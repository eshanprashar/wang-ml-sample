dc.contributor.advisor,dc.contributor.author,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.relation.ispartofseries,dc.title,dc.description.abstract,dc.identifier.other,dc.format.extent,dc.format.mimetype,dc.language.iso,dc.subject,dc.contributor.other,dc.description.sponsorship,dc.rights,dc.rights.uri,dc.description,dc.identifier.citation,dc.contributor,dc.relation.isversionof,dc.description.degree,dc.relation.isreplacedby,dc.relation.uri,dc.relation.replaces,dc.relation,dc.date.updated,dc.identifier,dc.relation.isreferencedby,dc.contributor.department,dc.type,dspace.orderedauthors,dc.date.submitted,dc.contributor.editor,dc.coverage.spatial,dc.coverage.temporal,eprint.grantNumber,dc.relation.requires,dc.language.rfc3066,dc.publisher,dc.relation.haspart,dc.identifier.oclc,year,research_paper_id
,"Evgeniou, Theodoros; Pontil, Massimiliano",2004-10-20T20:48:37Z,2004-10-20T20:48:37Z,2000-05-01,http://hdl.handle.net/1721.1/7169,AIM-1681; CBCL-184,A Note on the Generalization Performance of Kernel Classifiers with Margin,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,AIM-1681; CBCL-184,9 p.; 1149066 bytes; 253797 bytes,application/postscript; application/pdf,en_US,AI; MIT; Artificial Intelligence; missing data; mixture models; statistical learning; EM algorithm; neural networks; kernel classifiers; Support Vector Machine; regularization networks; statistical learning theory; V-gamma dimension.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,15
,"Kumar, Vinay; Poggio, Tomaso",2004-10-20T21:04:37Z,2004-10-20T21:04:37Z,2000-09-01,http://hdl.handle.net/1721.1/7264,AIM-1696; CBCL-191,Learning-Based Approach to Estimation of Morphable Model Parameters,"We describe the key role played by partial  evaluation in the Supercomputing Toolkit, a  parallel computing system for scientific  applications that effectively exploits the vast  amount of parallelism exposed by partial  evaluation. The Supercomputing Toolkit  parallel processor and its associated partial  evaluation-based compiler have been used  extensively by scientists at MIT, and have  made possible recent results in astrophysics  showing that the motion of the planets in our  solar system is chaotically unstable.",AIM-1696; CBCL-191,1037544 bytes; 218112 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,22
,"Kohler, Eddie; Chen, Benjie; Kaashoek, M. Frans; Morris, Robert T.; Poletto, Massimiliano",2023-03-29T15:33:47Z,2023-03-29T15:33:47Z,2000-08,https://hdl.handle.net/1721.1/149915,MIT-LCS-TR-812,Programming Language Techniques for Modular Router Configurations,"This paper applies programming language techniques to a high-level system description, both to optimize the system and to prove useful properties about it. The system in question is Click, a modular software router framework. Click routers are built from components called elements. Elements are written in C++, but the user creates a configuration using a simple, declarative data flow language. This language is amenable to data flow analysis and other conventional programming language techniques. Applied to a router configuration, these techniques have high-level results---for example, optimizing the router or verifying its high-level properties. This paper describes several programming language techniques that have been useful in practice, including optimization tools that remove virtual function calls from router definitions and remove redundant parts of adjacent routers. We also present performance results for an extensively optimized standards-compliant IP router. On conventional PC hardware, this router can forward up to 456,000 64-byte packets per second.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,19
"Liskov, Barbara H.; Morris, Robert T.","Ajmani, Sameer",2023-03-29T15:35:38Z,2023-03-29T15:35:38Z,2000-09,https://hdl.handle.net/1721.1/149948,MIT-LCS-TR-846,A Trusted Execution Platform for Multiparty Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,21
,"Fekete, Alan; Keidar, Idit",2023-03-29T14:42:00Z,2023-03-29T14:42:00Z,2000-11,https://hdl.handle.net/1721.1/149300,MIT-LCS-TM-610,A General Framework for Highly Available Services based on Group Communication,"We present a general framework for building highly available services. The framework uses group communication to coordinate a collection of servers. Our framework is configurable, in that one can adjust parameters such as the number of servers and the extent to which they are synchronized. We analyze the scenarios that can lead to the service availability being temporarily comprised, and we discuss the tradeoffs that govern the choice of parameters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,26
,"Riesenhuber, Maximilian; Poggio, Tomaso",2004-10-20T21:03:32Z,2004-10-20T21:03:32Z,2000-08-07,http://hdl.handle.net/1721.1/7231,AIM-1695; CBCL-190,Computational Models of Object Recognition in Cortex: A Review,"Understanding how biological visual systems perform object recognition is one of the ultimate goals in computational neuroscience. Among the biological models of recognition the main distinctions are between feedforward and feedback and between object-centered and view-centered. From a computational viewpoint the different recognition tasks - for instance categorization and identification - are very similar, representing different trade-offs between specificity and invariance. Thus the different tasks do not strictly require different classes of models. The focus of the review is on feedforward, view-based models that are supported by psychophysical and physiological data.",AIM-1695; CBCL-190,683319 bytes; 124521 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,20
"Amarasinghe, Saman; Agarwal, Anant","Barua, Rajeev",2023-03-29T15:32:37Z,2023-03-29T15:32:37Z,2000-01,https://hdl.handle.net/1721.1/149907,MIT-LCS-TR-799,Maps:  A Compiler-Managed Memory System for Software-Exposed Architectures,"Microprocessors must exploit both instruction-level parallelism (ILP) and memory parallelism for high performance.  Sophisticated techniques for ILP have boosted the ability of modern-day microprocessors to exploit ILP when available. Unfortunately, impro",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,4
,"Andersen, David; Bansal, Deccuk; Curtis, Dorothy; Seshan, Srinivasan; Balakrishnan, Hari",2023-03-29T15:33:11Z,2023-03-29T15:33:11Z,2000-05,https://hdl.handle.net/1721.1/149912,MIT-LCS-TR-808,System Support for Bandwidth Management and Content Adaptation in Internet Applications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,11
,"Snoeren, Alex C.; Andersen, David G.; Balakrishnan, Hari",2023-03-29T15:33:50Z,2023-03-29T15:33:50Z,2000-11,https://hdl.handle.net/1721.1/149916,MIT-LCS-TR-813,Fine-Grained Failover Using Connection Migration,"This paper presents a set of techniques for providing fine-grained failover of long-running connections across a distributed collection of replica servers, and is especially useful for fault-tolerant and load-balanced delivery of streaming media and telephony sessions. Our system achieves connection-level failover across both local- and wide-area server replication, without requiring a front-end transport- or application-layer switch. Our approach is enabled by the recently-developed end-to-end ``connection migration'' mechanism for transport protocols such as TCP, combined with a soft-state session synchronization protocol between replica servers.   The end result is a robust, fast, and fine-grained server failover mechanism that is transparent to both the client and server applications. We describe the details of our design and Linux implementation, as well as experiments with our implementation that show that this approach to failover is an attractive way to engineer robust systems for distributing long-running streams; connections suffer relatively low performance degradation even when server redirection occurs every few seconds, and overhead is negligible when compared to standard techniques. In particular, we observe the performance impact of migrating TCP connections depends on the length of time between migration and the most recent loss-recovery event.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,24
,"Serre, Thomas; Heisele, Bernd; Mukherjee, Sayan; Poggio, Tomaso",2004-10-20T21:03:34Z,2004-10-20T21:03:34Z,2000-09-01,http://hdl.handle.net/1721.1/7232,AIM-1697; CBCL-192,Feature Selection for Face Detection,We present a new method to select features for a face detection system using Support Vector Machines (SVMs). In the first step we reduce the dimensionality of the input space by projecting the data into a subset of eigenvectors. The dimension of the subset is determined by a classification criterion based on minimizing a bound on the expected error probability of an SVM. In the second step we select features from the SVM feature space by removing those that have low contributions to the decision function of the SVM.,AIM-1697; CBCL-192,7211022 bytes; 1034240 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,23
,"Katabi, Dina; Bazzi, Issam; Yang, Xiaowei",2023-03-29T14:41:46Z,2023-03-29T14:41:46Z,2000-03,https://hdl.handle.net/1721.1/149295,MIT-LCS-TM-604,An Information Theoretic Approach for Shared Bottleneck Inference Based on End-to-end Measurements,"Recent years have marked a growing interest in studying Internet path characteristics. However, most of the currently available tools to an end system to perform such measurements are slow inaccurate and generate an excessive amount of probing traffic. This paper introduces entropy as a novel and efficient metric for discovering Internet path characteristics based on data collected by an end system. In particular, the paper presents an entropy-based technique that enables an end system to cluster flows it receives according to their shared bottleneck. Our mechanism relies solely on information extracted from the packets' inter-arrivals at the receiver. It does not generate any probing traffic and can use data extracted from both TCP and UDP flows. Moreover, it requires only a small number of packets from each flow, which makes it useful for short-lived flows. We report the result of running the algorithm on simulated data and Internet traffic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,5
,"Bar-Joseph, Ziv; Keidar, Idit; Anker, Tal; Lynch, Nancy A.",2023-03-29T15:32:34Z,2023-03-29T15:32:34Z,2000-01,https://hdl.handle.net/1721.1/149906,MIT-LCS-TR-796,QoS Preserving Totally Ordered Multicast,This paper studies the Quality of Service (QoS) guarantees of totally ordered multicast algorithms. The paper shows that totally ordered multicast can coexist with guaranteed predictable delays in certain network models. The paper considers two reservatio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,3
,"Ingols, Kyle; Keidar, Idit",2023-03-29T14:42:03Z,2023-03-29T14:42:03Z,2000-11,https://hdl.handle.net/1721.1/149301,MIT-LCS-TM-611,Availability Study of Dynamic Voting Algorithms,"Fault tolerant distributed systems often select a primary component to allow a subset of the processes to function when failures occur. The dynamic voting paradigm defines rules for selecting the primary component adaptively: when a partition occurs, if a majority of the previous primary component is connected, a new and possibly smaller primary is chosen. Several studies have shown that dynamic voting leads to more available solutions than other paradigms for maintaining a primary component. However, these studies have assumed that every attempt made by the algorithm to form a new primary component terminates successfully. Unfortunately, in real systems, this is not always the case: a change in connectivity can interrupt the algorithm whiel it is still attempting to form a new primary component; in such cases, algorithms typically block until processes can resolve the outcome of the interrupted attempt. This paper uses simulations to evaluate the effect of interruptions on the availability of dynamic voting algorithms. We study four dynamic voting algorithms, and identify two important characteristics that impact an algorithm's availability in runs with frequent connectivity changes. First, we show that the number of communication rounds exchanged in an algorithm plays a significant role in the availability achieved, especially in the degradation of availability as connectivity changes become more frequent. Second, we show that the number of processes that need to be present in order to resolve past attempts impacts the availability, especially during long runs with numerous connectivity changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,25
,"Taylor, Michael Bedford; Lee, Walter; Frank, Matthew; Amarasinghe, Saman; Agarwal, Anant",2023-03-29T14:42:44Z,2023-03-29T14:42:44Z,2000-04,https://hdl.handle.net/1721.1/149317,MIT-LCS-TM-628,How to Build Scalable On-Chip ILP Networks for a Decentralized Architecture,"The era of billion transistors-on-a-chip is creating a completely different set of design constraints, forcing radically new microprocessor archiecture designs. This paper examines a few of the possible microarchitectures that are capable of obtaining scalable ILP performance. First, we observe that the network that interconnects the processing elements is the critical design point in the microarchitecture. Next, we characterize four fundamental properties that have to be satisfied by the interconnection network. Next, we provide case studies of two different networks that satisfy these properties. Finally, a detailed evaluation of these networks is presented to highlight the scalability and performance of these microarchitectures. We show that by using compile time information, we can build simpler networks and use them efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,7
,"Micali, Silvio; Reyzin, Leonid",2023-03-29T14:41:58Z,2023-03-29T14:41:58Z,2000-08,https://hdl.handle.net/1721.1/149299,MIT-LCS-TM-609,Concurrent/Resettable Zero-Knowledge Protocols for NP in the Public Key Model,"We propose a four-round protocol for concurrent and resettable zero-knowledge arguments for any langauge in NP, assuming the verifier has a pre-registered public-key. We also propose a three-round protocol with an additional timing assumption.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,18
,"Heisele, Bernd; Poggio, Tomaso; Pontil, Massimiliano",2004-10-20T21:03:29Z,2004-10-20T21:03:29Z,2000-05-01,http://hdl.handle.net/1721.1/7229,AIM-1687; CBCL-187,Face Detection in Still Gray Images,"We present a trainable system for detecting frontal and near-frontal views of faces in still gray images using Support Vector Machines (SVMs). We first consider the problem of detecting the whole face pattern by a single SVM classifer. In this context we compare different types of image features, present and evaluate a new method for reducing the number of features and discuss practical issues concerning the parameterization of SVMs and the selection of training data. The second part of the paper describes a component-based method for face detection consisting of a two-level hierarchy of SVM classifers. On the first level, component classifers independently detect components of a face, such as the eyes, the nose, and the mouth. On the second level, a single classifer checks if the geometrical configuration of the detected components in the image matches a geometrical model of a face.",AIM-1687; CBCL-187,6267853 bytes; 482304 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,14
,"Bansal, Deepak; Balakrishnan, Hari",2023-03-29T15:33:06Z,2023-03-29T15:33:06Z,2000-05,https://hdl.handle.net/1721.1/149910,MIT-LCS-TR-806,TCP-friendly Congestion Control for Real-time Streaming Applications,"This paper introduces and analyzes a class of nonlinear congestion control algorithms called binomial algorithms, motivated in part by the needs of streaming audio and video applications for which a drastic reduction in transmission rate upon congestion i",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,8
,"Thies, William F.; Viven, Frederic; Sheldon, Jeffery W.; Amarasinghe, Saman",2023-03-29T14:42:05Z,2023-03-29T14:42:05Z,2000-11,https://hdl.handle.net/1721.1/149302,MIT-LCS-TM-613,A Unified Framework for Schedule and Storage Optimization,"We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find the best storage mapping for a given schedule, the best schedule for a given storage mapping, and the best storage mapping that is valid for all legal schedules. Our techniques combines affine scheduling techniques with occupancy vector analysis, and incorporates general affine dependencies across statements and loop nests. We formulate the constraints imposed by the data dependencies and the storage mapping as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the best occupancy vector. We consider out method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,27
,"Micali, Silvio",2023-03-29T14:40:47Z,2023-03-29T14:40:47Z,2000,https://hdl.handle.net/1721.1/149277,MIT-LCS-TM-577,Copmutationally Sound Proofs,"This paper puts forward a new notion of a proof based on computational complexity and explores its implications for computation at large. Computationally sound proofs provide, in a novel and meaningful framework, answer to old and new questions in complexity theory. In particular, given a random oracle or a new complexity assumption, they enable us to 1. prove that verifying is easier than deciding for all theorems; 2. provides a quite effective way to prove membership in computationally hard languages (such as C-NP-complete ones); and 3. show that every computation possesses a short certificate vouching its correctness. FInally, if a special type of computationally sound proof exists, we show that Blum's notion of program checking can be meaningfully broadened so as to prove that NP-complete languages are checkable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,2
,"Shrestha, Govinda",2023-03-29T15:32:24Z,2023-03-29T15:32:24Z,2000-07,https://hdl.handle.net/1721.1/149903,MIT-LCS-TR-793,Information Technology Use in Developing Countries,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,17
"Szolovits, Peter","Tsien, Christine L.",2023-03-29T15:33:14Z,2023-03-29T15:33:14Z,2000,https://hdl.handle.net/1721.1/149913,MIT-LCS-TR-809,TrendFinder: Automated Detection of Alarmable Trends,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,1
,"Nakajima, Chikahito; Pontil, Massimiliano; Heisele, Bernd; Poggio, Tomaso",2004-10-20T21:03:31Z,2004-10-20T21:03:31Z,2000-06-01,http://hdl.handle.net/1721.1/7230,AIM-1688; CBCL-188,People Recognition in Image Sequences by Supervised Learning,We describe a system that learns from examples to recognize people in images taken indoors. Images of people are represented by color-based and shape-based features. Recognition is carried out through combinations of Support Vector Machine classifiers (SVMs). Different types of multiclass strategies based on SVMs are explored and compared to k-Nearest Neighbors classifiers (kNNs). The system works in real time and shows high performance rates for people recognition throughout one day.,AIM-1688; CBCL-188,4611797 bytes; 373760 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,16
,"Antone, Matthew E.; Teller, Seth",2023-03-29T15:33:52Z,2023-03-29T15:33:52Z,2000-12,https://hdl.handle.net/1721.1/149917,MIT-LCS-TR-814,Automatic Recovery of Camera Positions in Urban Scenes,"Accurate camera calibration is crucial to the reconstruction of three-dimensional geometry and the recovery of photometric scene properties. Calibration involves the determination of intrinsic parameters (e.g. focal length, principal point, and radial lens distortion) and extrinsic parameters (orientation and position).  In urban scenes and other environments containing sufficient geometric structure, it is possible to decouple extrinsic calibration into rotational and translational components that can be treated separately, simplifying the registration problem. Here we present such a decoupled formulation and describe methods for automatically recovering the positions of a large set of cameras given intrinsic calibration, relative rotations, and approximate positions.  Our algorithm first estimates the directions of translation (up to an unknown scale factor) between adjacent camera pairs using point features but without requiring explicit correspondence between them. This technique combines the robustness and simplicity of a Hough transform with the accuracy of Monte Carlo expectation maximization. We then find a set of distances between the pairs that produces globally-consistent camera positions. Novel uncertainty formulations and match plausibility criteria improve reliability and accuracy.  We assess our system's performance using both synthetic data and a large set of real panoramic imagery. The system produces camera positions accurate to within 5 centimeters in image networks extending over hundreds of meters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,29
,"Shrestha, Govinda; Amarasinghe, Saman",2023-03-29T15:33:59Z,2023-03-29T15:33:59Z,2000-11,https://hdl.handle.net/1721.1/149918,MIT-LCS-TR-815,Perspectives on the Use of the Internet in Sri Lanka,"The survey examines the use of computers and the Internet in Sri Lanka from the perspective of the Internet Service Provider (ISP) members. It attempts to describe the general nature of IT use in terms of the availability, access, familiarity and general conditions associated with using computers and the Internet in the country.  The survey was conducted in July 1999. Questionnaires were e-mailed to 9448 ISP members in Sri Lanka, using e-mail addresses available to us at that time. Altogether, 560 members completed and returned questionnaires via e-mail to MIT's Laboratory for Computer Science.  Descriptive analysis of both quantitative and qualitative data was then conducted.    Major quantitative findings include:  *Over 60% of the respondents were members of their respective ISPs for two or less years, and over half had first used a computer sometime during the 1990-99 period. *Sixty-two percent of the respondents had sent 10 or more e-mails per week over the past six (or less) months, and 52% had received 15 or more e-mails per week during the same period. *Nearly half of the respondents used a computer at home, and 48% indicated 33.6K as the baud rate to connect their ISPs. *Seventy-eight percent of the respondents spent 1-9 hours per week sending and receiving e-mails, and a large majority (68%) spent 1-9 hours surfing the Web. *A majority of the respondents were positive about conditions in the workplace, such as the number and quality of opportunities for training and skill development, the quality of telecommunications facilities, and the quality and reliability of Internet connections. *An overwhelming majority of the respondents indicated that ISP subscriber fees, computer hardware and software costs, and telecommunications charges were generally high. *Most respondents were generally positive about 1) the quality of access to the Internet, 2) the quality of access to e-mails, Web pages and other Internet-based features, and 3) various benefits of Internet access. *Seventy-one percent of the respondents were male; nearly half were younger than 35, and a large majority were educated (with at least a high school diploma.)  Private company employees and people in business comprised over half of the respondents.  Major qualitative findings include: * It is crucially important to have faster access to information, increased communication at low costs, online-education and training, and increased efficiency in business, professional and organizational activities. * Matters of considerable concern include the low bandwidth, the high telecommunications charges, the low quality of Internet services, and the lack of organized information and databases. * Greatly needed is a raising of awareness, a change in the current regulatory environment, an open government, and a set of local information resources to support commerce.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,28
,"Heinz, Ernst",2023-03-29T14:41:55Z,2023-03-29T14:41:55Z,2000-05,https://hdl.handle.net/1721.1/149298,MIT-LCS-TM-608,A New Self-Play Experiment in Computer Chess,"This paper presents the results of a new self-play experiment in computer chess. It is the _x000C_rst such experiment ever to feature search depths beyond 9 plies and thousands of games for every single match. Overall, we executed 17,150 self-play games (1,050{3,000 per match) in one \\calibration"" match and seven \\depth X+1 , X"" handicap matches at _x000C_xed iteration depths ranging from 5{12 plies. For the experiment to be realistic and independently repeatable, we relied on a state-of-the-art commercial contestant: Fritz 6 , one of the strongest modern chess pro- grams available. The main result of our new experimentis thatit shows the existence of diminishing returns for additional search in computer chess self-play by Fritz 6 with 95% statistical con_x000C_dence. The dimin- ishing returns manifest themselves by declining rates of won games and reversely increasing rates of drawn games for the deeper searching pro- gram versions. The rate of lost games, however, remains quite steady for the whole depth range of 5{12 plies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,10
,"Arkoudas, Konstantine",2004-10-08T20:37:19Z,2004-10-08T20:37:19Z,2001-11-13,http://hdl.handle.net/1721.1/6680,AIM-2001-031,Simplifying transformations for type-alpha certificates,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",AIM-2001-031,45 p.; 2306816 bytes; 532283 bytes,application/postscript; application/pdf,en_US,AI; deduction; proofs; simplifiation; proof optimization; deduction complexity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,95
,"Banks, Jessica",2004-10-20T20:28:07Z,2004-10-20T20:28:07Z,2001-05-01,http://hdl.handle.net/1721.1/7070,AITR-2001-005,Design and Control of an Anthropomorphic Robotic Finger with Multi-point Tactile Sensation,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",AITR-2001-005,88 p.; 17699541 bytes; 1837341 bytes,application/postscript; application/pdf,en_US,AI; tactile sensation; finger; robot; anthropomorphic; skin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,56
,"Katabi, Dina; Blake, Charles",2023-03-29T15:34:40Z,2023-03-29T15:34:40Z,2001-12,https://hdl.handle.net/1721.1/149931,MIT-LCS-TR-828,Inferring Congestion Sharing and Path Characteristics from Packet Interarrival Times,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,97
,"Yu, Angela J.; Giese, Martin A.; Poggio, Tomaso A.",2004-10-20T21:03:51Z,2004-10-20T21:03:51Z,2001-09-01,http://hdl.handle.net/1721.1/7240,AIM-2001-022; CBCL-207,Biologically Plausible Neural Circuits for Realization of Maximum Operations,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",AIM-2001-022; CBCL-207,28 p.; 2197042 bytes; 930880 bytes,application/postscript; application/pdf,en_US,AI; maximum operation; invariance; recurrent inhibition; shunting inhibition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,84
,"Ucko, Aaron Mark",2004-10-20T20:28:09Z,2004-10-20T20:28:09Z,2001-05-01,http://hdl.handle.net/1721.1/7071,AITR-2001-006,Predicate Dispatching in the Common Lisp Object System,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",AITR-2001-006,74 p.; 2463955 bytes; 977046 bytes,application/postscript; application/pdf,en_US,AI; predicate dispatching; Common Lisp; CLOS; Weyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,57
,"Larsen, Samuel; Witchel, Emmett; Amarasinghe, Saman",2023-03-29T14:42:27Z,2023-03-29T14:42:27Z,2001-11,https://hdl.handle.net/1721.1/149310,MIT-LCS-TM-621,Techniques for Increasing and Detecting Memory Alignment,"Memory alignment is an important property in memory system performance. Extraction of alignment information at compile-time enables the possibility for new classes of program optimization. In this paper, we present methods for increasing and detecting the alignment of memory references in a program. Our transformations and analyses do not require interprocedural analysis and introduce almost no overhead. As a result, they can be incorporated into real compilation systems. On average, our techniques are able to achieve a five-fold increase in the number of dynamically aligned memory references. We are then able to detect 94% of these operations. This success is invaluable in providing performance gains in a range of different areas. When alignment information is incorporated into a vectorizing compiler, we can increase the performance of a G4 AltiVec processor by more than a factor of two. Using the same methods, we are able to reduce energy consumption in a data cache by as much as 35%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,91
,"Bryson, Joanna J.",2004-10-20T20:29:10Z,2004-10-20T20:29:10Z,2001-09-01,http://hdl.handle.net/1721.1/7080,AITR-2002-003,Intelligence by Design: Principles of Modularity and Coordination for Engineerin,"All intelligence relies on search --- for  example, the search for an intelligent agent's  next action. Search is only likely to succeed in resource-bounded agents if they have already  been biased towards finding the right answer.  In artificial agents, the primary source of bias is engineering.   This dissertation describes an approach,  Behavior-Oriented Design (BOD) for  engineering complex agents. A complex agent  is one that must arbitrate between potentially  conflicting goals or behaviors.  Behavior-oriented design builds on work in  behavior-based and hybrid architectures for agents, and the object  oriented approach to software engineering.   The primary contributions of this dissertation  are:     1.The BOD architecture: a modular  architecture with each module providing  specialized representations to facilitate  learning.    This includes one pre-specified module  and representation for action selection or  behavior arbitration. The specialized    representation underlying BOD action  selection is Parallel-rooted, Ordered,  Slip-stack Hierarchical (POSH) reactive plans.     2.The BOD development process: an  iterative process that alternately scales the  agent's capabilities then optimizes the agent  for    simplicity, exploiting tradeoffs between the  component representations. This ongoing  process for controlling complexity not only    provides bias for the behaving agent, but  also facilitates its maintenance and  extendibility.   The secondary contributions of this  dissertation include two implementations of  POSH action selection, a procedure for  identifying useful idioms in agent architectures and  using them to distribute knowledge across  agent paradigms, several examples of  applying BOD idioms to established architectures, an  analysis and comparison of the attributes and  design trends of a large number of agent architectures, a comparison of biological  (particularly mammalian) intelligence to  artificial agent architectures, a novel model of primate transitive inference, and many other  examples of BOD agents and BOD  development.",AITR-2002-003,232 p.; 4544378 bytes; 1027952 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,79
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:41Z,2004-10-20T21:03:41Z,2001-07-25,http://hdl.handle.net/1721.1/7236,AIM-2001-015; CBCL-202,Recognizing Indoor Scenes,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,AIM-2001-015; CBCL-202,17 p.; 14931961 bytes; 3219314 bytes,application/postscript; application/pdf,en_US,AI; Scene classification; Navigation; scene representation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,66
,"Hong, Won",2004-10-20T20:28:24Z,2004-10-20T20:28:24Z,2001-09-01,http://hdl.handle.net/1721.1/7075,AITR-2001-007,"Modeling, Estimation, and Control of Robot-Soil Interactions","This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",AITR-2001-007,225 p.; 66603884 bytes; 4629577 bytes,application/postscript; application/pdf,en_US,AI; Robotics; Soil Modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,78
,"Nagpal, Radhika",2004-10-20T20:28:28Z,2004-10-20T20:28:28Z,2001-06-01,http://hdl.handle.net/1721.1/7076,AITR-2001-008,Programmable Self-Assembly: Constructing Global Shape using Biologically-inspire,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",AITR-2001-008,118 p.; 27221557 bytes; 1541086 bytes,application/postscript; application/pdf,en_US,AI; self-organisation; multi agent; developmental biology; amorphous computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,61
,"Castro, Miguel",2023-03-29T15:34:03Z,2023-03-29T15:34:03Z,2001-01,https://hdl.handle.net/1721.1/149920,MIT-LCS-TR-817,Practical Byzantine Fault Tolerance,"Our growing reliance on online services accessible on the Internet demands highly-available systemsthat provide correct service without interruptions. Byzantine faults such as software bugs, operatormistakes, and malicious attacks are the major cause of service interruptions. This thesis describesa new replication algorithm, BFT, that can be used to build highly-available systems that tolerateByzantine faults. It shows, for the first time, how to build Byzantine-fault-tolerant systems that canbe used in practice to implement real services because they do not rely on unrealistic assumptionsand they perform well. BFT works in asynchronous environments like the Internet, it incorporatesmechanisms to defend against Byzantine-faulty clients, and it recovers replicas proactively. Therecovery mechanism allows the algorithm to tolerate any number of faults over the lifetime of thesystem provided fewer than 1=3 of the replicas become faulty within a small windowof vulnerability.The window may increase under a denial-of-service attack but the algorithm can detect and respondto such attacks and it can also detect when the state of a replica is corrupted by an attacker.BFT has been implemented as a generic program library with a simple interface. The BFTlibrary provides a complete solution to the problem of building real services that tolerate Byzantinefaults. We used the library to implement the first Byzantine-fault-tolerant NFS file system, BFS. TheBFT library and BFS perform well because the library incorporates several important optimizations.The most important optimization is the use of symmetric cryptography to authenticate messages.Public-key cryptography, which was the major bottleneck in previous systems, is used only toexchange the symmetric keys. The performance results show that BFS performs 2% faster to 24%slower than production implementations of the NFS protocol that are not replicated. Therefore, webelieve that the BFT library can be used to build practical systems that tolerate Byzantine faults.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,32
,"Zollei, Lilla",2004-10-20T20:28:33Z,2004-10-20T20:28:33Z,2001-08-01,http://hdl.handle.net/1721.1/7078,AITR-2002-001,2D-3D Rigid-Body Registration of X-Ray Fluoroscopy and CT Images,"The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation.   Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",AITR-2002-001,128 p.; 21043480 bytes; 1712245 bytes,application/postscript; application/pdf,en_US,AI; registration; medical imaging,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,69
,"Miller, Erik G.; Tieu, Kinh; Stauffer, Chris P.",2004-10-08T20:36:37Z,2004-10-08T20:36:37Z,2001-09-01,http://hdl.handle.net/1721.1/6659,AIM-2001-021,Learning Object-Independent Modes of Variation with Feature Flow Fields,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",AIM-2001-021,9 p.; 8233900 bytes; 814636 bytes,application/postscript; application/pdf,en_US,AI; Invariance; Optical Flow; Color Constancy; Object Recognition; image manifold,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,83
,"Russell, Richard; Sinha, Pawan",2004-10-20T21:03:39Z,2004-10-20T21:03:39Z,2001-07-01,http://hdl.handle.net/1721.1/7235,AIM-2001-014; CBCL-201,Perceptually-based Comparison of Image Similarity Metrics,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",AIM-2001-014; CBCL-201,13 p.; 9714300 bytes; 2612761 bytes,application/postscript; application/pdf,en_US,AI; Image matching; vector quantization; Minkowski metric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,65
,"Yeo, Gene; Poggio, Tomaso",2004-10-20T21:03:45Z,2004-10-20T21:03:45Z,2001-08-25,http://hdl.handle.net/1721.1/7238,AIM-2001-018; CBCL-206,Multiclass Classification of SRBCTs,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",AIM-2001-018; CBCL-206,17 p.; 6552074 bytes; 816114 bytes,application/postscript; application/pdf,en_US,AI; multiclass; SVM; feature selection; SRBCT; tumors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,73
,"Arkoudas, Konstantine",2004-10-08T20:36:42Z,2004-10-08T20:36:42Z,2001-10-16,http://hdl.handle.net/1721.1/6662,AIM-2001-027,Type-omega DPLs,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",AIM-2001-027,62 p.; 3794425 bytes; 787916 bytes,application/postscript; application/pdf,en_US,AI; deduction; computation; proof search; soundness; logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,87
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:55Z,2004-10-20T21:03:55Z,2001-11-05,http://hdl.handle.net/1721.1/7242,AIM-2001-028; CBCL-208,Detecting Faces in Impoverished Images,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",AIM-2001-028; CBCL-208,14 p.; 20987363 bytes; 1810477 bytes,application/postscript; application/pdf,en_US,AI; Face detection; image resolution; contrast negation; vertical inversion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,93
,"Lee, Lily",2004-10-08T20:36:33Z,2004-10-08T20:36:33Z,2001-09-01,http://hdl.handle.net/1721.1/6657,AIM-2001-019,Gait Dynamics for Recognition and Classification,"This paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. Our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. We claim that our gait dynamics representation is rich enough for the task of recognition and classification. The use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. In addition, preliminary results are shown on gender classification using our gait dynamics features.",AIM-2001-019,12 p.; 1128480 bytes; 92054 bytes,application/postscript; application/pdf,en_US,AI; gait; recognition; gender classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,82
,"Taycher, Leonid; Darrell, Trevor",2004-10-08T20:36:35Z,2004-10-08T20:36:35Z,2001-09-01,http://hdl.handle.net/1721.1/6658,AIM-2001-024,Range Segmentation Using Visibility Constraints,"Visibility constraints can aid the segmentation of foreground objects observed with multiple range images. In our approach, points are defined as foreground if they can be determined to occlude some {em empty space} in the scene. We present an efficient algorithm to estimate foreground points in each range view using explicit epipolar search. In cases where the background pattern is stationary, we show how visibility constraints from other views can generate virtual background values at points with no valid depth in the primary view. We demonstrate the performance of both algorithms for detecting people in indoor office environments.",AIM-2001-024,10 p.; 15686301 bytes; 1574798 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,85
,"Rennie, Jason D. M.",2004-10-20T20:28:16Z,2004-10-20T20:28:16Z,2001-09-01,http://hdl.handle.net/1721.1/7074,AITR-2001-004,Improving Multi-class Text Classification with Naive Bayes,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",AITR-2001-004,49 p.; 2017370 bytes; 687421 bytes,application/postscript; application/pdf,en_US,AI; naive bayes; text; classification; feature selection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,77
,"Dror, Ron O.; Adelson, Edward H.; Willsky, Alan S.",2004-10-08T20:36:32Z,2004-10-08T20:36:32Z,2001-09-01,http://hdl.handle.net/1721.1/6656,AIM-2001-023,Surface Reflectance Estimation and Natural Illumination Statistics,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",AIM-2001-023,22 p.; 7750699 bytes; 706071 bytes,application/postscript; application/pdf,en_US,AI; reflectance; lighting; BRDF; surface; illumination statistics; natural images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,81
,"Chan, Nicholas T.; Dahan, Ely; Lo, Andrew W.; Poggio, Tomaso",2004-10-20T21:03:35Z,2004-10-20T21:03:35Z,2001-07-01,http://hdl.handle.net/1721.1/7233,AIM-2001-013; CBCL-200,Experimental Markets for Product Concepts,"Market prices are well known to efficiently collect and aggregate diverse information regarding the value of commodities and assets. The role of markets has been particularly suitable to pricing financial securities. This article provides an alternative application of the pricing mechanism to marketing research - using pseudo-securities markets to measure preferences over new product concepts. Surveys, focus groups, concept tests and conjoint studies are methods traditionally used to measure individual and aggregate preferences. Unfortunately, these methods can be biased, costly and time-consuming to conduct. The present research is motivated by the desire to efficiently measure preferences and more accurately predict new product success, based on the efficiency and incentive-compatibility of security trading markets. The article describes a novel market research method, pro-vides insight into why the method should work, and compares the results of several trading experiments against other methodologies such as concept testing and conjoint analysis.",AIM-2001-013; CBCL-200,3069806 bytes; 287156 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,64
"Szolovits, Peter","Koh, Waikit",2023-03-29T15:34:59Z,2023-03-29T15:34:59Z,2001-05,https://hdl.handle.net/1721.1/149933,MIT-LCS-TR-830,An Information-Theoretic Approach to Interest Making,"The Internet has brought a new meaning to the term communities. Geography is no longer a barrier to international communications. However, the paradigm of meeting new interesting people remains entrenched in traditional means; meeting new interesting people on the Internet still relies on chance and contacts. This thesis explores a new approach towards matching users in online communities in an effective fashion.  Instead of using the conventional feature vector scheme to profile users, each user is represented by a personalized concept hierarchy (or an ontology) that is learnt from the user's behavior in the system. Each concept hierarchy is then interpreted within the Information Theory framework as a probabilistic decision tree. The matching algorithm uses the Kullback-Leiber distance as a measure of deviation between two probabilistic decision trees. Thus, in an online community, where a personalized concept hierarchy represents each user, the Kullback-Leiber distance imposes a full- order rank on the level of similarity of all the users with respect to a particular user in question.  The validity and utility of the proposed scheme of matching users is then applied in a set of simulations, using the feature-vector-overlap measure as a baseline. The results of the simulations show that the Kullback Leiber distance, when used in conjunction with the concept hierarchy, is more robust to noise and is able to make a stronger and more distinctive classification of users into similar groups in comparison to the conventional keyword-overlap scheme. A graphical agent system that relies upon the ontology-based interest matching algorithm, called the Collaborative Sanctioning Network, is also described in this thesis.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,51
,"Fleming, Roland W.; Dror, Ron O.; Adelson, Edward H.",2004-10-08T20:36:44Z,2004-10-08T20:36:44Z,2001-10-21,http://hdl.handle.net/1721.1/6663,AIM-2001-032,How do Humans Determine Reflectance Properties under Unknown Illumination?,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",AIM-2001-032,9 p.; 7609556 bytes; 945959 bytes,application/postscript; application/pdf,en_US,AI; illumination; reflectance; natural image statistics; human vision; BRDF,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,89
,"Koile, Kimberle",2004-10-20T20:28:12Z,2004-10-20T20:28:12Z,2001-01-01,http://hdl.handle.net/1721.1/7072,AITR-2001-001,The Architect's Collaborator: Toward Intelligent Tools for Conceptual Design,"In early stages of architectural design, as in  other design domains, the language used is often very abstract. In architectural design, for  example, architects and their clients use experiential terms such as ""private"" or ""open""  to describe spaces. If we are to build programs that can help designers during this  early-stage design, we must give those programs the capability to deal with concepts  on the level of such abstractions. The work reported in this thesis sought to do that,  focusing on two key questions: How are  abstract terms such as ""private"" and ""open"" translated  into physical form? How might one build a tool to assist designers with this process? The Architect's Collaborator (TAC) was built to  explore these issues. It is a design assistant that supports iterative design refinement, and  that represents and reasons about how experiential qualities are manifested in  physical form. Given a starting design and a  set of design goals, TAC explores the space of  possible designs in search of solutions that  satisfy the goals. It employs a strategy we've called  dependency-directed redesign: it evaluates a design with respect to a set of goals, then  uses an explanation of the evaluation to guide proposal and refinement of repair  suggestions; it then carries out the repair  suggestions to create new designs. A series of experiments was run to study  TAC's behavior. Issues of control structure,  goal set size, goal order, and modification operator  capabilities were explored. In addition, TAC's use as a design assistant was studied  in an experiment using a house in the  process of being redesigned. TAC's use as an  analysis tool was studied in an experiment  using Frank Lloyd Wright's Prairie houses.",AITR-2001-001,20962265 bytes; 1471552 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,33
,"Lynch, Nancy A.; Segala, Roberto; Vaandrager, Frits",2023-03-30T15:32:47Z,2023-03-29T15:34:34Z; 2023-03-30T15:32:47Z,2002-02,https://hdl.handle.net/1721.1/149930.2,MIT-LCS-TR-827b,Hybrid I/O Automata*,"Hybrid systems are systems that exhibit a combination of discrete and continuous behavior. Typical hybrid systems include computer components, which operate in discrete program steps, and real-world components, whose behavior over time intervals evolves according to physical constraints. Important examples of hybrid systems include automated transportation systems, robotics systems, process control systems, systems of embedded devices, and mobile computing systems. Such systems can be very complex, and very difficult to describe and analyze. This paper presents the Hybrid Input/Output Automaton (HIOA) modeling framework, a basic mathematical framework to support description and analysis of hybrid systems. An important feature of this model is its support for decomposing hybrid system descriptions. In particular, the framework includes a notion of external behavior for a hybrid I/O automaton, which captures its discrete and continuous interactions with its environment. The framework also defines what it means for one HIOA to implement another, based on an inclusion relationship between their external behavior sets, and defines a notion of simulation, which provides a sufficient condition for demonstrating implementation relationships. The framework also includes a composition operation for HIOAs, which respects external behavior, and a notion of receptiveness, which implies that an HIOA does not block the passage of time. The framework is intended to support analysis methods from both computer science and control theory. This work is a simplification of an earlier version of the HIOA model [49, 50]. The main simplification in the new model is a clearer separation between the mechanisms used to model discrete and continuous interaction between components. In particular, the new model removes the dual use of external variables for discrete and continuous interaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,111
,"Attie, Paul C.; Lynch, Nancy A.; Rajsbaum, Sergio",2023-03-29T15:36:40Z,2023-03-29T15:36:40Z,2002-12,https://hdl.handle.net/1721.1/149973,MIT-LCS-TR-877,Boosting Fault-Tolerance in Asynchronous Message Passing Systems is Impossible,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,174
,"Thies, William; Lin, Jasper; Amarasinghe, Saman",2023-03-29T14:42:53Z,2023-03-29T14:42:53Z,2002-08,https://hdl.handle.net/1721.1/149319,MIT-LCS-TM-630,Phased Computation Graphs in the Polyhedral Model,"We present a translation scheme that allows a broad class of dataflow graphs to be considered under the optimization framework of the polyhedral model. The input to our analysis is a Phased Computation Graph, which we define as a generalization of the most widely used dataflow representations, including synchronous dataflow, cyclo-static dataflow, and computation graphs. The output of our analysis is a System of Affine Recurrence Equations (SARE) that exactly captures the data dependencies between the nodes of the original graph. Using the SARE representation, one can apply many techniques from the scientific community that are new to the DSP domain. For example, we propose simple optimizations such as node splitting, decimation propagation, and stead-state invariant code motion that leverage the fine-grained dependence information of the SARE to perform novel transformations on a stream graph. We also propose ways in which the polyhedral model can offer new approaches to classic problems of the DSP community, such as minimizing buffer size, code size, and optimizing the schedule.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,149
,"Kumar, Vinay P.",2004-10-01T14:00:07Z,2004-10-01T14:00:07Z,2002-09-01,http://hdl.handle.net/1721.1/5569,AITR-2002-008; CBCL-221,Towards Man-Machine Interfaces: Combining Top-down Constraints with Bottom-up Learning in Facial Analysis,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",AITR-2002-008; CBCL-221,68 p.; 21293042 bytes; 2473001 bytes,application/postscript; application/pdf,en_US,AI; Facial Expression Recognition; Pose Estimation; Viseme Recognition; SVM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,158
,"Boyapati, Chandrasekhar; Liskov, Barbara H.; Shrira, Liuba",2023-03-29T15:36:02Z,2023-03-29T15:36:02Z,2002-07,https://hdl.handle.net/1721.1/149958,MIT-LCS-TR-858,Ownership Types and Safe Lazy Upgrades in Object-Oriented Databases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,141
,"Salcianu, Alexandru; Boyapati, Chandrasekhar; Beebee, William S., Jr.; Rinard, Martin",2023-03-29T15:36:20Z,2023-03-29T15:36:20Z,2002-11,https://hdl.handle.net/1721.1/149965,MIT-LCS-TR-869,A Type System for Safe Region-Based Memory Management in Real-Time Java,"The Real-Time Specification for Java (RTSJ) allows a program to create real-time threads with hard real time constraints. Real-time threads use immortal memory and region-based memory management to avoid unbounded pauses caused by interference from the garbage collector. The RTSJ uses runtime checks to ensure that deleting a region does not create dangling references and that real-time threads do not access references to objects allocated in the garbage-collected heap. This paper presents a static type system that guarantees that these runtime checks will never fail for well-typed programs. Our type system therefore 1) provides an important safety guarantee for real-time programs and 2) makes it possible to eliminate the runtime checks and their associated overhead. Our system also makes several contributions over previous work on region types. For object-oriented programs, it combines region types and ownership types in a unified type system framework. For multithreaded programs, it allows long-lived threads to share objects without using the heap and without having memory leaks. For real-time programs, it ensures that real-time threads do not interfere with the garbage collector. We have implemented several programs in our system. Our experience indicates that our type system is sufficiently expressive and requires little programming overhead. We also ran these programs on our RTSJ platform. Our experiments show that eliminating the RTSJ runtime checks using a static type system can significantly decrease the execution time of a real-time program.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,167
,"Tan, Godfrey",2023-03-29T15:36:12Z,2023-03-29T15:36:12Z,2002-10,https://hdl.handle.net/1721.1/149962,MIT-LCS-TR-866,Blueware: Bluetooth Simulator for ns,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,161
Gerald Sussman,"Newton, Ryan; Beal, Jacob",2006-03-01T19:47:25Z,2006-03-01T19:47:25Z,2002-12-10,http://hdl.handle.net/1721.1/31221,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Amorphous Infrastructure for Language Implementation,"We propose a method for the robust implementation of simple graphical automataon an amorphous computer. This infrastructure is applied to the implementationof purely functional programming languages. Specifically, it is usedin conjunction with data-flow techniques to implement a toy language homologousto recurrence equations, exploiting control-flow parallelism through paralleloperand evaluation. Also, data parallelism is explored in a separate implementation,in which a simple mark-up syntax enables Scheme programs to performspatially-distributed tree-walking without modifying their semantics. This additionenables an idiomatically expressed interpreter to be trivially instrumented,producing a spatially distributed universal machine, and once again achievingcontrol flow parallelism in the interpreted language.",MIT-CSAIL-TR-2006-015,20 p.; 21433070 bytes; 757210 bytes,application/postscript; application/pdf,en_US,,Mathematics and Computation,,,,,6.978 Final Project,,,,,,,,,,,,,,,,,,,,,,,,2002,183
,"De Prisco, Roberto; Fekete, Alan; Lynch, Nancy A.; Shvartsman, Alexander A.",2023-03-29T15:36:30Z,2023-03-29T15:36:30Z,2002-11,https://hdl.handle.net/1721.1/149969,MIT-LCS-TR-873,A Dynamic Primary View Group Communication Service,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,166
,"Clarke, Dwaine; Gassend, Blaise; Suh, G. Edward; van Dijk, Marten; Devadas, Srinivas",2023-03-29T14:42:58Z,2023-03-29T14:42:58Z,2002-08,https://hdl.handle.net/1721.1/149320,MIT-LCS-TM-631,Offline Authentication of Untrusted Storage,"We extend the offline memory correctness checking scheme presented by Blum et. al [BEG+91], by using incremental cryptography, to detect attacks by an active adversary. We also introduce a hybrid o_ine-online checking scheme designed for untrusted storages in file systems and databases. Previous work [GSC+02] [FKM00] [MVS00] describe systems in which Merkle trees are used to verify the authenticity of data stored on untrusted storage. The Merkle trees [Mer79] are used to check, after each operation, whether the storage performed correctly. The offline and hybrid checkers are designed for checking sequences of operations on an untrusted storage and, in the common case, require only a constant overhead on the number of accesses to the storage, as compared to the logarithmic overhead incurred by online Merkle tree schemes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,147
,"Bouvrie, Jake V.",2004-10-08T20:38:35Z,2004-10-08T20:38:35Z,2002-12-01,http://hdl.handle.net/1721.1/6705,AIM-2002-022,Multiple Resolution Image Classification,"Binary image classifiction is a problem that  has received much attention  in recent years. In this paper we evaluate a  selection of popular  techniques in an effort to find a feature set/ classifier combination which  generalizes well to full resolution image data.  We then apply that system  to images at one-half through one-sixteenth  resolution, and consider the  corresponding error rates. In addition, we  further observe generalization  performance as it depends on the number of  training images, and lastly,  compare the system's best error rates to that  of a human performing an  identical classification task given teh same  set of test images.",AIM-2002-022,1054982 bytes; 824527 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,178
,"Livadas, Carolos; Lynch, Nancy A.",2023-03-29T15:36:17Z,2023-03-29T15:36:17Z,2002-11,https://hdl.handle.net/1721.1/149964,MIT-LCS-TR-868,A Formal Venture into Reliable Multicast Territory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,168
,"Demaine, Erik D.; Hohenberger, Susan; Liben-Nowell, David",2023-03-29T15:36:10Z,2023-03-29T15:36:10Z,2002-10,https://hdl.handle.net/1721.1/149961,MIT-LCS-TR-865,"Tetris is Hard, Even to Approximate",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,159
,"Huang, Andrew ""bunnie""",2004-10-20T20:29:51Z,2004-10-20T20:29:51Z,2002-06-01,http://hdl.handle.net/1721.1/7096,AITR-2002-006,ADAM: A Decentralized Parallel Computer Architecture Featuring Fast Thread and Data Migration and a Uniform Hardware Abstraction,"The furious pace of Moore's Law is driving  computer architecture into a realm where the the speed of light is the  dominant factor in system latencies. The number of clock cycles to span  a chip are increasing, while the number of bits that can be accessed  within a clock cycle is decreasing. Hence, it is becoming more  difficult to hide latency. One alternative solution is to reduce latency by  migrating threads and data, but the overhead of existing  implementations has previously made migration an unserviceable solution so  far.  I present an architecture, implementation, and  mechanisms that reduces the overhead of migration to the point where  migration is a viable supplement to other latency hiding  mechanisms, such as multithreading. The architecture is abstract,  and presents programmers with a simple, uniform fine-grained  multithreaded parallel programming model with implicit memory management. In  other words, the spatial nature and implementation details (such as  the number of processors) of a parallel machine are entirely hidden from  the programmer. Compiler writers are  encouraged to devise programming languages for the machine that guide a  programmer to express their ideas in terms of objects, since objects exhibit  an inherent physical locality of data and code. The machine  implementation can then leverage this locality to automatically distribute  data and threads across the physical machine by using a set of  high performance migration mechanisms.  An implementation of this architecture could  migrate a null thread in 66 cycles -- over a factor of 1000 improvement  over previous work. Performance also scales well; the time  required to move a typical thread is only 4 to 5 times that of a null  thread. Data migration performance is similar, and scales  linearly with data block size. Since the performance of the migration  mechanism is on par with that of an L2 cache, the implementation  simulated in my work has no data caches and relies instead on  multithreading and the migration mechanism to hide and reduce access  latencies.",AITR-2002-006,299 p.; 13404896 bytes; 2307234 bytes,application/postscript; application/pdf,en_US,AI; HPC parallel computer architecture queues fault tolerance programmability ADAM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,137
,"Beal, Jacob",2004-10-04T14:15:29Z,2004-10-04T14:15:29Z,2002-12-01,http://hdl.handle.net/1721.1/5933,AIM-2002-021,Leaderless Distributed Hierarchy Formation,"I present a system for robust leaderless  organization of an amorphous network into hierarchical clusters. This  system, which assumes that nodes are spatially embedded and can only  talk to neighbors within a given radius, scales to networks of arbitrary  size and converges rapidly. The amount of data stored at each  node is logarithmic in the diameter of the network, and the hierarchical  structure produces an addressing scheme such that there is an  invertible relation between distance and address for any pair of nodes.  The system adapts automatically to stopping failures, network  partition, and reorganization.",AIM-2002-021,27 p.; 7370490 bytes; 1660395 bytes,application/postscript; application/pdf,en_US,AI; amorphous computing hierarchy leaderless distributed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,177
,"Ma, Albert; Asanovi_, Krste",2023-03-29T15:35:33Z,2023-03-29T15:35:33Z,2002-05,https://hdl.handle.net/1721.1/149946,MIT-LCS-TR-844,A Double-Pulsed Set-Conditional-Reset Flip-Flop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,128
,"Nimmer, Jeremy",2023-03-29T15:35:52Z,2023-03-29T15:35:52Z,2002-06,https://hdl.handle.net/1721.1/149954,MIT-LCS-TR-852,Automatic Generation and Checking of Program Specifications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,136
,"Liskov, Barbara H.; Moh, Chuang-Hue; Richman, Steven; Shrira, Liuba; Chueng, Yin; Boyapati, Chandrasekhar",2023-03-29T15:35:50Z,2023-03-29T15:35:50Z,2002-06,https://hdl.handle.net/1721.1/149953,MIT-LCS-TR-851,Safe Lazy Software Upgrades in Object-Oriented Databases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,132
,"Hajiaghayi, Mohammad Taghi; Mahdian, Mohammad; Mirrokni, Vahab S.",2023-03-29T15:36:07Z,2023-03-29T15:36:07Z,2002-09,https://hdl.handle.net/1721.1/149960,MIT-LCS-TR-864,The Facility Location Problem with Concave Cost Functions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,153
,"III, Teodoro Arvizo",2004-10-20T20:29:40Z,2004-10-20T20:29:40Z,2002-06-01,http://hdl.handle.net/1721.1/7092,AITR-2002-004,A Virtual Machine for a Type-omega Denotational Proof Language,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",AITR-2002-004,106 p.; 2935187 bytes; 816842 bytes,application/postscript; application/pdf,en_US,AI; virtual machine; SECD; SECD machine; denotational proof language; Athena,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,140
,"Beal, Jacob",2004-10-20T20:29:08Z,2004-10-20T20:29:08Z,2002-01-01,http://hdl.handle.net/1721.1/7079,AITR-2002-002,Generating Communications Systems Through Shared Context,"In a distributed model of intelligence, peer components need to communicate with one another. I present a system which enables two agents connected by a thick twisted bundle of wires to bootstrap a simple communication system from observations of a shared environment. The agents learn a large vocabulary of symbols, as well as inflections on those symbols which allow thematic role-frames to be transmitted. Language acquisition time is rapid and linear in the number of symbols and inflections. The final communication system is robust and performance degrades gradually in the face of problems.",AITR-2002-002,58 p.; 7876447 bytes; 588901 bytes,application/postscript; application/pdf,en_US,AI; distributed amorphous human intelligence genesis robust communication network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,103
,"Gassend, Blaise; Clarke, Dwaine; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:35:57Z,2023-03-29T15:35:57Z,2002-06,https://hdl.handle.net/1721.1/149956,MIT-LCS-TR-854,Delay-Based Circuit Authentication With Application to Key Cards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,134
,"Gassend, Blaise; Clarke, Dwaine; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:35:36Z,2023-03-29T15:35:36Z,2002-06,https://hdl.handle.net/1721.1/149947,MIT-LCS-TR-845,Controlled Physical Unknown Functions: Applications to Secure Smartcards and Certified Execution,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,135
,"Aftab, Omar",2023-03-29T15:36:38Z,2023-03-29T15:36:38Z,2002-08,https://hdl.handle.net/1721.1/149972,MIT-LCS-TR-876b,Economic Mechanisms for Efficient Wireless Coexistence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,148
,"Chen, Benjie; Morris, Robert T.",2023-03-29T15:35:15Z,2023-03-29T15:35:15Z,2002-03,https://hdl.handle.net/1721.1/149939,MIT-LCS-TR-837,L+: Scalable Landmark Routing and Address Lookup for Multi-hop Wireless Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,116
,"Felzenszwalb, Pedro F.",2005-12-19T22:44:55Z,2005-12-19T22:44:55Z,2003-08-08,http://hdl.handle.net/1721.1/30400,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Representation and Detection of Shapes in Images,"We present a set of techniques that can be used to represent anddetect shapes in images.  Our methods revolve around a particularshape representation based on the description of objects usingtriangulated polygons.  This representation is similar to the medialaxis transform and has important properties from a computationalperspective.  The first problem we consider is the detection ofnon-rigid objects in images using deformable models.  We present anefficient algorithm to solve this problem in a wide range ofsituations, and show examples in both natural and medical images.  Wealso consider the problem of learning an accurate non-rigid shapemodel for a class of objects from examples.  We show how to learn goodmodels while constraining them to the form required by the detectionalgorithm.  Finally, we consider the problem of low-level imagesegmentation and grouping.  We describe a stochastic grammar thatgenerates arbitrary triangulated polygons while capturing Gestaltprinciples of shape regularity.  This grammar is used as a prior modelover random shapes in a low level algorithm that detects objects inimages.",MIT-CSAIL-TR-2003-008; AITR-2003-016,80 p.; 38103057 bytes; 1889641 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,250
,"Clarke, Dwaine; Devadas, Srinivas; van Dijk, Marten; Gassend, Blaise; Suh, G. Edward",2023-03-29T15:37:19Z,2023-03-29T15:37:19Z,2003-05,https://hdl.handle.net/1721.1/149987,MIT-LCS-TR-899,Incremental Multiset Hash Functions and their Application to Memory Integrity Checking,"We introduce a new cryptographic tool: multiset hash functions. Unlike standard hash functions which take strings as input, multiset hash functions operate on multisets (or sets). They map multisets of arbitrary finite size to strings (hashes) of fixed length. They are incremental in that, when new members are added to the multiset, the hash can be updated in time proportional to the change. The functions may be multiset-collision resistant in that it is diÔøΩcult to find two multisets which produce the same hash, or just set-collision resistant in that it is diÔøΩcult to find a set and a multiset which produce the same hash. In particular, we introduce four multiset hash functions, each with its own advantages. MSet-XOR-Hash uses the XOR operation and is very eÔøΩcient; however, it uses a secret key and is only set-collision resistant. MSet-Add-Hash uses addition modulo a large integer and, thus, is slightly less eÔøΩcient than MSet-XOR-Hash; MSet-Add-Hash also uses a secret key but it is multiset-collision resistant. MSet-Mu-Hash uses finite field arithmetic and is not as eÔøΩcient as the other two hash functions; however, MSet-Mu-Hash is multiset-collision resistant, and unlike the other two hash functions, does not require a secret key. MSet-VAdd-Hash is more eÔøΩcient than MSet-Mu-Hash; it is also multiset-collision resistant, and does not use a secret key, but the hashes it produces are significantly longer than the hashes of the other functions. The proven security of MSet-XOR-Hash and MSet-Add-Hash is quantitative. We reduce the hardness of finding collisions to the hardness of breaking the underlying pseudorandom functions. The proven security of MSet-Mu-Hash is in the random oracle model and is based on the hardness of the discrete logarithm problem. The proven security of MSet-VAdd-Hash is also in the random oracle model and is based on the hardness of the worst-case shortest vector problem. We demonstrate how set-collision resistant multiset hash functions make an existing oÔøΩine memory integrity checker secure against active adversaries. We improve on this checker such that it can use smaller time stamps without increasing the frequency of checks. The improved checker uses multiset-collision resistant hash functions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,219
,"Hajiaghayi, MohammadTaghi; Leighton, F. Thomson",2005-12-12T23:22:48Z,2005-12-12T23:22:48Z,2003-07-05,http://hdl.handle.net/1721.1/29829,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On the Max-Flow Min-Cut Ratio for Directed Multicommodity Flows,"We give a pure combinatorial problem whose solution determines max-flow
min-cut ratio for directed multicommodity flows. In addition, this
combinatorial problem has applications in improving the approximation  factor of Greedy algorithm for maximum edge disjoint path problem. More
precisely, our upper bound improves the approximation factor for this
problem to O(n^{3/4}). Finally, we demonstrate how even for very simple
graphs the aforementioned ratio might be very large.",MIT-CSAIL-TR-2003-002; MIT-LCS-TR-910,5 p.; 7867417 bytes; 389570 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,243
,"Mukherjee, Sayan; Golland, Polina; Panchenko, Dmitry",2004-10-08T20:39:06Z,2004-10-08T20:39:06Z,2003-08-28,http://hdl.handle.net/1721.1/6723,AIM-2003-019,Permutation Tests for Classification,"We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",AIM-2003-019,22 p.; 1135156 bytes; 662639 bytes,application/postscript; application/pdf,en_US,AI; Classification; Permutation testing; Statistical significance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,261
,"Morency, Louis-Philippe",2004-10-20T20:31:42Z,2004-10-20T20:31:42Z,2003-05-01,http://hdl.handle.net/1721.1/7102,AITR-2003-006,Stereo-Based Head Pose Tracking Using Iterative Closest Point and Normal Flow Constraint,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",AITR-2003-006,60 p.; 5276045 bytes; 2896854 bytes,application/postscript; application/pdf,en_US,AI; Head pose estimation; Stereo processing; Cursor control; 3D model acquisition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,223
,"Lee, Lily",2004-10-20T20:32:06Z,2004-10-20T20:32:06Z,2003-06-26,http://hdl.handle.net/1721.1/7109,AITR-2003-014,Gait Analysis for Classification,"This thesis describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. A suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. Despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. Each of the integration methods are investigated for their advantages and disadvantages. An improved gait representation is built based on our experiences with the initial set of gait representations. In addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.",AITR-2003-014,110 p.; 4040471 bytes; 994319 bytes,application/postscript; application/pdf,en_US,AI; gait recognition; gender classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,240
,"Marjanovic, Matthew J.",2004-10-20T20:32:04Z,2004-10-20T20:32:04Z,2003-06-20,http://hdl.handle.net/1721.1/7108,AITR-2003-013,Teaching an Old Robot New Tricks: Learning Novel Tasks via Interaction with People and Things,"As AI has begun to reach out beyond its symbolic, objectivist roots into the embodied, experientialist realm, many projects are exploring different aspects of creating machines which interact with and respond to the world as humans do. Techniques for visual processing, object recognition, emotional response, gesture production and recognition, etc., are necessary components of a complete humanoid robot. However, most projects invariably concentrate on developing a few of these individual components, neglecting the issue of how all of these pieces would eventually fit together.  The focus of the work in this dissertation is on creating a framework into which such specific competencies can be embedded, in a way that they can interact with each other and build layers of new functionality. To be of any practical value, such a framework must satisfy the real-world constraints of functioning in real-time with noisy sensors and actuators. The humanoid robot Cog provides an unapologetically adequate platform from which to take on such a challenge.  This work makes three contributions to embodied AI. First, it offers a general-purpose architecture for developing behavior-based systems distributed over networks of PC's. Second, it provides a motor-control system that simulates several biological features which impact the development of motor behavior. Third, it develops a framework for a system which enables a robot to learn new behaviors via interacting with itself and the outside world. A few basic functional modules are built into this framework, enough to demonstrate the robot learning some very simple behaviors taught by a human trainer.  A primary motivation for this project is the notion that it is practically impossible to build an ""intelligent"" machine unless it is designed partly to build itself. This work is a proof-of-concept of such an approach to integrating multiple perceptual and motor systems into a complete learning agent.",AITR-2003-013,181 p.; 13057317 bytes; 13082678 bytes,application/postscript; application/pdf,en_US,AI; cog humanoid robot embodied learning phd thesis metaphor pancake reaching vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,239
,"Liskov, Moses",2005-12-22T12:00:00Z,2005-12-22T12:00:00Z,2003-10-14,http://hdl.handle.net/1721.1/30427,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Electronic Cash with Blind Deposits: How to Have No Spare Change,"Electronic cash schemes in which the bank authenticates many coins at once suffer from the problem that coins that are authenticated together can be linked to one another. Unfortunately, unless a user spends coins in a closely prescribed manner, different batches of coins (""wallets"") will be linked together in these schemes. This is illustrated by the problem of what a customer does with the ""spare change"" - an unusable small amount of money left in a wallet. We propose a new protocol to be used in e-cash schemes: blind deposits. In a blind deposit, a customer returns a coin to the bank without revealing the coin. We present a secure and efficient e-cash scheme with this added feature based on that of Liskov-Micali [LM01].",MIT-CSAIL-TR-2003-022; MIT-LCS-TM-639,13 p.; 12754502 bytes; 526739 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,271
,"Christoudias, Chris Mario; Morency, Louis-Philippe; Darrell, Trevor",2004-10-08T20:38:54Z,2004-10-08T20:38:54Z,2003-04-18,http://hdl.handle.net/1721.1/6716,AIM-2003-010,Light Field Morphable Models,"Statistical shape and texture appearance models are  powerful image representations, but previously had  been restricted to 2D or simple 3D shapes. In this paper  we present a novel 3D morphable model based on  image-based rendering techniques, which can  represent complex lighting conditions, structures, and  surfaces. We describe how to construct a manifold of  the multi-view appearance of an object class using light  fields and show how to match a 2D image of an object  to a point on this manifold. In turn we use the  reconstructed light field to render novel views of the  object. Our technique overcomes the limitations of  polygon based appearance models and uses light  fields that are acquired in real-time.",AIM-2003-010,1375810 bytes; 716555 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,215
,"Timoner, Samson",2005-12-12T23:24:20Z,2005-12-12T23:24:20Z,2003-07-04,http://hdl.handle.net/1721.1/29830,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Compact Representations for Fast Nonrigid Registration of Medical Images,"We develop efficient techniques for the non-rigid registration of medical
images by using representations that adapt to the anatomy found in such
images.

 Images of anatomical structures typically have uniform intensity interiors
and smooth boundaries. We create methods to represent such regions
compactly using tetrahedra.  Unlike voxel-based representations, tetrahedra
can accurately describe the expected smooth surfaces of medical
objects. Furthermore, the interior of such objects can be represented using
a small number of tetrahedra. Rather than describing a medical object using
tens of thousands of voxels, our representations generally contain only a few
thousand elements.

Tetrahedra facilitate the creation of efficient non-rigid registration
algorithms based on finite element methods (FEM).  We create a fast,
FEM-based method to non-rigidly register segmented anatomical structures
from two subjects. Using our compact tetrahedral representations, this
method generally requires less than one minute of processing time on a desktop
PC.

We also create a novel method for the non-rigid registration of gray scale
images. To facilitate a fast method, we create a tetrahedral representation
of a displacement field that automatically adapts to both the anatomy in an
image and to the displacement field.  The resulting algorithm has a
computational cost that is dominated by the number of nodes in the mesh
(about 10,000), rather than the number of voxels in an image (nearly
10,000,000). For many non-rigid registration problems, we can find a
transformation from one image to another in five minutes. This speed is
important as it allows use of the algorithm during surgery.

We apply our algorithms to find correlations between the shape of
anatomical structures and the presence of schizophrenia. We show that a
study based on our representations outperforms studies based on other
representations. We also use the results of our non-rigid registration
algorithm as the basis of a segmentation algorithm. That algorithm also
outperforms other methods in our tests, producing smoother segmentations
and more accurately reproducing manual segmentations.",MIT-CSAIL-TR-2003-001; AITR-2003-015,183 p.; 160218641 bytes; 8166856 bytes,application/postscript; application/pdf,en_US,AI; non-rigid registration; medical image processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,242
,"Grauman, Kristen; Darrell, Trevor",2005-12-22T01:15:24Z,2005-12-22T01:15:24Z,2003-12-05,http://hdl.handle.net/1721.1/30438,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Fast Contour Matching Using Approximate Earth Mover's Distance,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",MIT-CSAIL-TR-2003-033; AIM-2003-026,16 p.; 18655633 bytes; 2291372 bytes,application/postscript; application/pdf,en_US,AI; contour matching; shape matching; EMD; image retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,284
,"Fitzpatrick, Paul",2004-10-20T20:31:57Z,2004-10-20T20:31:57Z,2003-06-01,http://hdl.handle.net/1721.1/7105,AITR-2003-008,From First Contact to Close Encounters: A Developmentally Deep Perceptual System for a Humanoid Robot,"This thesis presents a perceptual system for a  humanoid robot that integrates abilities such as object  localization and recognition with the deeper  developmental machinery required to forge those  competences out of raw physical experiences. It shows  that a robotic platform can build up and maintain a  system for object localization, segmentation, and  recognition, starting from very little. What the robot  starts with is a direct solution to achieving figure/ground  separation: it simply 'pokes around' in a region of visual  ambiguity and watches what happens. If the arm  passes through an area, that area is recognized as free  space. If the arm collides with an object, causing it to  move, the robot can use that motion to segment the  object from the background. Once the robot can  acquire reliable segmented views of objects, it learns  from them, and from then on recognizes and segments  those objects without further contact. Both low-level and  high-level visual features can also be learned in this  way, and examples are presented for both: orientation  detection and affordance recognition, respectively. The  motivation for this work is simple. Training on large  corpora of annotated real-world data has proven crucial  for creating robust solutions to perceptual problems  such as speech recognition and face detection. But the  powerful tools used during training of such systems are  typically stripped away at deployment. Ideally they  should remain, particularly for unstable tasks such as  object detection, where the set of objects needed in a  task tomorrow might be different from the set of objects  needed today. The key limiting factor is access to  training data, but as this thesis shows, that need not be  a problem on a robotic platform that can actively probe  its environment, and carry out experiments to resolve  ambiguity. This work is an instance of a general  approach to learning a new perceptual judgment:  find special situations in which the perceptual judgment  is easy and study these situations to find correlated  features that can be observed more generally.",AITR-2003-008,19935684 bytes; 7552671 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,232
,"Demsky, Brian; Cadar, Cristian; Roy, Daniel; Rinard, Martin",2005-12-22T01:14:41Z,2005-12-22T01:14:41Z,2003-11-13,http://hdl.handle.net/1721.1/30433,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Efficient Specification-Assisted Error Localization and Correction,"We present a new error localization tool, Archie, that accepts aspecification of key data structure consistency constraints, then generatesan algorithm that checks if the data structures satisfy theconstraints. We also present a set of specification analyses and optimizationsthat (for our benchmark software system) improve theperformance of the generated checking algorithm by over a factorof 3,900 as compared with the initial interpreted implementation,enabling Archie to efficiently support interactive debugging.We evaluate ArchieÂ’s effectiveness by observing the actions oftwo developer populations (one using Archie, the other using standarderror localization techniques) as they attempted to localize andcorrect three errors in a benchmark software system. With Archie,the developers were able to localize each error in less than 10 minutesand correct each error in (usually much) less than 20 minutes.Without Archie, the developers were, with one exception, unableto locate each error after more than an hour of effort. These resultsillustrate ArchieÂ’s potential to substantially improve current errorlocalization and correction techniques.",MIT-CSAIL-TR-2003-028; MIT-LCS-TR-927,10 p.; 19768606 bytes; 820085 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,277
,"Koile, Kimberle; Tollmar, Konrad; Demirdjian, David; Shrobe, Howard; Darrell, Trevor",2004-10-08T20:39:02Z,2004-10-08T20:39:02Z,2003-06-10,http://hdl.handle.net/1721.1/6720,AIM-2003-015,Activity Zones for Context-Aware Computing,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",AIM-2003-015,12 p.; 17075202 bytes; 8771896 bytes,application/postscript; application/pdf,en_US,AI; context-aware; activity; intelligent environment; ubiquitous; 3d tracker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,237
,"Jamieson, Kyle; Balakrishnan, Hari; Tay, Y.C.",2023-03-29T15:37:15Z,2023-03-29T15:37:15Z,2003-05,https://hdl.handle.net/1721.1/149985,MIT-LCS-TR-894,Sift: A MAC Protocol for Event-Driven Wireless Sensor Networks,"Nodes in sensor networks often encounter spatially-correlated contention, where multiple nodes in the same neighborhood all sense an event they need to transmit information about. Furthermore, in many sensor network applications, it is sufficient if a subset of the nodes that observe the same event report it. We show that traditional carrier-sense multiple access (CSMA) protocols like 802.11 do not handle the first constraint adequately, and do not take advantage of the second property, leading to degraded latency and throughput as the network scales in size.   We present Sift, a medium access protocol for wireless sensor networks designed with the above observations in mind. Sift is a randomized CSMA protocol, but unlike previous protocols, does not use a time-varying contention window from which a node randomly picks a transmission slot. Rather, to reduce the latency for the delivery of event reports, Sift uses a fixed-size contention window and a carefully-chosen, non-uniform probability distribution of transmitting in each slot within the window. We show using simulations that Sift can offer up to a 7-fold latency reduction compared to 802.11 as the size of the sensor network scales up to 500 nodes. We then analytically prove bounds on the best latency achievable by a decentralized CSMA-based MAC protocol for sensor networks where one report of each event is enough, and show that Sift comes close to meeting this bound.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,217
,"Eisenstein, Jacob",2005-12-22T01:12:20Z,2005-12-22T01:12:20Z,2003-10-28,http://hdl.handle.net/1721.1/30431,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Evolving Robocode Tank Fighters,"In this paper, I describe the application of genetic programming to evolve a controller for a robotic tank in a simulated environment.The purpose is to explore how genetic techniques can best be applied to produce controllers based on subsumption and behavior oriented languages such as REX.  As part of my implementation, I developed TableRex, a modification of REX that can be expressed on a fixed-lengthgenome.  Using a fixed subsumption architecture of TableRex modules, I evolved robots that beat some of the most competitive hand-coded adversaries.",MIT-CSAIL-TR-2003-026; AIM-2003-023,24 p.; 21707085 bytes; 716288 bytes,application/postscript; application/pdf,en_US,AI; genetic programming; robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,274
,"Moh, Chuang-Hue",2023-03-29T15:37:22Z,2023-03-29T15:37:22Z,2003-05,https://hdl.handle.net/1721.1/149988,MIT-LCS-TR-901,Snapshots in a Distributed Persistent Object Storage System,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,222
,"Balas, Benjamin J.; Sinha, Pawan",2004-10-20T21:05:11Z,2004-10-20T21:05:11Z,2003-08-13,http://hdl.handle.net/1721.1/7276,AIM-2003-018; CBCL-229,Dissociated Dipoles: Image representation via non-local comparisons,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . We introduce the ""Dissociated Dipole,"" or ""Sticks"" operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",AIM-2003-018; CBCL-229,15 p.; 691165 bytes; 974071 bytes,application/postscript; application/pdf,en_US,AI; image representation; recognition; non-local filtering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,254
,"Donovan, Alan; Ernst, Michael D.",2023-03-29T15:37:03Z,2023-03-29T15:37:03Z,2003-03,https://hdl.handle.net/1721.1/149981,MIT-LCS-TR-889,Inference of Generic Types in Java,"Future versions of Java will include support for parametric polymorphism, or generic classes.  This will bring many benefits to Java programmers, not least because current Java practise makes heavy use of pseudo-generic classes.  Such classes (for example, those in package java.util) have logically generic specifications and documentation, but the type system cannot prove their patterns of use to be safe.   This work aims to solve the problem of automatic translation of Java source code into Generic Java (GJ) source code.  We present two algorithms that together can be used to translate automatically a Java source program into a semantically-equivalent GJ program with generic types.   The first algorithm infers a candidate generalisation for any class, based on the methods of that class in isolation.  The second algorithm analyses the whole program; it determines a precise parametric type for every value in the program.  Optionally, it also refines the generalisations produced by the first analysis as required by the patterns of use of those classes in client code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,201
,"Kaynar, Dilsun K.; Lynch, Nancy; Segala, Roberto; Vaandrager, Frits",2005-12-19T22:48:27Z,2005-12-19T22:48:27Z,2003-08-27,http://hdl.handle.net/1721.1/30403,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Theory of Timed I/O Automata,"Revised version -- November 23, 2004.This paper presents the Timed Input/Output Automaton (TIOA) modeling framework, a basic mathematical framework to support description and analysis of timed systems.",MIT-CSAIL-TR-2003-014; MIT-LCS-TR-917,130 p.; 112222505 bytes; 4311471 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,258
,"Beal, Jacob; Gilbert, Seth",2004-10-08T20:43:07Z,2004-10-08T20:43:07Z,2003-12-17,http://hdl.handle.net/1721.1/6734,AIM-2003-027,RamboNodes for the Metropolitan Ad Hoc Network,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",AIM-2003-027,22 p.; 1312502 bytes; 499111 bytes,application/postscript; application/pdf,en_US,AI; ad-hoc networks distributed algorithms atomic distributed shared memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,287
,"Strumpen, Volker; Hoffmann, Henry; Agarwal, Anant",2005-12-22T01:09:48Z,2005-12-22T01:09:48Z,2003-10-22,http://hdl.handle.net/1721.1/30429,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Stream Algorithm for the SVD,"We present a stream algorithm for the Singular-Value Decomposition (SVD) of anM X N matrix A. Our algorithm trades speed of numerical convergence for parallelism,and derives from a one-sided, cyclic-by-rows Hestenes SVD. Experimental results showthat we can create O(M) parallelism, at the expense of increasing the computationalwork by less than a factor of about 2. Our algorithm qualifes as a stream algorithmin that it requires no more than a small, bounded amount of local storage per processor and its compute efficiency approaches an optimal 100% asymptotically for largenumbers of processors and appropriate problem sizes.",MIT-CSAIL-TR-2003-024; MIT-LCS-TM-641,31 p.; 30567456 bytes; 1124918 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,272
,"Ross, Michael G.; Kaelbling, Leslie Pack",2005-12-19T22:46:46Z,2005-12-19T22:46:46Z,2003-09-08,http://hdl.handle.net/1721.1/30401,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning object segmentation from video data,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",MIT-CSAIL-TR-2003-018; AIM-2003-022,15 p.; 23365488 bytes; 1821447 bytes,application/postscript; application/pdf,en_US,AI; learning; image segmentation; motion; Markov random field; belief propagation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,266
,"Demaine, Erik D.; Fomin, Fedor V.; Hajiaghayi, Mohammad Taghi; Thilikos, Dimitrios M.",2023-03-29T15:37:28Z,2023-03-29T15:37:28Z,2003-06,https://hdl.handle.net/1721.1/149991,MIT-LCS-TR-905,Subexponential Parameterized Algorithms on Graphs of Bounded Genus and H-minor-free Graphs,"We introduce a new framework for designing fixed-parameter algorithms with subexponential running time---2^O(sqrt k) n^O(1).  Our results apply to a broad family of graph problems, called bidimensional problems, which includes many domination and covering problems such as vertex cover, feedback vertex set, minimum maximal matching, dominating set, edge dominating set, clique-transversal set, and many others restricted to bounded genus graphs. Furthermore, it is fairly straightforward to prove that a problem is bidimensional.  In particular, our framework includes as special cases all previously known problems to have such subexponential algorithms.  Previously, these algorithms applied to planar graphs, single-crossing-minor-free graphs, and/or map graphs; we extend these results to apply to bounded-genus graphs as well.  In a parallel development of combinatorial results, we establish an upper bound on the treewidth (or branchwidth) of a bounded-genus graph that excludes some planar graph H as a minor.  This bound depends linearly on the size |V(H)| of the excluded graph H and the genus g(G) of the graph G, and applies and extends the graph-minors work of Robertson and Seymour.   Building on these results, we develop subexponential fixed-parameter algorithms for dominating set, vertex cover, and set cover in any class of graphs excluding a fixed graph H as a minor.  In particular, this general category of graphs includes planar graphs, bounded-genus graphs, single-crossing-minor-free graphs, and any class of graphs that is closed under taking minors. Specifically, the running time is 2^O(sqrt k) n^h, where h is a constant depending only on H, which is polynomial for k = O(log^2 n).  We introduce a general approach for developing algorithms on H-minor-free graphs, based on structural results about H-minor-free graphs at the heart of Robertson and Seymour's graph-minors work.  We believe this approach opens the way to further development on problems in H-minor-free graphs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,230
,"Wehowsky, Andreas F.",2004-10-20T20:31:59Z,2004-10-20T20:31:59Z,2003-05-30,http://hdl.handle.net/1721.1/7106,AITR-2003-012,Safe Distributed Coordination of Heterogeneous Robots through Dynamic Simple Temporal Networks,"Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman-Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",AITR-2003-012,95 p.; 3611933 bytes; 908879 bytes,application/postscript; application/pdf,en_US,AI; model-based autonomy; distributed planning; distributed constraint satisfaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,228
,"Srebro, Nathan",2005-12-22T02:16:24Z,2005-12-22T02:16:24Z,2004-11-22,http://hdl.handle.net/1721.1/30507,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning with Matrix Factorizations,"Matrices that can be factored into a product of two simpler matricescan serve as a useful and often natural model in the analysis oftabulated or high-dimensional data.  Models based on matrixfactorization (Factor Analysis, PCA) have been extensively used instatistical analysis and machine learning for over a century, withmany new formulations and models suggested in recent years (LatentSemantic Indexing, Aspect Models, Probabilistic PCA, Exponential PCA,Non-Negative Matrix Factorization and others).  In this thesis weaddress several issues related to learning with matrix factorizations:we study the asymptotic behavior and generalization ability ofexisting methods, suggest new optimization methods, and present anovel maximum-margin high-dimensional matrix factorizationformulation.",MIT-CSAIL-TR-2004-076; AITR-2004-009,132 p.; 96239481 bytes; 5561927 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,386
,"Zollei, Lilla; Fisher, John; Wells, William",2004-10-08T20:43:13Z,2004-10-08T20:43:13Z,2004-04-28,http://hdl.handle.net/1721.1/6738,AIM-2004-011,A Unified Statistical and Information Theoretic Framework for Multi-modal Image Registration,"We formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  A unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. Additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. We analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. Among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",AIM-2004-011,21 p.; 2760680 bytes; 531001 bytes,application/postscript; application/pdf,en_US,AI; registration; information theory; unified framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,325
,"Lam, Patrick; Kuncak, Viktor; Rinard, Martin",2005-12-19T23:39:47Z,2005-12-19T23:39:47Z,2004-10-04,http://hdl.handle.net/1721.1/30421,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Our Experience with Modular Pluggable Analyses,"We present a technique that enables the focused applicationof multiple analyses to di erent modules in thesame program. In our approach, each module encapsulatesone or more data structures and uses membershipin abstract sets to characterize how objects participatein data structures. Each analysis veri es that the implementationof the module 1) preserves important internaldata structure consistency properties and 2) correctlyimplements an interface that uses formulas in a set algebrato characterize the e ects of operations on theencapsulated data structures. Collectively, the analysesuse the set algebra to 1) characterize how objects participatein multiple data structures and to 2) enable theinter-analysis communication required to verify propertiesthat depend on multiple modules analyzed by differentanalyses.We have implemented our system and deployed threepluggable analyses into it: a ag analysis for modulesin which abstract set membership is determined by aag  eld in each object, a plugin for modules that encapsulatelinked data structures such as lists and trees,and an array plugin in which abstract set membershipis determined by membership in an array. Our experimentalresults indicate that our approach makes it possibleto e ectively combine multiple analyses to verifyproperties that involve objects shared by multiple modules,with each analysis analyzing only those modulesfor which it is appropriate.",MIT-CSAIL-TR-2004-061; MIT-LCS-TR-965,21 p.; 36771229 bytes; 1343453 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,371
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2004-10-08T20:43:16Z,2004-10-08T20:43:16Z,2004-06-25,http://hdl.handle.net/1721.1/6740,AIM-2004-013,Contextual models for object detection using boosted random fields,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",AIM-2004-013,10 p.; 2184856 bytes; 906515 bytes,application/postscript; application/pdf,en_US,AI; Object detection; context; boosting; BP; random fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,349
,"Arkoudas, Konstantine; Zee, Karen; Kuncak, Viktor; Rinard, Martin",2005-12-22T01:30:49Z,2005-12-22T01:30:49Z,2004-05-06,http://hdl.handle.net/1721.1/30468,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Verifying a File System Implementation,"We present a correctness proof for a basic file system implementation. This implementation contains key elements of standard Unix file systems such as inodes and fixed-size disk blocks. We prove the implementation correct by establishing a simulation relation between the specification of the file system (which models the file system as an abstract map from file names to sequences of bytes) and its implementation (which uses fixed-size disk blocks to store the contents of the files).We used the Athena proof checker to represent and validate our proof. Our experience indicates that Athena's use of block-structured natural deduction, support for structural induction and proof abstraction, and seamless connection with high-performance automated theorem provers were essential to our ability to successfully manage a proof of this size.",MIT-CSAIL-TR-2004-028; MIT-LCS-TR-946,31 p.; 26287249 bytes; 1119970 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,328
,"Sand, Peter; Teller, Seth",2005-12-22T01:30:57Z,2005-12-22T01:30:57Z,2004-05-11,http://hdl.handle.net/1721.1/30469,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Video Matching,"This paper describes a method for bringing two videos (recorded at different times) into spatiotemporal alignment, then comparing and combining corresponding pixels for applications such as background subtraction, compositing, and increasing dynamic range. We align a pair of videos by searching for frames that best match according to a robust image registration process. This process uses locally weighted regression to interpolate and extrapolate high-likelihood image correspondences, allowing new correspondences to be discovered and refined. Image regions that cannot be matched are detected and ignored, providing robustness to changes in scene content and lighting, which allows a variety of new applications.",MIT-CSAIL-TR-2004-029; MIT-LCS-TR-947,8 p.; 23656728 bytes; 1181560 bytes,application/postscript; application/pdf,en_US,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,329
,"Hearn, Robert A.",2005-12-22T01:34:55Z,2005-12-22T01:34:55Z,2004-06-16,http://hdl.handle.net/1721.1/30479,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Building Grounded Abstractions for Artificial Intelligence Programming,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",MIT-CSAIL-TR-2004-040; AITR-2004-004,58 p.; 45433655 bytes; 1795607 bytes,application/postscript; application/pdf,en_US,AI; Artificial Intelligence; Society of Mind; Multi-Agent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,343
,"Steinkraus, Kurt; Kaelbling, Leslie Pack",2005-12-22T01:41:41Z,2005-12-22T01:41:41Z,2004-10-21,http://hdl.handle.net/1721.1/30496,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining dynamic abstractions in large MDPs,"One of the reasons that it is difficult to plan and act in real-worlddomains is that they are very large.  Existing research generallydeals with the large domain size using a static representation andexploiting a single type of domain structure.  In this paper, wecreate a framework that encapsulates existing and new abstraction andapproximation methods into modules, and combines arbitrary modulesinto a system that allows for dynamic representation changes.  We showthat the dynamic changes of representation allow our framework tosolve larger and more interesting domains than were previouslypossible, and while there are no optimality guarantees, suitablemodule choices gain tractability at little cost to optimality.",MIT-CSAIL-TR-2004-065; AIM-2004-023,12 p.; 9975204 bytes; 424481 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,375
,"Perez-Breva, Luis",2005-12-22T01:27:18Z,2005-12-22T01:27:18Z,2004-04-21,http://hdl.handle.net/1721.1/30463,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Cascading Regularized Classifiers,"Among the various methods to combine classifiers, Boosting was originally thought as an stratagem to cascade pairs of classifiers through their disagreement. I recover the same idea from the work of Niyogi et al. to show how to loosen the requirement of weak learnability, central to Boosting, and introduce a new cascading stratagem. The paper concludes with an empirical study of an implementation of the cascade that, under assumptions that mirror the conditions imposed by Viola and Jones in [VJ01], has the property to preserve the generalization ability of boosting.",MIT-CSAIL-TR-2004-023; AIM-2004-028,8 p.; 8847621 bytes; 505102 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,321
,"Teevan, Jaime",2005-12-22T01:35:01Z,2005-12-22T01:35:01Z,2004-06-18,http://hdl.handle.net/1721.1/30480,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,How People Re-find Information When the Web Changes,"This paper investigates how people return to information in a dynamic information environment.  For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed.  Changes can benefit users by providing new information, but they hinder returning to previously viewed information.  The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  The implications of these observations for systems that support re-finding in dynamic environments are discussed.",MIT-CSAIL-TR-2004-041; AIM-2004-012,9 p.; 16783533 bytes; 654765 bytes,application/postscript; application/pdf,en_US,AI; re-finding; information management; dynamic information,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,346
,"Harvey, Nicholas J.; Kleinberg, Robert D.; Lehman, April Rasala",2005-12-22T02:16:27Z,2005-12-22T02:16:27Z,2004-11-24,http://hdl.handle.net/1721.1/30508,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Comparing Network Coding with Multicommodity Flow for the k-pairs Communication Problem,"Given a graph G = (V,E) and k source-sink pairs of vertices, this papers investigates the maximum rate r at which all pairs can simultaneously communicate. We view this problem from two perspectives and compare their advantages. In the multicommodity flow formulation, a solution provides dedicated bandwidth r between each source-sink pair. In the information flow formulation, a vertex can transmit a function of the information it received thereby allowing multiple source-sink pairs to share bandwidth. For directed acyclic graphs with n vertices, we show that the rate achievable in the information flow formulation can be a multiplicative factor n larger than the rate achievable in the multicommodity flow formulation. It is well known [5] that for undirected graphs with n vertices, in the multicommodity flow formulation, the maximum rate achievable can be an O(1/log|V|) multiplicative factor smaller than the value of the sparsest cut. We extend this result to show that the maximum rate achievable in the information flow setting can be an O(1/log|V|) multiplicative factor smaller than the sparsest cut value.For directed acyclic graphs G, we define a parameter called the value of the most meager cut which is an upper bound for the maximum rate achievable in the information flow setting.We also present an example illustrating that this upper bound is not always tight.",MIT-CSAIL-TR-2004-078; MIT-LCS-TR-964,13 p.; 14089646 bytes; 597139 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,388
,"Stamatoiu, Oana L.",2005-12-22T01:31:32Z,2005-12-22T01:31:32Z,2004-05-18,http://hdl.handle.net/1721.1/30473,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning Commonsense Categorical Knowledge in a Thread Memory System,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",MIT-CSAIL-TR-2004-033; AITR-2004-001,96 p.; 68735708 bytes; 2432875 bytes,application/postscript; application/pdf,en_US,AI; learning; context; categorization; similarity; Bridge; thread memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,336
,"Tan, Godfrey; Guttag, John",2005-12-22T02:15:36Z,2005-12-22T02:15:36Z,2004-11-12,http://hdl.handle.net/1721.1/30503,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Capacity Allocation in Wireless LANs,"Today's access point based wireless LANs (WLANs) are inefficient and unfair. For many traffic loads they provide far less total throughput than they should, and do a poor job allocating what throughput they do deliver. Inappropriate association of nodes to access points and rates to flows plays a large role in these problems. We address a major root cause of this problem in this paper.Current practice ignores the distinction between flows that connect two wireless nodes via an access point and flows that connect wireless nodes to the wired infrastructure. As wireless devices and applications become more pervasive, ignoring this distinction will lead to a significant degradation in perceived performance.In this paper, we i) describe a series of examples that illustrates the impact of two-hop flows on the performance of the system, ii) provide a practical algorithm to solve the AP-assignment problem and iii) evaluate the performance of our algorithm against other approaches. Our preliminary results show that our algorithm can increase average achieved throughput by as much as 50% for some traffic loads.",MIT-CSAIL-TR-2004-073; MIT-LCS-TR-973,17 p.; 20308180 bytes; 807306 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,381
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2005-12-22T01:35:14Z,2005-12-22T01:35:14Z,2004-06-25,http://hdl.handle.net/1721.1/30482,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Contextual models for object detection using boosted random fields,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",MIT-CSAIL-TR-2004-043; AIM-2004-013,10 p.; 11085755 bytes; 604755 bytes,application/postscript; application/pdf,en_US,AI; Object detection; context; boosting; BP; random fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,348
,"Yokono, Jerry Jun; Poggio, Tomaso",2004-10-20T21:05:26Z,2004-10-20T21:05:26Z,2004-04-27,http://hdl.handle.net/1721.1/7285,AIM-2004-010; CBCL-238,Rotation Invariant Object Recognition from One Training Example,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",AIM-2004-010; CBCL-238,15 p.; 5162833 bytes; 968095 bytes,application/postscript; application/pdf,en_US,AI; object recognition; local descriptor; rotation invariant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,323
,"Monteleoni, Claire; Balakrishnan, Hari; Feamster, Nick; Jaakkola, Tommi",2005-12-22T02:15:02Z,2005-12-22T02:15:02Z,2004-10-27,http://hdl.handle.net/1721.1/30499,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Managing the 802.11 Energy/Performance Tradeoff with Machine Learning,"This paper addresses the problem of managing the tradeoff betweenenergy consumption and performance in wireless devices implementingthe IEEE 802.11 standard. To save energy, the 802.11 specificationproposes a power-saving mode (PSM), where a device can sleep to saveenergy, periodically waking up to receive packets from a neighbor(e.g., an access point) that may have buffered packets for thesleeping device. Previous work has shown that a fixed polling time forwaking up degrades the performance of Web transfers, because networkactivity is bursty and time-varying. We apply a new online machinelearning algorithm to this problem and show, using ns simulation andtrace analysis, that it is able to adapt well to network activity. Thelearning process makes no assumptions about the underlying networkactivity being stationary or even Markov. Our learning power-savingalgorithm, LPSM, guides the learning using a ""loss function"" thatcombines the increased latency from potentially sleeping too long andthe wasted use of energy in waking up too soon.  In our nssimulations, LPSM saved 7%-20% more energy than 802.11 in power-savingmode, with an associated increase in average latency by a factor of1.02, and not more than 1.2.  LPSM is straightforward to implementwithin the 802.11 PSM framework.",MIT-CSAIL-TR-2004-068; MIT-LCS-TR-971,14 p.; 23210224 bytes; 1542849 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,378
,"Richards, Whitman; Seung, H. Sebastian",2005-12-22T02:20:05Z,2005-12-22T02:20:05Z,2004-12-31,http://hdl.handle.net/1721.1/30513,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Neural Voting Machines,"Â“Winner-take-allÂ” networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",MIT-CSAIL-TR-2004-083; AIM-2004-029,12 p.; 13714512 bytes; 523751 bytes,application/postscript; application/pdf,en_US,AI; WTA; Borda machine; Condorcet procedure; neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,393
,"Tucker-Kellogg, Lisa",2005-12-19T23:32:53Z,2005-12-19T23:32:53Z,2004-10-01,http://hdl.handle.net/1721.1/30419,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Systematic Conformational Search with Constraint Satisfaction,"Throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  This thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state NMR spectroscopy.  A separate portion of the thesis isabout protein-DNA binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.The search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromNMR experiments.  Systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  Our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) OmniMerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  Although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  Propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of OmniMerge.(3) The search may be run in two stages, once at low-resolutionusing a side-effect of OmniMerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) An A* function prioritizes each subchain based onestimated future search costs.  Subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.A common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.In addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.These novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",MIT-CSAIL-TR-2004-060; AITR-2004-007,177 p.; 127791565 bytes; 5501537 bytes,application/postscript; application/pdf,en_US,AI; Distance Geometry; Nuclear Magnetic Resonance (NMR); Molecular Modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,370
,"Stephenson, Mark; Amarasinghe, Saman",2005-12-22T01:20:19Z,2005-12-22T01:20:19Z,2004-03-22,http://hdl.handle.net/1721.1/30453,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Predicting Unroll Factors Using Nearest Neighbors,"In order to deliver the promise of MooreÂ’s Law to the enduser, compilers must make decisions that are intimately tiedto a specific target architecture. As engineers add architecturalfeatures to increase performance, systems becomeharder to model, and thus, it becomes harder for a compilerto make effective decisions.Machine-learning techniques may be able to help compilerwriters model modern architectures. Because learning techniquescan effectively make sense of high dimensional spaces,they can be a valuable tool for clarifying and discerningcomplex decision boundaries. In our work we focus on loopunrolling, a well-known optimization for exposing instructionlevel parallelism. Using the Open Research Compileras a testbed, we demonstrate how one can use supervisedlearning techniques to model the appropriateness of loopunrolling.We use more than 1,100 loops Â— drawn from 46 benchmarksÂ— to train a simple learning algorithm to recognizewhen loop unrolling is advantageous. The resulting classifiercan predict with 88% accuracy whether a novel loop(i.e., one that was not in the training set) benefits fromloop unrolling. Furthermore, we can predict the optimal ornearly optimal unroll factor 74% of the time. We evaluatethe ramifications of these prediction accuracies using theOpen Research Compiler (ORC) and the Itanium r  2 architecture.The learned classifier yields a 6% speedup (overORCÂ’s unrolling heuristic) for SPEC benchmarks, and a 7%speedup on the remainder of our benchmarks. Because thelearning techniques we employ run very quickly, we wereable to exhaustively determine the four most salient loopcharacteristics for determining when unrolling is beneficial.",MIT-CSAIL-TR-2004-012; MIT-LCS-TR-938,9 p.; 15158625 bytes; 629381 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,307
,"Liang, Percy; Srebro, Nathan",2005-12-22T02:19:52Z,2005-12-22T02:19:52Z,2004-12-30,http://hdl.handle.net/1721.1/30511,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Methods and Experiments With Bounded Tree-width Markov Networks,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",MIT-CSAIL-TR-2004-081; AIM-2004-030,10 p.; 10714507 bytes; 473643 bytes,application/postscript; application/pdf,en_US,AI; tree-width; hypertrees; Markov Networks; maximum likelihood; MDL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,391
,"Kuncak, Viktor; Nguyen, Huu Hai; Rinard, Martin",2005-12-22T01:35:58Z,2005-12-22T01:35:58Z,2004-07-19,http://hdl.handle.net/1721.1/30488,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,An Algorithm for Deciding BAPA: Boolean Algebra with Presburger Arithmetic,"We describe an algorithm for deciding the first-order multisorted theory BAPA, which combines 1) Boolean algebras of sets of uninterpreted elements (BA) and 2) Presburger arithmetic operations (PA). BAPA can express the relationship between integer variables and cardinalities of sets, and supports arbitrary quantification over both sets and integers.Our motivation for BAPA is deciding verification conditions that arise in the static analysis of data structure consistency properties. Data structures often use an integer variable to keep track of the number of elements they store; an invariant of such a data structure is that the value of the integer variable is equal to the number of elements stored in the data structure. When the data structure content is represented by a set, the resulting constraints can be captured in BAPA. BAPA formulas with quantifier alternations arise when annotations contain quantifiers themselves, or when proving simulation relation conditions for refinement and equivalence of program fragments. Furthermore, BAPA constraints can be used to extend the techniques for proving the termination of integer programs to programs that manipulate data structures, and have applications in constraint databases.We give a formal description of a decision procedure for BAPA, which implies the decidability of the satisfiability and validity problems for BAPA. We analyze our algorithm and obtain an elementary upper bound on the running time, thereby giving the first complexity bound for BAPA. Because it works by a reduction to PA, our algorithm yields the decidability of a combination of sets of uninterpreted elements with any decidable extension of PA. Our algorithm can also be used to yield an optimal decision procedure for BA though a reduction to PA with bounded quantifiers.We have implemented our algorithm and used it to discharge verification conditions in the Jahob system for data structure consistency checking of Java programs; our experience with the algorithm is promising.",MIT-CSAIL-TR-2004-049; MIT-LCS-TR-958,26 p.; 26003902 bytes; 1120584 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,357
,"Georgiou, Chryssis; Mavrommatis, Panayiotis P.; Tauber, Joshua A.",2005-12-19T23:25:19Z,2005-12-19T23:25:19Z,2004-10-06,http://hdl.handle.net/1721.1/30412,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Implementing Asynchronous Distributed Systems Using the IOA Toolkit,"This document is a report about the capabilities and performance of the IOA Toolkit, and in particularthe tools that provide support for implementing and running distributed systems (checker,composer, code generator). The Toolkit compiles distributed systems specified in IOA into Javaclasses, which run on a network of workstations and communicate using the Message Passing Interface(MPI). In order to test the toolkit, several distributed algorithms were implemented, rangingfrom simple algorithms such as LCR leader election in a ring network to more complex algorithmssuch as the GHS algorithm for computing the minimum spanning tree in an arbitrary graph. Allof our experiments completed successfully, and several runtime measurements were made.",MIT-CSAIL-TR-2004-062; MIT-LCS-TR-966,107 p.; 63601552 bytes; 2713486 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,372
,"Tauber, Joshua A.; Garland, Stephen J.",2005-12-22T01:35:52Z,2005-12-22T01:35:52Z,2004-07-19,http://hdl.handle.net/1721.1/30487,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Definition and Expansion of Composite Automata in IOA,"The IOA language provides notations for defining both primitive and composite I/O automata.This note describes, both formally and with examples, the constraints on these definitions, thecomposability requirements for the components of a composite automaton, and the transformationof a composite automaton into an equivalent primitive automaton.Section 2 introduces four examples used throughout this note to illustrate new definitions andoperations. Section 3 treats IOA programs for primitive I/O automata: it introduces notationsfor describing the syntactic structures that appear in these programs, and it lists syntactic andsemantic conditions that these programs must satisfy to represent valid primitive I/O automata.Section 4 describes how to reformulate primitive IOA programs into an equivalent but more regular(desugared) form that is used in later definitions in this note. Section 5 treats IOA programsfor composite I/O automata: it introduces notations for describing the syntactic structures thatappear in these programs, describes resortings induced by them, and lists syntactic and semanticconditions that these programs must satisfy to represent valid composite I/O automata. Section 6describes the translation of the name spaces of component automata into a unified name spacefor the composite automaton. Section 7 shows how to expand an IOA program for a compositeautomaton into an equivalent IOA program for a primitive automaton. The expansion is generatedby combining syntactic structures of the desugared programs for the component automata afterapplying appropriate replacements of sorts and variables. Section 8 details the expansion of thecomposite automaton introduced in Section 2 using the desugared forms developed throughoutSections 4Â–6 and the techniques described in Section 7. Finally, Section 9 gives a precise definitionof the resortings and substitutions used to replace sorts and variables.",MIT-CSAIL-TR-2004-048; MIT-LCS-TR-959,91 p.; 77470182 bytes; 2979546 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,356
,"Kennell, Jonathan",2005-12-22T01:31:24Z,2005-12-22T01:31:24Z,2004-05-18,http://hdl.handle.net/1721.1/30472,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Generative Temporal Planning with Complex Processes,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems.  This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander.  To support such a system, this thesis presents the Spock generative planner.  To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes.  This is in contrast to traditional planners, whose operators represent simple atomic or durative actions.  Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes.  RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators.  Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution.  Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts.  This thesis describes the Spock algorithm in detail, along with example problems and test results.",MIT-CSAIL-TR-2004-032; AITR-2004-002,90 p.; 56421512 bytes; 2495752 bytes,application/postscript; application/pdf,en_US,"AI; planning ""temporal planning""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,334
,"Kennell, Jonathan",2004-10-20T20:32:23Z,2004-10-20T20:32:23Z,2004-05-18,http://hdl.handle.net/1721.1/7113,AITR-2004-002,Generative Temporal Planning with Complex Processes,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems. This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander. To support such a system, this thesis presents the Spock generative planner. To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes. This is in contrast to traditional planners, whose operators represent simple atomic or durative actions. Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes. RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators. Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution. Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts. This thesis describes the Spock algorithm in detail, along with example problems and test results.",AITR-2004-002,90 p.; 15726143 bytes; 1269432 bytes,application/postscript; application/pdf,en_US,"AI; planning ""temporal planning""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,332
,"Dolev, Shlomi; Lahiani, Limor; Lynch, Nancy; Nolte, Tina",2005-12-22T02:36:07Z,2005-12-22T02:36:07Z,2005-08-11,http://hdl.handle.net/1721.1/30563,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Self-Stabilizing Mobile Node Location Management and Message,"We present simple algorithms for achieving self-stabilizing locationmanagement and routing in mobile ad-hoc networks. While mobile clients maybe susceptible to corruption and stopping failures, mobile networks areoften deployed with a reliable GPS oracle, supplying frequent updates ofaccurate real time and location information to mobile nodes. Informationfrom a GPS oracle provides an external, shared source of consistency formobile nodes, allowing them to label and timestamp messages, and henceaiding in identification of, and eventual recovery from, corruption andfailures. Our algorithms use a GPS oracle.Our algorithms also take advantage of the Virtual Stationary Automataprogramming abstraction, consisting of mobile clients, virtual timedmachines called virtual stationary automata (VSAs), and a local broadcastservice connecting VSAs and mobile clients. VSAs are distributed at knownlocations over the plane, and emulated in a self-stabilizing manner by themobile nodes in the system. They serve as fault-tolerant building blocksthat can interact with mobile clients and each other, and can simplifyimplementations of services in mobile networks.We implement three self-stabilizing, fault-tolerant services, each builton the prior services: (1) VSA-to-VSA geographic routing, (2) mobileclient location management, and (3) mobile client end-to-end routing. Weuse a greedy version of the classical depth-first search algorithm toroute messages between VSAs in different regions. The mobile clientlocation management service is based on home locations: Each clientidentifier hashes to a set of home locations, regions whose VSAs areperiodically updated with the client\'s location. VSAs maintain thisinformation and answer queries for client locations. Finally, theVSA-to-VSA routing and location management services are used to implementmobile client end-to-end routing.",MIT-CSAIL-TR-2005-052; MIT-LCS-TR-999,20 p.; 27793653 bytes; 1205701 bytes,application/postscript; application/pdf,en_US,,Theory of Distributed Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,452
,"Wies, Thomas; Kuncak, Viktor; Lam, Patrick; Podelski, Andreas; Rinard, Martin",2005-12-22T02:40:34Z,2005-12-22T02:40:34Z,2005-11-03,http://hdl.handle.net/1721.1/30582,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Field Constraint Analysis,"We introduce field constraint analysis, a new  technique for verifying data structure invariants.  A  field constraint for a field is a formula specifying a set of objects  to which the field can point.  Field constraints enable  the application of decidable logics to data structures  which were originally beyond the scope of these logics, by verifying the  backbone of the data structure and then verifying  constraints on fields that cross-cut the backbone in  arbitrary ways.  Previously, such cross-cutting fields  could only be verified when they were uniquely determined by  the backbone, which significantly limited the range of  analyzable data structures.  Our field constraint analysis permits \\emph{non-deterministic} field  constraints on cross-cutting fields, which allows to verify  invariants of data structures such as skip lists.  Non-deterministic  field constraints also enable the verification of invariants between  data structures, yielding an expressive generalization of static  type declarations.  The generality of our field constraints requires new  techniques, which are orthogonal to the traditional use of  structure simulation.  We present one such technique and  prove its soundness.  We have implemented this technique  as part of a symbolic shape analysis deployed in  the context of the Hob system for verifying data structure  consistency.  Using this implementation we were able to  verify data structures that were previously beyond the  reach of similar techniques.",MIT-CSAIL-TR-2005-072; MIT-LCS-TR-1010,23 p.; 22680509 bytes; 928319 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,472
,"Kumar, Ravi; Liben-Nowell, David; Novak, Jasmine; Raghavan, Prabhakar; Tomkins, Andrew",2005-12-22T02:32:28Z,2005-12-22T02:32:28Z,2005-06-03,http://hdl.handle.net/1721.1/30551,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Theoretical Analysis of Geographic Routing in Social Networks,"We introduce a formal model for geographic social networks, and introduce the notion of rank-based friendship, in which the probability that a person v is a friend of a person u is inversely proportional to the number of people w who live closer to u than v does.  We then prove our main theorem, showing that rank-based friendship is a sufficient explanation of the navigability of any geographic social network that adheres to it.",MIT-CSAIL-TR-2005-040; MIT-LCS-TR-990,8 p.; 8282908 bytes; 444233 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,437
,"Marnette, Bruno; Kuncak, Viktor; Rinard, Martin",2005-12-22T02:33:49Z,2005-12-22T02:33:49Z,2005-08-03,http://hdl.handle.net/1721.1/30561,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Algorithms and Complexity for Sets with Cardinality Constraints,"Typestate systems ensure many desirable properties of imperativeprograms, including initialization of object fields and correct use ofstateful library interfaces.  Abstract sets with cardinalityconstraints naturally generalize typestate properties: relationshipsbetween the typestates of objects can be expressed as subset anddisjointness relations on sets, and elements of sets can berepresented as sets of cardinality one.  In addition, sets withcardinality constraints provide a natural language for specifyingoperations and invariants of data structures.Motivated by these program analysis applications, thispaper presents new algorithms and new complexity results forconstraints on sets and their cardinalities.  We studyseveral classes of constraints and demonstrate a trade-offbetween their expressive power and their complexity.Our first result concerns a quantifier-free fragment of BooleanAlgebra with Presburger Arithmetic.  We give a nondeterministicpolynomial-time algorithm for reducing the satisfiability of sets withsymbolic cardinalities to constraints on constant cardinalities, andgive a polynomial-space algorithm for the resulting problem.  The bestpreviously existing algorithm runs in exponential space andnondeterministic exponential time.In a quest for more efficient fragments, we identify severalsubclasses of sets with cardinality constraints whose satisfiabilityis NP-hard.  Finally, we identify a class of constraints that haspolynomial-time satisfiability and entailment problems and can serveas a foundation for efficient program analysis.  We give a system ofrewriting rules for enforcing certain consistency properties of theseconstraints and show how to extract complete information fromconstraints in normal form.  This result implies the soundness andcompleteness of our algorithms.",MIT-CSAIL-TR-2005-050; MIT-LCS-TR-997,19 p.; 33171425 bytes; 1663929 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,449
,"Taycher, Leonid; Shakhnarovich, Gregory; Demirdjian, David; Darrell, Trevor",2005-12-22T02:41:53Z,2005-12-22T02:41:53Z,2005-12-01,http://hdl.handle.net/1721.1/30588,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Conditional Random People: Tracking Humans with CRFs and Grid Filters,"We describe a state-space tracking approach based on a Conditional Random Field(CRF) model, where the observation potentials are \emph{learned} from data. Wefind functions that embed both state and observation into a space wheresimilarity corresponds to $L_1$ distance, and define an observation potentialbased on distance in this space. This potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. We show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse HMM, resulting in real-time system performance. The resulting system isused for human pose tracking in video sequences.",MIT-CSAIL-TR-2005-079; AIM-2005-034,9 p.; 21558399 bytes; 932744 bytes,application/postscript; application/pdf,en_US,AI; articulated tracking; grid filter; conditional random field,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,481
,"Drake, Matthew; Hoffmann, Hank; Rabbah, Rodric; Amarasinghe, Saman",2005-12-22T02:40:17Z,2005-12-22T02:40:17Z,2005-10-22,http://hdl.handle.net/1721.1/30578,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,MPEG-2 in a Stream Programming Language,"Image and video codecs are prevalent in multimedia applications, ranging from embedded systems, to desktop computers, to high-end servers such as HDTV editing consoles. It is not uncommon however that developers create (from scratch) and customize their codec implementations for each of the architecture targets they intend their coders and decoders to run on. This practice is time consuming anderror prone, leading to code that is not malleable or portable.  In this paper we describe an implementation of the MPEG-2 codec using the StreamIt programming language. StreamIt is an architecture-independent stream language that aims to improve programmer productivity, while concomitantly exposing the inherent parallelism and communication topology of the application. We describe why MPEG is a good match forthe streaming programming model, and illustrate the malleability of the implementation using a simple modification to the decoder to support alternate color compression formats. StreamIt allows for modular application development, which also reduces the complexity of the debugging process since stream components can be verifiedindependently. This in turn leads to greater programmer productivity. We implement a fully functional MPEG-2 decoder in StreamIt. The decoder was developed in eight weeks by a single student programmer who did not have any prior experience with MPEG or other video codecs. Many of the MPEG-2 components were subsequently reused to assemble a JPEG codec.",MIT-CSAIL-TR-2005-068; MIT-LCS-TM-652,15 p.; 22871205 bytes; 786081 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,468
,"Richards, Whitman",2005-12-22T02:36:19Z,2005-12-22T02:36:19Z,2005-08-16,http://hdl.handle.net/1721.1/30565,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Collective Choice with Uncertain Domain Moldels,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a Â“top-cycleÂ”.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voterÂ’s view of how alternatives are related. As more and more voterÂ’s preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",MIT-CSAIL-TR-2005-054; AIM-2005-024,18 p.; 17793797 bytes; 614937 bytes,application/postscript; application/pdf,en_US,AI; collective choice; uncertainty; voting; top-cycles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,454
,"Chockler, Gregory; Lynch, Nancy; Mitra, Sayan; Tauber, Joshua",2005-12-22T02:33:34Z,2005-12-22T02:33:34Z,2005-07-22,http://hdl.handle.net/1721.1/30559,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Proving Atomicity: An Assertional Approach,"Atomicity (or linearizability) is a commonly used  consistency criterion for distributed services and objects. Although  atomic object implementations are abundant, proving that algorithms  achieve atomicity has turned out to be a challenging problem. In  this paper, we initiate the study of systematic ways of verifying  distributed implementations of atomic objects, beginning with  read/write objects (registers).  Our general approach is to replace  the existing operational reasoning about events and partial orders  with assertional reasoning about invariants and simulation  relations.  To this end, we define an abstract state machine that  captures the atomicity property and prove correctness of the object  implementations by establishing a simulation mapping between the  implementation and the specification automata. We demonstrate the  generality of our specification by showing that it is implemented by  three different read/write register constructions (the  message-passing register emulation of Attiya, Bar-Noy and Dolev, its  optimized version based on real time, and the shared memory register  construction of Vitanyi and Awerbuch), and by a general atomic  object implementation based on the Lamport\'s replicated state  machine algorithm.",MIT-CSAIL-TR-2005-048; MIT-LCS-TR-995,15 p.; 14829213 bytes; 699536 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,446
,"Muthitacharoen, Athicha; Gilbert, Seth; Morris, Robert",2005-12-22T02:32:54Z,2005-12-22T02:32:54Z,2005-06-15,http://hdl.handle.net/1721.1/30555,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Etna: a Fault-tolerant Algorithm for Atomic Mutable DHT Data,"This paper presents Etna, an algorithm for atomic reads and writes of replicated data stored in a distributed hash table. Etna correctly handles dynamically changing sets of replica hosts, and is optimized for reads, writes, and reconfiguration, in that order.Etna maintains a series of replica configurations as nodes in the system change, using new sets of replicas from the pool supplied by the distributed hash table system. It uses the Paxos protocol to ensure consensus on the members of each new configuration. For simplicity and performance, Etna serializes all reads and writes through a primary during the lifetime of each configuration. As a result, Etna completes read and write operations in only a single round from the primary.Experiments in an environment with high network delaysshow that Etna's read latency is determined by round-tripdelay in the underlying network, while write and reconfiguration latency is determined by the transmission time required to send data to each replica. Etna's write latency is about the same as that of a non-atomic replicating DHT, and Etna's read latency is about twice that of a non-atomic DHT due to Etna assembling a quorum for every read.",MIT-CSAIL-TR-2005-044; MIT-LCS-TR-993,10 p.; 16474627 bytes; 693416 bytes,application/postscript; application/pdf,en_US,,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,440
,"Caponnetto, Andrea; Rosasco, Lorenzo; Vito, Ernesto De; Verri, Alessandro",2005-12-22T02:29:53Z,2005-12-22T02:29:53Z,2005-05-27,http://hdl.handle.net/1721.1/30548,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Empirical Effective Dimension and Optimal Rates for Regularized Least Squares Algorithm,"This paper presents an approach to model selection for regularized least-squares on reproducing kernel Hilbert spaces in the semi-supervised setting.  The role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  The main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  The empirical effective dimension can be computed from independent unlabelled samples.  This makes the approach particularly appealing in the semi-supervised setting.",MIT-CSAIL-TR-2005-036; AIM-2005-019; CBCL-252,14 p.; 11158573 bytes; 526018 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; effective dimension; semi-supervised learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,433
,"Taylor, Christopher J.",2005-12-22T02:30:00Z,2005-12-22T02:30:00Z,2005-05-31,http://hdl.handle.net/1721.1/30549,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Simultaneous Localization and Tracking in Wireless Ad-hoc Sensor Networks,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",MIT-CSAIL-TR-2005-037; AITR-2005-003,69 p.; 81859537 bytes; 3510560 bytes,application/postscript; application/pdf,en_US,AI; Localization; Target Tracking; Sensor Network; Calibration,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,434
,"Taylor, Christopher; Rahimi, Ali; Bachrach, Jonathan; Shrobe, Howard",2005-12-22T02:28:41Z,2005-12-22T02:28:41Z,2005-04-26,http://hdl.handle.net/1721.1/30541,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Simultaneous Localization, Calibration, and Tracking in an ad Hoc Sensor Network","We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",MIT-CSAIL-TR-2005-029; AIM-2005-016,18 p.; 40655574 bytes; 2128443 bytes,application/postscript; application/pdf,en_US,AI; sensor network; localization; bayesian filter; extended kalman filter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,425
,"Das, Sanmay",2005-12-22T02:37:21Z,2005-12-22T02:37:21Z,2005-10-07,http://hdl.handle.net/1721.1/30573,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning to Trade with Insider Information,"This paper introduces algorithms for learning how to trade usinginsider (superior) information in Kyle's model of financial markets.Prior results in finance theory relied on the insider having perfectknowledge of the structure and parameters of the market. I show herethat it is possible to learn the equilibrium trading strategy whenits form is known even without knowledge of the parameters governingtrading in the model. However, the rate of convergence toequilibrium is slow, and an approximate algorithm that does notconverge to the equilibrium strategy achieves better utility whenthe horizon is limited. I analyze this approximate algorithm fromthe perspective of reinforcement learning and discuss the importanceof domain knowledge in designing a successful learning algorithm.",MIT-CSAIL-TR-2005-063; AIM-2005-028; CBCL-255,15 p.; 16523384 bytes; 613212 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,463
,"Su, Sara L.; Durand, Fredo; Agrawala, Maneesh",2005-12-22T02:28:16Z,2005-12-22T02:28:16Z,2005-04-12,http://hdl.handle.net/1721.1/30537,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,De-Emphasis of Distracting Image Regions Using Texture Power Maps,"A major obstacle in photography is the presence of distracting elements that pull attention away from the main subject and clutter the composition. In this article, we present a new image-processing technique that reduces the salience of distracting regions. It is motivated by computational models of attention that predict that texture variation influences bottom-up attention mechanisms. Our method reduces the spatial variation of texture using power maps, high-order features describing local frequency content in an image. We show how modification of power maps results in  powerful image de-emphasis. We validate our results using a user search experiment and eye tracking data.",MIT-CSAIL-TR-2005-025; MIT-LCS-TR-987,12 p.; 24844567 bytes; 1742248 bytes,application/postscript; application/pdf,en_US,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,421
,"Wu, Jia Jane",2005-12-22T02:29:44Z,2005-12-22T02:29:44Z,2005-05-25,http://hdl.handle.net/1721.1/30547,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Comparing Visual Features for Morphing Based Recognition,"This thesis presents a method of object classification using the idea of deformable shape matching.  Three types of visual features, geometric blur, C1 and SIFT, are used to generate feature descriptors.  These feature descriptors are then used to find point correspondences between pairs of images.  Various morphable models are created by small subsets of these correspondences using thin-plate spline.  Given these morphs, a simple algorithm, least median of squares (LMEDS), is used to find the best morph.  A scoring metric, using both LMEDS and distance transform, is used to classify test images based on a nearest neighbor algorithm.  We perform the experiments on the Caltech 101 dataset [5].  To ease computation, for each test image, a shortlist is created containing 10 of the most likely candidates.  We were unable to duplicate the performance of [1] in the shortlist stage because we did not use hand-segmentation to extract objects for our training images.  However, our gain from the shortlist to correspondence stage is comparable to theirs.  In our experiments, we improved from 21% to 28% (gain of 33%), while [1] improved from 41% to 48% (gain of 17%).  We find that using a non-shape based approach, C2 [14], the overall classification rate of 33.61% is higher than all of the shaped based methods tested in our experiments.",MIT-CSAIL-TR-2005-035; AITR-2005-002; CBCL-251,42 p.; 39773758 bytes; 1459526 bytes,application/postscript; application/pdf,en_US,AI; object recognition; shape-based,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,432
,"Ajmani, Sameer; Liskov, Barbara; Shrira, Liuba; Curtis, Dorothy",2005-12-22T02:37:15Z,2005-12-22T02:37:15Z,2005-10-06,http://hdl.handle.net/1721.1/30572,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems,"Upgrading the software of long-lived, highly-available distributedsystems is difficult.  It is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  Instead, upgrades must happengradually, and there may be long periods of time when different nodesrun different software versions and need to communicate usingincompatible protocols.  We present a methodology and infrastructurethat make it possible to upgrade distributed systems automatically whilelimiting service disruption.  We introduce new ways to reason aboutcorrectness in a multi-version system. We also describe a prototypeimplementation that supports automatic upgrades with modest overhead.",MIT-CSAIL-TR-2005-062; MIT-LCS-TR-1005,14 p.; 26794595 bytes; 1207166 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,462
,"Abbott, Tim; Kane, Daniel; Valiant, Paul",2005-12-22T02:24:28Z,2005-12-22T02:24:28Z,2005-02-08,http://hdl.handle.net/1721.1/30523,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Complexity of finding Nash equilibria in 0-1 bimatrix games,We exhibit a polynomial reduction from the problem of finding a Nashequilibrium of a bimatrix game with rational coefficients to the problemof finding a Nash equilibrium of a bimatrix game with 0-1 coefficients.,MIT-CSAIL-TR-2005-010; MIT-LCS-TM-648,6 p.; 6499249 bytes; 336493 bytes,application/postscript; application/pdf,en_US,,Cryptography and Information Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,404
,"Liang, Percy; Srebro, Nati",2005-12-22T02:20:16Z,2005-12-22T02:20:16Z,2005-01-03,http://hdl.handle.net/1721.1/30514,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Dynamic Data Structure for Checking Hyperacyclicity,"We present a dynamic data structure that keeps track of an acyclic hypergraph (equivalently, a triangulated graph) and enables verifying that adding a candidate hyperedge (clique) will not break the acyclicity of the augmented hypergraph. This is a generalization of the use of Tarjan's Union-Find data structure for maintaining acyclicity when augmenting forests, and the amortized time per operation has a similar almost-constant dependence on the size of the hypergraph. Such a data structure is useful when augmenting acyclic hypergraphs, e.g.\~in order to greedily construct a high-weight acyclic hypergraph. In designing this data structure, we introduce a hierarchical decomposition of acyclic hypergraphs that aid in understanding {\em hyper-connectivity}, and introduce a novel concept of a {\em hypercycle} which is excluded from acyclic hypergraphs.",MIT-CSAIL-TR-2005-001; MIT-LCS-TR-977,12 p.; 17306554 bytes; 656257 bytes,application/postscript; application/pdf,en_US,,Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,396
,"Serre, T.; Kouh, M.; Cadieu, C.; Knoblich, U.; Kreiman, G.; Poggio, T.",2007-03-12T16:41:47Z,2007-03-12T16:41:47Z,2005-12-19,http://hdl.handle.net/1721.1/36407,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Theory of Object Recognition: Computations and Circuits in the Feedforward Path of the Ventral Stream in Primate Visual Cortex,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",MIT-CSAIL-TR-2005-082; AIM-2005-36; CBCL-259,130 p.,,,AI; object recognition; standard model; theory; visual cortex; ventral stream; hmax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,483
,"Zhang, MIchael; Asanovic, Krste",2005-12-22T02:37:29Z,2005-12-22T02:37:29Z,2005-10-10,http://hdl.handle.net/1721.1/30574,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Victim Migration: Dynamically Adapting Between Private and Shared CMP Caches,"Future CMPs will have more cores and greater onchip cache capacity. The on-chip cache can either be divided into separate private L2 caches for each core, or treated as a large shared L2 cache. Private caches provide low hit latency but low capacity, while shared caches have higher hit latencies but greater capacity. Victim replication was previously introduced as a way of reducing the average hit latency of a shared cache by allowing a processor to make a replica of a primary cache victim in its local slice of the global L2 cache. Although victim replication performs well on multithreaded and single-threaded codes, it performs worse than the private scheme for multiprogrammed workloads where there is little sharing between the different programs running at the same time. In this paper, we propose victim migration, which improves on victim replication by adding an additional set of migration tags on each node which are used to implement an exclusive cache policy for replicas. When a replica has been created on a remote node, it is not also cached on the home node, but only recorded in the migration tags. This frees up space on the home node to store shared global lines or replicas for the local processor. We show that victim migration performs better than private, shared, and victim replication schemes across a range of single threaded, multithreaded, and multiprogrammed workloads, while using less area than a private cache design. Victim migration provides a reduction in average memory access latency of up to 10% over victim replication.",MIT-CSAIL-TR-2005-064; MIT-LCS-TR-1006,17 p.; 18487877 bytes; 796263 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,464
,"Hsu, Eugene; Pulli, Kari; Popovic, Jovan",2008-08-28T18:45:44Z,2008-08-28T18:45:44Z,2005-08-01,http://hdl.handle.net/1721.1/42004,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Style Translation for Human Motion (Supplemental Material),"Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel correspondence algorithm to align motions, and a linear time-invariant model to represent stylistic differences. Once the model is estimated with system identification, our system is capable of translating streaming input with simple linear operations at each frame.",,,,,,,,,,,"Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel corres",Jovan Popovic; Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,2005,447
,"Grauman, Kristen; Darrell, Trevor",2005-12-19T23:36:26Z,2005-12-19T23:36:26Z,2005-03-17,http://hdl.handle.net/1721.1/30420,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pyramid Match Kernels: Discriminative Classification with Sets of Image Features,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",MIT-CSAIL-TR-2005-017; AIM-2005-007,12 p.; 68553886 bytes; 13808123 bytes,application/postscript; application/pdf,en_US,AI; kernel; unordered sets; correspondence; object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,413
,"Zeng, Gang; Paris, Sylvain; Quan, Long; Sillion, Francois",2005-12-22T02:41:10Z,2005-12-22T02:41:10Z,2005-11-18,http://hdl.handle.net/1721.1/30586,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Accurate and Scalable Surface Representation and Reconstruction from Images,"We introduce a new surface representation, the patchwork, to extend the problem of surface reconstruction from multiple images. A patchwork is the combination of several patches that are built one by one. This design potentially allows the reconstruction of an object of arbitrarily large dimensions while preserving a fine level of detail. We formally demonstrate that this strategy leads to a spatial complexity independent of the dimensions of the reconstructed object, and to a time complexity linear with respect to the object area. The former property ensures that we never run out of storage (memory) and the latter means that reconstructing an object can be done in a reasonable amount of time. In addition, we show that the patchwork representation handles equivalently open and closed surfaces whereas most of the existing approaches are limited to a specific scenario (open or closed surface but not both).Most of the existing optimization techniques can be cast into this framework. To illustrate the possibilities offered by this approach, we propose two applications that expose how it dramatically extends a recent accurate graph-cut technique. We first revisit the popular carving techniques. This results in a well-posed reconstruction problem that still enjoys the tractability of voxel space. We also show how we can advantageously combine several image-driven criteria to achieve a finely detailed geometry by surface propagation. The above properties of the patchwork representation and reconstruction are extensively demonstrated on real image sequences.",MIT-CSAIL-TR-2005-076; MIT-LCS-TR-1011,35 p.; 126753884 bytes; 4493080 bytes,application/postscript; application/pdf,en_US,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,478
,"Nahnsen, Thade; Uzuner, Ozlem; Katz, Boris",2005-12-22T02:29:37Z,2005-12-22T02:29:37Z,2005-05-19,http://hdl.handle.net/1721.1/30546,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Lexical Chains and Sliding Locality Windows in Content-based Text Similarity Detection,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",MIT-CSAIL-TR-2005-034; AIM-2005-017,9 p.; 17827888 bytes; 7011726 bytes,application/postscript; application/pdf,en_US,AI; Natural Language Processing; N-grams; Text Similarity; Lexical Chains,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,430
,"Vito, Ernesto De; Caponnetto, Andrea",2005-12-22T02:28:54Z,2005-12-22T02:28:54Z,2005-05-16,http://hdl.handle.net/1721.1/30543,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Risk Bounds for Regularized Least-squares Algorithm with Operator-valued kernels,We show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel Hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  We first briefly introduce central concepts on operator-valued kernels.  Then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,MIT-CSAIL-TR-2005-031; AIM-2005-015; CBCL-249,17 p.; 12090406 bytes; 642646 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; reproducing kernel Hilbert space; effective dimension,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,427
Michael Ernst,"Artzi, Shay; Ernst, Michael D.; Glasser, David; Kiezun, Adam",2006-09-18T17:55:18Z,2006-09-18T17:55:18Z,2006-09-17,http://hdl.handle.net/1721.1/33968,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combined static and dynamic mutability analysis,"Knowing which method parameters may be mutated during a method'sexecution is useful for many software engineering tasks.  We presentan approach to discovering parameter immutability, in which severallightweight, scalable analyses are combined in stages, with each stagerefining the overall result.  The resulting analysis is scalable andcombines the strengths of its component analyses.  As one of thecomponent analyses, we present a novel, dynamic mutability analysisand show how its results can be improved by random input generation.Experimental results on programs of up to 185 kLOC demonstrate that,compared to previous approaches, our approach increases both scalabilityand overall accuracy.",MIT-CSAIL-TR-2006-065,10 p.; 176363 bytes; 1038435 bytes,application/pdf; application/postscript,en_US,immutability; mutability; side effect analysis; purity; pointer analysis; dynamic analysis; mutation,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,547
Brian Williams,"Leaute, Thomas",2006-04-28T18:22:21Z,2006-04-28T18:22:21Z,2006-04-28,http://hdl.handle.net/1721.1/32537,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Coordinating Agile Systems through the Model-based Execution of Temporal Plans,"Agile autonomous systems are emerging, such as unmanned aerial vehicles (UAVs), that must robustly perform tightly coordinated time-critical missions; for example, military surveillance or search-and-rescue scenarios. In the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness, in the context of space probes and planetary rovers, modeled as discrete systems. We address the challenge of extending plan execution to systems with continuous dynamics, such as air vehicles and robot manipulators, and that are controlled indirectly through the setting of continuous state variables.Systems with continuous dynamics are more challenging than discrete systems, because they require continuous, low-level control, and cannot be controlled by issuing simple sequences of discrete commands. Hence, manually controlling these systems (or plants) at a low level can become very costly, in terms of the number of human operators necessary to operate the plant. For example, in the case of a fleet of UAVs performing a search-and-rescue scenario, the traditional approach to controlling the UAVs involves providing series of close waypoints for each aircraft, which incurs a high workload for the human operators, when the fleet consists of a large number of vehicles.Our solution is a novel, model-based executive, called Sulu, that takes as input a qualitative state plan, specifying the desired evolution of the state of the system. This approach elevates the interaction between the human operator and the plant, to a more abstract level where the operator is able to Â“coachÂ” the plant by qualitatively specifying the tasks, or activities, the plant must perform. These activities are described in a qualitative manner, because they specify regions in the plantÂ’s state space in which the plant must be at a certain point in time. Time constraints are also described qualitatively, in the form of flexible temporal constraints between activities in the state plan. The design of low-level control inputs in order to meet this abstract goal specification is then delegated to the autonomous controller, hence decreasing the workload per human operator. This approach also provides robustness to the executive, by giving it room to adapt to disturbances and unforeseen events, while satisfying the qualitative constraints on the plant state, specified in the qualitative state plan.Sulu reasons on a model of the plant in order to dynamically generate near-optimal control sequences to fulfill the qualitative state plan. To achieve optimality and safety, Sulu plans into the future, framing the problem as a disjunctive linear programming problem. To achieve robustness to disturbances and maintain tractability, planning is folded within a receding horizon, continuous planning and execution framework. The key to performance is a problem reduction method based on constraint pruning. We benchmark performance using multi-UAV firefighting scenarios on a real-time, hardware-in-the-loop testbed.",MIT-CSAIL-TR-2006-029,155 p.; 41524644 bytes; 17696521 bytes,application/postscript; application/pdf,en_US,Model-based Programming; Qualitative Reasoning; Temporal Reasoning; Continuous Scheduling; Path Planning; Model Predictive Control,Model-based Embedded and Robotic Systems,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,511
Hari Balakrishnan,"Walfish, Michael; Zamfirescu, J.D.; Balakrishnan, Hari; Karger, David; Shenker, Scott",2006-05-05T19:42:05Z,2006-05-05T19:42:05Z,2006-04-29,http://hdl.handle.net/1721.1/32542,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Supplement to ""Distributed Quota Enforcement for Spam Control""","This report is a supplement to our paper ""Distributed Quota Enforcement forSpam Control"" (NSDI 2006). We assume here that the reader has readthe main paper. In this report, we first analyze the enforcer nodes'key-value maps and then analyze two of the experiments from the main paper.",MIT-CSAIL-TR-2006-033,6 p.; 611915 bytes; 214834 bytes,application/postscript; application/pdf,en_US,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,515
Martin Rinard,"Bouillaguet, Charles; Kuncak, Viktor; Wies, Thomas; Zee, Karen; Rinard, Martin",2006-11-09T15:26:55Z,2006-11-09T15:26:55Z,2006-11-09,http://hdl.handle.net/1721.1/34874,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Using First-Order Theorem Provers in the Jahob Data Structure Verification System,"This paper presents our integration of efficient  resolution-based theorem provers into the Jahob data  structure verification system.  Our experimental results  show that this approach enables Jahob to automatically  verify the correctness of a range of complex dynamically  instantiable data structures, including data structures  such as hash tables and search trees, without the need for  interactive theorem proving or techniques tailored to  individual data structures.  Our primary technical results include: (1) a translation  from higher-order logic to first-order logic that enables  the application of resolution-based theorem provers and  (2) a proof that eliminating type (sort) information in  formulas is both sound and complete, even in the presence  of a generic equality operator.  Our  experimental results show that the elimination of   type information dramatically decreases the time required  to prove the resulting formulas.  These techniques enabled us to verify complex correctness  properties of Java programs such as a mutable set  implemented as an imperative linked list, a finite map  implemented as a functional ordered tree, a hash table  with a mutable array, and a simple library system example  that uses these container data structures.  Our system  verifies (in a matter of minutes) that data structure  operations correctly update the finite map, that they  preserve data structure invariants (such as ordering of  elements, membership in appropriate hash table buckets, or  relationships between sets and relations), and that there  are no run-time errors such as null dereferences or array  out of bounds accesses.",MIT-CSAIL-TR-2006-072,32 p.; 397902 bytes; 1759318 bytes,application/pdf; application/postscript,en_US,program verification; shape analysis; multisorted logic,Computer Architecture,,,,,"Short version to appear in VMCAI'07, Nice, January 2007",,,,,,,,,,,,,,,,,,,,,,,,2006,553
Dina Katabi,"Teow, Loo Nin; Katabi, Dina",2006-07-12T17:06:52Z,2006-07-12T17:06:52Z,2006-07-04,http://hdl.handle.net/1721.1/33234,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Iterative Collaborative Ranking of  Customers and Providers,"This paper introduces a new application: predicting the Internet provider-customer market. We cast the problem in the collaborative filtering framework, where we use current and past customer-provider relationships to compute for each Internet customer a ranking of potential future service providers. Furthermore, for each Internet service provider (ISP), we rank potential future customers. We develop a novel iterative ranking algorithm that draws inspiration from several sources, including collaborative filtering, webpage ranking, and kernel methods. Further analysis of our algorithm shows that it can be formulated in terms of an affine eigenvalue problem. Experiments on the actual Internet customer-provider data show promising results.",MIT-CSAIL-TR-2006-050,9 p.; 1308662 bytes; 269609 bytes,application/postscript; application/pdf,en_US,Network Analysis; Autonomous Systems,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,530
Tomaso Poggio,"Das, Sanmay",2006-07-13T11:26:51Z,2006-07-13T11:26:51Z,2006-07-12,http://hdl.handle.net/1721.1/33235,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Dealers, Insiders and Bandits: Learning and its Effects on Market Outcomes","This thesis seeks to contribute to the understanding of markets populated by boundedly rational agents who learn from experience. Bounded rationality and learning have both been the focus of much research in computer science, economics and finance theory. However, we are at a critical stage in defining the direction of future research in these areas. It is now clear that realistic learning problems faced by agents in market environments are often too hard to solve in a classically rational fashion. At the same time, the greatly increased computational power available today allows us to develop and analyze richer market models and to evaluate different learning procedures and algorithms within these models. The danger is that the ease with which complex markets can be simulated could lead to a plethora of models that attempt to explain every known fact about different markets. The first two chapters of this thesis define a principled approach to studying learning in rich models of market environments, and the rest of the thesis provides a proof of concept by demonstrating the applicability of this approach in modeling settings drawn from two different broad domains, financial market microstructure and search theory. In the domain of market microstructure, this thesis extends two important models from the theoretical finance literature. The third chapter introduces an algorithm for setting prices in dealer markets based on the model of Glosten and Milgrom (1985), and produces predictions about the behavior of prices in securities markets. In some cases, these results confirm economic intuitions in a significantly more complex setting (like the existence of a local profit maximum for a monopolistic market-maker) and in others they can be used to provide quantitative guesses for variables such as rates of convergence to efficient market conditions following price jumps that provide insider information. The fourth chapter studies the problem faced by a trader with insider information in KyleÂ’s (1985) model. I show how the insider trading problem can be usefully analyzed from the perspective of reinforcement learning when some important market parameters are unknown, and that the equilibrium behavior of an insider who knows these parameters can be learned by one who does not, but also that the time scale of convergence to the equilibrium behavior may be impractical, and agents with limited time horizons may be better off using approximate algorithms that do not converge to equilibrium behavior. The fifth and sixth chapters relate to search problems. Chapter 5 introduces models for a class of problems in which there is a search Â“seasonÂ” prior to hiring or matching, like academic job markets. It solves for expected values in many cases, and studies the difference between a Â“high informationÂ” process where applicants are immediately told when they have been rejected and a Â“low informationÂ” process where employers do not send any signal when they reject an applicant. The most important intuition to emerge from the results is that the relative benefit of the high information process is much greater when applicants do not know their own Â“attractiveness,Â” which implies that search markets might be able to eliminate inefficiencies effectively by providing good information, and we do not always have to think about redesigning markets as a whole. Chapter 6 studies two-sided search explicitly and introduces a new class of multi-agent learning problems, two-sided bandit problems, that capture the learning and decision problems of agents in matching markets in which agents must learn their preferences. It also empirically studies outcomes under different periodwise matching mechanisms and shows that some basic intuitions about the asymptotic stability of matchings are preserved in the model. For example, when agents are matched in each period using the Gale-Shapley algorithm, asymptotic outcomes are always stable, while a matching mechanism that induces a stopping problem for some agents leads to the lowest probabilities of stability. By contributing to the state of the art in modeling different domains using computational techniques, this thesis demonstrates the success of the approach to modeling complex economic and social systems that is prescribed in the first two chapters.",MIT-CSAIL-TR-2006-051; CBCL-262,149 p.; 860631 bytes; 5532424 bytes,application/pdf; application/postscript,en_US,,Center for Biological and Computational Learning (CBCL),,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,531
Peter Szolovits,"Hug, Caleb W.",2006-08-31T14:29:11Z,2006-08-31T14:29:11Z,2006-08-30,http://hdl.handle.net/1721.1/33957,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Predicting the Risk and Trajectory of Intensive Care Patients Using Survival Models,"Using artificial intelligence to assist physicians in patient care has received sustained interest over the past several decades.  Recently, with automated systems at most bedsides, the amount of patient information collected continues to increase, providing specific impetus for intelligent systems that can interpret this information. In fact, the large set of sensors and test results, often measured repeatedly over long periods of time, make it challenging for caregivers to quickly utilize all of the data for optimal patient treatment.This research focuses on predicting the survival of ICU patients throughout their stay.  Unlike traditional static mortality models, this survival prediction is explored as an indicator of patient state and trajectory.  Using survival analysis techniques and machine learning, models are constructed that predict individual patient survival probabilities at fixed intervals in the future.  These models seek to help physicians interpret the large amount of data available in order to provide optimal patient care.We find that the survival predictions from our models are comparable to survival predictions using the SAPS score, but are available throughout the patient's ICU course instead of only at 24 hours after admission.  Additionally, we demonstrate effective prediction of patient mortality over fixed windows in the future.",MIT-CSAIL-TR-2006-055,126 p.; 3075332 bytes; 103809024 bytes,application/pdf; application/postscript,en_US,Intelligent Monitoring,Clinical Decision-Making,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,537
Sam Madden,"Abadi, Daniel J.; Myers, Daniel S.; DeWitt, David J.; Madden, Samuel R.",2006-11-28T19:34:39Z,2006-11-28T19:34:39Z,2006-11-27,http://hdl.handle.net/1721.1/34929,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Materialization Strategies in a Column-Oriented DBMS,"There has been renewed interest in column-oriented database architectures in recent years. For read-mostly query workloads such as those found in data warehouse and decision support applications, ``column-stores'' have been shown to perform particularly well relative to ``row-stores.'' In order for column-stores to be readily adopted as a replacement for row-stores, however, they must present the same interface to client applications as do row stores, which implies that they must output row-store-style tuples.Thus, the input columns stored on disk must be converted to rows at some point in the query plan, but the optimal point at which to do the conversion is not obvious. This problem can be considered as the opposite of the projection problem in row-store systems: while row-stores need to determine where in query plans to place projection operators to make tuples narrower, column-stores need to determine when to combine single-column projections into wider tuples. This paper describes a variety of strategies for tuple construction and intermediate result representations and provides a systematic evaluation of these strategies.",MIT-CSAIL-TR-2006-078,13 p.; 952582 bytes; 2497163 bytes,application/pdf; application/postscript,en_US,,Database,,,,,Extension of Publication (of the same title) in the Proceedings of ICDE 2007,,,,,,,,,,,,,,,,,,,,,,,,2006,558
Eric Grimson,"Dalley, Gerald; Izo, Tomas",2006-06-12T18:36:58Z,2006-06-12T18:36:58Z,2006-06-12,http://hdl.handle.net/1721.1/32999,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Schematic Querying of Large Tracking Databases,"In dealing with long-term tracking databases withwide-area coverage, an important problem is in formulating anintuitive and fast query system for analysis. In such a querysystem, a user who is not a computer vision research should beable to readily specify a novel query to the system and obtainthe desired results. Furthermore, these queries should be able tonot only search out individual actors (e.g. ""find all white cars"")but also find interactions amongst multiple actors (e.g. ""find alldrag racing activities in the city""). Informally, we have foundthat people often use sketches when describing activities andinteractions. In this paper, we demonstrate a preliminary systemthat automatically interprets schematic drawings of activities.The system transforms the schematics into executable code thatsearches a tracking database. Through our query optimization,these queries tend to take orders of magnitude less time to executethan equivalent queries running on a partially-optimized SQLdatabase.",MIT-CSAIL-TR-2006-043,7 p.; 391836 bytes; 7529339 bytes,application/pdf; application/postscript,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,523
Michael Ernst,"McCamant, Stephen; Ernst, Michael D.",2006-11-17T11:12:32Z,2006-11-17T11:12:32Z,2006-11-17,http://hdl.handle.net/1721.1/34892,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Quantitative Information-Flow Tracking for C and Related Languages,"We present a new approach for tracking programs' use of data througharbitrary calculations, to determine how much information about secretinputs is revealed by public outputs.  Using a fine-grained dynamicbit-tracking analysis, the technique measures the information revealedduring a particular execution.  The technique accounts for indirectflows, e.g. via branches and pointer operations.  Two kinds ofuntrusted annotation improve the precision of the analysis.  Animplementation of the technique based on dynamic binary translation isdemonstrated on real C, C++, and Objective C programs of up to half amillion lines of code.  In case studies, the tool checked multiplesecurity policies, including one that was violated by a previouslyunknown bug.",MIT-CSAIL-TR-2006-076,18 p.; 450616 bytes; 1216950 bytes,application/pdf; application/postscript,en_US,Confidentiality; Privacy; Information disclosure; Tainting; Implicit flows; Valgrind; Memcheck; OpenSSH,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,557
Gerald Sussman,"Werfel, Justin",2006-08-14T12:49:01Z,2006-08-14T12:49:01Z,2006-05-12,http://hdl.handle.net/1721.1/33791,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Anthills Built to Order: Automating Construction with Artificial Swarms,"Social insects build large, complex structures, which emerge through the collective actions of many simple agents acting with no centralized control or preplanning.  These natural systems motivate investigating the use of artificial swarms to automate construction or fabrication.  The goal is to be able to take an unspecified number of simple robots and a supply of building material, give the system a high-level specification for any arbitrary structure desired, and have a guarantee that it will produce that structure without further intervention.In this thesis I describe such a distributed system for automating construction, in which autonomous mobile robots collectively build user-specified structures from square building blocks.  The approach preserves many desirable features of the natural systems, such as considerable parallelism and robustness to factorslike robot loss and variable order or timing of actions.  Further, unlike insect colonies, it can build particular desired structures according to a high-level design provided by the user.Robots in this system act without explicit communication or cooperation, instead using the partially completed structure to coordinate their actions.  This mechanism is analogous to that of stigmergy used by social insects, in which insects take actions that affect the environment, and the environmental state influences further actions.  I introduce a framework of ""extended stigmergy"" in which building blocks are allowed to store, process or communicate information.  Increasing the capabilities of the building material (rather than of the robots) in this way increases the availability of nonlocal structure information.  Benefits include significant improvements in construction speed and in ability to take advantage of the parallelism of the swarm.This dissertation describes system design and control rules for decentralized teams of robots that provably build arbitrary solid structures in two dimensions.  I present a hardware prototype, and discuss extensions to more general structures, including those built with multiple block types and in three dimensions.",MIT-CSAIL-TR-2006-052,116 p.; 3106746 bytes; 130023424 bytes,application/pdf; application/postscript,en_US,,Mathematics and Computation,,,,PhD thesis,"Ph.D. thesis, Massachusetts Institute of Technology",,,,,,,,,,,,,,,,,,,,,,,,2006,518
Srini Devadas,"Sarmenta, Luis F. G.; van Dijk, Marten; O'Donnell, Charles W.; Rhodes, Jonathan; Devadas, Srinivas",2006-09-11T22:20:24Z,2006-09-11T22:20:24Z,2006-09-11,http://hdl.handle.net/1721.1/33966,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Virtual Monotonic Counters and Count-Limited Objects using a TPM without a Trusted OS (Extended Version),"A trusted monotonic counter is a valuable primitive thatenables a wide variety of highly scalable offlineand decentralized applications that would otherwise be prone to replay attacks, including offline payment, e-wallets, virtual trusted storage, and digital rights management (DRM).In this paper, we show how one can implement a very large number of virtual monotonic counters on an untrusted machine with a Trusted Platform Module (TPM) or similar device, without relying on a trusted OS.  We first present a log-based scheme that can be implemented with the current version of the TPM (1.2) and used incertain applications.We then show how the addition of a few simple features tothe TPM makes it possible to implement a hash-tree-based schemethat not only offers improved performance and scalability compared to the log-based scheme, but also makes it possible to implement count-limited objects (or ``clobs'' for short) -- i.e., encrypted keys, data, and other objectsthat can only be used when an associated virtual monotonic counter is within a certain range.Such count-limited objects include n-time use keys, n-out-of-m data blobs,n-copy migratable objects, and other variants, which have many potential uses in digital rights management (DRM), digital cash, digital voting, itinerant computing,and other application areas.",MIT-CSAIL-TR-2006-064,18 p.; 430350 bytes; 694048 bytes,application/pdf; application/postscript,en_US,trusted storage; key delegation; stored-value; e-wallet; smartcard; memory integrity checking; certified execution,Computation Structures,,,,,A shorter version of this paper will appear in the 1st ACM CCS Workshop on Scalable Trusted Computing (STC'06).,,,,,,,,,,,,,,,,,,,,,,,,2006,546
Daniel Jackson,"Edwards, Jonathan",2006-05-23T11:16:56Z,2006-05-23T11:16:56Z,2006-05-22,http://hdl.handle.net/1721.1/32980,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,First Class Copy & Paste,"The Subtext project seeks to make programming fundamentally easier by altering the nature of programming languages and tools. This paper defines an operational semantics for an essential subset of the Subtext language. It also presents a fresh approach to the problems of mutable state, I/O, and concurrency.Inclusions reify copy & paste edits into persistent relationships that propagate changes from their source into their destination. Inclusions formulate a programming language in which there is no distinction between a programÂ’s representation and its execution. Like spreadsheets, programs are live executions within a persistent runtime, and programming is direct manipulation of these executions via a graphical user interface. There is no need to encode programs into source text.Mutation of state is effected by the computation of hypothetical recursive variants of the state, which can then be lifted into new versions of the state. Transactional concurrency is based upon queued single-threaded execution. Speculative execution of queued hypotheticals provides concurrency as a semantically transparent implementation optimization.",MIT-CSAIL-TR-2006-037,20 p.; 610092 bytes; 11137489 bytes,application/pdf; application/postscript,en_US,prototypes; copy and paste; modularity; reactivity; transactions,Software Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,520
Barbara Liskov,"Leong, Ben",2006-06-14T14:57:35Z,2006-06-14T14:57:35Z,2006-06-14,http://hdl.handle.net/1721.1/33000,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New Techniques for Geographic Routing,"As wireless sensor networks continue to grow in size, we are facedwith the prospect of emerging wireless networks with hundreds orthousands of nodes. Geographic routing algorithms are a promisingalternative to tradition ad hoc routing algorithms in this new domainfor point-to-point routing, but deployments of such algorithms arecurrently uncommon because of some practical difficulties.This dissertation explores techniques that address two major issues inthe deployment of geographic routing algorithms: (i) the costsassociated with distributed planarization and (ii) the unavailabilityof location information.  We present and evaluate two new algorithmsfor geographic routing: Greedy Distributed Spanning Tree Routing(GDSTR) and Greedy Embedding Spring Coordinates (GSpring).Unlike previous geographic routing algorithms which require theplanarization of the network connectivity graph, GDSTR switches torouting on a spanning tree instead of a planar graph when packets endup at dead ends during greedy forwarding. To choose a direction on thetree that is most likely to make progress towards the destination,each GDSTR node maintains a summary of the area covered by the subtreebelow each of its tree neighbors using convex hulls. This distributeddata structure is called a hull tree. GDSTR not only requires an orderof magnitude less bandwidth to maintain these hull trees than CLDP,the only distributed planarization algorithm that is known to workwith practical radio networks, it often achieves better routingperformance than previous planarization-based geographic routingalgorithms.GSpring is a new virtual coordinate assignment algorithm that derivesgood coordinates for geographic routing when location information isnot available. Starting from a set of initial coordinates for a set ofelected perimeter nodes, GSpring uses a modified spring relaxationalgorithm to incrementally adjust virtual coordinates to increase theconvexity of voids in the virtual routing topology. This reduces theprobability that packets will end up in dead ends during greedyforwarding, and improves the routing performance of existinggeographic routing algorithms.The coordinates derived by GSpring yield comparable routingperformance to that for actual physical coordinates and significantlybetter performance than that for NoGeo, the best existing algorithmfor deriving virtual coordinates for geographic routing. Furthermore,GSpring is the first known algorithm that is able to derivecoordinates that achieve better geographic routing performance thanactual physical coordinates for networks with obstacles.",MIT-CSAIL-TR-2006-044,150 p.; 3957081 bytes; 32739307 bytes,application/pdf; application/postscript,en_US,,Programming Methodology,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,524
Michael Ernst,"Tschantz, Matthew S.",2006-09-07T18:51:29Z,2006-09-07T18:51:29Z,2006-09-05,http://hdl.handle.net/1721.1/33963,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Javari: Adding Reference Immutability to Java,"This paper describes a programming language, Javari, that is capable of expressing and enforcing immutability constraints.  The specific constraint expressed is that the abstract state of the object to which an immutable reference refers cannot be modified using that reference.  The abstract state is (part of) the transitively reachable state: that is, the state of the object and all state reachable from it by following references.  The type system permits explicitly excluding fields from the abstract state of an object.  For a statically type-safe language, the type system guarantees reference immutability.The type system is distinguishes the notions of assignability and mutability; integrates with Java's generic types and with multi-dimensional arrays; provides a mutability polymorphism approach to avoiding code duplication; and has type-safe support for reflection and serialization.  This paper describes a core calculus including formal type rules for the language.Additionally, this paper describes a type inference algorithm that can be used convert existing Java programs to Javari.  Experimental results from a prototype implementation of the algorithm are presented.",MIT-CSAIL-TR-2006-059,133 p.; 1217863 bytes; 4225617 bytes,application/pdf; application/postscript,en_US,assignable; languages; mutable; readonly; type system; verification,Program Analysis,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,542
Brian Williams,"Blackmore, Lars; Williams, Brian",2006-02-28T22:36:51Z,2006-02-28T22:36:51Z,2006-02-28,http://hdl.handle.net/1721.1/31219,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Finite Horizon Control Design for Optimal Discrimination between Several Models,"Multiple-Model fault detection is a powerful method for detecting changes, such as faults, in dynamic systems. In many cases, the ability of such a detection scheme to distinguish between possible models for the system dynamics depends critically on the control inputs applied to the system. Prior work has therefore aimed to design control inputs in order to improve fault detection. We previously developed a new method that uses constrained finite horizon control design to create control inputs that minimize an upper bound on the probability of model selection error. This method is limited, however, to the problem of selection between two models. In this paper we describe a new method that extends this approach to handle an arbitrary number of models. By optimizing subject to hard constraints, the new method can ensure that a defined task is fulfilled, while optimally discriminating between models. This means that the discrimination power of the designed control input can be much greater than that created by other approaches, which typically design Â‘auxiliaryÂ’ signals with limited power so that the effect on the system state is small. Furthermore, the optimization criterion, which is an upper bound on the probability of model selection error, has a more meaningful interpretation than alternative approaches that are based on information gain, for example.We demonstrate the method using an aircraft fault detectionscenario and show that the new method significantly reducesthe bound on the probability of error when compared to amanually generated identification sequence and a fuel-optimalsequence.",MIT-CSAIL-TR-2006-013,6 p.; 11551717 bytes; 591011 bytes,application/postscript; application/pdf,en_US,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,494
Howard Shrobe,"Bachrach, Jonathan; Beal, Jacob",2006-10-03T14:06:44Z,2006-10-03T14:06:44Z,2006-06,http://hdl.handle.net/1721.1/34223,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Programming a Sensor Network as an Amorphous Medium,"In many sensor network applications, the network is deployedto approximate a physical space. The network itself is not ofinterest: rather, we are interested in measuring the propertiesof the space it fills, and of establishing control over thebehavior of that space.The spatial nature of sensor network applications meansthat many can be expressed naturally and succinctly in termsof the global behavior of an amorphous medium---a continuouscomputational material filling the space of interest. Althoughwe cannot construct such a material, we can approximateit using a sensor network.Using this amorphous medium abstraction separates sensornetwork problems into two largely independent domains.Above the abstraction barrier we are concerned with longrangecoordination and concise description of applications,while below the barrier we are concerned with fast, efficient,and robust communication between neighboring devices.We apply the amorphous medium abstraction with Proto,a high-level language for programming sensor/actuator networks.Existing applications, such as target tracking andthreat avoidance, can be expressed in only a few lines of Protocode. The applications are then compiled for execution on akernel that approximates an amorphous medium. Programswritten using our Proto implementation have been verified insimulation on over ten thousand nodes, as well as on a networkof Berkeley Motes.",MIT-CSAIL-TR-2006-069,6 p.; 374454 bytes; 4831946 bytes,application/pdf; application/postscript,en_US,amorphous computing,AIRE,,,,,DCOSS 2006 (Extended Abstract),,,,,,,,,,,,,,,,,,,,,,,,2006,522
David Clark,"Masiello, Elizabeth",2006-01-12T16:07:26Z,2006-01-12T16:07:26Z,2006-01-11,http://hdl.handle.net/1721.1/30606,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Service Identification in TCP/IP: Well-Known versus Random Port Numbers,"The sixteen-bit well-known port number is often overlooked as a network identifier in Internet communications. Its purpose at the most fundamental level is only to demultiplex flows of traffic. Several unintended uses of the port number evolved from associating services with a list of well-known port numbers. This thesis documents those unintended consequences in an effort to describe the port number's influence on Internet players from ISPs to application developers to individual users. Proposals and examples of moving away from well-known port numbers to randomly assigned ones are then presented, with analysis of impacts on the political and economic systems on which Internet communication is dependent.",MIT-CSAIL-TR-2006-004,52 p.; 66099985 bytes; 3014151 bytes,application/postscript; application/pdf,en_US,,Advanced Network Architecture,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,486
Tomaso Poggio,"Wolf, Lior",2006-05-16T22:16:59Z,2006-05-16T22:16:59Z,2006-05-16,http://hdl.handle.net/1721.1/32978,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning using the Born Rule,"In Quantum Mechanics the transition from a deterministic descriptionto a probabilistic one is done using a simple rule termed the Bornrule. This rule states that the probability of an outcome ($a$)given a state ($\Psi$) is the square of their inner products($(a^\top\Psi)^2$).In this paper, we unravel a new probabilistic justification forpopular algebraic algorithms, based on the Born rule. Thesealgorithms include two-class and multiple-class spectral clustering,and algorithms based on Euclidean distances.",MIT-CSAIL-TR-2006-036; CBCL-261,28 p.; 650559 bytes; 1572192 bytes,application/pdf; application/postscript,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,519
Michael Ernst,"McCamant, Stephen",2006-05-11T19:32:15Z,2006-05-11T19:32:15Z,2006-05-11,http://hdl.handle.net/1721.1/32546,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Machine-Checked Safety Proof for a CISC-Compatible SFI Technique,"Executing untrusted code while preserving security requires that thecode be prevented from modifying memory or executing instructionsexcept as explicitly allowed.  Software-based fault isolation (SFI) or""sandboxing"" enforces such a policy by rewriting code at theinstruction level.  In previous work, we developed a new SFI techniquethat is applicable to CISC architectures such as the Intel IA-32,based on enforcing additional alignment constraints to avoiddifficulties with variable-length instructions.  This report describesa machine-checked proof we developed to increase our confidence in thesafety provided by the technique.  The proof, constructed for asimplified model of the technique using the ACL2 theorem provingenvironment, certifies that if the code rewriting has been checked tohave been performed correctly, the resulting program cannot perform adangerous operation when run.  We describe the high-level structure ofthe proof, then give the intermediate lemmas with interspersedcommentary, and finally evaluate the process of the proof'sconstruction.",MIT-CSAIL-TR-2006-035,33 p.; 499398 bytes; 1456845 bytes,application/pdf; application/postscript,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,517
Dina Katabi,"Chachulski, Szymon; Jennings, Michael; Katti, Sachin; Katabi, Dina",2006-07-03T14:16:54Z,2006-07-03T14:16:54Z,2006-06-30,http://hdl.handle.net/1721.1/33230,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,MORE: A Network Coding Approach to Opportunistic Routing,"Opportunistic routing has the potential to substantially increase wireless network throughput. Prior work on opportunistic routing, however, requires tight node coordination. Different nodes in a network must have knowledge of which packets other nodes have received. Furthermore, the nodes have to agree on which nodes should transmit which packets. Such coordination becomes fragile in dense or large networks.This paper introduces MORe, a new opportunistic routing protocol that avoids node-coordination. Our design is rooted in the theory of network coding.Routers code packets going to the same destination and forward the coded versions. The destination decodes and recovers the original packets. This approach needs no coordination and provably maximizes network throughput. We have implemented our design and evaluated it in a 25-node testbed. Our results show that MORE provides an average throughput increase of 60% and a maximum of 10-fold, demonstrating that the theoretical gains promised by network coding are realizable in practice.",MIT-CSAIL-TR-2006-049,12 p.; 22051455 bytes; 883727 bytes,application/postscript; application/pdf,en_US,Network Coding; Opportunistic Routing; Wireless Networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,529
Howard Shrobe,"Look, Gary; Peters, Stephen; Shrobe, Howard",2006-03-02T14:01:55Z,2006-03-02T14:01:55Z,2006-03-01,http://hdl.handle.net/1721.1/31222,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Plan-Driven Pervasive Computing,"The goal of human-centered, pervasive computing should be to hide the details of the computing environment, allowing users to concentrate on their goals, rather than on the direct management of devices. This paper describes a system that operates at the level of goals and plans, rather than individual resources. It adaptively selects from its plan library that plan which is likely to best achieve the userÂ’s goal in view of his preferences and current resource availability. Once the plan and resources are selected, it monitors the execution of the plan, dispatching subtasks when they are ready to be executed.",MIT-CSAIL-TR-2006-016,14p.; 18134831 bytes; 577156 bytes,application/postscript; application/pdf,en_US,,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,499
Daniel Jackson,"Torlak, Emina; Jackson, Daniel",2006-09-29T21:46:41Z,2006-09-29T21:46:41Z,2006-09-29,http://hdl.handle.net/1721.1/34218,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Design of a Relational Engine,"The key design challenges in the construction of a SAT-based relational engine are described, and novel techniques are proposed to address them.  An efficient engine must have a mechanism for specifying partial solutions, an effective symmetry detection and breaking scheme, and an economical translation from relational to boolean logic.  These desiderata are addressed with three new techniques: a symmetry detection algorithm that works in the presence of partial solutions, a sparse-matrix representation of relations, and a compact representation of boolean formulas inspired by boolean expression diagrams and reduced boolean circuits.  The presented techniques have been implemented and evaluated, with promising results.",MIT-CSAIL-TR-2006-068,11 p.; 444636 bytes; 1204261 bytes,application/pdf; application/postscript,en_US,,Software Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,550
William Freeman,"Fergus, Rob; Torralba, Antonio; Freeman, William T.",2006-09-07T18:46:57Z,2006-09-07T18:46:57Z,2006-09-02,http://hdl.handle.net/1721.1/33962,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Random Lens Imaging,"We call a random lens one for which the function relating the input light ray to the output sensor location is pseudo-random. Imaging systems with random lensescan expand the space of possible camera designs, allowing new trade-offs in optical design and potentially adding new imaging capabilities. Machine learningmethods are critical for both camera calibration and image reconstruction from the sensor data. We develop the theory and compare two different methods for calibration and reconstruction: an MAP approach, and basis pursuit from compressive sensing. We show proof-of-concept experimental results from a random lens made from a multi-faceted mirror, showing successful calibration and image reconstruction. We illustrate the potential for super-resolution and 3D imaging.",MIT-CSAIL-TR-2006-058,9 p.; 52671890 bytes; 1133927 bytes,application/postscript; application/pdf,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,540
Trevor Darrell,"Grauman, Kristen; Darrell, Trevor",2006-03-20T19:24:36Z,2006-03-20T19:24:36Z,2006-03-18,http://hdl.handle.net/1721.1/31338,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pyramid Match Kernels: Discriminative Classification with Sets of Image Features (version 2),"Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering.  Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences -- generally a computationally expensive task that becomes impractical for largeset sizes.  We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space.  This ``pyramid match"" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kerneldoes not penalize the presence of extra features, it is robust to clutter.  We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels.  We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches.  (This tech report updates MIT-CSAIL-TR-2005-017 and the paper ""The Pyramid Match Kernel: Discriminative Classification with Sets of Images Features"" which appeared in the proceedings of ICCV 2005.)",MIT-CSAIL-TR-2006-020,10 p.; 7963746 bytes; 1702359 bytes,application/postscript; application/pdf,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,503
Michael Ernst,"Kim, Sunghun; Artzi, Shay; Ernst, Michael D.",2007-11-20T15:45:09Z,2007-11-20T15:45:09Z,2007-11-20,http://hdl.handle.net/1721.1/39639,,ReCrash: Making Crashes Reproducible,"It is difficult to fix a problem without being able to reproduce it.However, reproducing a problem is often difficult and time-consuming.This paper proposes a novel algorithm, ReCrash, that generatesmultiple unit tests that reproduce a given program crash.ReCrash dynamically tracks method calls during every execution of the target program. If the program crashes, ReCrash saves information about the relevant method calls and uses the saved information to create unit tests reproducing the crash.We present reCrashJ an implementation of ReCrash for Java. reCrashJ reproducedreal crashes from javac, SVNKit, Eclipse JDT, and BST. reCrashJ is efficient, incurring 13%-64% performance overhead. If this overhead is unacceptable, then reCrashJ has another mode that has negligible overhead until a crash occurs and 0%-1.7% overhead until a second crash, at which point the test cases are generated.",MIT-CSAIL-TR-2007-054,9 p.,,,,Program Analysis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2007,624
Howard Shrobe,"Shrobe, Howard; Knight, Thomas; Hon, Andre de",2007-05-30T18:01:22Z,2007-05-30T18:01:22Z,2007-05-30,http://hdl.handle.net/1721.1/37589,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"TIARA:  Trust Management, Intrusion-tolerance, Accountability, and Reconstitution Architecture","The last 20 years have led to unprecedented improvements in chipdensity and system performance fueled mainly by Moore's Law.  Duringthe same time, system and application software have bloated, leadingto unmanageable complexity, vulnerability to attack, rigidity and lackof robustness and accountability. These problems arise from the factthat all key elements of the computational environment, from hardwarethrough system software and middleware to application code regard theworld as consisting of unconstrained ``raw seething bits''.  No elementof the entire stack is responsible for enforcing over-archingconventions of memory structuring or access control.  Outsiders mayeasily penetrate the system by exploiting vulnerabilities (e.g. bufferoverflows) arising from this lack of basic constraints. Attacks arenot easily contained, whether they originate from the clever outsiderwho penetrates the defenses or from the insider who exploits existingprivileges.  Finally, because there are no facilities for tracing theprovenance of data, even when an attack is detected, it is difficultif not impossible to tell which data are traceable to the attack andwhat data may still be trusted. We have abundant computational resources allowing us to fix thesecritical problems using a combination of hardware, system software,and programming language technology: In this report, we describe theTIARAproject, which is using these resources to design a newcomputer system thatis less vulnerable, more tolerant of intrusions, capable of recoveryfrom attacks, and accountable for their actions.  TIARA provides thesecapabilities without significant impact on overall system performance.  Itachieves these goals through the judicious use of a modest amountof extra, but reasonably generable purpose, hardware that is dedicatedto tracking the provenance of data at a very fine grained level, toenforcing access control policies, and to constructing a coherentobject-oriented model of memory.  This hardware runs in parallel withthe main data-paths of the system and operates on a set of extra bitstagging each word with data-type, bounds, access control andprovenance information. Operations that violate the intendedinvariants are trapped, while normal results are tagged withinformation derived from the tags of the input operands.This hardware level provides fine-grained support for a series ofsoftware layers that enable a variety of comprehensive access controlpolicies, self-adaptive computing, and fine-grained recoveryprocessing.  The first of these software layers establishes aconsistent object-oriented level of computing while higher layersestablish wrappers that may not be bypassed, access controls, dataprovenance tracking.  At the highest level we create the ``planlevel'' of computing in which code is executed in parallel with anabstract model (or executable specification) of the system that checkswhether the code behaves as intended.",MIT-CSAIL-TR-2007-028,18 p.,,,Security; Intrusion Tolerance; Tagged Architecture,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,597
Hal Abelson,"Ehrmann, Stephen C.; Gilbert, Steven W.; McMartin, Flora",2007-08-23T14:41:46Z,2007-08-23T14:41:46Z,2007-08-20,http://hdl.handle.net/1721.1/38482,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Factors Affecting the Adoption of Faculty-Developed Academic Software: A Study of Five iCampus Projects,"Initiated in 1999, iCampus is a research collaboration between Microsoft Research and MIT whose goal is to create and demonstrate technologies with the potential for revolutionary change throughout the university curriculum.” The program was made possible by a $25 million research grant from Microsoft to MIT, and involves extensive collaboration between MIT and Microsoft staff.<p />This assessment study by the TLT Group addresses the question: The TLT Group has been asked, “In light of the experience of iCampus, especially those projects selected by MIT and Microsoft for close study, what can be learned about priorities for educational technology initiatives in the future and about how the spread of such innovations can be more effectively supported?”<p />The major conclusions are that the five projects studied improved important elements of an MIT education by making learning more authentic, active, collaborative, and feedback-rich.  Nevertheless, wider adoption beyond MIT was extremely difficult to achieve, largely due to structure issues in universities that make it difficult for educational technology to spread beyond the initial innovators, even to other departments within the same institution.  The report includes recommendations for universities, external sponsors, and for MIT in particular, about steps to take to achieve more effective dissemination.",,149 p.,,,educational technology; educational assessment,iCampus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,611
Michael Ernst,"Papi, Matthew M.; Ali, Mahmood; Correa Jr., Telmo Luis; Perkins, Jeff H.; Ernst, Michael D.",2007-09-20T19:21:48Z,2007-09-20T19:21:48Z,2007-09-17,http://hdl.handle.net/1721.1/38878,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pluggable type-checking for custom type qualifiers in Java,"We have created a framework for adding custom type qualifiers to the Javalanguage in a backward-compatible way.  The type system designer definesthe qualifiers and creates a compiler plug-in that enforces theirsemantics.  Programmers can write the type qualifiers in their programs andbe informed of errors or assured that the program is free of those errors.The system builds on existing Java tools and APIs.In order to evaluate our framework, we have written four type-checkersusing the framework:  for a non-null type system that can detect andprevent null pointer errors; for an interned type system that can detectand prevent equality-checking errors; for a reference immutability typesystem, Javari, that can detect and prevent mutation errors; and for areference and object immutability type system, IGJ, that can detect andprevent even more mutation errors.  We have conducted case studies usingeach checker to find real errors in existing software.  These case studiesdemonstrate that the checkers and the framework are practical and useful.",MIT-CSAIL-TR-2007-047,10 p.,,,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,617
Michael Ernst,"McCamant, Stephen; Ernst, Michael D.",2007-12-10T14:00:11Z,2007-12-10T14:00:11Z,2007-12-10,http://hdl.handle.net/1721.1/39812,,Quantitative Information Flow as Network Flow Capacity,"We present a new technique for determining how much information abouta program's secret inputs is revealed by its public outputs. Incontrast to previous techniques based on reachability from secretinputs (tainting), it achieves a more precise quantitative result bycomputing a maximum flow of information between the inputs andoutputs. The technique uses static control-flow regions to soundlyaccount for implicit flows via branches and pointer operations, butoperates dynamically by observing one or more program executions andgiving numeric flow bounds specific to them (e.g., ""17 bits""). Themaximum flow in a network also gives a minimum cut (a set of edgesthat separate the secret input from the output), which can be used toefficiently check that the same policy is satisfied on futureexecutions. We performed case studies on 5 real C, C++, and ObjectiveC programs, 3 of which had more than 250K lines of code. The toolchecked multiple security policies, including one that was violated bya previously unknown bug.",MIT-CSAIL-TR-2007-057,12 p.,,,Confidentiality; Privacy; Information disclosure; Tainting; Implicit flows; Valgrind; Memcheck,Program Analysis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2007,627
Martin Rinard,"Lam, Patrick; Zee, Karen; Kuncak, Viktor; Rinard, Martin",2007-11-02T18:45:31Z,2007-11-02T18:45:31Z,2007-10-31,http://hdl.handle.net/1721.1/39419,,Set Interfaces for Generalized Typestate and Data Structure Consistency Verification,"Typestate systems allow the type of an object to change during its lifetime in the computation. Unlike standard type systems, they can enforce safety properties that depend on changing object states. We present a new, generalized formulation of typestate that models the typestate of an object through membership in abstract sets. This abstract set formulation enables developers to reason about cardinalities of sets, and in particular to state and verify the condition that certain sets are empty. We support hierarchical typestate classifications by specifying subset and disjointness properties over the typestate sets.We present our formulation of typestate in the context of the Hob program specification and verification framework. The Hob framework allows the combination of typestate analysis with powerful independently developed analyses such as shape analyses or theorem proving techniques. We implemented our analysis and annotated several programs (75-2500 lines of code) with set specifications. Our implementation includes several optimizations that improve the scalability of the analysis and a novel loop invariant inferencealgorithm that eliminates the need to specify loop invariants. We present experimental data demonstrating the effectiveness of our techniques.",MIT-CSAIL-TR-2007-049,30 p.,,,,Computer Architecture,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2007,620
Dina Katabi,"Woo, Grace Rusi; Kheradpour, Pouya; Katabi, Dina",2007-05-29T18:21:50Z,2007-05-29T18:21:50Z,2007-05-29,http://hdl.handle.net/1721.1/37587,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Beyond the Bits: Cooperative Packet Recovery Using Physical Layer Information,"Wireless networks can suffer from high packet loss rates.  This paper shows that the loss rate can be significantly reduced by exposing information readily available at the physical layer. We make the physical layer convey an estimate of its confidence that a particular bit is ``0'' or ``1'' to the higher layers.  When used with cooperative design, this information dramatically improves the throughput of the wireless network. Access points that hear the same transmission combine their information to correct bits in a packet with minimal overhead. Similarly, a receiver may combine multiple erroneous transmissions to recover a correct packet.  We analytically prove that our approach minimizes the errors in packet recovery.  We also experimentally demonstrate its benefits using a testbed of GNU software radios. The results show that our approach can reduce loss rate by up to 10x in comparison with the current approach, and significantly outperforms prior cooperation proposals.",MIT-CSAIL-TR-2007-027,12 p.,,,,Networks & Mobile Systems,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2007,596
Howard Shrobe,"Shrobe, Howard; Laddaga, Robert; Balzer, Robert; Goldman, Neil; Wile, Dave; Tallis, Marcelo; Hollebeek, Tim; Egyed, Alexander",2007-04-10T21:41:47Z,2007-04-10T21:41:47Z,2007-04-10,http://hdl.handle.net/1721.1/37151,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Self-Adaptive Systems for Information Survivability: PMOP and AWDRAT,"Information systems form the backbones of the critical infrastructures of modern societies. Unfortunately, these systems are highly vulnerable to attacks that can result in enormous damage. Furthermore, traditional approaches to information security have not provided all the protections necessary to defeat and recover from a concerted attack; in particular, they are largely irrelevant to the problem of defending against attacks launched by insiders.This paper describes two related systems PMOP and AWDRAT that were developed during the DARPA Self Regenerative Systems program. PMOP defends against insider attacks while AWDRAT is intended to detect compromises to software systems. Both rely on self-monitoring, diagnosis and self-adaptation. We describe both systems and show the results of experiments with each.",MIT-CSAIL-TR-2007-023,10 p.,,,Information Survivability; Model Based Diagnosis; Adaptive Software,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,590
Hal Abelson,"Ehrmann, Stephen C.; Gilbert, Steven W.; McMartin, Flora; Abelson, Harold; Long, Philip D.",2007-08-27T19:01:47Z,2007-08-27T19:01:47Z,2007-10-20,http://hdl.handle.net/1721.1/38487,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Factors Affecting the Adoption of Faculty-Developed Academic Software: A Study of Five iCampus Projects,"Instruction in higher education must adapt more rapidly to: changes in workforce needs, global issues, advances in disciplines, and resource constraints.  The pace of such improvement depends on the speed with which new ideas and materials are adopted across institutions. In 1999 Microsoft pledged $25 million and staff support for iCampus, a seven-year MIT project to develop pioneering uses of educational technology. The TLT Group studied five iCampus projects in order to identify factors affecting institutionalization and widespread dissemination. Among the factors impeding adoption: lack of rewards and support for faculty to adopt innovations; faculty isolation; and a lack of attention to adoption issues among projects selected for funding. The study made recommendations for universities, foundations, government agencies and corporations: 1) continue making education more authentic, active, collaborative, and feedback-rich; 2) create demand to adopt ideas and materials from other sources by encouraging all faculty members to improve and document learning in their programs, year after year; 3) nurture coalitions for instructional improvement, across and within institutions; 4) create more effective higher education – corporate alliances; and 5) improve institutional services to support faculty in educational design, software development, assessment methods, formative evaluation, and/or in sharing ideas with others who teach comparable courses.",MIT-CSAIL-TR-2007-045,6 p.,,,educational technology; educational innovation,iCampus,,,,,37th ASEE/IEEE Frontiers in Education Conference,,,,,,,,,,,,,,,,,,,,,,,,2007,618
Silvio Micali,"Micali, Silvio; Valiant, Paul",2008-06-30T13:00:26Z,2008-06-30T13:00:26Z,2007- 11-02,http://hdl.handle.net/1721.1/41872,,Revenue in Truly Combinatorial Auctions and Adversarial Mechanism Design,"Little is known about generating revenue in UNRESTRICTED combinatorial auctions. (In particular, the VCG mechanism has no revenue guarantees.) In this paper we determine how much revenue can be guaranteed in such auctions. Our analysis holds both in the standard model, when all players are independent and rational, as well as in a most adversarial model, where some players may bid collusively or even totally irrationally.",MIT-CSAIL-TR-2008-039,30 p.,,,Revenue Benchmarks; Revenue lowerbounds; revenue upperbounds; Probabilistic DST mechanisms,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2007,565
Una-May O'Reilly,"Salameh, Lynne Rafik",2007-06-06T12:21:53Z,2007-06-06T12:21:53Z,2007-06-05,http://hdl.handle.net/1721.1/37597,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,An Analysis of Posynomial MOSFET Models Using Genetic Algorithms and Visualization,"Analog designers are interested in optimization tools which automate the process of circuit sizing. Geometric programming, which uses posynomial models of MOSFET parameters, represents one such tool. Genetic algorithms have been used to evolve posynomial models for geometric programs, with a reasonable mean error when modeling MOSFET parameters. By visualizing MOSFET data using two dimensional plots, this thesis investigates the behavior of various MOSFET small and large signal parameters and consequently proposes a lower bound on the maximum error, which a posynomial cannot improve upon. It then investigates various error metrics which can be used to balance the mean and maximum errors generated by posynomial MOSFET models. Finally, the thesis uses empirical data to verify the existence of the lower bound, and compares the maximum error from various parameters modeled by the genetic algorithm and by monomial fitting. It concludes that posynomial MOSFET models suffer from inherent inaccuracies. Additionally, although genetic algorithms improve on the maximum model error, the improvement, in general, does not vastly surpass results obtained through monomial fitting, which is a less computationally intensive method. Genetic algorithms are hence best used when modeling partially convex MOSFET parameters, such as r0 .",MIT-CSAIL-TR-2007-032,90 p,,,limiations; convex optimization,Humanoid Robotics,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,,2007,599
Gerald Sussman,"Beal, Jacob; Bachrach, Jonathan; Tobenkin, Mark",2007-08-27T14:23:35Z,2007-08-27T14:23:35Z,2007-08-24,http://hdl.handle.net/1721.1/38484,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Constraint and Restoring Force,"Long-lived sensor network applications must be able to self-repair and adapt to changing demands. We introduce a new approach for doing so: Constraint and Restoring Force. CRF is a physics-inspired framework for computing scalar fields across a sensor network with occasional changes. We illustrate CRF’s usefulness by applying it to gradients, a common building block for sensor network systems. The resulting algorithm, CRF-Gradient, determines locally when to self-repair and when to stop and save energy. CRF-Gradient is self-stabilizing, converges in O(diameter) time, and has been verified experimentally in simulation and on a network of Mica2 motes. Finally we show how CRF can be applied to other algorithms as well, such as the calculation of probability fields.",MIT-CSAIL-TR-2007-044,12 p.,,,amorphous computing; spatial computing; Proto,Mathematics and Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,614
Karen Sollins,"Hansen, Richard E.",2007-06-22T12:41:46Z,2007-06-22T12:41:46Z,2007-06-21,http://hdl.handle.net/1721.1/37601,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Stateful Anycast for DDoS Mitigation,"Distributed denial-of-service (DDoS) attacks can easily cripple victim hosts or networks, yet effective defenses remain elusive.  Normal anycast can be used to force the diffusion of attack traffic over a group of several hosts to increase the difficulty of saturating resources at or near any one of the hosts.  However, because a packet sent to the anycast group may be delivered to any member, anycast does not support protocols that require a group member to maintain state (such as TCP).  This makes anycast impractical for most applications of interest.This document describes the design of Stateful Anycast, a conceptual anycast-like network service based on IP anycast.  Stateful Anycast is designed to support stateful sessions without losing anycast’s ability to defend against DDoS attacks.  Stateful Anycast employs a set of anycasted proxies to direct packets to the proper stateholder.  These proxies provide DDoS protection by dropping a session’s packets upon group member request.  Stateful Anycast is incrementally deployable and can scale to support many groups.",MIT-CSAIL-TR-2007-035,103 p.,,,,Advanced Network Architecture,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,,2007,602
Rodney Brooks,"Aryananda, Lijin",2007-04-04T17:21:51Z,2007-04-04T17:21:51Z,2007-04-03,http://hdl.handle.net/1721.1/37144,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Few Days of A Robot's Life in the Human's World: Toward Incremental Individual Recognition,"This thesis presents an integrated framework and implementation for Mertz, an expressive robotic creature for exploring the task of face recognition through natural interaction in an incremental and unsupervised fashion.  The goal of this thesis is to advance toward a framework which would allow robots to incrementally ``get to know'' a set of familiar individuals in a natural and extendable way.  This thesis is motivated by the increasingly popular goal of integrating robots in the home.  In order to be effective in human-centric tasks, the robots must be able to not only recognize each family member, but also to learn about the roles of various people in the household.In this thesis, we focus on two particular limitations of the current technology.  Firstly, most of face recognition research concentrate on the supervised classification problem.  Currently, one of the biggest problems in face recognition is how to generalize the system to be able to recognize new test data that vary from the training data.  Thus, until this problem is solved completely, the existing supervised approaches may require multiple manual introduction and labelling sessions to include training data with enough variations. Secondly, there is typically a large gap between research prototypes and commercial products, largely due to lack of robustness and scalability to different environmental settings.In this thesis, we propose an unsupervised approach which wouldallow for a more adaptive system which can incrementally update thetraining set with more recent data or new individuals over time.Moreover, it gives the robots a more natural {\em socialrecognition} mechanism to learn not only to recognize each person'sappearance, but also to remember some relevant contextualinformation that the robot observed during previous interactionsessions. Therefore, this thesis focuses on integrating anunsupervised and incremental face recognition system within aphysical robot which interfaces directly with humans through naturalsocial interaction.  The robot autonomously detects, tracks, andsegments face images during these interactions and automaticallygenerates a training set for its face recognition system.  Moreover,in order to motivate robust solutions and address scalabilityissues, we chose to put the robot, Mertz, in unstructured publicenvironments to interact with naive passersby, instead of with onlythe researchers within the laboratory environment.While an unsupervised and incremental face recognition system is acrucial element toward our target goal, it is only a part of thestory.  A face recognition system typically receives eitherpre-recorded face images or a streaming video from a static camera.As illustrated an ACLU review of a commercial face recognitioninstallation, a security application which interfaces with thelatter is already very challenging.  In this case, our target goalis a robot that can recognize people in a home setting. Theinterface between robots and humans is even more dynamic.  Both therobots and the humans move around.We present the robot implementation and its unsupervised incremental face recognition framework.  We describe analgorithm for clustering local features extracted from a large set of automatically generated face data.  We demonstrate the robot's capabilities and limitations in a series of experiments at a public lobby. In a final experiment, the robot interacted with a few hundred individuals in an eight day period and generated a training set of over a hundred thousand face images. We evaluate the clustering algorithm performance across a range of parameters on this automatically generated training data and also the Honda-UCSD video face database. Lastly, we present some recognition results using the self-labelled clusters.",MIT-CSAIL-TR-2007-022,244 p.,,,Humanoid robotic; Human robot interaction; Face recognition,Humanoid Robotics,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2007,589
Gerald Sussman,"Beal, Jacob",2007-05-15T17:41:55Z,2007-05-15T17:41:55Z,2007-05-15,http://hdl.handle.net/1721.1/37336,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Developmental Cost for Models of Intelligence,"We can evaluate models of natural intelligence, as well as theirindividual components, by using a model of hardware and developmentcosts, ignoring almost all the details of biology.  The basic argumentis that neither the gross anatomy of the brain nor the behavior ofindividual cells nor the behavior of the whole poses sufficientconstraint on the algorithms that might run within the brain, but thatthe process of engineering an intelligence under this cost model posessimilar challenges to those faced by a human growing from a singlecell to an adult.  This will allow us to explore architectural ideasfreely, yet retain confidence that when a system works, the principlesallowing it to work are likely to be similar to those that allow humanintelligence to work.",MIT-CSAIL-TR-2007-026,4 p.,,,cognitive architectures; artificial intelligence,Mathematics and Computation,,,,,AAAI 2007 Workshop on Evaluating Architectures for Intelligence,,,,,,,,,,,,,,,,,,,,,,,,2007,594
Silvio Micali,"Valiant, Paul; Micali, Silvio",2007-11-02T20:30:07Z,2007-11-02T20:30:07Z,2007-11-02,http://hdl.handle.net/1721.1/39420,,Collusion-Resilient Revenue In Combinatorial Auctions,"In auctions of a single good, the second-price mechanism achieves, in dominantstrategies, a revenue benchmark that is naturally high and resilient to anypossible collusion.We show how to achieve, to the maximum extent possible, the same propertiesin combinatorial auctions.",MIT-CSAIL-TR-2007-052,20 p.,,,Worst Rational Setting; Natural Solution Pairs; Player-Monotone Benchmarks; Revenue Guarantees; Guaranteed Revenue,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,,,,,,,,,,,,,,2007,622
Gerald Sussman,"Beal, Jacob",2007-08-27T14:21:47Z,2007-08-27T14:21:47Z,2007-08-23,http://hdl.handle.net/1721.1/38483,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning by Learning To Communicate,"Human intelligence is a product of cooperation among many different specialists.  Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the ""high-level"" agile cooperation that permeates our daily lives.I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of ""communication bootstrapping,"" in which shared experiences form a basis for agreement on a system of signals.  In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence.I then demonstrate progress along the course of investigation laid out in my roadmap:* I establish a measure of ""developmental cost"" that allows me to eliminate many possible designs* I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using ""failure simplification.""* I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system.* I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping.",MIT-CSAIL-TR-2007-042,218 p.,,,artificial intelligence; cognitive science,Mathematics and Computation,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2007,612
Dave Gifford,"Gerber, Georg K.; Dowell, Robin D.; Jaakkola, Tommi S.; Gifford, David K.",2007-07-09T17:43:48Z,2007-07-09T17:43:48Z,2007-07-06,http://hdl.handle.net/1721.1/37817,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Hierarchical Dirichlet Process-Based Models For Discovery of Cross-species Mammalian Gene Expression,"An important research problem in computational biology is theidentification of expression programs, sets of co-activatedgenes orchestrating physiological processes, and thecharacterization of the functional breadth of these programs.  Theuse of mammalian expression data compendia for discovery of suchprograms presents several challenges, including: 1) cellularinhomogeneity within samples, 2) genetic and environmental variationacross samples, and 3) uncertainty in the numbers of programs andsample populations. We developed GeneProgram, a new unsupervisedcomputational framework that uses expression data to simultaneouslyorganize genes into overlapping programs and tissues into groups toproduce maps of inter-species expression programs, which are sortedby generality scores that exploit the automatically learnedgroupings.  Our method addresses each of the above challenges byusing a probabilistic model that: 1) allocates mRNA to differentexpression programs that may be shared across tissues, 2) ishierarchical, treating each tissue as a sample from a population ofrelated tissues, and 3) uses Dirichlet Processes, a non-parametricBayesian method that provides prior distributions over numbers ofsets while penalizing model complexity.  Using real gene expressiondata, we show that GeneProgram outperforms several popularexpression analysis methods in recovering biologically interpretablegene sets.  From a large compendium of mouse and human expressiondata, GeneProgram discovers 19 tissue groups and 100 expressionprograms active in mammalian tissues.  Our method automaticallyconstructs a comprehensive, body-wide map of expression programs andcharacterizes their functional generality. This map can be used forguiding future biological experiments, such as discovery of genesfor new drug targets that exhibit minimal ""cross-talk"" withunintended organs, or genes that maintain general physiologicalresponses that go awry in disease states.  Further, our method isgeneral, and can be applied readily to novel compendia of biologicaldata.",MIT-CSAIL-TR-2007-037,42 p.,,,,Computational & Systems Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,607
Howard Shrobe,"Bachrach, Jonathan; Beal, Jacob",2007-03-14T18:21:48Z,2007-03-14T18:21:48Z,2007-03-14,http://hdl.handle.net/1721.1/36840,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Building Spatial Computers,"Programmability is a major challenge in spatial computing, anaggregate control problem found in domains such as sensor networks,swarm robotics, and modular robotics.  We address this challenge witha model of a spatial computer (Proto Abstract Machine) and adistributed operating system, ProtoKernel, which implements PAMapproximately.  ProtoKernel has been demonstrated on platforms inthree spatial computing domains: sensor networks, swarm robotics, andmodular robotics.",MIT-CSAIL-TR-2007-017,5 p.,,,amorphous medium; amorphous computing,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,584
Dave Gifford,"Gerber, Georg K.; Dowell, Robin D.; Jaakkola, Tommi S.; Gifford, David K.",2007-06-26T22:24:22Z,2007-06-26T22:24:22Z,2007-06-25,http://hdl.handle.net/1721.1/37603,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Table 2 (Supplemental): Complete data for all 100 expression programs discovered by GeneProgram from the Novartis Gene Atlas v2,"Table 2 (Supplemental): Complete data for all 100 recurrent expression programs (EPs) discovered by GeneProgram.  Each EP has two identifying rows, a list of meta-genes, and a list of significantly enriched GO categories.  The first identifying row has three columns: (1) the EP identifier (an arbitrarily assigned number), (2) the number of meta-genes in the EP, and (3) the percentage of samples the EP occurs in.  The identifying row lists all tissues that use the EP (h_ = human tissue, m_ = mouse tissue).  Numbers in parentheses next to each tissue indicate the degree to which the tissue uses the EP.After the identifying rows the set of meta-genes in the EP are listed. Each meta-gene has eight columns: (1) the human RefSeq identifier, (2) the mouse RefSeq identifier, (3) the empirical mean expression level, (4) the empirical mean occurrence percentage, (5) the human gene name, (6) the human Swis-Prot description, (7) the mouse gene name, and (8) the mouse Swis-Prot description.Following the meta-genes are lists of significant GO categories (the first list uses human annotations, and the second uses mouse annotations).  The columns for each line in this list are: (1) GO term, (2) enrichment p-value, (3) number of genes in the EP in the category/total genes in the EP with some GO category, (4) category description, and (5) total number of genes in the category that are also in the dataset analyzed.",,,,,,Computational & Systems Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,603
Boris Katz,"Radul, Alexey",2007-12-19T19:15:14Z,2007-12-19T19:15:14Z,2007-10-22,http://hdl.handle.net/1721.1/39831,,Report on the Probabilistic Language Scheme,"Reasoning with probabilistic models is a widespread andsuccessful technique in areas ranging from computer vision, to naturallanguage processing, to bioinformatics. Currently, these reasoningsystems are either coded from scratch in general-purpose languages oruse formalisms such as Bayesian networks that have limited expressivepower. In both cases, the resulting systems are difficult to modify,maintain, compose, and interoperate with. This work presents ProbabilisticScheme, an embedding of probabilistic computation into Scheme. Thisgives programmers an expressive language for implementing modularprobabilistic models that integrate naturally with the rest of Scheme.",MIT-CSAIL-TR-2007-059,9 p.,,,probability,Infolab,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2007,619
Martin Rinard,"Kuncak, Viktor",2007-01-02T20:21:50Z,2007-01-02T20:21:50Z,2007-01-01,http://hdl.handle.net/1721.1/35258,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Quantifier-Free Boolean Algebra with Presburger Arithmetic is NP-Complete,"Boolean Algebra with Presburger Arithmetic (BAPA) combines1) Boolean algebras of sets of uninterpreted elements (BA)and 2) Presburger arithmetic operations (PA).  BAPA canexpress the relationship between integer variables andcardinalities of unbounded finite sets and can be used toexpress verification conditions in verification of datastructure consistency properties.In this report I consider the Quantifier-Free fragment ofBoolean Algebra with Presburger Arithmetic (QFBAPA).Previous algorithms for QFBAPA had non-deterministicexponential time complexity.  In this report I show thatQFBAPA is in NP, and is therefore NP-complete.  My resultyields an algorithm for checking satisfiability of QFBAPAformulas by converting them to polynomially sized formulasof quantifier-free Presburger arithmetic.  I expect thisalgorithm to substantially extend the range of QFBAPAproblems whose satisfiability can be checked in practice.",MIT-CSAIL-TR-2007-001,14 p.; 315999 bytes; 842090 bytes,application/pdf; application/postscript,en_US,Caratheodory theorem; integer linear programming; integer cone; Hilbert basis,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,566
David Clark,"Lee, George J.",2007-06-05T14:21:56Z,2007-06-05T14:21:56Z,2007-06-04,http://hdl.handle.net/1721.1/37595,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,CAPRI: A Common Architecture for Distributed Probabilistic Internet Fault Diagnosis,"This thesis presents a new approach to root cause localization and fault diagnosis in the Internet based on a Common Architecture for Probabilistic Reasoning in the Internet (CAPRI) in which distributed, heterogeneous diagnostic agents efficiently conduct diagnostic tests and communicate observations, beliefs, and knowledge to probabilistically infer the cause of network failures.  Unlike previous systems that can only diagnose a limited set of network component failures using a limited set of diagnostic tests, CAPRI provides a common, extensible architecture for distributed diagnosis that allows experts to improve the system by adding new diagnostic tests and new dependency knowledge.To support distributed diagnosis using new tests and knowledge, CAPRI must overcome several challenges including the extensible representation and communication of diagnostic information, the description of diagnostic agent capabilities, and efficient distributed inference.  Furthermore, the architecture must scale to support diagnosis of a large number of failures using many diagnostic agents.  To address these challenges, this thesis presents a probabilistic approach to diagnosis based on an extensible, distributed component ontology to support the definition of new classes of components and diagnostic tests; a service description language for describing new diagnostic capabilities in terms of their inputs and outputs; and a message processing procedure for dynamically incorporating new information from other agents, selecting diagnostic actions, and inferring a diagnosis using Bayesian inference and belief propagation.To demonstrate the ability of CAPRI to support distributed diagnosis of real-world failures, I implemented and deployed a prototype network of agents on Planetlab for diagnosing HTTP connection failures.  Approximately 10,000 user agents and 40 distributed regional and specialist agents on Planetlab collect information from over 10,000 users and diagnose over 140,000 failures using a wide range of active and passive tests, including DNS lookup tests, connectivity probes, Rockettrace measurements, and user connection histories.  I show how to improve accuracy and cost by learning new dependency knowledge and introducing new diagnostic agents.  I also show that agents can manage the cost of diagnosing many similar failures by aggregating related requests and caching observations and beliefs.",MIT-CSAIL-TR-2007-031,222 p.,,,,Advanced Network Architecture,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2007,598
,"Hsu, Eugene; Silva, Marco da; Popovic, Jovan",2008-08-25T19:01:02Z,2008-08-25T19:01:02Z,2007-08-01,http://hdl.handle.net/1721.1/41946,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Guided Time Warping for Motion Editing,"Time warping allows users to modify timing without affecting poses. It has many applications in animation systems for motion editing, such as refining motions to meet new timing constraints or modifying the acting of animated characters. However, time warping typically requires many manual adjustments to achieve the desired results. We present a technique which simplifies this process by allowing time warps to be guided by a provided reference motion. Given few timing constraints, it computes a warp that both satisfies these constraints and maximizes local timing similarities to the reference. The algorithm is fast enough to incorporate into standard animation workflows. We apply the technique to two common tasks: preserving the natural timing of motions under new time constraints and modifying the timing of motions for stylistic effects.",,,,,,,,,,,"In ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pages 45-52, July 2007.",Jovan Popovic; Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,2007,609
Tomaso Poggio,"Rifkin, Ryan; Bouvrie, Jake; Schutte, Ken; Chikkerur, Sharat; Kouh, Minjoon; Ezzat, Tony; Poggio, Tomaso",2007-02-01T18:26:47Z,2007-02-01T18:26:47Z,2007-02-01,http://hdl.handle.net/1721.1/35835,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Phonetic Classification Using Hierarchical, Feed-forward, Spectro-temporal Patch-based Architectures","A preliminary set of experiments are described in which a biologically-inspired computer vision system (Serre, Wolf et al. 2005; Serre 2006; Serre, Oliva et al. 2006; Serre, Wolf et al. 2006) designed for visual object recognition was applied to the task of phonetic classification. During learning, the systemprocessed 2-D wideband magnitude spectrograms directly as images, producing a set of 2-D spectrotemporal patch dictionaries at different spectro-temporal positions, orientations, scales, and of varying complexity. During testing, features were computed by comparing the stored patches with patches fromnovel spectrograms. Classification was performed using a regularized least squares classifier (Rifkin, Yeo et al. 2003; Rifkin, Schutte et al. 2007) trained on the features computed by the system. On a 20-class TIMIT vowel classification task, the model features achieved a best result of 58.74% error, compared to 48.57% error using state-of-the-art MFCC-based features trained using the same classifier. This suggests that hierarchical, feed-forward, spectro-temporal patch-based architectures may be useful for phoneticanalysis.",MIT-CSAIL-TR-2007-007; CBCL-266,16 p.; 2265616 bytes; 383591 bytes,application/postscript; application/pdf,en_US,phonetic classification; hierarchical models; regularized least-squares; spectrotemporal patches,Center for Biological and Computational Learning (CBCL),,,,,,,,,http://hdl.handle.net/1721.1/36865,http://hdl.handle.net/1721.1/36865,,,,,,,,,,,,,,,,,,,2007,573
Trevor Darrell,"Lee, John J.",2008-04-07T20:45:20Z,2008-04-07T20:45:20Z,2008-04-07,http://hdl.handle.net/1721.1/41070,,LIBPMK: A Pyramid Match Toolkit,"LIBPMK is a C++ implementation of Grauman and Darrell's pyramid match algorithm. This toolkit provides a flexible framework with which developers can quickly match sets of image features and run experiments. LIBPMK provides functionality for $k$-means and hierarchical clustering, dealing with data sets too large to fit in memory, building multi-resolution histograms, quickly performing pyramid matches, and training and testing support vector machines (SVMs). This report provides a tutorial on how to use the LIBPMK code, and gives the specifications of the LIBPMK API.",MIT-CSAIL-TR-2008-017,217 p.,,,pmk; vgpmk; pyramid match kernel,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,650
Trevor Darrell,"Stiefelhagen, Rainer; Darrell, Trevor; Urtasun, Raquel; Geiger, Andreas",2008-09-29T20:15:10Z,2008-09-29T20:15:10Z,2008-09-26,http://hdl.handle.net/1721.1/42840,MIT-CSAIL-TR-2008-056,Rank Priors for Continuous Non-Linear Dimensionality Reduction,"Non-linear dimensionality reduction methods are powerful techniques to deal with high-dimensional datasets. However, they often are susceptible to local minima and perform poorly when initialized far from the global optimum, even when the intrinsic dimensionality is known a priori. In this work we introduce a prior over the dimensionality of the latent space, and simultaneously optimize both the latent space and its intrinsic dimensionality. Ad-hoc initialization schemes are unnecessary with our approach; we initialize the latent space to the observation space and automatically infer the latent dimensionality using an optimization scheme that drops dimensions in a continuous fashion. We report results applying our prior to various tasks involving probabilistic non-linear dimensionality reduction, and show that our method can outperform graph-based dimensionality reduction techniques as well as previously suggested ad-hoc initialization strategies.",,8 p.,,,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,686
Peter Szolovits,"Rudin, Robert",2008-02-19T13:45:08Z,2008-02-19T13:45:08Z,2008-02-17,http://hdl.handle.net/1721.1/40285,,Making Medical Records More Resilient,"Hurricane Katrina showed that the current methods for handling medicalrecords are minimally resilient to large scale disasters. This research presents a preliminary model for measuring the resilience of medical records systemsagainst public policy goals and uses the model to illuminate the current state of medical record resilience. From this analysis, three recommendations for how to make medical records more resilient are presented.The recommendations are: 1) Federal and state governments should use the preliminary resiliencemodel introduced here as the basis for compliance requirements for electronicmedical record technical architectures. 2) Regional Health Information Organizations (RHIOs) should consideroffering services in disaster management to healthcare organizations. This willhelp RHIOs create sustainable business models. 3) Storage companies should consider developing distributed storagesolutions based on Distributed Hash Table (DHT) technology for medical recordstorage. Distributed storage would alleviate public concerns over privacy withcentralized storage of medical records. Empirical evidence is presenteddemonstrating the performance of DHT technology using a prototype medicalrecord system.",MIT-CSAIL-TR-2008-010,92 p.,,,,Clinical Decision-Making,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,641
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-12-16T19:15:07Z,2008-12-16T19:15:07Z,2008-12-02,http://hdl.handle.net/1721.1/43946,MIT-CSAIL-TR-2008-073,"Resilient  Provision of a Public and/or Private Good,  or: Resilient Auctions of One Good in Unlimited Supply","We present two resilient mechanisms: the first for the provision of a public good, and the second for the provision of a private good. Both mechanisms adopt a knowledge-based benchmark.",,3 p.,,,"Single-good, unlimited-supply auctions; Resilient mechanism design; Provision of a public good; Knowledge-Based Benchmarks",Theory of Computation,,,,,,,,,,http://hdl.handle.net/1721.1/43716,,MIT-CSAIL-TR-2008-072,,,,,,,,,,,,,,,,,2008,701
David Karger,"Nikolova, Evdokia",2008-09-25T19:00:16Z,2008-09-25T19:00:16Z,2008-09-13,http://hdl.handle.net/1721.1/42837,MIT-CSAIL-TR-2008-055,Stochastic Combinatorial Optimization with Risk,"We consider general combinatorial optimization problems that can be formulated as minimizing the weight of a feasible solution wT x over an arbitrary feasible set. For these problems we describe a broad class of corresponding stochastic problems where the weight vector W has independent random components, unknown at the time of solution. A natural and important objective which incorporates risk in this stochastic setting, is to look for a feasible solution whose stochastic weight has a small tail or a small linear combination of mean and standard deviation. Our models can be equivalently reformulated as deterministic nonconvex programs for which no efficient algorithms are known. In this paper, we make progress on these hard problems.  Our results are several efficient general-purpose approximation schemes. They use as a black-box (exact or approximate) the solution to the underlying deterministic combinatorial problem and thus immediately apply to arbitrary combinatorial problems. For example, from an available ?-approximation algorithm to the deterministic problem, we construct a ?(1 + ?)-approximation algorithm that invokes the deterministic algorithm only a logarithmic number of times in the input and polynomial in 1/?, for any desired accuracy level ? > 0. The algorithms are based on a geometric analysis of the curvature and approximability of the nonlinear level sets of the objective functions.",,25 p.,,,"approximation algorithms, combinatorial optimization, stochastic optimization, risk, nonconvex optimization, concave programming",Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,685
Piotr Indyk,"Andoni, Alexandr; Ba, Khanh Do; Indyk, Piotr",2008-05-05T15:45:27Z,2008-05-05T15:45:27Z,2008-05-02,http://hdl.handle.net/1721.1/41514,,Block Heavy Hitters,"e study a natural generalization of the heavy hitters problem in thestreaming context. We term this generalization *block heavy hitters* and define it as follows. We are to stream over a matrix$A$, and report all *rows* that are heavy, where a row is heavy ifits ell_1-norm is at least phi fraction of the ell_1 norm ofthe entire matrix $A$. In comparison, in the standard heavy hittersproblem, we are required to report the matrix *entries* that areheavy. As is common in streaming, we solve the problem approximately:we return all rows with weight at least phi, but also possibly someother rows that have weight no less than (1-eps)phi. To solve theblock heavy hitters problem, we show how to construct a linear sketchof A from which we can recover the heavy rows of A.The block heavy hitters problem has already found applications forother streaming problems. In particular, it is a crucial buildingblock in a streaming algorithm that constructs asmall-size sketch for the Ulam metric, a metric on non-repetitivestrings under the edit (Levenshtein) distance.",MIT-CSAIL-TR-2008-024,3 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,657
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-07-14T19:45:18Z,2008-07-14T19:45:18Z,2008-07,http://hdl.handle.net/1721.1/41878,,Knowledge Benchmarks in Adversarial Mechanism Design (Part I) and Implementation in Surviving Strategies (Part I),"We put forward new benchmarks and solution concepts for Adversarial Mechanism Design, as defined by [MV07.a], and we exemplify them in the case of truly combinatorial auctions.We benchmark the combined performance (the sum of the auction's effciency and revenue)of a truly combinatorial auction against a very relevant but private knowledge of the players: essentially, the maximum revenue that the best informed player could guarantee if he were the seller. (I.e., by offering each other player a subset of the goods for a take-it-or-leave-it price.) We achieve this natural benchmark within a factor of 2, by means of a new and probabilisticauction mechanism, in KNOWLINGLY SURVIVING STRATEGIES. That is, the above performance of our mechanism is guaranteed in any rational play, independent of any possible beliefs of the players. Indeed, our performance guarantee holds for any possible choice of strategies, so long as each player chooses a strategy among those surviving iterated elimination of knowingly dominated strategies.Our mechanism is extremely robust. Namely, its performance guarantees hold even if all but one of the players collude (together or in separate groups) in any possible but reasonable way. Essentially, the only restriction for the collective utility function of a collusive subset S of the players is the following: the collective utility increases when one member of S is allocated asubset of the goods ""individually better"" for him and/or his ""individual price"" is smaller, while the allocations and prices of all other members of S stay the same.Our results improve on the yet unpublished ones of [MV07.b]. The second part of this paper, dealing with a more aggressive benchmark (essentially, the maximum welfare privately known to the players) is forthcoming.",MIT-CSAIL-TR-2008-042,30 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,671
Seth Teller,"Huang, Albert S.; Teller, Seth",2008-06-11T20:15:09Z,2008-06-11T20:15:09Z,2008-06-06,http://hdl.handle.net/1721.1/41860,,Non-Metrical Navigation Through Visual Path Control,"We describe a new method for wide-area, non-metrical robot navigationwhich enables useful, purposeful motion indoors. Our method has twophases: a training phase, in which a human user directs a wheeledrobot with an attached camera through an environment while occasionallysupplying textual place names; and a navigation phase in which theuser specifies goal place names (again as text), and the robot issueslow-level motion control in order to move to the specified place. We show thatdifferences in the visual-field locations and scales of features matched acrosstraining and navigation can be used to construct a simple and robust controlrule that guides the robot onto and along the training motion path.Our method uses an omnidirectional camera, requires approximateintrinsic and extrinsic camera calibration, and is capable of effective motioncontrol within an extended, minimally-prepared building environment floorplan.We give results for deployment within a single building floor with 7 rooms, 6corridor segments, and 15 distinct place names.",MIT-CSAIL-TR-2008-032,8 p.,,,,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,664
Trevor Darrell,"Yeh, Tom; Lee, John J.; Darrell, Trevor",2008-06-11T20:15:30Z,2008-06-11T20:15:30Z,2008-06-10,http://hdl.handle.net/1721.1/41862,,Fast concurrent object classification and localization,"Object localization and classification are important problems incomputer vision. However, in many applications, exhaustive searchover all class labels and image locations is computationallyprohibitive. While several methods have been proposed to makeeither classification or localization more efficient, few havedealt with both tasks simultaneously. This paper proposes anefficient method for concurrent object localization andclassification based on a data-dependent multi-classbranch-and-bound formalism. Existing bag-of-featuresclassification schemes, which can be expressed as weightedcombinations of feature counts can be readily adapted to ourmethod. We present experimental results that demonstrate the meritof our algorithm in terms of classification accuracy, localizationaccuracy, and speed, compared to baseline approaches includingexhaustive search, the ISM method, and single-class branch andbound.",MIT-CSAIL-TR-2008-033,9 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,665
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-10-08T20:15:07Z,2008-10-08T20:15:07Z,2008-10-08,http://hdl.handle.net/1721.1/42893,MIT-CSAIL-TR-2008-059,New Resiliency in  Truly Combinatorial Auctions  (and Implementation in Surviving Strategies),"Following Micali and Valiant [MV07.a], a mechanism is resilient if it achieves its objective without any problem of (1) equilibrium selection and (2) player collusion. To advance resilient mechanism design,We put forward a new meaningful benchmark for the COMBINED social welfare-revenue performance of any mechanism in truly combinatorial auctions.We put forward a NEW notion of implementation, much more general than the ones used so far, which we believe to be of independent interest.We put forward a new RESILIENT mechanism that, by leveraging the knowledge that the players have about each other, guarantees at least one half of our benchmark under a very general collusion model.",,32 p.,,,knowledge benchmarks; implementation in surviving strategies; equilibrium-less implementation; combinatorial auctions; resilient mechanisms; collusion; truly combinatorial auctions,Theory of Computation,,,,,,,,,,,,MIT-CSAIL-TR-2008-041,,,,,,,,,,,,,,,,,2008,691
Tomaso Poggio,"De Mol, Christine; Rosasco, Lorenzo; De Vito, Ernesto",2008-07-24T20:00:33Z,2008-07-24T20:00:33Z,2008-07-24,http://hdl.handle.net/1721.1/41889,MIT-CSAIL-TR-2008-046; CBCL-273,Elastic-Net Regularization in Learning Theory,"Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie [""Regularization and variable selection via the elastic net"" J. R. Stat. Soc. Ser. B, 67(2):301-320, 2005] for the selection of groups of correlated variables. To investigate on the statistical properties of this scheme and in particular on its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combination of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular ""elastic-net representation"" of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in ""Regularization and variable selection via the elastic net"".",,32 p.,,,machine learning; regularization; feature selection,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,675
Leslie Kaelbling,"Gardiol, Natalia H.; Kaelbling, Leslie Pack",2008-08-01T21:30:16Z,2008-08-01T21:30:16Z,2008-07-29,http://hdl.handle.net/1721.1/41920,,Adaptive Envelope MDPs for Relational Equivalence-based Planning,"We describe a method to use structured representations of the environmentâ€™s dynamics to constrain and speed up the planning process. Given a problem domain described in a probabilistic logical description language, we develop an anytime technique that incrementally improves on an initial, partial policy. This partial solution is found by ï¬rst reducing the number of predicates needed to represent a relaxed version of the problem to a minimum, and then dynamically partitioning the action space into a set of equivalence classes with respect to this minimal representation. Our approach uses the envelope MDP framework, which creates a Markov decision process out of a subset of the full state space as de- termined by the initial partial solution. This strategy permits an agent to begin acting within a restricted part of the full state space and to expand its envelope judiciously as resources permit.",MIT-CSAIL-TR-2008-050,17 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,678
Jovan Popovic,"Silva, Marco da; Abe, Yeuhi; Popovic, Jovan",2008-01-16T13:45:10Z,2008-01-16T13:45:10Z,2008-01-15,http://hdl.handle.net/1721.1/40091,,Simulation of Human Motion Data using Short-Horizon Model-Predictive Control,"Many data-driven animation techniques are capable of producing high quality motions of human characters. Few techniques, however, are capable of generating motions that are consistent with physically simulated environments. Physically simulated characters, in contrast, are automatically consistent with the environment, but their motionsare often unnatural because they are difficult to control. We present a model-predictive controller that yields natural motions by guiding simulated humans toward real motion data. During simulation, the predictive component of the controller solves a quadratic program to compute the forces for a short window of time into the future. These forces are then applied by a low-gain proportional-derivative component, which makes minor adjustments until the next planning cycle. The controller is fast enough for interactive systems such as games and training simulations. It requires no precomputation and little manual tuning. The controller is resilient to mismatches between the character dynamics and the input motion, which allows it to track motion capture data even where the real dynamics are not known precisely. The same principled formulation can generate natural walks, runs, and jumps in a number of different physically simulated surroundings.",,,,,Computer Graphics; Three Dimensional Graphics and Realism; Animation,Computer Graphics,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,633
,"Kiezun, Adam; Guo, Philip J.; Jayaraman, Karthick; Ernst, Michael D.",2008-09-25T19:00:06Z,2008-09-25T19:00:06Z,2008-09-10,http://hdl.handle.net/1721.1/42836,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Creation of SQL Injection and Cross-Site Scripting Attacks,"We present a technique for finding security vulnerabilitiesin Web applications. SQL Injection (SQLI) and cross-sitescripting (XSS) attacks are widespread forms of attackin which the attacker crafts the input to the application toaccess or modify user data and execute malicious code. Inthe most serious attacks (called second-order, or persistent,XSS), an attacker can corrupt a database so as to causesubsequent users to execute malicious code.This paper presents an automatic technique for creatinginputs that expose SQLI and XSS vulnerabilities. The techniquegenerates sample inputs, symbolically tracks taintsthrough execution (including through database accesses),and mutates the inputs to produce concrete exploits. Oursis the first analysis of which we are aware that preciselyaddresses second-order XSS attacks.Our technique creates real attack vectors, has few falsepositives, incurs no runtime overhead for the deployed application,works without requiring modification of applicationcode, and handles dynamic programming-languageconstructs. We implemented the technique for PHP, in a toolArdilla. We evaluated Ardilla on five PHP applicationsand found 68 previously unknown vulnerabilities (23 SQLI,33 first-order XSS, and 12 second-order XSS).",,11 p.,,,reliability; dynamic analysis; dynamic taint,,,,,,,Michael Ernst; Program Analysis,,,,,,,,local: MIT-CSAIL-TR-2008-054,,,,,,,,,,,,,,,2008,684
Leslie Kaelbling,"McAllester, David; Milch, Brian; Goodman, Noah D.",2008-05-05T15:45:52Z,2008-05-05T15:45:52Z,2008-05-03,http://hdl.handle.net/1721.1/41516,,Random-World Semantics and Syntactic Independence for Expressive Languages,"We consider three desiderata for a language combining logic and probability: logical expressivity, random-world semantics, and the existence of a useful syntactic condition for probabilistic independence. Achieving these three desiderata simultaneously is nontrivial. Expressivity can be achieved by using a formalism similar to a programming language, but standard approaches to combining programming languages with probabilities sacrifice random-world semantics. Naive approaches to restoring random-world semantics undermine syntactic independence criteria. Our main result is a syntactic independence criterion that holds for a broad class of highly expressive logics under random-world semantics. We explore various examples including Bayesian networks, probabilistic context-free grammars, and an example from Mendelian genetics. Our independence criterion supports a case-factor inference technique that reproduces both variable elimination for BNs and the inside algorithm for PCFGs.",MIT-CSAIL-TR-2008-025,6 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,658
Barbara Liskov,"Vandiver, Benjamin Mead",2008-07-02T06:00:10Z,2008-07-02T06:00:10Z,2008-06-30,http://hdl.handle.net/1721.1/41873,,Detecting and Tolerating Byzantine Faults in Database Systems,"This thesis describes the design, implementation, and evaluation of a replication scheme to handle Byzantine faults in transaction processing database systems. The scheme compares answers from queries and updates on multiple replicas which are off-the-shelf database systems, to provide a single database that is Byzantine fault tolerant. The scheme works when the replicas are homogeneous, but it also allows heterogeneous replication in which replicas come from different vendors. Heterogeneous replicas reduce the impact of bugs and security compromises because they are implemented independently and are thus less likely to suffer correlated failures. A final component of the scheme is a repair mechanism that can correct the state of a faulty replica, ensuring the longevity of the scheme.The main challenge in designing a replication scheme for transaction processingsystems is ensuring that the replicas state does not diverge while allowing a high degree of concurrency. We have developed two novel concurrency control protocols, commit barrier scheduling (CBS) and snapshot epoch scheduling (SES) that provide strong consistency and good performance. The two protocols provide different types of consistency: CBS provides single-copy serializability and SES provides single-copy snapshot isolation. We have implemented both protocols in the context of a replicated SQL database. Our implementation has been tested with production versions of several commercial and open source databases as replicas. Our experiments show a configuration that can tolerate one faulty replica has only a modest performance overhead (about 10-20% for the TPC-C benchmark). Our implementation successfully masks several Byzantine faults observed in practice and we have used it to find a new bug in MySQL.",MIT-CSAIL-TR-2008-040,174 p.,,,,Programming Methodology,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,670
Brian Williams,"Ono, Masahiro; Williams, Brian C.",2008-03-06T14:30:20Z,2008-03-06T14:30:20Z,2008-03-06,http://hdl.handle.net/1721.1/40804,,Two-stage Optimization Approach to Robust Model Predictive Control with a Joint Chance Constraint,"When controlling dynamic systems such as mobile robots in uncertain environments, there is a trade off between risk and reward. For example, a race car can turn a corner faster by taking a more challenging path. This paper proposes a new approach to planning a control sequence with guaranteed risk bound. Given a stochastic dynamic model, the problem is to find a control sequence that optimizes a performance metric, while satisfying chance constraints i.e. constraints on the upper bound of the probability of failure. We propose a two-stage optimization approach, with the upper stage optimizing the risk allocation and the lower stage calculating the optimal control sequence that maximizes the reward. In general, upper-stage is a non-convex optimization problem, which is hard to solve. We develop a new iterative algorithm for this stage that efficiently computes the risk allocation with a small penalty to optimality. The algorithm is implemented and tested on the autonomous underwater vehicle (AUV) depth planning problem, which demonstrates the substantial improvement in computation cost and suboptimality compared to the prior arts.",MIT-CSAIL-TR-2008-014,8 p.,,,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,646
Patrick Winston,"Bonawitz, Keith A",2008-07-23T19:30:16Z,2008-07-23T19:30:16Z,2008-07-23,http://hdl.handle.net/1721.1/41887,,Composable Probabilistic Inference with Blaise,"Probabilistic inference provides a unified, systematic framework for specifying and solving these problems. Recent work has demonstrated the great value of probabilistic models defined over complex, structured domains. However, our ability to imagine probabilistic models has far outstripped our ability to programmatically manipulate them and to effectively implement inference, limiting the complexity of the problems that we can solve in practice.This thesis presents Blaise, a novel framework for composable probabilistic modeling and inference, designed to address these limitations. Blaise has three components: * The Blaise State-Density-Kernel (SDK) graphical modeling language that generalizes factor graphs by: (1) explicitly representing inference algorithms (and their locality) using a new type of graph node, (2) representing hierarchical composition and repeated substructures in the state space, the interest distribution, and the inference procedure, and (3) permitting the structure of the model to change during algorithm execution. * A suite of SDK graph transformations that may be used to extend a model (e.g. to construct a mixture model from a model of a mixture component), or to make inference more effective (e.g. by automatically constructing a parallel tempered version of an algorithm or by exploiting conjugacy in a model). * The Blaise Virtual Machine, a runtime environment that can efficiently execute the stochastic automata represented by Blaise SDK graphs. Blaise encourages the construction of sophisticated models by composing simpler models, allowing the designer to implement and verify small portions of the model and inference method, and to reuse model components from one task to another. Blaise decouples the implementation of the inference algorithm from the specification of the interest distribution, even in cases (such as Gibbs sampling) where the shape of the interest distribution guides the inference. This gives modelers the freedom to explore alternate models without slow, error-prone reimplementation. The compositional nature of Blaise enables novel reinterpretations of advanced Monte Carlo inference techniques (such as parallel tempering) as simple transformations of Blaise SDK graphs.In this thesis, I describe each of the components of the Blaise modeling framework, as well as validating the Blaise framework by highlighting a variety of contemporary sophisticated models that have been developed by the Blaise user community. I also present several surprising findings stemming from the Blaise modeling framework, including that an Infinite Relational Model can be built using exactly the same inference methods as a simple mixture model, that constructing a parallel tempered inference algorithm should be a point-and-click/one-line-of-code operation, and that Markov chain Monte Carlo for probabilistic models with complicated long-distance dependencies, such as a stochastic version of Scheme, can be managed using standard Blaise mechanisms.",MIT-CSAIL-TR-2008-044,190 p.,,,Bayesian; MCMC,Genesis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,674
Karen Sollins,"Li, Ji",2008-06-11T20:15:18Z,2008-06-11T20:15:18Z,2008-06-11,http://hdl.handle.net/1721.1/41861,,Agent Organization in the Knowledge Plane,"In designing and building a network like the Internet, we continue to face the problems of scale and distribution. With the dramatic expansion in scale and heterogeneity of the Internet, network management has become an increasingly difficult task. Furthermore, network applications often need to maintain efficient organization among the participants by collecting information from the underlying networks. Such individual information collection activities lead to duplicate efforts and contention for network resources.The Knowledge Plane (KP) is a new common construct that provides knowledge and expertise to meet the functional, policy and scaling requirements of network management, as well as to create synergy and exploit commonality among many network applications. To achieve these goals, we face many challenging problems, including widely distributed data collection, efficient processing of that data, wide availability of the expertise, etc.In this thesis, to provide better support for network management and large-scale network applications, I propose a knowledge plane architecture that consists of a network knowledge plane (NetKP) at the network layer, and on top of it, multiple specialized KPs (spec-KPs). The NetKP organizes agents to provide valuable knowledge and facilities about the Internet to the spec-KPs. Each spec-KP is specialized in its own area of interest. In both the NetKP and the spec-KPs, agents are organized into regions based on different sets of constraints. I focus on two key design issues in the NetKP: (1) a regionbased architecture for agent organization, in which I design an efficient and non-intrusive organization among regions that combines network topology and a distributed hash table; (2) request and knowledge dissemination, in which I design a robust and efficient broadcast and aggregation mechanism using a tree structure among regions. In the spec-KPs, I build two examples: experiment management on the PlanetLab testbed and distributed intrusion detection on the DETER testbed. The experiment results suggest a common approach driven by the design principles of the Internet and more specialized constraints can derive productive organization for network management and applications.",MIT-CSAIL-TR-2008-034,191 p.,,,broadcast; region; knowledge plane; network pnowledge plane; agent; intrusion detection; specialized knowledge plane; distributed hash table; aggregation,Advanced Network Architecture,,,,,,,,,,,,; Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,,,,,,,,,,,,,,2008,666
Joshua Tenenbaum,"Tenenbaum, Joshua B.; Jonas, Eric M.; Mansinghka, Vikash K.",2008-11-24T16:30:26Z,2008-11-24T16:30:26Z,2008-11-24,http://hdl.handle.net/1721.1/43712,MIT-CSAIL-TR-2008-069,Stochastic Digital Circuits for Probabilistic Inference,"We introduce combinational stochastic logic, an abstraction that generalizes deterministic digital circuit design (based on Boolean logic gates) to the probabilistic setting. We show how this logic can be combined with techniques from contemporary digital design to generate stateless and stateful circuits for exact and approximate sampling from a range of probability distributions. We focus on Markov chain Monte Carlo algorithms for Markov random fields, using massively parallel circuits. We implement these circuits on commodity reconfigurable logic and estimate the resulting performance in time, space and price. Using our approach, these simple and general algorithms could be affordably run for thousands of iterations on models with hundreds of thousands of variables in real time.",,10 p.,,,cognitive science; robustness; Bayesian inference; artificial intelligence,Computational Cognitive Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,699
Anant Agarwal,"Agarwal, Anant; Wentzlaff, David",2008-10-08T23:15:04Z,2008-10-08T23:15:04Z,2008-10-08,http://hdl.handle.net/1721.1/42894,MIT-CSAIL-TR-2008-060,The Case for a Factored Operating System (fos),"The next decade will afford us computer chips with 1,000 - 10,000 cores on a single piece of silicon. Contemporary operating systems have been designed to operate on a single core or small number of cores and hence are not well suited to manage and provide operating system services at such large scale. Managing 10,000 cores is so fundamentally different from managing two cores that the traditional evolutionary approach of operating system optimization will cease to work. The fundamental design of operating systems and operating system data structures must be rethought. This work begins by documenting the scalability problems of contemporary operating systems. These studies are used to motivate the design of a factored operating system (fos). fos is a new operating system targeting 1000+ core multicore systems where space sharing replaces traditional time sharing to increase scalability. fos is built as a collection of Internet inspired services. Each operating system service is factored into a fleet of communicating servers which in aggregate implement a system service. These servers are designed much in the way that distributed Internet services are designed, but instead of providing high level Internet services, these servers provide traditional kernel services and manage traditional kernel data structures in a factored, spatially distributed manner. The servers are bound to distinct processing cores and by doing so do not fight with end user applications for implicit resources such as TLBs and caches. Also, spatial distribution of these OS services facilitates locality as many operations only need to communicate with the nearest server for a given service.",,12 p.,,,multicore; operating system design; manycore,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,690
Silvio Micali,"Micali, Silvio; Chen, Jing",2008-12-17T21:00:03Z,2008-12-17T21:00:03Z,2008-12-17,http://hdl.handle.net/1721.1/43947,MIT-CSAIL-TR-2008-074,Resilient Auctions of One Good in Limited Supply,We present various resilient auction mechanisms for a good in limited supply. Our mechanisms achieve both player-knowledge and aggregated player-knowledge benchmarks.,,3 p.,,,Resilient mechanism design; Knowledge-Based Benchmarks; Player-Knowledge Benchmarks; Aggregated Knowledge-Based Benchmarks; Aggregated Player-Knowledge Benchmarks,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,703
Silvio Micali,"Micali, Silvio; Chen, Jing",2008-12-03T16:15:09Z,2008-12-03T16:15:09Z,2008-10-08,http://hdl.handle.net/1721.1/43715,MIT-CSAIL-TR-2008-071,Resilient Knowledge-Based  Mechanisms  For Truly Combinatorial Auctions (And Implementation in Surviving Strategies),"We put forward a new mechanism achieving a high benchmark for (both revenue and) the sum of revenue and efficiency in truly combinatorial auctions. Notably, our mechanism guarantees its performance (1) in a very adversarial collusion model; (2) for any profile of strategies surviving the iterated elimination of dominated strategies; and (3) by leveraging the knowledge that the players have about each other (in a non-Bayesian setting).Our mechanism also is computationally efficient, and preserves the players' privacy to an unusual extent.",,18 p.,,,Implementation in surviving strategies; Resilient Mechanism Design; Privacy-preserving mechanisms; Equilibrium-less mechanism design; Knowledge benchmarks,Theory of Computation,,,,,,Silvio Micali; Theory of Computation,,,,,,MIT-CSAIL-TR-2008-059,,,,,,,,,,,,,,,,,2008,689
Michael Ernst,"Ernst, Michael D.; Marrero, John; Dig, Danny",2008-09-30T19:15:06Z,2008-09-30T19:15:06Z,2008-09-30,http://hdl.handle.net/1721.1/42841,MIT-CSAIL-TR-2008-057,Refactoring Sequential Java Code for Concurrency via Concurrent Libraries,"Parallelizing existing sequential programs to run efficiently on multicores is hard. The Java 5 packagejava.util.concurrent (j.u.c.) supports writing concurrent programs: much of the complexity of writing threads-safe and scalable programs is hidden in the library.  To use this package, programmers still need to reengineer existing code. This is tedious because it requires changing many lines of code, is error-prone because programmers can use the wrong APIs, and is omission-prone because programmers can miss opportunities to use the enhanced APIs.  This paper presents our tool, CONCURRENCER, which enables programmers to refactor sequential code into parallel code that uses j.u.c. concurrent utilities. CONCURRENCER does not require any program annotations, although the transformations are very involved: they span multiple program statements and use custom program analysis.  A find-and-replace tool can not perform such transformations.  Empirical evaluation shows that CONCURRENCER refactors code effectively: CONCURRENCER correctly identifies and applies transformations that some open-source developers overlooked, and the converted code exhibits good speedup.",,12 p.,,,program transformations; concurrency; refactoring; library,Program Analysis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2008,687
Chris Terman,"Carli, Roberto",2008-07-02T06:00:36Z,2008-07-02T06:00:36Z,2008-06-16,http://hdl.handle.net/1721.1/41874,,Flexible MIPS Soft Processor Architecture,"The flexible MIPS soft processor architecture borrows selected technologies from high-performance computing to deliver a modular, highly customizable CPU targeted towards FPGA implementations for embedded systems; the objective is to provide a more flexible architectural alternative to coprocessor-based solutions. The processor performs out-of-order execution on parallel functional units, it delivers in-order instruction commit and it is compatible with the MIPS-1 Instruction Set Architecture. Amongst many available options, the user can introduce custom instructions and matching functional units; modify existing units; change the pipelining depth within functional units to any fixed or variable value; customize instruction definitions in terms of operands, control signals and register file interaction; insert multiple redundant functional units for improved performance. The flexibility provided by the architecture allows the user to expand the processor functionality to implement instructions of coprocessor-level complexity through additional functional units. The processor design was implemented and simulated on two FPGA platforms, tested on multiple applications, and compared to three commercially available soft processor solutions in terms of features, area, clock frequency and benchmark performance.",MIT-CSAIL-TR-2008-036,49 p.,,,,Computer Architecture,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,667
Tomaso Poggio,"Chikkerur, Sharat; Poggio, Tomaso; Serre, Thomas",2009-10-06T22:45:05Z,2009-10-06T22:45:05Z,2009-10-02,http://hdl.handle.net/1721.1/49415,CBCL-279; MIT-CSAIL-TR-2009-046,Attentive processing improves object recognition,"The human visual system can recognize several thousand object categories irrespective of their position and size. This combination of selectivity and invariance is built up gradually across several stages of visual processing. However, the recognition of multiple objects in cluttered visual scenes presents a difficult problem for human as well as machine vision systems. The human visual system has evolved to perform two stages of visual processing: a pre-attentive parallel processing stage, in which the entire visual field is processed at once and a slow serial attentive processing stage, in which aregion of interest in an input image is selected for ""specialized"" analysis by an attentional spotlight. We argue that this strategy evolved to overcome the limitation of purely feed forward processing in the presence of clutter and crowding. Using a Bayesian model of attention along with a hierarchical model of feed forward recognition on a data set of real world images, we show that this two stage attentive processing can improve recognition in cluttered and crowded conditions.",,12 p.,,,,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,751
Nancy Lynch,"Oshman, Rotem; Lynch, Nancy; Kuhn, Fabian",2009-11-12T19:30:03Z,2009-11-12T19:30:03Z,2009-11-10,http://hdl.handle.net/1721.1/49814,MIT-CSAIL-TR-2009-058,Distributed Computation in Dynamic Networks,"In this report we investigate distributed computation in dynamic networks in which the network topology changes from round to round. We consider a worst-case model in which the communication links for each round are chosen by an adversary, and nodes do not know who their neighbors for the current round are before they broadcast their messages. The model is intended to capture mobile networks and wireless networks, in which mobility and interference render communication unpredictable. The model allows the study of the fundamental computation power of dynamic networks. In particular, it captures mobile networks and wireless networks, in which mobility and interference render communication unpredictable. In contrast to much of the existing work on dynamic networks, we do not assume that the network eventually stops changing; we require correctness and termination even in networks that change continually. We introduce a stability property called T-interval connectivity (for T >= 1), which stipulates that for every T consecutive rounds there exists a stable connected spanning subgraph. For T = 1 this means that the graph is connected in every round, but changes arbitrarily between rounds. Algorithms for the dynamic graph model must cope with these unceasing changes. We show that in 1-interval connected graphs it is possible for nodes to determine the size of the network and compute any computable function of their initial inputs in O(n^2) rounds using messages of size O(log n + d), where d is the size of the input to a single node. Further, if the graph is T-interval connected for T > 1, the computation can be sped up by a factor of T, and any function can be computed in O(n + n^2 / T) rounds using messages of size O(log n + d). We also give two lower bounds on the gossip problem, which requires the nodes to disseminate k pieces of information to all the nodes in the network. We show an Omega(n log k) bound on gossip in 1-interval connected graphs against centralized algorithms, and an Omega(n + nk / T) bound on exchanging k pieces of information in T-interval connected graphs for a restricted class of randomized distributed algorithms. The T-interval connected dynamic graph model is a novel model, which we believe opens new avenues for research in the theory of distributed computing in wireless, mobile and dynamic networks.",,40 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,762
Tomaso Poggio,"Chikkerur, Sharat; Serre, Thomas; Poggio, Tomaso",2009-10-06T22:45:10Z,2009-10-06T22:45:10Z,2009-10-03,http://hdl.handle.net/1721.1/49416,CBCL-280; MIT-CSAIL-TR-2009-047,A Bayesian inference theory of attention: neuroscience and algorithms,"The past four decades of research in visual neuroscience has generated a large and disparate body of literature on the role of attention [Itti et al., 2005]. Although several models have been developed to describe specific properties of attention, a theoretical framework that explains the computational role of attention and is consistent with all known effects is still needed. Recently, several authors have suggested that visual perception can be interpreted as a Bayesian inference process [Rao et al., 2002, Knill and Richards, 1996, Lee and Mumford, 2003]. Within this framework, topdown priors via cortical feedback help disambiguate noisy bottom-up sensory input signals. Building on earlier work by Rao [2005], we show that this Bayesian inference proposal can be extended to explain the role and predict the main properties of attention: namely to facilitate the recognition of objects in clutter. Visual recognition proceeds by estimating the posterior probabilities for objects and their locations within an image via an exchange of messages between ventral and parietal areas of the visual cortex. Within this framework, spatial attention is used to reduce the uncertainty in feature information; feature-based attention is used to reduce the uncertainty in location information. In conjunction, they are used to recognize objects in clutter. Here, we find that several key attentional phenomena such such as pop-out, multiplicative modulation and change in contrast response emerge naturally as a property of the network. We explain the idea in three stages. We start with developing a simplified model of attention in the brain identifying the primary areas involved and their interconnections. Secondly, we propose a Bayesian network where each node has direct neural correlates within our simplified biological model. Finally, we elucidate the properties of the resulting model, showing that the predictions are consistent with physiological and behavioral evidence.",,18 p.,,,,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,752
Michael Ernst,"Ernst, Michael D.; Kiezun, Adam; Ganesh, Vijay; Guo, Philip J.; Hooimeijer, Pieter",2009-02-04T19:00:04Z,2009-02-04T19:00:04Z,2009-02-04,http://hdl.handle.net/1721.1/44584,MIT-CSAIL-TR-2009-004,HAMPI: A Solver for String Constraints,"Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable tools. The increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few, if any, effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis techniques for string-manipulating programs. We designed and implemented Hampi, a solver for string constraints over bounded string variables. Hampi constraints express membership in regular languages and bounded context-free languages. Hampi constraints may contain context-free-language definitions, regular-language definitions and operations, and the membership predicate. Given a set of constraints, Hampi outputs a string that satisfies all the constraints, or reports that the constraints are unsatisfiable. Hampi is expressive and efficient, and can be successfully applied to testing and analysis of real programs. Our experiments use Hampi in: static and dynamic analyses for finding SQL injection vulnerabilities in Web applications; automated bug finding in C programs using systematic testing; and compare Hampi with another string solver. Hampi's source code, documentation, and the experimental data are available at http://people.csail.mit.edu/akiezun/hampi.",,11 p.,,,Constraint solvers; Automated testing,Program Analysis,,,,,,,,,,http://people.csail.mit.edu/akiezun/hampi,,,,,,,,,,,,,,,,,,,2009,707
Martin Rinard,"Ganesh, Vijay; Singh, Rishabh; Near, Joseph P.; Rinard, Martin",2009-08-26T22:00:06Z,2009-08-26T22:00:06Z,2009-08-26,http://hdl.handle.net/1721.1/46691,MIT-CSAIL-TR-2009-039,AvatarSAT: An Auto-tuning Boolean SAT Solver,"We present AvatarSAT, a SAT solver that uses machine-learning classifiers to automatically tune the heuristics of an off-the-shelf SAT solver on a per-instance basis. The classifiers use features of both the input and conflict clauses to select parameter settings for the solver's tunable heuristics. On a randomly selected set of SAT problems chosen from the 2007 and 2008 SAT competitions, AvatarSAT is, on average, over two times faster than MiniSAT based on the geometric mean speedup measure and 50% faster based on the arithmeticmean speedup measure. Moreover, AvatarSAT is hundreds to thousands of times faster than MiniSAT on many hard SAT instances and is never more than twenty times slower than MiniSAT on any SAT instance.",,7 p.,,,self-tuning; machine learning; SAT solvers,Computer Architecture,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2009,743
Anant Agarwal,"Agarwal, Anant; Santambrogio, Marco D.; Wingate, David; Eastep, Jonathan",2009-11-09T21:30:03Z,2009-11-09T21:30:03Z,2009-11-09,http://hdl.handle.net/1721.1/49808,MIT-CSAIL-TR-2009-055,Smartlocks: Self-Aware Synchronization through Lock Acquisition Scheduling,"As multicore processors become increasingly prevalent, system complexity is skyrocketing. The advent of the asymmetric multicore compounds this -- it is no longer practical for an average programmer to balance the system constraints associated with today's multicores and worry about new problems like asymmetric partitioning and thread interference. Adaptive, or self-aware, computing has been proposed as one method to help application and system programmers confront this complexity. These systems take some of the burden off of programmers by monitoring themselves and optimizing or adapting to meet their goals. This paper introduces an open-source self-aware synchronization library for multicores and asymmetric multicores called Smartlocks. Smartlocks is a spin-lock library that adapts its internal implementation during execution using heuristics and machine learning to optimize toward a user-defined goal, which may relate to performance, power, or other problem-specific criteria. Smartlocks builds upon adaptation techniques from prior work like reactive locks, but introduces a novel form of adaptation designed for asymmetric multicores that we term lock acquisition scheduling. Lock acquisition scheduling is optimizing which waiter will get the lock next for the best long-term effect when multiple threads (or processes) are spinning for a lock. Our results demonstrate empirically that lock scheduling is important for asymmetric multicores and that Smartlocks significantly outperform conventional and reactive locks for asymmetries like dynamic variations in processor clock frequencies caused by thermal throttling events.",,16 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,760
Nancy Lynch,"Locher, Thomas; Kuhn, Fabian; Oshman, Rotem",2009-05-29T16:00:04Z,2009-05-29T16:00:04Z,2009-05-29,http://hdl.handle.net/1721.1/45549,MIT-CSAIL-TR-2009-022,Gradient Clock Synchronization in Dynamic Networks,"Over the last years, large-scale decentralized computer networks such as peer-to-peer and mobile ad hoc networks have become increasingly prevalent. The topologies of many of these networks are often highly dynamic. This is especially true for ad hoc networks formed by mobile wireless devices. In this paper, we study the fundamental problem of clock synchronization in dynamic networks. We show that there is an inherent trade-off between the skew S guaranteed along sufficiently old links and the time needed to guarantee a small skew along new links. For any sufficiently large initial skew on a new link, there are executions in which the time required to reduce the skew on the link to O(S) is at least Omega(n/S). We show that this bound is tight for moderately small values of S. Assuming a fixed set of n nodes and an arbitrary pattern of edge insertions and removals, a weak dynamic connectivity requirement suffices to prove the following results. We present an algorithm that always maintains a skew of O(n) between any two nodes in the network. For a parameter S = Omega(sqrt{rho n}), where rho is the maximum hardware clock drift, it is further guaranteed that if a communication link between two nodes u, v persists in the network for at least Omega(n/S) time, the clock skew between u and v is reduced to no more than O(S).",,33 p.,,,time synchronization; distributed algorithms; lower bound,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,726
Srini Devadas,"Devadas, Srinivas; Agarwal, Anant; Hoffmann, Henry",2009-06-16T17:15:03Z,2009-06-16T17:15:03Z,2009-06-16,http://hdl.handle.net/1721.1/45567,MIT-CSAIL-TR-2009-026,Partitioning Strategies for Concurrent Programming,"This work presents four partitioning strategies, or patterns, useful for decomposing a serial application into multiple concurrently executing parts. These partitioning strategies augment the commonly used task and data parallel design patterns by recognizing that applications are spatiotemporal in nature. Therefore, data and instruction decomposition are further distinguished by whether the partitioning is done in the spatial or in temporal dimension. Thus, this work describes four decomposition strategies: spatial data partitioning (SDP), temporal data partitioning (TDP), spatial instruction partitioning (SIP), and temporal instruction partitioning (TIP), while cataloging the benefits and drawbacks of each. In addition, the practical use of these strategies is demonstrated through a case study in which they are applied to implement several different parallelizations of a multicore H.264 encoder for HD video. This case study illustrates both the application of the patterns and their effects on the performance of the encoder.",,16 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,731
Polina Golland,"Golland, Polina; Lashkari, Danial",2009-11-03T20:30:11Z,2009-11-03T20:30:11Z,2009-11-03,http://hdl.handle.net/1721.1/49526,MIT-CSAIL-TR-2009-054,Co-Clustering with Generative Models,"In this paper, we present a generative model for co-clustering and develop algorithms based on the mean field approximation for the corresponding modeling problem. These algorithms can be viewed as generalizations of the traditional model-based clustering; they extend hard co-clustering algorithms such as Bregman co-clustering to include soft assignments. We show empirically that these model-based algorithms offer better performance than their hard-assignment counterparts, especially with increasing problem complexity.",,9 p.,,,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,758
Martin Rinard,"Hoffmann, Henry; Misailovic, Sasa; Sidiroglou, Stelios; Agarwal, Anant; Rinard, Martin",2009-09-04T15:45:14Z,2009-09-04T15:45:14Z,2009-09-03,http://hdl.handle.net/1721.1/46709,MIT-CSAIL-TR-2009-042,"Using Code Perforation to Improve Performance, Reduce Energy Consumption, and Respond to Failures","Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are designed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. We present a new general technique, code perforation, for automatically augmenting existing computations with the capability of trading off accuracy in return for performance. In contrast to existing approaches, which typically require the manual development of new algorithms, our implemented SpeedPress compiler can automatically apply code perforation to existing computations with no developer intervention whatsoever. The result is a transformed computation that can respond almost immediately to a range of increased performancedemands while keeping any resulting output distortion within acceptable user-defined bounds. We have used SpeedPress to automatically apply code perforation to applications from the PARSEC benchmark suite. The results show that the transformed applications can run as much as two to three times faster than the original applications while distorting the output by less than 10%. Because the transformed applications can operate successfully at many points in the performance/accuracy tradeoff space, they can (dynamically and on demand) navigate the tradeoff space to either maximize performance subject to a given accuracy constraint, or maximize accuracy subject to a given performance constraint. We also demonstrate the SpeedGuard runtime system which uses code perforation to enable applications to automatically adapt to challenging execution environments such as multicore machines that suffer core failures or machines that dynamically adjust the clock speed to reduce power consumption or to protect the machine from overheating.",,19 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,"Hoffmann, Henry; Misailovic, Sasa; Sidiroglou, Stelios; Agarwal, Anant; Rinard, Martin",,,,,,,,,,,2009,746
Rob Miller,"Miller, Rob; Karger, David; Marcus, Adam; Bernstein, Michael",2009-10-13T12:31:44Z,2009-10-13T12:31:44Z,2009-10-07,http://hdl.handle.net/1721.1/49426,MIT-CSAIL-TR-2009-048,Understanding and Supporting Directed Content Sharing on the Web,"To find interesting, personally relevant web content, we often rely on friends and colleagues to pass links along as they encounter them. In this paper, we study and augment link-sharing via e-mail, the most popular means of sharing web content today. Armed with survey data indicating that active sharers of novel web content are often those that actively seek it out, we present FeedMe, a plug-in for Google Reader that makes directed sharing of content a more salient part of the user experience. Our survey research indicates that sharing is moderated by concern about relevancy to the recipient, a desire to send only novel content to the recipient, and the effort required to share. FeedMe allays these concerns by recommending friends who may be interested in seeing the content, providing information on what the recipient has seen and how many emails they have received recently, and giving recipients the opportunity to provide lightweight feedback when they appreciate shared content. FeedMe introduces a novel design space for mixed-initiative social recommenders: friends who know the user voluntarily vet the material on the userâ  s behalf. We present a two week field experiment (N=60) demonstrating that FeedMeâ  s recommendations and social awareness features made it easier and more enjoyable to share content that recipients appreciated and would not have found otherwise.",,10 p.,,,friendsourcing; Social link sharing; blogs; RSS,User Interface Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,753
Seth Teller,"Moore, David; Olson, Edwin; Huang, Albert",2009-09-04T15:45:08Z,2009-09-04T15:45:08Z,2009-09-02,http://hdl.handle.net/1721.1/46708,MIT-CSAIL-TR-2009-041,Lightweight Communications and Marshalling for Low-Latency Interprocess Communication,"We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, targeted at real-time robotics applications. LCM is comprised of several components: a data type specification language, a message passing system, logging/playback tools, and real-time analysis tools. LCM provides a platform- and language-independent type specification language. These specifications can be compiled into platform and language specific implementations, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. Messages can be transmitted between different processes using LCM's message-passing system, which implements a publish/subscribe model. LCM's implementation is notable in providing low-latency messaging and eliminating the need for a central communications ""hub"". This architecture makes it easy to mix simulated, recorded, and live data sources. A number of logging, playback, and traffic inspection tools simplify common development and debugging tasks. LCM is targeted at robotics and other real-time systems where low latency is critical; its messaging model permits dropping messages in order to minimize the latency of new messages. In this paper, we explain LCM's design, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots.",,15 p.,,,message passing; interprocess communication; robotics middleware; real-time systems,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,745
,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2009-09-17T18:30:10Z,2009-09-17T18:30:10Z,2009-09-12,http://hdl.handle.net/1721.1/46722,MIT-CSAIL-TR-2009-043,Finding aircraft collision-avoidance strategies using policy search methods,A progress report describing the application of policy gradient and policy search by dynamic programming methods to an aircraft collision avoidance problem inspired by the requirements of next-generation TCAS.,,11 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2009,748
Ted Adelson,"Pfister, Hanspeter; Freeman, William T.; Avidan, Shai; Dale, Kevin; Johnson, Micah K.; Matusik, Wojciech",2009-07-21T21:30:34Z,2009-07-21T21:30:34Z,2009-07-15,http://hdl.handle.net/1721.1/46335,MIT-CSAIL-TR-2009-034,CG2Real: Improving the Realism of Computer Generated Images using a Large Collection of Photographs,"Computer Graphics (CG) has achieved a high level of realism, producing strikingly vivid images. This realism, however, comes at the cost of long and often expensive manual modeling, and most often humans can still distinguish between CG images and real images. We present a novel method to make CG images look more realistic that is simple and accessible to novice users. Our system uses a large collection of photographs gathered from online repositories. Given a CG image, we retrieve a small number of real images with similar global structure. We identify corresponding regions between the CG and real images using a novel mean-shift cosegmentation algorithm. The user can then automatically transfer color, tone, and texture from matching regions to the CG image. Our system only uses image processing operations and does not require a 3D model of the scene, making it fast and easy to integrate into digital content creation workflows. Results of a user study show that our improved CG images appear more realistic than the originals.",,10 p.,,,image databases; image synthesis; computer graphics,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,738
Nancy Lynch,"Lynch, Nancy; Ley-Wild, Ruy; Kuhn, Fabian; Cornejo, Alejandro",2009-06-17T21:15:03Z,2009-06-17T21:15:03Z,2009-06-17,http://hdl.handle.net/1721.1/45568,MIT-CSAIL-TR-2009-027,Keeping Mobile Robots Connected,"Designing robust algorithms for mobile agents with reliable communication is difficult due to the distributed nature of computation, in mobile ad hoc networks (MANETs) the matter is exacerbated by the need to ensure connectivity. Existing distributed algorithms provide coordination but typically assume connectivity is ensured by other means. We present a connectivity service that encapsulates an arbitrary motion planner and can refine any plan to preserve connectivity (the graph of agents remains connected) and ensure progress (the agents advance towards their goal). The service is realized by a distributed algorithm that is modular in that it makes no assumptions of the motion-planning mechanism except the ability for an agent to query its position and intended goal position, local in that it uses 1-hop broadcast to communicate with nearby agents but doesn't need any network routing infrastructure, and \emph{oblivious} in that it does not depend on previous computations. We prove the progress of the algorithm in one round is at least Omega(min(d,r)), where d is the minimum distance between an agent and its target and r is the communication radius. We characterize the worst case configuration and show that when d >= r this bound is tight and the algorithm is optimal, since no algorithm can guarantee greater progress. Finally we show all agents get epsilon-close to their targets within O(D_0/r+n^2/epsilon) rounds where n is the number of agents and D_0 is the initial distance to the targets.",,21 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,732
Tomaso Poggio,"Poggio, Tomaso; Rosasco, Lorenzo; Wibisono, Andre",2009-12-01T21:15:05Z,2009-12-01T21:15:05Z,2009-12-01,http://hdl.handle.net/1721.1/49868,CBCL-284; MIT-CSAIL-TR-2009-060,Sufficient Conditions for Uniform Stability of Regularization Algorithms,"In this paper, we study the stability and generalization properties of penalized empirical-risk minimization algorithms. We propose a set of properties of the penalty term that is sufficient to ensure uniform ?-stability: we show that if the penalty function satisfies a suitable convexity property, then the induced regularization algorithm is uniformly ?-stable. In particular, our results imply that regularization algorithms with penalty functions which are strongly convex on bounded domains are ?-stable. In view of the results in [3], uniform stability implies generalization, and moreover, consistency results can be easily obtained. We apply our results to show that â  p regularization for 1 < p <= 2 and elastic-net regularization are uniformly ?-stable, and therefore generalize.",,16 p.,,,artificial intelligence; theory; computation; learning,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,765
Tomaso Poggio,"Yu, Xinlin; Steele, Andrew D.; Khilnani, Vinita; Garrote, Estibaliz; Jhuang, Hueihan; Serre, Thomas; Poggio, Tomaso",2009-11-03T20:30:21Z,2009-11-03T20:30:21Z,2009-10-26,http://hdl.handle.net/1721.1/49527,CBCL-283; MIT-CSAIL-TR-2009-052,Automated home-cage behavioral phenotyping of mice,"We describe a trainable computer vision system enabling the automated analysis of complex mouse behaviors. We provide software and a very large manually annotated video database used for training and testing the system. Our system outperforms leading commercial software and performs on par with human scoring, as measured from the ground-truth manual annotations of thousands of clips of freely behaving animals. We show that the home-cage behavior profiles provided by the system is sufficient to accurately predict the strain identity of individual animals in the case of two standard inbred and two non-standard mouse strains. Our software should complement existing sensor-based automated approaches and help develop an adaptable, comprehensive, high-throughput, fine-grained, automated analysis of rodent behavior.",,27 p.,,,animal monitoring; rodents,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-Noncommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2009,757
Dina Katabi,"Katabi, Dina; Rahul, Hariharan; Jakubczak, Szymon",2009-02-09T16:30:14Z,2009-02-09T16:30:14Z,2009-02-07,http://hdl.handle.net/1721.1/44585,MIT-CSAIL-TR-2009-005,SoftCast: One Video to Serve All Wireless Receivers,"The main challenge in wireless video multicast is to scalably serve multiple receivers who have different channel characteristics. Current wireless transmission schemes, however, cannot support smooth degradation. Specifically, each packet is transmitted at a particular bitrate and is decodable only by receivers that support the chosen bitrate. Broadcasting a video stream to all receivers requires transmitting at the lowest bitrate, and hence reduces everyone to the performance of the worst receiver in the multicast group.This paper introduces SoftCast, an alternative design for wireless video multicast, in which a sender broadcasts a single stream and each receiver watches a video quality that matches its channel quality. SoftCast achieves this by making the magnitude of the transmitted signal proportional to the pixel value. Hence, channel noise directly translates to a small perturbation in pixel values, allowing graceful degradation with increasing noise. SoftCast introduces a novel power allocation scheme that allows the transmission of real-valued video signals in a compact and resilient manner. We implement SoftCast in the WARP radio platform. Our results show that SoftCast improves the average video quality across multicast receivers by 3-7dB over the current approach. Further, it stays competitive with the current approach even for regular unicast.",,14 p.,,,Wireless networks; Multicast; Wireless video; Video coding,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,708
"Robotics, Vision & Sensor Networks; John Leonard","Benjamin, Michael R.; Newman, Paul M.; Schmidt, Henrik; Leonard, John J.",2009-08-20T18:30:06Z,2009-08-20T18:30:06Z,2009-08-20,http://hdl.handle.net/1721.1/46361,MIT-CSAIL-TR-2009-037,Extending a MOOS-IvP Autonomy System and Users Guide to the IvPBuild Toolbox,"This document describes how to extend the suite of MOOS applications and IvP Helm behaviors distributed with the MOOS-IvP software bundle from www.moos-ivp.org. It covers (a) a straw-man repository with a place-holder MOOS application and IvP Behavior, with a working CMake build structure, (b) a brief overview of the MOOS application class with an example application, (c) an overview of the IvP Behavior class with an example behavior, and (d) the IvPBuild Toolbox for generation of objective functions within behaviors.",,102 p.,,,UUV; Behavior Based Control; Unmanned Vehicles; Multi-objective Optimization; Autonomous Marine Vehicles; Behavior Based Architecture; Autonomous Vehicles; AUV; Arbitration; MOOSDB; Unmanned Marine Vehicles; Action Selection; Multi-Objective Optimization; Autonomous Helm; USV; Unmanned Surface Vehicles; MOOS; Behaviors; Artificial Intelligence; Autonomous Decision Making; Underwater Vehicles; ZAIC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,741
Dina Katabi,"Katabi, Dina; Raskar, Ramesh; Mohan, Ankit; Woo, Grace",2009-06-15T23:15:04Z,2009-06-15T23:15:04Z,2009-06-15,http://hdl.handle.net/1721.1/45565,,Simple LCD Transmitter Camera Receiver Data Link,"We demonstrate a freespace optical system using a consumer camera and projector in indoor environments using available devices for visual computing. Through design, prototype and experimentation with this commodity hardware, we analyze a practical optical solution as well as the drawbacks for current wireless challenges unmet by classic RF wireless communication. We summarize and introduce some new applications enabled by such similar setups.",,5 p.,,,Visible Light Communication,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,729
Whitman Richards,"Wormald, Nicholas; Richards, Whitman",2009-03-30T18:00:14Z,2009-03-30T18:00:14Z,2009-03-30,http://hdl.handle.net/1721.1/44959,MIT-CSAIL-TR-2009-012,Representing Small Group Evolution,"Understanding the dynamics of network evolution rests in part on the representation chosen to characterize the evolutionary process. We offer a simple, three-parameter representation based on subgraphs that capture three important properties of social networks: leadership, team alignment or bonding among members, and diversity of expertise. When plotted on this representation, the evolution of a typical small group such as start-ups or street gangs has a spiral trajectory, moving toward a tentative fixed point as membership increases to two dozen or so. We show that a simple probabilistic model for recruitment and bonding can not explain these observations, and suggest that strategic moves among group members may come into play.",,15 p.,,,simplex representation; network evolution,Belief Dynamics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,715
Anant Agarwal,"Modzelewski, Kevin; Miller, Jason; Belay, Adam; Beckmann, Nathan; Gruenwald, Charles, III; Wentzlaff, David; Youseff, Lamia; Agarwal, Anant",2009-11-20T23:45:04Z,2009-11-20T23:45:04Z,2009-11-20,http://hdl.handle.net/1721.1/49844,MIT-CSAIL-TR-2009-059,A Unified Operating System for Clouds and Manycore: fos,"Single chip processors with thousands of cores will be available in the next ten years and clouds of multicore processors afford the operating system designer thousands of cores today. Constructing operating systems for manycore and cloud systems face similar challenges. This work identifies these shared challenges and introduces our solution: a factored operating system (fos) designed to meet the scalability, faultiness, variability of demand, and programming challenges of OSâ  s for single-chip thousand-core manycore systems as well as current day cloud computers. Current monolithic operating systems are not well suited for manycores and clouds as they have taken an evolutionary approach to scaling such as adding fine grain locks and redesigning subsystems, however these approaches do not increase scalability quickly enough. fos addresses the OS scalability challenge by using a message passing design and is composed out of a collection of Internet inspired servers. Each operating system service is factored into a set of communicating servers which in aggregate implement a system service. These servers are designed much in the way that distributed Internet services are designed, but provide traditional kernel services instead of Internet services. Also, fos embraces the elasticity of cloud and manycore platforms by adapting resource utilization to match demand. fos facilitates writing applications across the cloud by providing a single system image across both future 1000+ core manycores and current day Infrastructure as a Service cloud computers. In contrast, current cloud environments do not provide a single system image and introduce complexity for the user by requiring different programming models for intra- vs inter-machine communication, and by requiring the use of non-OS standard management tools.",,11 p.,,,Infrastructure as a Service; Cloud Computing; Manycore; Operating System; Multicore,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,764
Nancy Lynch,"Kuhn, Fabian; Newport, Calvin; Lynch, Nancy",2009-05-11T17:30:04Z,2009-05-11T17:30:04Z,2009-05-11,http://hdl.handle.net/1721.1/45515,MIT-CSAIL-TR-2009-021,The Abstract MAC Layer,"A diversity of possible communication assumptions complicates the study of algorithms and lower bounds for radio networks. We address this problem by defining an Abstract MAC Layer. This service provides reliable local broadcast communication, with timing guarantees stated in terms of a collection of abstract \emph{delay functions} applied to the relevant contention. Algorithm designers can analyze their algorithms in terms of these functions, independently of specific channel behavior. Concrete implementations of the Abstract MAC Layer over basic radio network models generate concrete definitions for these delay functions, automatically adapting bounds proven for the abstract service to bounds for the specific radio network under consideration. To illustrate this approach, we use the Abstract MAC Layer to study the new problem of Multi-Message Broadcast, a generalization of standard single-message broadcast, in which any number of messages arrive at any processes at any times.We present and analyze two algorithms for Multi-Message Broadcast in static networks: a simple greedy algorithm and one that uses regional leaders. We then indicate how these results can be extended to mobile networks.",,27 p.,,en,network modeling; mobile networks; wireless networks; medium-acccess protocols,Theory of Computation,,,,,,,,,,http://hdl.handle.net/1721.1/44620,MIT-CSAIL-TR-2009-009,,,,,,,,,,,,,,,,,,2009,724
Anant Agarwal,"Miller, Jason; Agarwal, Anant; Santambrogio, Marco; Eastep, Jonathan; Hoffmann, Henry",2009-08-07T17:45:05Z,2009-08-07T17:45:05Z,2009-08-07,http://hdl.handle.net/1721.1/46351,MIT-CSAIL-TR-2009-035,Application Heartbeats for Software Performance and Health,"Adaptive, or self-aware, computing has been proposed as one method to help application programmers confront the growing complexity of multicore software development. However, existing approaches to adaptive systems are largely ad hoc and often do not manage to incorporate the true performance goals of the applications they are designed to support. This paper presents an enabling technology for adaptive computing systems: Application Heartbeats. The Application Heartbeats framework provides a simple, standard programming interface that applications can use to indicate their performance and system software (and hardware) can use to query an applicationâ  s performance. Several experiments demonstrate the simplicity and efficacy of the Application Heartbeat approach. First the PARSEC benchmark suite is instrumented with Application Heartbeats to show the broad applicability of the interface. Then, an adaptive H.264 encoder is developed to show how applications might use Application Heartbeats internally. Next, an external resource scheduler is developed which assigns cores to an application based on its performance as specified with Application Heartbeats. Finally, the adaptive H.264 encoder is used to illustrate how Application Heartbeats can aid fault tolerance.",,10 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,739
Whitman Richards,"Richards, Whitman; Winston, Patrick Henry; Finlayson, Mark Alan",2009-12-17T21:15:04Z,2009-12-17T21:15:04Z,2009-12-17,http://hdl.handle.net/1721.1/50232,MIT-CSAIL-TR-2009-063,Advancing Computational Models of Narrative,"Report of a Workshop held at the Wylie Center, Beverly, MA, Oct 8-10 2009",,32 p.,,en,literary theory; stories; narrative; computational linguistics; natural language generation; text understanding; discourse parsing,Belief Dynamics,Sponsored by the AFOSR under MIT-MURI contract #FA9550-05-1-0321,Creative Commons Attribution-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,Belief Dynamics; Whitman Richards,,,,,,,,,,,,,,,"Beverly, Massachusetts, United States",2009-10-08/2009-10-10,FA9550-05-1-0321,,,,,,2009,768
Tomaso Poggio,"Bouvrie, Jake; Poggio, Tomaso; Rosasco, Lorenzo; Smale, Steve; Wibisono, Andre",2010-11-22T22:15:09Z,2010-11-22T22:15:09Z,2010-11-19,http://hdl.handle.net/1721.1/60024,MIT-CSAIL-TR-2010-051; CBCL-292,Generalization and Properties of the Neural Response,"Hierarchical learning algorithms have enjoyed tremendous growth in recent years, with many new algorithms being proposed and applied to a wide range of applications. However, despite the apparent success of hierarchical algorithms in practice, the theory of hierarchical architectures remains at an early stage. In this paper we study the theoretical properties of hierarchical algorithms from a mathematical perspective. Our work is based on the framework of hierarchical architectures introduced by Smale et al. in the paper ""Mathematics of the Neural Response"", Foundations of Computational Mathematics, 2010. We propose a generalized definition of the neural response and derived kernel that allows us to integrate some of the existing hierarchical algorithms in practice into our framework. We then use this generalized definition to analyze the theoretical properties of hierarchical architectures. Our analysis focuses on three particular aspects of the hierarchy. First, we show that a wide class of architectures suffers from range compression; essentially, the derived kernel becomes increasingly saturated at each layer. Second, we show that the complexity of a linear architecture is constrained by the complexity of the first layer, and in some cases the architecture collapses into a single-layer linear computation. Finally, we characterize the discrimination and invariance properties of the derived kernel in the case when the input data are one-dimensional strings. We believe that these theoretical results will provide a useful foundation for guiding future developments within the theory of hierarchical algorithms.",,59 p.,,,hierarchical learning; kernel methods; learning theory,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2010,818
Anant Agarwal,"Modzelewski, Kevin; Miller, Jason; Belay, Adam; Beckmann, Nathan; Gruenwald, Charles, III; Wentzlaff, David; Youseff, Lamia; Agarwal, Anant",2010-02-08T20:00:07Z,2010-02-08T20:00:07Z,2010-02-08,http://hdl.handle.net/1721.1/51381,MIT-CSAIL-TR-2010-003,An Operating System for Multicore and Clouds: Mechanisms and Implementation,"Cloud computers and multicore processors are two emerging classes of computational hardware that have the potential to provide unprecedented compute capacity to the average user. In order for the user to effectively harness all of this computational power, operating systems (OSes) for these new hardware platforms are needed.  Existing multicore operating systems do not scale to large numbers of cores, and do not support clouds. Consequently, current-day cloud systems push much complexity onto the user, requiring the user to manage individual Virtual Machines (VMs) and deal with many system-level concerns. In this work we describe the mechanisms and implementation of a factored operating system named fos. fos is a single system image operating system across both multicore and Infrastructure as a Service (IaaS) cloud systems.  fos tackles OS scalability challenges by factoring the OS into its component system services. Each system service is further factored into a collection of Internet-inspired servers which communicate via messaging. Although designed in a manner similar to distributed Internet services, OS services instead provide traditional kernel services such as file systems, scheduling, memory management,and access to hardware. fos also implements new classes of OS services like fault tolerance and demand elasticity. In this work, we describe our working fos implementation, and provide early performance measurements of fos for both intra-machine and inter-machine operations.",,12 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,773
John Leonard,"Benjamin, Michael R.",2010-08-23T21:15:27Z,2010-08-23T21:15:27Z,2010-08-23,http://hdl.handle.net/1721.1/57509,MIT-CSAIL-TR-2010-039,MOOS-IvP Autonomy Tools Users Manual,This document describes fifteen MOOS-IvP autonomy tools. uHelmScope provides a run-time scoping window into the state of an active IvP Helm executing its mission. pMarineViewer is a geo-based GUI tool for rendering marine vehicles and geometric data in their operational area. uXMS is a terminal based tool for scoping on a MOOSDB process. uTermCommand is a terminal based tool for poking a MOOSDB with a set of MOOS file pre-defined variable-value pairs selectable with aliases from the command-line. pEchoVar provides a way of echoing a post to one MOOS variable with a new post having the same value to a different variable. uProcessWatch monitors the presence or absence of a set of MOOS processes and summarizes the collective status in a single MOOS variable. uPokeDB provides a way of poking the MOOSDB from the command line with one or more variable-value pairs without any pre-existing configuration of a MOOS file. uTimerScript will execute a pre-defined timed pausable script of poking variable-value pairs to a MOOSDB. pNodeReporter summarizes a platforms critical information into a single node report string for sharing beyond the vehicle. pBasicContactMgr provides a basic contact management service with the ability to generate range-dependent configurable alerts. The Alog Toolbox is a set of offline tools for analyzing and manipulating log files in the .alog format.,,113 p.,,,MOOS; Contact Manager,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,805
Anant Agarwal,"Hoffmann, Henry; Maggio, Martina; Santambrogio, Marco D.; Leva, Alberto; Agarwal, Anant",2010-10-22T23:15:19Z,2010-10-22T23:15:19Z,2010-10-13,http://hdl.handle.net/1721.1/59519,MIT-CSAIL-TR-2010-049,SEEC: A Framework for Self-aware Computing,"As the complexity of computing systems increases, application programmers must be experts in their application domain and have the systems knowledge required to address the problems that arise from parallelism, power, energy, and reliability concerns. One approach to relieving this burden is to make use of self-aware computing systems, which automatically adjust their behavior to help applications achieve their goals. This paper presents the SEEC framework, a unified computational model designed to enable self-aware computing in both applications and system software. In the SEEC model, applications specify goals, system software specifies possible actions, and the SEEC framework is responsible for deciding how to use the available actions to meet the application-specified goals. The SEEC framework is built around a general and extensible control system which provides predictable behavior and allows SEEC to make decisions that achieve goals while optimizing resource utilization. To demonstrate the applicability of the SEEC framework, this paper presents fivedifferent self-aware systems built using SEEC. Case studies demonstrate how these systems can control the performance of the PARSEC benchmarks, optimize performance per Watt for a video encoder, and respond to unexpected changes in the underlying environment. In general these studies demonstrate that systems built using the SEEC framework are goal-oriented, predictable, adaptive, and extensible.",,13 p.,,,Autonomic Computing; Adaptive Computing; Multicore; Control Theory,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,816
Nancy Lynch,"Cornejo, Alejandro; Lynch, Nancy",2010-09-11T00:00:21Z,2010-09-11T00:00:21Z,2010-09-09,http://hdl.handle.net/1721.1/58484,MIT-CSAIL-TR-2010-043,Reliably Detecting Connectivity using Local Graph Traits,"Local distributed algorithms can only gather sufficient information to identify local graph traits, that is, properties that hold within the local neighborhood of each node. However, it is frequently the case that global graph properties (connectivity, diameter, girth, etc) have a large influence on the execution of a distributed algorithm. This paper studies local graph traits and their relationship with global graph properties. Specifically, we focus on graph k-connectivity. First we prove a negative result that shows there does not exist a local graph trait which perfectly captures graph k-connectivity. We then present three different local graph traits which can be used to reliably predict the k-connectivity of a graph with varying degrees of accuracy. As a simple application of these results, we present upper and lower bounds for a local distributed algorithm which determines if a graph is k-connected. As a more elaborate application of local graph traits, we describe, and prove the correctness of, a local distributed algorithm that preserves k-connectivity in mobile ad hoc networks while allowing nodes to move independently whenever possible.",,16 p,,,Connectivity; Graph Traits; Distributed,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,809
Martin Rinard,"Misailovic, Sasa; Kim, Deokhwan; Rinard, Martin",2010-08-05T16:00:10Z,2010-08-05T16:00:10Z,2010-08-05,http://hdl.handle.net/1721.1/57475,MIT-CSAIL-TR-2010-038,Parallelizing Sequential Programs With Statistical Accuracy Tests,"We present QuickStep, a novel system for parallelizing sequential programs. QuickStep deploys a set of parallelization transformations that together induce a search space of candidate parallel programs. Given a sequential program, representative inputs, and an accuracy requirement, QuickStep uses performance measurements, profiling information, and statistical accuracy tests on the outputs of candidate parallel programs to guide its search for a parallelizationthat maximizes performance while preserving acceptable accuracy. When the search completes, QuickStep produces an interactive report that summarizes the applied parallelization transformations, performance, and accuracy results for the automatically generated candidate parallel programs. In our envisioned usage scenarios, the developer examines this report to evaluate the acceptability of the final parallelization and to obtain insight into how the original sequential program responds to different parallelization strategies. Itis also possible for the developer (or even a user of the program who has no software development expertise whatsoever) to simply use the best parallelization out of the box without examining the report or further investigating the parallelization. Results from our benchmark set of applications show that QuickStep can automatically generate accurate and efficient parallel programs---the automatically generated parallel versions of five of our six benchmark applications run between 5.0 and 7.7 times faster on 8 cores than the original sequential versions. Moreover, a comparison with the Intel icc compiler highlights how QuickStep can effectively parallelize applications with features (such as the use of modern object-oriented programming constructs or desirable parallelizations with infrequent but acceptable data races) that place them inherently beyond the reach of standard approaches.",,22 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,804
Nick Roy,"Banerjee, Ashis Gopal; Roy, Nicholas",2010-09-20T22:45:36Z,2010-09-20T22:45:36Z,2010-09-18,http://hdl.handle.net/1721.1/58609,MIT-CSAIL-TR-2010-045,Learning Solutions of Similar Linear Programming Problems using Boosting Trees,"In many optimization problems, similar linear programming (LP) problems occur in the nodes of the branch and bound trees that are used to solve integer (mixed or pure, deterministic or stochastic) programming problems. Similar LP problems are also found in problem domains where the objective function and constraint coefficients vary due to uncertainties in the operating conditions. In this report, we present a regression technique for learning a set of functions that map the objective function and the constraints to the decision variables of such an LP system by modifying boosting trees, an algorithm we term the Boost-LP algorithm. Matrix transformations and geometric properties of boosting trees are utilized to provide theoretical performance guarantees on the predicted values. The standard form of the loss function is altered to reduce the possibility of generating infeasible LP solutions. Experimental results on three different problems, one each on scheduling, routing, and planning respectively, demonstrate the effectiveness of the Boost-LP algorithm in providing significant computational benefits over regular optimization solvers without generating solutions that deviate appreciably from the optimum values.",,18 p.,,,,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,811
Patrick Winston,"Finlayson, Mark Alan; Hervas, Raquel",2010-08-19T18:15:22Z,2010-08-19T18:15:22Z,2010-05-12,http://hdl.handle.net/1721.1/57507,,"UCM/MIT Indications, Referring Expressions, and Coreference Corpus (UMIREC corpus) v1.1","The corpus comprises 62 files in ""Story Workbench"" annotation format: 30 folktales in English from a variety of sources, and 32 Wall Street Journal articles selected to coincide with articles found in the Penn Treebank. The files are annotated with the location of referring expressions, coreference relations between the referring expressions, and so-called ""indication structures"", which split referring expressions into constituents (nuclei and modifiers) and mark each constituent as either 'distinctive' or 'descriptive', indicating whether or not the constituent contains information required for uniquely identifying the referent. The files distributed in this corpus archive are the gold-standard files, which were constructed by merging annotations done by two trained annotators. The contents of this corpus, the annotation procedure, and the indication structures are described in more detail in a paper titled ""The Prevalence of Descriptive Referring Expressions in News and Narrative"" published in the proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, held in July 2010 in Uppsala, Sweden (ACL-2010). A near-final version of the paper is included in the doc/ directory of the compressed corpus archive file.
This is version 1.1 of the UMIREC corpus, in which the coreference annotations have been fixed relative to version 1.0. UMIREC v1.0 suffered from a bug in the export script that corrupted the coreference data.",,877 ko,,,,Genesis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,"Finlayson, M.A. & Hervás, R. (2010) UCM/MIT Indications, Referring Expressions, and Co-Reference Corpus v1.1 (UMIREC corpus). MIT CSAIL Work Product.",Patrick Winston; Genesis,,,,,http://hdl.handle.net/1721.1/54766,,,,http://hdl.handle.net/1721.1/54765,,,,,,,,,,,,,,2010,790
Patrick Winston,"Hervas, Raquel; Finlayson, Mark Alan",2010-05-12T15:30:19Z,2010-05-12T15:30:19Z,2010-05-12,http://hdl.handle.net/1721.1/54766,,"UCM/MIT Indications, Referring Expressions, and Coreference Corpus (UMIREC corpus)","This version of the UMIREC corpus has been superseded by version 1.1, found at http://hdl.handle.net/1721.1/57507.  Please do not use version 1.0, as it contains corrupted coreference information.  The correct, uncorrupted data is found in version 1.1.",,,,,,Genesis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,"Finlayson, M.A. & Hervás, R. (2010) UCM/MIT Indications, Referring Expressions, and Co-Reference Corpus v1.0 (UMIREC corpus). MIT CSAIL Work Product.",,,,http://hdl.handle.net/1721.1/57507,,,,,,http://hdl.handle.net/1721.1/54765,,,,,,,,,,,,,,2010,792
Martin Rinard,"Kim, Deokhwan; Rinard, Martin C.",2010-12-03T21:00:05Z,2010-12-03T21:00:05Z,2010-12-03,http://hdl.handle.net/1721.1/60078,MIT-CSAIL-TR-2010-056,Verification of Semantic Commutativity Conditions and Inverse Operations on Linked Data Structures,"Commuting operations play a critical role in many parallel computing systems. We present a new technique for verifying commutativity conditions, which are logical formulas that characterize when operations commute. Because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not identical) data structure states in different execution orders. We have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (ListSet and HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and ArrayList). This effort involved the specification and verification of 765 commutativity conditions. Many speculative parallel systems need to undo the effects of speculatively executed operations. Inverse operations, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). We present a new technique for verifying such inverse operations. We have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state. Together, the commutativity conditions and inverse operations provide a key resource that language designers and system developers can draw on to build parallel languages and systems with strong correctness guarantees.",,673 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,821
Tomaso Poggio,"Leibo, Joel Z; Mutch, Jim; Rosasco, Lorenzo; Ullman, Shimon; Poggio, Tomaso",2011-01-04T22:30:15Z,2011-01-04T22:30:15Z,2010-12-30,http://hdl.handle.net/1721.1/60378,MIT-CSAIL-TR-2010-061; CBCL-294,Learning Generic Invariances in Object Recognition:  Translation and Scale,"Invariance to various transformations is key to object recognition but existing definitions of invariance are somewhat confusing while discussions of invariance are often confused. In this report, we provide an operational definition of invariance by formally defining perceptual tasks as classification problems. The definition should be appropriate for physiology, psychophysics and computational modeling. For any specific object, invariance can be trivially ``learned'' by memorizing a sufficient number of example images of the transformed object. While our formal definition of invariance also covers such cases, this report focuses instead on invariance from very few images and mostly on invariances from one example. Image-plane invariances -- such as translation, rotation and scaling -- can be computed from a single image for any object. They are called generic since in principle they can be hardwired or learned (during development) for any object. In this perspective, we characterize the invariance range of a class of feedforward architectures for visual recognition that mimic the hierarchical organization of the ventral stream. We show that this class of models achieves essentially perfect translation and scaling invariance for novel images. In this architecture a new image is represented in terms of weights of ""templates"" (e.g. ""centers"" or ""basis functions"") at each level in the hierarchy. Such a representation inherits the invariance of each template, which is implemented through replication of the corresponding ""simple"" units across positions or scales and their ""association"" in a ""complex"" unit. We show simulations on real images that characterize the type and number of templates needed to support the invariant recognition of novel objects. We find that 1) the templates need not be visually similar to the target objects and that 2) a very small number of them is sufficient for good recognition. These somewhat surprising empirical results have intriguing implications for the learning of invariant recognition during the development of a biological organism, such as a human baby. In particular, we conjecture that invariance to translation and scale may be learned by the association -- through temporal contiguity -- of a small number of primal templates, that is patches extracted from the images of an object moving on the retina across positions and scales. The number of templates can later be augmented by bootstrapping mechanisms using the correspondence provided by the primal templates -- without the need of temporal contiguity.",,27 p.,,,vision; object recognition; generic transformations; selectivity-invariance trade-off; primal templates,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,CBCL-291,,,,,,,,,,,,,,,,,,2010,826
Nancy Lynch,"Lynch, Nancy; Kuhn, Fabian; Kowalski, Dariusz; Khabbazian, Majid",2010-02-09T18:30:02Z,2010-02-09T18:30:02Z,2010-02-09,http://hdl.handle.net/1721.1/51667,MIT-CSAIL-TR-2010-005,The Cost of Global Broadcast Using Abstract MAC Layers,"We analyze greedy algorithms for broadcasting messages throughout a multi-hop wireless network, using a slot-based model that includes message collisions without collision detection. Our algorithms are split formally into two pieces: a high-level piece for broadcast and a low-level piece for contention management. We accomplish the split using abstract versions of the MAC layer to encapsulate the contention management. We use two different abstract MAC layers: a basic non-probabilistic one, which our contention management algorithm implements with high probability, and a probabilistic one, which our contention management algorithm implements precisely. Using this approach, we obtain the following complexity bounds: Single-message broadcast, using the basic abstract MAC layer, takes time O(D log(n/epsilon) log(Delta)) to deliver the message everywhere with probability 1 - epsilon, where D is the network diameter, n is the number of nodes, and Delta is the maximum node degree. Single-message broadcast, using the probabilistic abstract MAC layer, takes time only O((D + log(n/epsilon)) log(Delta)). For multi-message broadcast, the bounds are O((D + k' Delta) log(n/epsilon) log(Delta)) using the basic layer and O((D + k' Delta log(n/epsilon)) log(Delta)) using the probabilistic layer,for the time to deliver a single message everywhere in the presence of at most k' concurrent messages.",,42 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,774
Dina Katabi,"Wang, Jue; Katabi, Dina",2010-07-07T21:15:12Z,2010-07-07T21:15:12Z,2010-07-05,http://hdl.handle.net/1721.1/56252,MIT-CSAIL-TR-2010-031,ChitChat: Making Video Chat Robust to Packet Loss,"Video chat is increasingly popular among Internet users. Often, however, chatting sessions suffer from packet loss, which causes video outage and poor quality. Existing solutions however are unsatisfying. Retransmissions increase the delay and hence can interact negatively with the strict timing requirements of interactive video. FEC codes introduce extra overhead and hence reduce the bandwidth available for video data even in the absence of packet loss. This paper presents ChitChat, a new approach for reliable video chat that neither delays frames nor introduces bandwidth overhead. The key idea is to ensure that the information in each packet describes the whole frame. As a result, even when some packets are lost, the receiver can still use the received packets to decode a smooth version of the original frame. This reduces frame loss and the resulting video freezes and improves the perceived video quality. We have implemented ChitChat and evaluated it over multiple Internet paths. In comparison to Windows Live Messenger 2009, our method reduces the occurrences of video outage events by more than an order of magnitude.",,12 p.,,,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,797
Jovan Popovic,"Wang, Robert; Paris, Sylvain; Popovic, Jovan",2010-09-11T00:00:37Z,2010-09-11T00:00:37Z,2010-09-10,http://hdl.handle.net/1721.1/58485,MIT-CSAIL-TR-2010-044,Practical Color-Based Motion Capture,"Motion capture systems have been widely used for high quality content creation and virtual reality but are rarely used in consumer applications due to their price and setup cost. In this paper, we propose a motion capture system built from commodity components that can be deployed in a matter of minutes. Our approach uses one or more webcams and a color shirt to track the upper-body at interactive rates. We describe a robust color calibration system that enables our color-based tracking to work against cluttered backgrounds and under multiple illuminants. We demonstrate our system in several real-world indoor and outdoor settings.",,6 p.,,,augmented reality,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,810
John Leonard,"Benjamin, Michael R.; Newman, Paul; Schmidt, Henrik; Leonard, John J.",2010-08-27T16:00:16Z,2010-08-27T16:00:16Z,2010-08-27,http://hdl.handle.net/1721.1/57583,MIT-CSAIL-TR-2010-041,An Overview of MOOS-IvP and a Users Guide to the IvP Helm Autonomy Software,"This document describes the IvP Helm -- an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming -- a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",,255 p.,,,pMarineViewer; uTimerScript; Adaptive Autonomy; USV; UUV; UxV; Unmanned Marine Vehicles; Marine Autonomy; AUV; NURC,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,807
Fredo Durand,"Ragan-Kelley, Jonathan; Doggett, Michael; Lehtinen, Jaakko; Chen, Jiawen; Durand, Fredo",2010-03-29T18:45:17Z,2010-03-29T18:45:17Z,2010-03-29,http://hdl.handle.net/1721.1/53330,MIT-CSAIL-TR-2010-015,Decoupled Sampling for Real-Time Graphics Pipelines,"We propose decoupled sampling, an approach that decouples shading from visibility sampling in order to enable motion blur and depth-of-field at reduced cost. More generally, it enables extensions of modern real-time graphics pipelines that provide controllable shading rates to trade off quality for performance. It can be thought of as a generalization of GPU-style multisample antialiasing (MSAA) to support unpredictable shading rates, with arbitrary mappings from visibility to shading samples as introduced by motion blur, depth-of-field, and adaptive shading. It is inspired by the Reyes architecture in offline rendering, but targets real-time pipelines by driving shading from visibility samples as in GPUs, and removes the need for micropolygon dicing or rasterization. Decoupled Sampling works by defining a many-to-one hash from visibility to shading samples, and using a buffer to memoize shading samples and exploit reuse across visibility samples. We present extensions of two modern GPU pipelines to support decoupled sampling: a GPU-style sort-last fragment architecture, and a Larrabee-style sort-middle pipeline. We study the architectural implications and derive end-to-end performance estimates on real applications through an instrumented functional simulator. We demonstrate high-quality motion blur and depth-of-field, as well as variable and adaptive shading rates.",,16 p.,,,Computer Graphics; Graphics Systems; Graphics Hardware,Computer Graphics,,Creative Commons Attribution-Share Alike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,"RAGAN-KELLEY, J., LEHTINEN, J., CHEN, J., DOGGETT, M., and DURAND, F. 2010. Decoupled Sampling for Real-Time Graphics Pipelines. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series, MIT-CSAIL-TR-2010-015.",,,,,,,,,,,,,,,,,,,,,,,,2010,781
Arvind,"Adler, Michael; Fleming, Kermin E.; Parashar, Angshuman; Pellauer, Michael; Emer, Joel",2010-11-29T19:15:06Z,2010-11-29T19:15:06Z,2010-11-23,http://hdl.handle.net/1721.1/60045,MIT-CSAIL-TR-2010-054,LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic [Extended Version],"Developers accelerating applications on FPGAs or other reconfigurable logic have nothing but raw memory devices in their standard toolkits. Each project typically includes tedious development of single-use memory management. Software developers expect a programming environment to include automatic memory management. Virtual memory provides the illusion of very large arrays and processor caches reduce access latency without explicit programmer instructions. LEAP scratchpads for reconfigurable logic dynamically allocate and manage multiple, independent, memory arrays in a large backing store. Scratchpad accesses are cached automatically in multiple levels, ranging from shared on-board, RAM-based, set-associative caches to private caches stored in FPGA RAM blocks. In the LEAP framework, scratchpads share the same interface as on-die RAM blocks and are plug-in replacements. Additional libraries support heap management within a storage set. Like software developers, accelerator authors using scratchpads may focus more on core algorithms and less on memory management. Two uses of FPGA scratchpads are analyzed: buffer management in an H.264 decoder and memory management within a processor microarchitecture timing model.",,11 p.,,,FPGA; memory management; caches,Computation Structures,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,"CORRECTION: The authors for entry [4] in the references should have been ""E. S. Chung, 
J. C. Hoe, and K. Mai"".",,,,,,,,,,,,,,,,,,,,,,,,,2010,820
Karen Sollins,"Cheng, Tiffany",2010-09-22T20:15:24Z,2010-09-22T20:15:24Z,2010-09-22,http://hdl.handle.net/1721.1/58668,MIT-CSAIL-TR-2010-047,"A File Location, Replication, and Distribution System for Network Information to Aid Network Management","This thesis demonstrates and evaluates the design, architecture, and implementation of a file location, replication, and distribution system built with the objective of managing information in an Internet network. The system's goal is to enable the availability of information by providing alternative locations for files in case of situations where the original piece of information cannot be found in the network due to failures or other problems. The system provides the mechanism for duplicating files and executes the act of placing them in multiple locations according to predefined rules for distribution. The resulting system is a working model for a file management system that can exist over the Internet and will aid in overall network management by organizing and overseeing the information found within a network.",,105 p.,,,failure; copying,Advanced Network Architecture,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,,2010,813
Silvio Micali,"Chen, Jing; Micali, Silvio",2010-12-30T09:30:03Z,2010-12-30T09:30:03Z,2010-12-20,http://hdl.handle.net/1721.1/60371,MIT-CSAIL-TR-2010-060,Conservative Rationalizability and The Second-Knowledge Mechanism,"In mechanism design, the traditional way of modeling the players' incomplete information about their opponents is ""assuming a Bayesian."" This assumption, however, is very strong and does not hold in many real applications. Accordingly, we put forward (1) a set-theoretic way to model the knowledge that a player might have about his opponents, and (2) a new class of mechanisms capable of leveraging such more conservative knowledge in a robust way. In auctions of a single good, we show that such a new mechanism can perfectly guarantee a revenue benchmark (always lying in between the second highest and the highest valuation) that no classical mechanism can even approximate in any robust way.",,23 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,824
Nancy Lynch,"Kuhn, Fabian; Lynch, Nancy; Newport, Calvin",2010-08-26T20:45:18Z,2010-08-26T20:45:18Z,2010-08-26,http://hdl.handle.net/1721.1/57577,MIT-CSAIL-TR-2010-040,The Abstract MAC Layer,"A diversity of possible communication assumptions complicates the study of algorithms and lower bounds for radio networks. We address this problem by defining an abstract MAC layer. This service provides reliable local broadcast communication, with timing guarantees stated in terms of a collection of abstract delay functions applied to the relevant contention. Algorithm designers can analyze their algorithms in terms of these functions, independently of specific channel behavior. Concrete implementations of the abstract MAC Layer over basic radio network models generate concrete definitions for these delay functions, automatically adapting bounds proven for the abstract service to bounds for the specific radio network under consideration. To illustrate this approach, we use the abstract MAC Layer to study the new problem of Multi-Message Broadcast, a generalization of standard single-message broadcast in which multiple messages can originate at different times and locations in the network. We present and analyze two algorithms for Multi-Message Broadcast in static networks: a simple greedy algorithm and one that uses regional leaders. We then indicate how these results can be extended to mobile networks.",,30 p.,,,Wireless networks; Abstraction; Medium access control; Broadcast,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,806
Whitman Richards,"Richards, Whitman; Macindoe, Owen",2010-07-27T20:15:27Z,2010-07-27T20:15:27Z,2010-07-27,http://hdl.handle.net/1721.1/57462,MIT-CSAIL-TR-2010-033,Characteristics of Small Social Networks,"Two dozen networks are analyzed using three parameters that attempt to capture important properties of social networks: leadership L, member bonding B, and diversity of expertise D. The first two of these parameters have antecedents, the third is new. A key part of the analysis is to examine networks at multiple scales by dissecting the entire network into its n subgraphs of a given radius of two edge steps about each of the n nodes. This scale-based analysis reveals constraints on what we have dubbed ""cognitive"" networks, as contrasted with biological or physical networks. Specifically, ""cognitive"" networks appear to maximize bonding and diversity over a range of leadership dominance. Asymptotic relations between the bonding and diversity measures are also found when small, nearly complete subgraphs are aggregated to form larger networks. This aggregation probably underlies changes in a regularity among the LBD parameters; this regularity is a U-shaped function of networks size, n, which is minimal for networks around 80 or so nodes.",,38 p.,,,constraints on social networks; network evolution; small group aggregation; multi-scale analysis,,,,,,,,,,,,,,,,,,,,,Belief Dynamics,,,,,,,,,2010,798
Antonio Torralba,"Choi, Myung Jin; Lim, Joseph J.; Torralba, Antonio; Willsky, Alan S.",2010-10-29T23:00:18Z,2010-10-29T23:00:18Z,2010-10-29,http://hdl.handle.net/1721.1/59799,MIT-CSAIL-TR-2010-050,A Tree-Based Context Model for Object Recognition,"There has been a growing interest in exploiting contextual information in addition to local features to detect and localize multiple object categories in an image. A context model can rule out some unlikely combinations or locations of objects and guide detectors to produce a semantically coherent interpretation of a scene. However, the performance benefit of context models has been limited because most of the previous methods were tested on datasets with only a few object categories, in which most images contain one or two object categories. In this paper, we introduce a new dataset with images that contain many instances of different object categories, and propose an efficient model that captures the contextual information among more than a hundred object categories using a tree structure. Our model incorporates global image features, dependencies between object categories, and outputs of local detectors into one probabilistic framework. We demonstrate that our context model improves object recognition performance and provides a coherent interpretation of a scene, which enables a reliable image querying system by multiple object categories. In addition, our model can be applied to scene understanding tasks that local detectors alone cannot solve, such as detecting objects out of context or querying for the most typical and the least typicalscenes in a dataset.",,14 p.,,,Object recognition; scene analysis; Markov random fields; structural models; image databases,Vision,"This research was partially funded by Shell International Exploration and Production Inc., by Army Research Office under award W911NF-06-1-0076, by NSF Career Award (ISI 0747120), and by the Air Force Office of Scientific Research under Award No.FA9550-06-1-0324. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the Air Force.",,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,817
Joshua Tenenbaum,"Salakhutdinov, Ruslan; Tenenbaum, Josh; Torralba, Antonio",2010-11-22T22:15:19Z,2010-11-22T22:15:19Z,2010-10-13,http://hdl.handle.net/1721.1/60025,MIT-CSAIL-TR-2010-052,One-Shot Learning with a Hierarchical Nonparametric Bayesian Model,"We develop a hierarchical Bayesian model that learns to learn categories from single training examples. The model transfers acquired knowledge from previously learned categories to a novel category, in the form of a prior over category means and variances. The model discovers how to group categories into meaningful super-categories that express different priors for new classes. Given a single example of a novel category, we can efficiently infer which super-category the novel category belongs to, and thereby estimate not only the new category's mean but also an appropriate similarity metric based on parameters inherited from the super-category. On MNIST and MSR Cambridge image datasets the model learns useful representations of novel categories based on just a single training example, and performs significantly better than simpler hierarchical Bayesian approaches. It can also discover new categories in a completely unsupervised fashion, given just one or a few examples.",,14 p.,,,hierarchical Bayes; semi-supervised learning; learning to learn,Computational Cognitive Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,815
Nancy Lynch,"Oshman, Rotem; Richa, Andrea; Newport, Calvin; Lynch, Nancy; Kuhn, Fabian",2010-06-08T16:00:03Z,2010-06-08T16:00:03Z,2010-06-08,http://hdl.handle.net/1721.1/55721,MIT-CSAIL-TR-2010-029,Broadcasting in Unreliable Radio Networks,"Practitioners agree that unreliable links, which fluctuate between working and not working, are an important characteristic of wireless networks. In contrast, most theoretical models of radio networks fix a static set of links and assume that these links work reliably throughout an execution. This gap between theory and practice motivates us to investigate how unreliable links affect theoretical bounds on broadcast in radio networks. To that end we consider a model that includes two types of links: reliable links, which always deliver messages, and unreliable links, which sometimes deliver messages and sometimes do not. It is assumed that the graph induced by the reliable links is connected, and unreliable links are controlled by a worst-case adversary. In the new model we show an(n log n) lower bound on deterministic broadcast in undirected graphs, even when all processes are initially awake and have collision detection, and an (n) lower bound on randomized broadcast in undirected networks of constant diameter. This clearly separates the new model from the classical, reliable model. On the positive side, we give two algorithms that tolerate the inherent unreliability: an O(n3=2plog n)-time deterministic algorithm and a randomized algorithm which terminates in O(n log2 n) rounds with high probability.",,25 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,795
Tomaso Poggio,"Wibisono, Andre; Bouvrie, Jake; Rosasco, Lorenzo; Poggio, Tomaso",2010-07-30T17:45:16Z,2010-07-30T17:45:16Z,2010-07-30,http://hdl.handle.net/1721.1/57464,CBCL-290; MIT-CSAIL-TR-2010-035,Learning and Invariance in a Family of Hierarchical Kernels,"Understanding invariance and discrimination properties of hierarchical models is arguably the key to understanding how and why such models, of which the the mammalian visual system is one instance, can lead to good generalization properties and reduce the sample complexity of a given learning task. In this paper we explore invariance to transformation and the role of layer-wise embeddings within an abstract framework of hierarchical kernels motivated by the visual cortex. Here a novel form of invariance is induced by propagating the effect of locally defined, invariant kernels throughout a hierarchy. We study this notion of invariance empirically. We then present an extension of the abstract hierarchical modeling framework to incorporate layer-wise embeddings, which we demonstrate can lead to improved generalization and scalable algorithms. Finally we analyze experimentally sample complexity properties as a function of architectural parameters.",,9 p.,,,artificial intelligence,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,801
Hal Abelson,"Kagal, Lalana; Jacobi, Ian; Khandelwal, Ankesh",2011-04-21T18:15:15Z,2011-04-21T18:15:15Z,2011-04-16,http://hdl.handle.net/1721.1/62294,MIT-CSAIL-TR-2011-023,Gasping for AIR   Why we need Linked Rules and Justifications on the Semantic Web,"The Semantic Web is a distributed model for publishing, utilizing and extending structured information using Web protocols. One of the main goals of this technology is to automate the retrieval and integration of data and to enable the inference of interesting results. This automation requires logics and rule languages that make inferences, choose courses of action, and answer questions. The openness of the Web, however, leads to several issues including the handling of inconsistencies, integration of diverse information, and the determination of the quality and trustworthiness of the data. AIR is a Semantic Web-based rule language that provides this functionality while focusing on generating and tracking explanations for its inferences and actions as well as conforming to Linked Data principles. AIR supports Linked Rules, which allow rules to be combined, re-used and extended in a manner similar to Linked Data. Additionally, AIR explanations themselves are Semantic Web data so they can be used for further reasoning. In this paper we present an overview of AIR, discuss its potential as a Web rule language by providing examples of how its features can be leveraged for different inference requirements, and describe how justifications are represented and generated.",,10 p.,,,,Decentralized Information Group,"This material is based upon work supported by the National Science Foundation under Award No. CNS-0831442, by the Air Force Office of Scientific Research under Award No. FA9550-09-1-0152, and by Intelligence Advanced Research Projects Activity under Award No. FA8750-07-2-0031.",Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2011,850
Martin Rinard,"Misailovic, Sasa; Roy, Daniel M.; Rinard, Martin",2011-01-19T23:45:03Z,2011-01-19T23:45:03Z,2011-01-19,http://hdl.handle.net/1721.1/60675,MIT-CSAIL-TR-2011-003,Probabilistic and Statistical Analysis of Perforated Patterns,"We present a new foundation for the analysis and transformation of computer programs.Standard approaches involve the use of logical reasoning to prove that the applied transformation does not change the observable semantics of the program. Our approach, in contrast, uses probabilistic and statistical reasoning to justify the application of transformations that may change, within probabilistic bounds, the result that the program produces. Loop perforation transforms loops to execute fewer iterations. We show how to use our basic approach to justify the application of loop perforation to a set of computational patterns. Empirical results from computations drawn from the PARSEC benchmark suite demonstrate that these computational patterns occur in practice. We also outline a specification methodology that enables the transformation of subcomputations and discuss how to automate the approach.",,22 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,830
,"Durand, Frédo",2011-12-14T19:45:12Z,2011-12-14T19:45:12Z,2011-12-14,http://hdl.handle.net/1721.1/67677,MIT-CSAIL-TR-2011-052,A Frequency Analysis of Monte-Carlo and other Numerical Integration Schemes,"The numerical calculation of integrals is central to many computer graphics algorithms such as Monte-Carlo Ray Tracing. We show that such methods can be studied using Fourier analysis. Numerical error is shown to correspond to aliasing and the link between properties of the sampling pattern and the integrand is studied. The approach also permits the unified study of image aliasing and numerical integration, by considering a multidimensional domain where some dimensions are integrated while others are sampled.",,6 p.,,,Numerical Analysis; Integration; Fourier; Monte-Carlo; Aliasing; Rendering; Ray Tracing,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,877
David Clark,"Heikkinen, Mikko V. J.; Berger, Arthur W.",2011-05-03T21:15:05Z,2011-05-03T21:15:05Z,2011-05-03,http://hdl.handle.net/1721.1/62579,MIT-CSAIL-TR-2011-028,Comparison of User Traffic Characteristics on Mobile-Access versus Fixed-Access Networks,"We compare Web traffic characteristics of mobile- versus fixed-access end-hosts, where herein the term ""mobile"" refers to access via cell towers, using for example the 3G/UMTS standard, and the term ""fixed"" includes Wi-Fi access. It is well-known that connection speeds are in general slower over mobile-access networks, and also that often there is higher packet loss. We were curious whether this leads mobile-access users to have smaller connections. We examined the distribution of the number of bytes-per-connection, and packet loss from a sampling of logs from servers of Akamai Technologies. We obtained 149 million connections, across 57 countries. The mean bytes-per-connection was typically larger for fixed-access: for two-thirds of the countries, it was at least one-third larger. Regarding distributions, we found that the difference between the bytes-per-connection for mobile- versus fixed-access, as well as the packet loss, was statistically significant for each of the countries; however the visual difference in plots is typically small. For some countries, mobile-access had the larger connections. As expected, mobile-access often had higher loss than fixed-access, but the reverse pertained for some countries. Typically packet loss increased during the busy period of the day, when mobile-access had a larger increase. Comparing our results from 2010 to those from 2009 of the same time period, we found that connections have become a bit smaller.",,20 p.,,,,Advanced Network Architecture,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,854
Tomaso Poggio,"Alvarez, Mauricio A.; Rosasco, Lorenzo; Lawrence, Neil D.",2011-06-30T19:30:08Z,2011-06-30T19:30:08Z,2011-06-30,http://hdl.handle.net/1721.1/64731,MIT-CSAIL-TR-2011-033; CBCL-301,Kernels for Vector-Valued Functions: a Review,"Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.",,38 p.,,,learning theory; kernel methods; multi-output learning,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,"Alvarez, Mauricio A.; Rosasco, Lorenzo; Lawrence, Neil D.",,,,,,,en-US,,,,2011,861
Nancy Lynch,"Ghaffari, Mohsen; Lynch, Nancy; Sastry, Srikanth",2011-10-12T18:30:07Z,2011-10-12T18:30:07Z,2011-10-12,http://hdl.handle.net/1721.1/66224,MIT-CSAIL-TR-2011-045,Leader Election Using Loneliness Detection,"We consider the problem of leader election (LE) in single-hop radio networks with synchronized time slots for transmitting and receiving messages. We assume that the actual number n of processes is unknown, while the size u of the ID space is known, but is possibly much larger. We consider two types of collision detection: strong (SCD), whereby all processes detect collisions, and weak (WCD), whereby only non-transmitting processes detect collisions. We introduce loneliness detection (LD) as a key subproblem for solving LE in WCD systems. LD informs all processes whether the system contains exactly one process or more than one. We show that LD captures the difference in power between SCD and WCD, by providing an implementation of SCD over WCD and LD. We present two algorithms that solve deterministic and probabilistic LD in WCD systems with time costs of O(log(u/n)) and O(min(log(u/n), (log(1/epsilon)/n)), respectively, where epsilon is the error probability. We also provide matching lower bounds. We present two algorithms that solve deterministic and probabilistic LE in SCD systems with time costs of O(log u) and O(min(log u, loglog n + log(1/epsilon))), respectively, where epsilon is the error probability. We provide matching lower bounds.",,37 p.,,,wireless networks,Theory of Computation,"This work partially supported by the NSF under award numbers CCF-0937274, and CCF-0726514, and by AFOSR under award number FA9550-08-1-0159. This work is also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.",,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,872
Karen Sollins,"Guo, Nina X.",2011-06-07T21:15:04Z,2011-06-07T21:15:04Z,2011-06-07,http://hdl.handle.net/1721.1/63260,MIT-CSAIL-TR-2011-030,Scalable Information-Sharing Network Management,"This thesis analyzes scalable information-sharing network management. It looks into one of the large problems in network management today: finding information across different network domains. Information-sharing network management is a method to solving the problem, though it is important to make it scalable. The solution proposed uses the Publish-Subscribe Internet Routing Paradigm (PSIRP) inter-domain design as the base structure. The design borrows from Border Gateway Protocol ideas and uses the Chord protocol as one of the key methods of finding information. The conclusion after analyzing the scalability of PSIRP is that its use of Chord gives it an advantage that allows a O(log^2 N) tradeoff between performance and distribution.",,56 p.,,,"network management, scalability, information-centric networking, inter-domain routing",Advanced Network Architecture,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,858
Nancy Lynch,"Censor-Hillel, Keren; Gilbert, Seth; Kuhn, Fabian; Lynch, Nancy; Newport, Calvin",2011-12-27T20:30:09Z,2011-12-27T20:30:09Z,2011-12-22,http://hdl.handle.net/1721.1/67885,MIT-CSAIL-TR-2011-053,Structuring Unreliable Radio Networks,"In this paper we study the problem of building a connected dominating set with constant degree (CCDS) in the dual graph radio network model.  This model includes two types of links:  reliable links, which
always deliver messages, and unreliable links, which sometimes fail to deliver messages.  Real networks compensate for this differing quality by deploying low-layer detection protocols to filter unreliable from reliable links.  With this in mind, we begin by presenting an algorithm that solves the CCDS problem in the dual graph model under the assumption that every process u is provided with a local ""link detector set"" consisting of every neighbor connected to u by a reliable link.  The algorithm solves the CCDS problem in O((Delta log2(n)/b) + log3(n)) rounds, with high probability, where Delta is the maximum degree in the reliable link graph, n is the network size, and b is an upper bound in bits on the message size.  The algorithm works by first building a Maximal Independent Set (MIS) in log3(n) time, and then leveraging the local topology knowledge to efficiently connect nearby MIS processes.  A natural follow up question is whether the link detector must be perfectly reliable to solve the CCDS problem.  To answer this question, we first describe an algorithm that builds a CCDS in O(Delta polylog(n)) time under the assumption of O(1) unreliable links included in each link detector set.  We then prove this algorithm to be (almost) tight by showing that the possible inclusion of only a single unreliable link in each process's local link detector set is sufficient to require Omega(Delta) rounds to solve the CCDS problem, regardless of message size.  We conclude by discussing how to apply our algorithm in the setting where the topology of reliable and unreliable links can change over time.",,21 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,878
Robert Morris,"Boyd-Wickizer, Silas; Kaashoek, M. Frans; Morris, Robert; Zeldovich, Nickolai",2011-06-28T21:45:22Z,2011-06-28T21:45:22Z,2011-06-28,http://hdl.handle.net/1721.1/64698,MIT-CSAIL-TR-2011-032,A Software Approach to Unifying Multicore Caches,"Multicore chips will have large amounts of fast on-chip cache memory, along with relatively slow DRAM interfaces. The on-chip cache memory, however, will be fragmented and spread over the chip; this distributed arrangement is hard for certain kinds of applications to exploit efficiently, and can lead to needless slow DRAM accesses. First, data accessed from many cores may be duplicated in many caches, reducing the amount of distinct data cached. Second, data in a cache distant from the accessing core may be slow to fetch via the cache coherence protocol. Third, software on each core can only allocate space in the small fraction of total cache memory that is local to that core. A new approach called software cache unification (SCU) addresses these challenges for applications that would be better served by a large shared cache. SCU chooses the on-chip cache in which to cache each item of data. As an application thread reads data items, SCU moves the thread to the core whose on-chip cache contains each item. This allows the thread to read the data quickly if it is already on-chip; if it is not, moving the thread causes the data to be loaded into the chosen on-chip cache. A new file cache for Linux, called MFC, uses SCU to improve performance of file-intensive applications, such as Unix file utilities. An evaluation on a 16-core AMD Opteron machine shows that MFC improves the throughput of file utilities by a factor of 1.6. Experiments with a platform that emulates future machines with less DRAM throughput per core shows that MFC will provide benefit to a growing range of applications.",,13 p.,,,,Parallel and Distributed Operating Systems,"This material is based upon work supported by the National Science
Foundation under grant number 0915164.",Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,860
Patrick Winston,"Finlayson, Mark Alan; Kulkarni, Nidhi",2011-05-09T19:30:09Z,2011-05-09T19:30:09Z,2011-05-09,http://hdl.handle.net/1721.1/62792,,Source code and data for MWE'2011 papers,"Contains the source code and data necessary to run all computations described in the following two papers: Finlayson, Mark A. and Kulkarni, Nidhi (2011) ""Detecting Multi-Word Expressions improves Word Sense Disambiguation"", in Proceedings of the 2011 Workshop on Multiword Expressions, held at ACL'2011 in Portland, OR; Kulkarni, Nidhi and Finlayson, Mark A. (2011) ""jMWE: A Java Toolkit for Detecting Multi-Word Expressions"" in Proceedings of the 2011 Workshop on Multiword Expressions, held at ACL'2011 in Portland, OR.",,45241266 bytes,application/zip,,Multi-word expressions; MWE; Java; Supplementary material,Genesis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,855
Tomaso Poggio,"Baldassarre, Luca; Rosasco, Lorenzo; Barla, Annalisa; Verri, Alessandro",2011-02-01T20:00:05Z,2011-02-01T20:00:05Z,2011-01-24,http://hdl.handle.net/1721.1/60875,MIT-CSAIL-TR-2011-004; CBCL-296,Multi-Output Learning via Spectral Filtering,"In this paper we study a class of regularized kernel methods for vector-valued learning which are based on filtering the spectrum of the kernel matrix. The considered methods include Tikhonov regularization as a special case, as well as interesting alternatives such as vector-valued extensions of L2 boosting. Computational properties are discussed for various examples of kernels for vector-valued functions and the benefits of iterative techniques are illustrated. Generalizing previous results for the scalar case, we show finite sample bounds for the excess risk of the obtained estimator and, in turn, these results allow to prove consistency both for regression and multi-category classification. Finally, we present some promising results of the proposed algorithms on artificial and real data.",,37 p.,,,"Computational Learning, Multi-Output Learning, Spectral Methods",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,831
Tomaso Poggio,"Mroueh, Youssef; Poggio, Tomaso; Rosasco, Lorenzo",2011-06-03T15:15:06Z,2011-06-03T15:15:06Z,2011-06-03,http://hdl.handle.net/1721.1/63175,MIT-CSAIL-TR-2011-029; CBCL-299,Regularization Predicts While Discovering Taxonomy,In this work we discuss a regularization framework to solve multi-category when the classes are described by an underlying class taxonomy. In particular we discuss how to learn the class taxonomy while learning a multi-category classifier.,,5 p.,,,computational learning; classification,Center for Biological and Computational Learning (CBCL),"This research was sponsored by grants from DARPA (IPTO and DSO), National Science Foundation (NSF-0640097, NSF-0827427), AFSOR-THRL (FA8650-05-C-7262) Additional support was provided by: Adobe, Honda Research Institute USA, King Abdullah University Science and Technology grant to B. DeVore, NEC, Sony and especially by the Eugene McDermott Foundation",,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,857
Frédo Durand,"Aubry, Mathieu; Paris, Sylvain; Hasinoff, Samuel W.; Kautz, Jan; Durand, Frédo",2011-11-15T16:30:09Z,2011-11-15T16:30:09Z,2011-11-15,http://hdl.handle.net/1721.1/67030,MIT-CSAIL-TR-2011-049,Fast and Robust Pyramid-based Image Processing,"Multi-scale manipulations are central to image editing but they are also prone to halos. Achieving artifact-free results requires sophisticated edgeaware techniques and careful parameter tuning. These shortcomings were recently addressed by the local Laplacian filters, which can achieve a broad range of effects using standard Laplacian pyramids. However, these filters are slow to evaluate and their relationship to other approaches is unclear. In this paper, we show that they are closely related to anisotropic diffusion and to bilateral filtering. Our study also leads to a variant of the bilateral filter that produces cleaner edges while retaining its speed. Building upon this result, we describe an acceleration scheme for local Laplacian filters that yields speed-ups on the order of 50x. Finally, we demonstrate how to use local Laplacian filters to alter the distribution of gradients in an image. We illustrate this property with a robust algorithm for photographic style transfer.",,11 p.,,,"Computational photography, photoediting, image processing, Laplacian pyramid, bilateral filter, photographic style transfer",Computer Graphics,,,,,,,,,,,,,,,,,,"Aubry, Mathieu; Paris, Sylvain; Hasinoff, Samuel W.; Kautz, Jan; Durand, Frédo",,,,,,,en-US,,,,2011,875
Tomaso Poggio,"Mosci, Sofia; Rosasco, Lorenzo; Santoro, Matteo; Verri, Alessandro; Villa, Silvia",2011-09-26T20:45:09Z,2011-09-26T20:45:09Z,2011-09-26,http://hdl.handle.net/1721.1/65964,MIT-CSAIL-TR-2011-041; CBCL-303,Nonparametric Sparsity and Regularization,"In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose and study a new regularizer and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art.",,38 p.,,,computational learning; machine learning,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,869
Tomaso Poggio,"Isik, Leyla; Leibo, Joel Z; Poggio, Tomaso",2011-09-12T16:00:13Z,2011-09-12T16:00:13Z,2011-09-10,http://hdl.handle.net/1721.1/65646,MIT-CSAIL-TR-2011-040; CBCL-302,Learning and disrupting invariance in visual recognition,"Learning by temporal association rules such as Foldiak's trace rule is an attractive hypothesis that explains the development of invariance in visual recognition. Consistent with these rules, several recent experiments have shown that invariance can be broken by appropriately altering the visual environment but found puzzling differences in the effects at the psychophysical versus single cell level. We show a) that associative learning provides appropriate invariance in models of object recognition inspired by Hubel and Wiesel b) that we can replicate the ""invariance disruption"" experiments using these models with a temporal association learning rule to develop and maintain invariance, and c) that we can thereby explain the apparent discrepancies between psychophysics and singe cells effects. We argue that these models account for the stability of perceptual invariance despite the underlying plasticity of the system, the variability of the visual world and expected noise in the biological mechanisms.",,13 p.,,,"vision, object recognition",Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,867
Tomaso Poggio,"Chikkerur, Sharat; Poggio, Tomaso",2011-04-21T18:15:06Z,2011-04-21T18:15:06Z,2011-04-14,http://hdl.handle.net/1721.1/62293,MIT-CSAIL-TR-2011-021; CBCL-298,Approximations in the HMAX Model,"The HMAX model is a biologically motivated architecture for computer vision whose components are in close agreement with existing physiological evidence. The model is capable of achieving close to human level performance on several rapid object recognition tasks. However, the model is computationally bound and has limited engineering applications in its current form. In this report, we present several approximations in order to increase the efficiency of the HMAX model. We outline approximations at several levels of the hierarchy and empirically evaluate the trade-offs between efficiency and accuracy. We also explore ways to quantify the representation capacity of the model.",,12 p.,,,"object recognition, approximation",Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2011,849
Daniela Rus,"Julian, Brian J.; Angermann, Michael; Schwager, Mac; Rus, Daniela",2011-07-15T17:15:17Z,2011-07-15T17:15:17Z,2011-09-25,http://hdl.handle.net/1721.1/64821,MIT-CSAIL-TR-2011-034,A Scalable Information Theoretic Approach to Distributed Robot Coordination,"This paper presents a scalable information theoretic approach to infer the state of an environment by distributively controlling robots equipped with sensors. The robots iteratively estimate the environment state using a recursive Bayesian filter, while continuously moving to improve the quality of the estimate by following the gradient of mutual information. Both the filter and the controller use a novel algorithm for approximating the robots' joint measurement probabilities, which combines consensus (for decentralization) and sampling (for scalability). The approximations are shown to approach the true joint measurement probabilities as the size of the consensus rounds grows or as the network becomes complete. The resulting gradient controller runs in constant time with respect to the number of robots, and linear time with respect to the number of sensor measurements and environment discretization cells, while traditional mutual information methods are exponential in all of these quantities. Furthermore, the controller is proven to be convergent between consensus rounds and, under certain conditions, is locally optimal. The complete distributed inference and coordination algorithm is demonstrated in experiments with five quad-rotor flying robots and simulations with 100 robots.",,10 p.,,,,Robotics (Rus),"This work is sponsored by the Department of the Air Force under Air Force contract number FA8721-05-C-0002. The opinions, interpretations, recommendations, and conclusions are those of the authors and are not necessarily endorsed by the United States Government. This work is also supported in part by ARO grant number W911NF-05-1-0219, ONR grant number N00014-09-1-1051, NSF grant number EFRI-0735953, ARL grant number W911NF-08-2-0004, MIT Lincoln Laboratory, the European Commission, and the Boeing Company.",,,,In Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS 11),,,,,,,,,,,,,,,,,,,,en-US,,,,2011,868
Russ Tedrake,"Platt, Robert, Jr.; Kaelbling, Leslie; Lozano-Perez, Tomas; Tedrake, Russ",2011-09-15T18:30:11Z,2011-09-15T18:30:11Z,2011-08-27,http://hdl.handle.net/1721.1/65856,MIT-CSAIL-TR-2011-039,A hypothesis-based algorithm for planning and control in non-Gaussian belief spaces,"We consider the partially observable control problem where it is potentially necessary to perform complex information-gathering operations in order to localize state. One approach to solving these problems is to create plans in belief-space, the space of probability distributions over the underlying state of the system. The belief-space plan encodes a strategy for performing a task while gaining information as necessary. Most approaches to belief-space planning rely upon representing belief state in a particular way (typically as a Gaussian). Unfortunately, this can lead to large errors between the assumed density representation and the true belief state. We propose a new computationally efficient algorithm for planning in non-Gaussian belief spaces. We propose a receding horizon re-planning approach where planning occurs in a low-dimensional sampled representation of belief state while the true belief state of the system is monitored using an arbitrary accurate high-dimensional representation. Our key contribution is a planning problem that, when solved optimally on each re-planning step, is guaranteed, under certain conditions, to enable the system to gain information. We prove that when these conditions are met, the algorithm converges with probability one. We characterize algorithm performance for different parameter settings in simulation and report results from a robot experiment that illustrates the application of the algorithm to robot grasping.",,22 p.,,,,Robot Locomotion Group,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,Russ Tedrake; Robot Locomotion Group,,,,,,,,,,,,,,,,,,,en-US,,,,2011,866
Fredo Durand,"Levin, Anat; Weiss, Yair; Durand, Fredo; Freeman, William T.",2011-04-04T15:45:25Z,2011-04-04T15:45:25Z,2011-04-04,http://hdl.handle.net/1721.1/62035,MIT-CSAIL-TR-2011-020,Efficient Marginal Likelihood Optimization in Blind Deconvolution,"In blind deconvolution one aims to estimate from an input blurred image y a sharp image x and an unknown blur kernel k. Recent research shows that a key to success is to consider the overall shape of the posterior distribution p(x, k|y) and not only its mode. This leads to a distinction between MAPx,k strategies which estimate the mode pair x, k and often lead to undesired results, and MAPk strategies which select the best k while marginalizing over all possible x images. The MAPk principle is significantly more robust than the MAPx,k one, yet, it involves a challenging marginalization over latent images. As a result, MAPk techniques are considered complicated, and have not been widely exploited. This paper derives a simple approximated MAPk algorithm which involves only a modest modification of common MAPx,k algorithms. We show that MAPk can, in fact, be optimized easily, with no additional computational complexity.",,12 p.,,,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,847
,"Jayaraman, Karthick; Ganesh, Vijay; Tripunitara, Mahesh; Rinard, Martin C.; Chapin, Steve J.",2011-04-28T20:30:11Z,2011-04-28T20:30:11Z,2011-04-27,http://hdl.handle.net/1721.1/62562,MIT-CSAIL-TR-2011-026,ARBAC Policy for a Large Multi-National Bank,"Administrative role-based access control (ARBAC) is the first comprehensive administrative model proposed for role-based access control (RBAC). ARBAC has several features for designing highly expressive policies, but current work has not highlighted the utility of these expressive policies. In this report, we present a case study of designing an ARBAC policy for a bank comprising 18 branches. Using this case study we provide an assessment about the features of ARBAC that are likely to be used in realistic policies.",,6 p.,,,ARBAC policy; computer security; access control,Program Analysis,,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,852
Li-Shiuan Peh,"Krishna, Tushar; Beckmann, Bradford M.; Peh, Li-Shiuan; Reinhardt, Steven K.",2011-03-14T19:45:24Z,2011-03-14T19:45:24Z,2011-03-14,http://hdl.handle.net/1721.1/61695,MIT-CSAIL-TR-2011-013,BOOM: Broadcast Optimizations for On-chip Meshes,"Future many-core chips will require an on-chip network that can support broadcasts and multicasts at good power-performance. A vanilla on-chip network would send multiple unicast packets for each broadcast packet, resulting in latency, throughput and power overheads. Recent research in on-chip multicast support has proposed forking of broadcast/multicast packets within the network at the router buffers, but these techniques are far from ideal, since they increase buffer occupancy which lowers throughput, and packets incur delay and power penalties at each router. In this work, we analyze an ideal broadcast mesh; show the substantial gaps between state-of-the-art multicast NoCs and the ideal; then propose BOOM, which comprises a WHIRL routing protocol that ideally load balances broadcast traffic, a mXbar multicast crossbar circuit that enables multicast traversal at similar energy-delay as unicasts, and speculative bypassing of buffering for multicast flits. Together, they enable broadcast packets to approach the delay, energy, and throughput of the ideal fabric. Our simulations show BOOM realizing an average network latency that is 5% off ideal, attaining 96% of ideal throughput, with energy consumption that is 9% above ideal. Evaluations using synthetic traffic show BOOM achieving a latency reduction of 61%, throughput improvement of 63%, and buffer power reduction of 80% as compared to a baseline broadcast. Simulations with PARSEC benchmarks show BOOM reducing average request and network latency by 40% and 15% respectively.",,12 p.,,,multicore,Computer Architecture,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2011,841
Tomaso Poggio,"Mroueh, Youssef; Poggio, Tomaso; Rosasco, Lorenzo; Slotine, Jean-Jacques E.",2011-09-27T20:30:07Z,2011-09-27T20:30:07Z,2011-09-27,http://hdl.handle.net/1721.1/66085,MIT-CSAIL-TR-2011-043; CBCL-305,Multi-Class Learning: Simplex Coding And Relaxation Error,"We study multi-category classification in the framework of computational learning theory. We show how a relaxation approach, which is commonly used in binary classification, can be generalized to the multi-class setting. We propose a vector coding, namely the simplex coding, that allows to introduce a new notion of multi-class margin and cast multi-category classification into a vector valued regression problem. The analysis of the relaxation error be quantified and the binary case is recovered as a special case of our theory. From a computational point of view we can show that using the simplex coding we can design regularized learning algorithms for multi-category classification that can be trained at a complexity which is independent to the number of classes.",,3 p.,,,computational learning; machine learning; convex relaxation,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,870
Nickolai Zeldovich,"Boneh, Dan; Mazieres, David; Popa, Raluca Ada",2011-03-31T20:15:08Z,2011-03-31T20:15:08Z,2011-03-30,http://hdl.handle.net/1721.1/62006,MIT-CSAIL-TR-2011-018,Remote Oblivious Storage: Making Oblivious RAM Practical,"Remote storage of data has become an increasingly attractive and advantageous option, especially due to cloud systems. While encryption protects the data, it does not hide the access pattern to the data. A natural solution is to access remote storage using an Oblivious RAM (ORAM) which provably hides all access patterns. While ORAM is asymptotically efficient, the best existing scheme (Pinkas and Reinman, Crypto'10) still has considerable overhead for a practical implementation: for M stored items, it stores 4 times and sometimes 6 times more items remotely, requires O(log2 M) round trips to storage server per request, and periodically blocks all data requests to shuffle all storage (which is a lengthy process). In this paper, we first define a related notion to ORAM, oblivious storage (OS), which captures more accurately and naturally the security setting of remote storage. Then, we propose a new ORAM/OS construction that solves the practicality issues just outlined: it has a storage constant of ~ 1, achieves O(1) round trips to the storage server per request, and allows requests to happen concurrently with shuffle without jeopardizing security. Our construction consists of a new organization of server memory into a flat main part and a hierarchical shelter, a client-side index for rapidly locating identifiers at the server, a new shelter serving requests concurrent with the shuffle, and a data structure for locating items efficiently in a partially shuffled storage.",,18 p.,,,access patterns; data privacy,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,845
Martin Rinard,"Carbin, Michael; Kim, Deokhwan; Misailovic, Sasa; Rinard, Martin C.",2011-11-15T18:15:04Z,2011-11-15T18:15:04Z,2011-11-15,http://hdl.handle.net/1721.1/67031,MIT-CSAIL-TR-2011-050,Reasoning about Relaxed Programs,"A number of approximate program transformations have recently emerged that enable transformed programs to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control program execution. We call such transformed programs relaxed programs -- they have been extended with additional nondeterminism to relax their semantics and offer greater execution flexibility. We present programming language constructs for developing relaxed programs and proof rules for reasoning about properties of relaxed programs. Our proof rules enable programmers to directly specify and verify acceptability properties that characterize the desired correctness relationships between the values of variables in a program's original semantics (before transformation) and its relaxed semantics. Our proof rules also support the verification of safety properties (which characterize desirable properties involving values in individual executions). The rules are designed to support a reasoning approach in which the majority of the reasoning effort uses the original semantics. This effort is then reused to establish the desired properties of the program under the relaxed semantics. We have formalized the dynamic semantics of our target programming language and the proof rules in Coq, and verified that the proof rules are sound with respect to the dynamic semantics. Our Coq implementation enables developers to obtain fully machine checked verifications of their relaxed programs.",,11 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,874
John Leonard,"Benjamin, Michael R.; Schmidt, Henrik; Newman, Paul; Leonard, John J.",2011-08-03T17:15:06Z,2011-08-03T17:15:06Z,2011-08-03,http://hdl.handle.net/1721.1/65073,MIT-CSAIL-TR-2011-037,An Overview of MOOS-IvP and a Users Guide to the IvP Helm - Release 4.2.1,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moos-ivp.org.",,262 p.,,,Unmanned Underwater Vehicles; UUV; Unmanned Surface Vehicles; USV; Unmanned Vehicles; Autonomy; Marine Vehicles; Marine Autonomy; pHelmIvP; MOOS; Autonomous Vehicles; Autonomous Marine Vehicles; Unmanned Marine Vehicles; Multi-objective Optimization; Behavior-Based Control; Behavior-Based Architecture,Marine Robotics,,Creative Commons Attribution-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,864
John Leonard,"Whelan, Thomas; Johannsson, Hordur; Kaess, Michael; Leonard, John J.; McDonald, John",2012-09-25T16:00:05Z,2012-09-25T16:00:05Z,2012-09-17,http://hdl.handle.net/1721.1/73167,MIT-CSAIL-TR-2012-031,Robust Tracking for Real-Time Dense RGB-D Mapping with Kintinuous,"This paper describes extensions to the Kintinuous algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused real-time surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.",,8 p.,,,,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,909
Fredo Durand,"Belcour, Laurent; Soler, Cyril; Subr, Kartic; Holzschuch, Nicolas; Durand, Fredo",2012-11-16T18:00:09Z,2012-11-16T18:00:09Z,2012-11-16,http://hdl.handle.net/1721.1/74662,MIT-CSAIL-TR-2012-034,5D Covariance Tracing for Efficient Defocus and Motion Blur,"The rendering of effects such as motion blur and depth-of-field requires costly 5D integrals. We dramatically accelerate their computation through adaptive sampling and reconstruction based on the prediction of the anisotropy and bandwidth of the integrand. For this, we develop a new frequency analysis of the 5D temporal light-field, and show that first-order motion can be handled through simple changes of coordinates in 5D. We further introduce a compact representation of the spectrum using the co- variance matrix and Gaussian approximations. We derive update equations for the 5 × 5 covariance matrices for each atomic light transport event, such as transport, occlusion, BRDF, texture, lens, and motion. The focus on atomic operations makes our work general, and removes the need for special-case formulas. We present a new rendering algorithm that computes 5D covariance matrices on the image plane by tracing paths through the scene, focusing on the single-bounce case. This allows us to reduce sampling rates when appropriate and perform reconstruction of images with complex depth-of-field and motion blur effects.",,19 p.,,,Rendering; Computer Graphics; Fourier,Computer Graphics,,,,,,Fredo Durand,,,,,,,,,,,,,,,,,,,,,,,2012,913
Tomaso Poggio,"Little, Anna V.; Maggioni, Mauro; Rosasco, Lorenzo",2012-09-10T18:00:08Z,2012-09-10T18:00:08Z,2012-09-08,http://hdl.handle.net/1721.1/72597,MIT-CSAIL-TR-2012-029; CBCL-310,"Multiscale Geometric Methods for Data Sets I: Multiscale SVD, Noise and Curvature","Large data sets are often modeled as being noisy samples from probability distributions in R^D, with D large. It has been noticed that oftentimes the support M of these probability distributions seems to be well-approximated by low-dimensional sets, perhaps even by manifolds. We shall consider sets that are locally well approximated by k-dimensional planes, with k << D, with k-dimensional manifolds isometrically embedded in R^D being a special case. Samples from this distribution; are furthermore corrupted by D-dimensional noise. Certain tools from multiscale geometric measure theory and harmonic analysis seem well-suited to be adapted to the study of samples from such probability distributions, in order to yield quantitative geometric information about them. In this paper we introduce and study multiscale covariance matrices, i.e. covariances corresponding to the distribution restricted to a ball of radius r, with a fixed center and varying r, and under rather general geometric assumptions we study how their empirical, noisy counterparts behave. We prove that in the range of scales where these covariance matrices are most informative, the empirical, noisy covariances are close to their expected, noiseless counterparts. In fact, this is true as soon as the number of samples in the balls where the covariance matrices are computed is linear in the intrinsic dimension of M. As an application, we present an algorithm for estimating the intrinsic dimension of M.",,59 p.,,,machine learning; high dimensional data,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,907
Nick Roy,"Tellex, Stefanie; Thaker, Pratiksha; Deits, Robin; Simeonov, Dimitar; Kollar, Thomas; Roy, Nicholas",2012-01-24T22:30:02Z,2012-01-24T22:30:02Z,2012-01-23,http://hdl.handle.net/1721.1/68651,MIT-CSAIL-TR-2012-002,Toward a Probabilistic Approach to Acquiring Information from Human Partners Using Language,"Our goal is to build robots that can robustly interact with humans using natural language. This problem is extremely challenging because human language is filled with ambiguity, and furthermore, the robot's model of the environment might be much more limited than the human partner. When humans encounter ambiguity in dialog with each other, a key strategy to resolve it is to ask clarifying questions about whatthey do not understand. This paper describes an approach for enabling robots to take the same approach: asking the human partner clarifying questions about ambiguous commands in order to infer better actions. The robot fuses information from the command, the question, and the answer by creating a joint probabilistic graphical model in the Generalized Grounding Graph framework. We demonstrate that by performing inference using information from the command, question and answer, the robot is able to infer object groundings and follow commands with higher accuracythan by using the command alone.",,2 p.,,,"dialog, robotics, question-asking","Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2012,882
Leslie Kaelbling; Tomas Lozano-Perez,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2012-07-03T18:00:04Z,2012-07-03T18:00:04Z,2012-07-03,http://hdl.handle.net/1721.1/71529,MIT-CSAIL-TR-2012-019,Integrated robot task and motion planning in belief space,"In this paper, we describe an integrated strategy for planning, perception, state-estimation and action in complex mobile manipulation domains. The strategy is based on planning in the belief space of probability distribution over states. Our planning approach is based on hierarchical goal regression (pre-image back-chaining). We develop a vocabulary of fluents that describe sets of belief states, which are goals and subgoals in the planning process. We show that a relatively small set of symbolic operators lead to task-oriented perception in support of the manipulation goals. An implementation of this method is demonstrated in simulation and on a real PR2 robot, showing robust, flexible solution of mobile manipulation problems with multiple objects and substantial uncertainty.",,80 p.,,,,Learning and Intelligent Systems,"This work was supported in part by the NSF under Grant No. 1117325. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We also gratefully acknowledge support from ONR MURI grant N00014-09-1-1051, from AFOSR grant AOARD-104135 and from the Singapore Ministry of Education under a grant to the Singapore-MIT International Design Center. We thank Willow Garage for the use of the PR2 robot as part of the PR2 Beta Program.",Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2012,898
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",2012-06-27T20:15:03Z,2012-06-27T20:15:03Z,2012-06-22,http://hdl.handle.net/1721.1/71232,MIT-CSAIL-TR-2012-017,Epistemic Implementation and The Arbitrary-Belief Auction,"In settings of incomplete information we put forward an epistemic framework for designing mechanisms that successfully leverage the players' arbitrary higher-order beliefs, even when such beliefs are totally wrong, and even when the players are rational in a very weak sense. Following Aumann (1995), we consider a player i rational if he uses a pure strategy s_i such that no alternative pure strategy s_i' performs better than s_i in every world i considers possible, and consider him order-k rational if he is rational and believes that all other players are order-(k-1) rational. We then introduce an iterative deletion procedure of dominated strategies and use it to precisely characterize the strategies consistent with the players being order-k rational. We exemplify the power of our framework in single-good auctions by introducing and achieving a new class of revenue benchmarks, defined over the players' arbitrary beliefs, that can be much higher than classical ones, and are unattainable by traditional mechanisms. Namely, we exhibit a mechanism that, for every k greater than or equal to 0 and epsilon>0 and whenever the players are order-(k+1) rational, guarantees revenue greater than or equal to G^k-epsilon, where G^k is the second highest belief about belief about ... (k times) about the highest valuation of some player, even when such a player's identity is not precisely known. Importantly, our mechanism is possibilistic interim individually rational. Essentially this means that, based on his beliefs, a player's utility is non-negative not in expectation, but in each world he believes possible. We finally show that our benchmark G^k is so demanding that it separates the revenue achievable with order-k rational players from that achievable with order-(k+1) rational ones. That is, no possibilistic interim individually rational mechanism can guarantee revenue greater than or equal to G^k-c, for any constant c>0, when the players are only order-k rational.",,31 p.,,,higher-order beliefs; higher-order rationality; revenue,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2012,896
Barbara Liskov,"Liskov, Barbara; Cowling, James",2012-07-23T19:15:02Z,2012-07-23T19:15:02Z,2012-07-23,http://hdl.handle.net/1721.1/71763,MIT-CSAIL-TR-2012-021,Viewstamped Replication Revisited,"This paper presents an updated version of Viewstamped Replication, a replication technique that handles failures in which nodes crash. It describes how client requests are handled, how the group reorganizes when a replica fails, and how a failed replica is able to rejoin the group. The paper also describes a number of important optimizations and presents a protocol for handling reconfigurations that can change both the group membership and the number of failures the group is able to handle.",,14 p.,,,VR; state machine replication; consensus; fault-tolerance; agreement,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,900
Armando Solar-Lezama,"Cheung, Alvin; Solar-Lezama, Armando; Madden, Samuel",2012-08-13T21:15:04Z,2012-08-13T21:15:04Z,2012-08-13,http://hdl.handle.net/1721.1/72106,MIT-CSAIL-TR-2012-025,Using Program Synthesis for Social Recommendations,"This paper presents a new approach to select events of interest to a user in a social media setting where events are generated by the activities of the user's friends through their mobile devices. We argue that given the unique requirements of the social media setting, the problem is best viewed as an inductive learning problem, where the goal is to first generalize from the users' expressed ""likes"" and ""dislikes"" of specific events, then to produce a program that can be manipulated by the system and distributed to the collection devices to collect only data of interest. The key contribution of this paper is a new algorithm that combines existing machine learning techniques with new program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application. The approach also improves on standard machine learning techniques in that it produces clear programs that can be manipulated to optimize data collection and filtering.",,10 p.,,,recommender systems; social networking applications; support vector machines,Computer-Aided Programming,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2012,902
Barbara Liskov,"Liskov, Barbara",2012-09-17T18:30:04Z,2012-09-17T18:30:04Z,2012-09-14,http://hdl.handle.net/1721.1/73017,MIT-CSAIL-TR-2012-030,Aeolus Reference Manual,This document describes the interface that the Aeolus information flow platform provides for users who are implementing applications using Java. The document explains how the Aeolus features are made available by means of a Java library.,,37 p.,,,information flow control; DIFC; data privacy,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,908
Nancy Lynch,"Censor-Hillel, Keren; Haeupler, Bernhard; Lynch, Nancy; Medard, Muriel",2012-09-05T22:00:07Z,2012-09-05T22:00:07Z,2012-08-27,http://hdl.handle.net/1721.1/72536,MIT-CSAIL-TR-2012-026,Bounded-Contention Coding for Wireless Networks in the High SNR Regime,"Efficient communication in wireless networks is typically challenged by the possibility of interference among several transmitting nodes. Much important research has been invested in decreasing the number of collisions in order to obtain faster algorithms for communication in such networks. This paper proposes a novel approach for wireless communication, which embraces collisions rather than avoiding them, over an additive channel. It introduces a coding technique called Bounded-Contention Coding (BCC) that allows collisions to be successfully decoded by the receiving nodes into the original transmissions and whose complexity depends on a bound on the contention among the transmitters. BCC enables deterministic local broadcast in a network with n nodes and at most a transmitters with information of L bits each within O(a log n + aL) bits of communication with full-duplex radios, and O((a log n + aL)(log n)) bits, with high probability, with half-duplex radios. When combined with random linear network coding, BCC gives global broadcast within O((D + a + log n)(a log n + L)) bits, with high probability. This also holds in dynamic networks that can change arbitrarily over time by a worst-case adversary. When no bound on the contention is given, it is shown how to probabilistically estimate it and obtain global broadcast that is adaptive to the true contention in the network.",,17 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,905
Silvio Micali,"Chiesa, Alessandro; Micali, Silvio; Zhu, Zeyuan Allen",2012-09-07T22:15:03Z,2012-09-07T22:15:03Z,2012-09-07,http://hdl.handle.net/1721.1/72584,MIT-CSAIL-TR-2012-028,A Social-Welfare Optimal Probabilistic Mechanism for Knightian Single-Good Auctions,"We provide an optimal probabilistic mechanism for maximizing social welfare in single-good auctions when each player does not know his true valuation for the good, but only a set of valuations that is guaranteed to include his true one.",,19 p.,,,Knightian Auctions; Probabilistic Mechanisms; Social Welfare,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,906
Fredo Durand,"Levin, Anat; Nadler, Boaz; Durand, Fredo; Freeman, William T.",2012-07-31T17:45:08Z,2012-07-31T17:45:08Z,2012-10-07,http://hdl.handle.net/1721.1/71919,MIT-CSAIL-TR-2012-022,"Patch complexity, finite pixel correlations and optimal denoising","Image restoration tasks are ill-posed problems, typically solved withpriors. Since the optimal prior is the exact unknown density of natural images,actual priors are only approximate and typically restricted to small patches. Thisraises several questions: How much may we hope to improve current restorationresults with future sophisticated algorithms? And more fundamentally, even withperfect knowledge of natural image statistics, what is the inherent ambiguity ofthe problem? In addition, since most current methods are limited to finite supportpatches or kernels, what is the relation between the patch complexity of naturalimages, patch size, and restoration errors? Focusing on image denoising, we makeseveral contributions. First, in light of computational constraints, we study the relation between denoising gain and sample size requirements in a non parametricapproach. We present a law of diminishing return, namely that with increasingpatch size, rare patches not only require a much larger dataset, but also gain littlefrom it. This result suggests novel adaptive variable-sized patch schemes for denoising. Second, we study absolute denoising limits, regardless of the algorithmused, and the converge rate to them as a function of patch size. Scale invarianceof natural images plays a key role here and implies both a strictly positive lowerbound on denoising and a power law convergence. Extrapolating this parametriclaw gives a ballpark estimate of the best achievable denoising, suggesting thatsome improvement, although modest, is still possible.",,23 p.,,,,Computer Graphics,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,supplemental material for conference paper at ECCV 2012 (European Conf. on Computer Vision),,,,,,,,,,,,,,,,,,,,,,,,2012,912
Brian Williams,"Dong, Shuonan",2018-01-30T23:46:40Z,2018-01-30T23:46:40Z,2012-08-23,http://hdl.handle.net/1721.1/113367,MIT-CSAIL-TR-2018-007,Learning and recognition of hybrid manipulation tasks in variable environments using probabilistic flow tubes,"Robots can act as proxies for human operators in environments where a human operator is not present or cannot directly perform a task, such as in dangerous or remote situations. Teleoperation is a common interface for controlling robots that are designed to be human proxies. Unfortunately, teleoperation may fail to preserve the natural fluidity of human motions due to interface limitations such as communication delays, non-immersive sensing, and controller uncertainty. I envision a robot that can learn a set of motions that a teleoperator commonly performs, so that it can autonomously execute routine tasks or recognize a user's motion in real time. Tasks can be either primitive activities or compound plans. During online operation, the robot can recognize a user's teleoperated motions on the fly and offer real-time assistance, for example, by autonomously executing the remainder of the task. I realize this vision by addressing three main problems: (1) learning primitive activities by identifying significant features of the example motions and generalizing the behaviors from user demonstration trajectories; (2) recognizing activities in real time by determining the likelihood that a user is currently executing one of several learned activities; and (3) learning complex plans by generalizing a sequence of activities, through auto-segmentation and incremental learning of previously unknown activities. To solve these problems, I first present an approach to learning activities from human demonstration that (1) provides flexibility and robustness when encoding a user's demonstrated motions by using a novel representation called a probabilistic flow tube, and (2) automatically determines the relevant features of a motion so that they can be preserved during autonomous execution in new situations. I next introduce an approach to real-time motion recognition that (1) uses temporal information to successfully model motions that may be non-Markovian, (2) provides fast real-time recognition of motions in progress by using an incremental temporal alignment approach, and (3) leverages the probabilistic flow tube representation to ensure robustness during recognition against varying environment states. Finally, I develop an approach to learn combinations of activities that (1) automatically determines where activities should be segmented in a sequence and (2) learns previously unknown activities on the fly. I demonstrate the results of autonomously executing motions learned by my approach on two different robotic platforms supporting user-teleoperated manipulation tasks in a variety of environments. I also present the results of real-time recognition in different scenarios, including a robotic hardware platform. Systematic testing in a two-dimensional environment shows up to a 27% improvement in activity recognition rates over prior art, while maintaining average computing times for incremental recognition of less than half of human reaction time.",,144 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,,,,,,,,2018-01-30T23:46:40Z,,,,,,,,,,,,,,,,2012,903
Nancy Lynch,"Musial, Peter M.",2012-09-05T22:00:14Z,2012-09-05T22:00:14Z,2012-08-27,http://hdl.handle.net/1721.1/72537,MIT-CSAIL-TR-2012-027,From Formal Methods to Executable Code,"The objective of this work is the derivation of software that is verifiably correct. Our approach is to abstract system specifications and model these in a formal framework called Timed Input/Output Automata, which provides a notation for expressing distributed systems and mathematical support for reasoning about their properties. Although formal reasoning is easier at an abstract level, it is not clear how to transform these abstractions into executable code. During system implementation, when an abstract system specification is left up to human interpretation, then this opens a possibility of undesirable behaviors being introduced into the final code, thereby nullifying all formal efforts. This manuscript addresses this issue and presents a set of transformation methods for systems described as a network to timed automata into Java code for distributed platforms. We prove that the presented transformation methods preserve guarantees of the source specifications, and therefore, result in code that is correct by construction.",,92 p.,,,,Theory of Computation,,,,Note: the cover page of this report shows an incorrect title.  The title given on the first page of the document itself is correct.,,,,,,,,,,,,,,,,,,,,,,,,,2012,904
Tomaso Poggio,"Tacchetti, Andrea; Mallapragada, Pavan S.; Santoro, Matteo; Rosasco, Lorenzo",2012-02-06T21:15:06Z,2012-02-06T21:15:06Z,2012-01-31,http://hdl.handle.net/1721.1/69034,MIT-CSAIL-TR-2012-003; CBCL-306,GURLS: a Toolbox for Regularized Least Squares Learning,"We present GURLS, a toolbox for supervised learning based on the regularized least squares algorithm. The toolbox takes advantage of all the favorable properties of least squares and is tailored to deal in particular with multi-category/multi-label problems. One of the main advantages of GURLS is that it allows training and tuning a multi-category classifier at essentially the same cost of one single binary classifier. The toolbox provides a set of basic functionalities including different training strategies and routines to handle computations with very large matrices by means of both memory-mapped storage and distributed task execution. The system is modular and can serve as a basis for easily prototyping new algorithms. The toolbox is available for download, easy to set-up and use.",,6 p.,,,"Matlab; Computational Learning; Regularized Least Squares; Large Scale, Multiclass problems; C++",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,MIT CSAIL,,,2012,883
Brian Williams,"Ono, Masahiro",2018-01-31T00:01:17Z,2018-01-31T00:01:17Z,2012-01-20,http://hdl.handle.net/1721.1/113370,MIT-CSAIL-TR-2018-010,Energy-efficient Control of a Smart Grid with Sustainable Homes based on Distributing Risk,"The goal of this thesis is to develop a distributed control system for a smart grid with sustainable homes. A central challenge is how to enhance energy efficiency in the presence of uncertainty. A major source of uncertainty in a smart grid is intermittent energy production by renewable energy sources. In the face of global climate change, it is crucial to reduce dependence on fossil fuels and shift to renewable energy sources, such as wind and solar. However, a large-scale introduction of wind and solar generation to an electrical grid poses a significant risk of blackouts since the energy supplied by the renewables is unpredictable and intermittent. The uncertain behavior of renewable energy sources increases the risk of blackouts. Therefore, an important challenge is to develop an intelligent control mechanism for the electrical grid that is both reliable and efficient. Uncertain weather conditions and human behavior pose challenges for a smart home. For example, autonomous room temperature control of a residential building may occasionally make the room environment uncomfortable for residents. Autonomous controllers must be able to take residents' preferences as an input, and to control the indoor environment in an energy-efficient manner while limiting the risk of failure to meet the residents' requirements in the presence of uncertainties. In order to overcome these challenges, we propose a distributed robust control method for a smart grid that includes smart homes as its building components. The proposed method consists of three algorithms: 1) market-based contingent energy dispatcher for an electrical grid, 2) a risk-sensitive plan executive for temperature control of a residential building, and 3) a chance-constrained model-predictive controller with a probabilistic guarantee of constraint satisfaction, which can control continuously operating systems such as an electrical grid and a building. We build the three algorithms upon the chance-constrained programming framework: minimization of a given cost function with chance constraints, which bound the probability of failure to satisfy given state constraints. Although these technologies provide promising capabilities, they cannot contribute to sustainability unless they are accepted by the society. In this thesis we specify policy challenges for a smart grid and a smart home, and discuss policy options that gives economical and regulatory incentives for the society to introduce these technologies on a large scale.",,145 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-31T00:01:18Z,,,,,,,,,,,,,,,,2012,881
John Leonard,"Johannsson, Hordur; Kaess, Michael; Fallon, Maurice; Leonard, John J.",2012-05-25T19:15:05Z,2012-05-25T19:15:05Z,2012-05-25,http://hdl.handle.net/1721.1/70952,MIT-CSAIL-TR-2012-013,Temporally Scalable Visual SLAM using a Reduced Pose Graph,"In this paper, we demonstrate a system for temporally scalable visual SLAM using a reduced pose graph representation. Unlike previous visual SLAM approaches that use keyframes, our approach continually uses new measurements to improve the map, yet achieves efficiency by avoiding adding redundant frames and not using marginalization to reduce the graph. To evaluate our approach, we present results using an online binocular visual SLAM system that uses place recognition for both robustness and multi-session operation. To allow large-scale indoor mapping, our system automatically handles elevator rides based on accelerometer data. We demonstrate long-term mapping in a large multi-floor building, using approximately nine hours of data collected over the course of six months. Our results illustrate the capability of our visual SLAM system to scale in size with the area of exploration instead of the time of exploration.",,9 p.,,,robotics navigation,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,893
Ron Weiss,"Beal, Jacob; Weiss, Ron; Yaman, Fusun; Davidsohn, Noah; Adler, Aaron",2012-04-09T17:15:07Z,2012-04-09T17:15:07Z,2012-04-07,http://hdl.handle.net/1721.1/69973,MIT-CSAIL-TR-2012-008,"A Method for Fast, High-Precision Characterization of Synthetic Biology Devices","Engineering biological systems with predictable behavior is a foundational goal of synthetic biology. To accomplish this, it is important to accurately characterize the behavior of biological devices. Prior characterization efforts, however, have generally not yielded enough high-quality information to enable compositional design. In the TASBE (A Tool-Chain to Accelerate Synthetic Biological Engineering) project we have developed a new characterization technique capable of producing such data. This document describes the techniques we have developed, along with examples of their application, so that the techniques can be accurately used by others.",,25 p.,,,Synthetic biology; characterization; TASBE,Synthetic Biology,,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2012,889
Brian Williams,"Levine, Steven J.",2012-10-09T16:45:14Z,2012-10-09T16:45:14Z,2012-10-04,http://hdl.handle.net/1721.1/73686,MIT-CSAIL-TR-2012-033,Monitoring the Execution of Temporal Plans for Robotic Systems,"To achieve robustness in dynamic and uncertain environments, robotic systems must monitor the progress of their plans during execution. This thesis develops a plan executive called Pike that is capable of executing and monitoring plans. The execution monitor at its core quickly and efficiently detects relevant disturbances that threaten future actions in the plan. We present a set of novel offline algorithms that extract sets of candidate causal links from temporally-flexible plans. A second set of algorithms uses these causal links to monitor the execution online and detect problems with low latency. We additionally introduce the TBurton executive, a system capable of robustly meeting a user s high-level goals through the combined use of Pike and a temporal generative planner. An innovative voice-commanded robot is demonstrated in hardware and simulation that robustly meets high level goals and verbalizes any causes of failure using the execution monitor",,125 p.,,,temporal plan; execution monitoring; causal link; disturbance; executive,Model-based Embedded and Robotic Systems,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,,2012,911
Silvio Micali,"Chen, Jing; Micali, Silvio",2012-08-01T21:30:09Z,2012-08-01T21:30:09Z,2012-07-31,http://hdl.handle.net/1721.1/71953,MIT-CSAIL-TR-2012-023,"The Order Independence of Iterated Dominance in Extensive Games, with Connections to Mechanism Design and Backward Induction","Shimoji and Watson (1998) prove that a strategy of an extensive game is rationalizable in the sense of Pearce if and only if it survives the maximal elimination of conditionally dominated strategies. Briefly, this process iteratively eliminates conditionally dominated strategies according to a specific order, which is also the start of an order of elimination of weakly dominated strategies. Since the final set of possible payoff profiles, or terminal nodes, surviving iterated elimination of weakly dominated strategies may be order-dependent, one may suspect that the same holds for conditional dominance. We prove that, although the sets of strategy profiles surviving two arbitrary elimination orders of conditional dominance may be very different from each other, they are equivalent in the following sense: for each player i and each pair of elimination orders, there exists a function phi_i mapping each strategy of i surviving the first order to a strategy of i surviving the second order, such that, for every strategy profile s surviving the first order, the profile (phi_i(s_i))_i induces the same terminal node as s does. To prove our results we put forward a new notion of dominance and an elementary characterization of extensive-form rationalizability (EFR) that may be of independent interest. We also establish connections between EFR and other existing iterated dominance procedures, using our notion of dominance and our characterization of EFR.",,35 p.,,,extensive-form rationalizability; distinguishable dominance; order independence,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,901
Fredo Durand,"Gharbi, Michael; Malisiewicz, Tomasz; Paris, Sylvain; Durand, Frédo",2012-10-09T16:45:04Z,2012-10-09T16:45:04Z,2012-10-01,http://hdl.handle.net/1721.1/73685,MIT-CSAIL-TR-2012-032,A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although exemplar-based SVM is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of- the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency.",,11 p.,,,"Image retrieval, object detection, computer vision, parametric model",Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,910
John Leonard,"Whelan, Thomas; Kaess, Michael; Fallon, Maurice; Johannsson, Hordur; Leonard, John; McDonald, John",2012-07-23T17:45:03Z,2012-07-23T17:45:03Z,2012-07-19,http://hdl.handle.net/1721.1/71756,MIT-CSAIL-TR-2012-020,Kintinuous: Spatially Extended KinectFusion,"In this paper we present an extension to the KinectFusion algorithm that permits dense mesh-based mapping of extended scale environments in real-time. This is achieved through (i) altering the original algorithm such that the region of space being mapped by the KinectFusion algorithm can vary dynamically, (ii) extracting a dense point cloud from the regions that leave the KinectFusion volume due to this variation, and, (iii) incrementally adding the resulting points to a triangular mesh representation of the environment. The system is implemented as a set of hierarchical multi-threaded components which are capable of operating in real-time. The architecture facilitates the creation and integration of new modules with minimal impact on the performance on the dense volume tracking and surface reconstruction modules. We provide experimental results demonstrating the system's ability to map areas considerably beyond the scale of the original KinectFusion algorithm including a two story apartment and an extended sequence taken from a car at night. In order to overcome failure of the iterative closest point (ICP) based odometry in areas of low geometric features we have evaluated the Fast Odometry from Vision (FOVIS) system as an alternative. We provide a comparison between the two approaches where we show a trade off between the reduced drift of the visual odometry approach and the higher local mesh quality of the ICP-based approach. Finally we present ongoing work on incorporating full simultaneous localisation and mapping (SLAM) pose-graph optimisation.",,8 p.,,,,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,899
Leslie Kaelbling; Tomas Lozano-Perez,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2012-07-02T17:15:07Z,2012-07-02T17:15:07Z,2012-06-29,http://hdl.handle.net/1721.1/71521,MIT-CSAIL-TR-2012-018,Integrated Robot Task and Motion Planning in the Now,"This paper provides an approach to integrating geometric motion planning with logical task planning for long-horizon tasks in domains with many objects. We propose a tight integration between the logical and geometric aspects of planning. We use a logical representation which includes entities that refer to poses, grasps, paths and regions, without the need for a priori discretization. Given this representation and some simple mechanisms for geometric inference, we characterize the pre-conditions and effects of robot actions in terms of these logical entities. We then reason about the interaction of the geometric and non-geometric aspects of our domains using the general-purpose mechanism of goal regression (also known as pre-image backchaining). We propose an aggressive mechanism for temporal hierarchical decomposition, which postpones the pre-conditions of actions to create an abstraction hierarchy that both limits the lengths of plans that need to be generated and limits the set of objects relevant to each plan. We describe an implementation of this planning method and demonstrate it in a simulated kitchen environment in which it solves problems that require approximately 100 individual pick or place operations for moving multiple objects in a complex domain.",,68 p.,,,robot manipulation; motion planning,Learning and Intelligent Systems,"This work was supported in part by the NSF under Grant No. 1117325. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We also gratefully acknowledge support from ONR MURI grant N00014-09-1-1051, from AFOSR grant AOARD-104135 and from
the Singapore Ministry of Education under a grant to the Singapore-MIT International Design Center. We thank Willow Garage for the use of the PR2 robot as part of the PR2 Beta Program.",,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,897
Brian Williams,"Ono, Masahiro",2018-01-31T00:01:14Z,2018-01-31T00:01:14Z,2012-02-02,http://hdl.handle.net/1721.1/113369,MIT-CSAIL-TR-2018-009,"Robust, Goal-directed Plan Execution with Bounded Risk","There is an increasing need for robust optimal plan execution for multi-agent systems in uncertain environments, while guaranteeing an acceptable probability of success. For ex- ample, a fleet of unmanned aerial vehicles (UAVs) and autonomous underwater vehicles (AUVs) are required to operate autonomously for an extensive mission duration in an uncertain environment. Previous work introduced the concept of a model-based executive, which increases the level of autonomy, elevating the level at which systems are commanded. This thesis develops model-based executives that reason explicitly from a stochastic plant model to find the optimal course of action, while ensuring that the probability of failure is within a user-specified risk bound. This thesis presents two robust mode-based executives: probabilisticSulu orp-Sulu, and distributedprobabilisticSulu or dp-Sulu. The objective for p-Sulu and dp-Sulu is to allow users to command continuous, stochastic multi-agent systems in a manner that is both intuitive and safe. The user specifies the desired evolution of the plant state, as well as the acceptable probabilities of failure, as a temporal plan on states called a chance-constrained qualitative state plan (CCQSP). An example of a CCQSP statement is ""go to A through B within 30 minutes, with less than 0.001% probability of failure."" p-Sulu and dp-Sulu take a CCQSP, a continuous plant model with stochastic uncertainty, and an objective function as inputs, and outputs an optimal continuous control sequence, as well as an optimal discrete schedule. The difference between p-Sulu and dp-Sulu is that p-Sulu plans in a centralized manner while dp-Sulu plans in a distributed manner. dp-Sulu enables robust CCQSP execution for multi-agent systems. We solve the problem based on the key concept of risk allocation, which achieves tractability by allocating the specified risk to individual constraints and mapping the result into an equivalent deterministic constrained optimization problem. Risk allocation also enables a distributed plan execution for multi-agent systems by distributing the risk among agents to decompose the optimization problem. Building upon the risk allocation approach, we develop our first CCQSP executive, p-Sulu, in four spirals. First, we develop the Convex Risk Allocation (CRA) algorithm, which can solve a CCQSP planning problem with a convex state space and a fixed schedule, highlighting the capability of optimally allocating risk to individual constraints. Second, we develop the Non-convex Iterative Risk Allocation (NIRA) algorithm, which can handle non-convex state space. Third, we build upon NIRA a full-horizon CCQSP planner, p-Sulu FH, which can optimize not only the control sequence but also the schedule. Fourth, we develop p-Sulu, which enables the real-time execution of CCQSPs by employing the receding horizon approach. Our second CCQSP executive, dp-Sulu, is developed in two spirals. First, we develop the Market-based Iterative Risk Allocation (MIRA) algorithm, which can control a multi-agent system in a distributed manner by optimally distributing risk among agents through the market-based method called tatonnement. Second and finally, we integrate the capability of MIRA into p-Sulu to build the robust model-based executive, dp-Sulu, which can execute CCQSPs on multi-agent systems in a distributed manner. Our simulation results demonstrate that our executives can efficiently execute CCQSP planning problems with significantly reduced suboptimality compared to prior art.",,283 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,Brian Williams; Model-based Embedded and Robotic Systems,,,,,,,2018-01-31T00:01:14Z,,,,,,,,,,,,,,,,2012,885
Srini Devadas,"Kurian, George; Khan, Omer; Devadas, Srinivas",2012-05-22T20:15:03Z,2012-05-22T20:15:03Z,2012-05-22,http://hdl.handle.net/1721.1/70909,MIT-CSAIL-TR-2012-012,A Case for Fine-Grain Adaptive Cache Coherence,"As transistor density continues to grow geometrically, processor manufacturers are already able to place a hundred cores on a chip (e.g., Tilera TILE-Gx 100), with massive multicore chips on the horizon. Programmers now need to invest more effort in designing software capable of exploiting multicore parallelism. The shared memory paradigm provides a convenient layer of abstraction to the programmer, but will current memory architectures scale to hundreds of cores? This paper directly addresses the question of how to enable scalable memory systems for future multicores. We develop a scalable, efficient shared memory architecture that enables seamless adaptation between private and logically shared caching at the fine granularity of cache lines. Our data-centric approach relies on in hardware runtime profiling of the locality of each cache line and only allows private caching for data blocks with high spatio-temporal locality. This allows us to better exploit on-chip cache capacity and enable low-latency memory access in large-scale multicores.",,11 p.,,,,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,892
Martin Rinard,"Carbin, Michael; Misailovic, Sasa; Rinard, Martin",2013-06-20T17:00:08Z,2013-06-20T17:00:08Z,2013-06-19,http://hdl.handle.net/1721.1/79355,MIT-CSAIL-TR-2013-014,Verifying Quantitative Reliability of Programs That Execute on Unreliable Hardware,"Emerging high-performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. Full detection and recovery from soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors. In this paper we present Rely, a programming language that enables developers to reason about the quantitative reliability of an application -- namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces. We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification, that characterizes the reliability of the underlying hardware components, and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely.",,22 p.,,,"unreliable hardware, probabilistic semantics, quantitative reliability",Computer Architecture,"This research was supported in part by the National Science Foundation (Grants CCF-0905244, CCF-1036241, CCF-1138967, CCF-1138967, and IIS-0835652), the United States Department of Energy (Grant DE-SC0008923), and DARPA (Grants FA8650-11-C-7192, FA8750-12-2-0110).",,,,,,,,,,,,2013-06-20T17:00:08Z,,,,,,,,,,,,en,,,,2013,929
,"Jackson, Daniel",2013-08-12T02:30:04Z,2013-08-12T02:30:04Z,2013-08-08,http://hdl.handle.net/1721.1/79826,MIT-CSAIL-TR-2013-020,Conceptual Design of Software: A Research Agenda,"A research agenda in software design is outlined, focusing on the role of concepts. The notions of concepts as ""abstract affordances"" and of conceptual integrity are discussed, and a series of small examples of conceptual models is given.",,17 p.,,,Software design; Usability; Conceptual integrity; Conceptual models,Software Design,,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,2013-08-12T02:30:05Z,,,,,,,,,,,,,,,,2013,934
Brian Williams,"Wang, Andrew J.",2018-01-31T00:01:19Z,2018-01-31T00:01:19Z,2013-01-31,http://hdl.handle.net/1721.1/113371,MIT-CSAIL-TR-2018-011,Risk Allocation for Temporal Risk Assessment,"Temporal uncertainty arises when performing any activity in the natural world. When activities are composed into temporal plans, then, there is a risk of not meeting the plan requirements. Currently, we do not have quantitatively precise methods for assessing temporal risk of a plan. Existing methods that deal with temporal uncertainty either forgo probabilistic models or try to optimize a single objective, rather than satisfy multiple objectives. This thesis offers a method for evaluating whether a schedule exists that meets a set of temporal constraints, with acceptable risk of failure. Our key insight is to assume a form of risk allocation to each source of temporal uncertainty in our plan, such that we may reformulate the probabilistic plan into an STNU parameterized on the risk allocation. We show that the problem becomes a deterministic one of finding a risk allocation which implies a schedulable STNU within acceptable risk. By leveraging the principles behind STNU analysis, we derive conditions which encode this problem as a convex feasibility program over risk allocations. Furthermore, these conditions may be learned incrementally as temporal conflicts. Thus, to boost computational efficiency, we employ a generate-and-test approach to determine whether a schedule may be found.",,64 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,MEng thesis,,,,,,,,,2018-01-31T00:01:19Z,,,,,,,,,,,,,,,,2013,918
Tomas Lozano-Perez,"Perez, Alejandro",2013-11-18T20:00:08Z,2013-11-18T20:00:08Z,2013-11-18,http://hdl.handle.net/1721.1/82462,MIT-CSAIL-TR-2013-027,On Randomized Path Coverage of Configuration Spaces,We present a sampling-based algorithm that generates a set of locally-optimal paths that differ in visibility.,,13 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,2013-11-18T20:00:09Z,,,,,,,,,,,,,,,,2013,944
Brian Williams,"Timmons, Eric",2018-01-30T23:46:03Z,2018-01-30T23:46:03Z,2013-12-11,http://hdl.handle.net/1721.1/113364,MIT-CSAIL-TR-2018-004,"Fast, Approximate State Estimation of Concurrent Probabilistic Hybrid Automata","It is an undeniable fact that autonomous systems are simultaneously becoming more common place, more complex, and deployed in more inhospitable environments. Examples include smart homes, smart cars, Mars rovers, unmanned aerial vehicles, and autonomous underwater vehicles. A common theme that all of these autonomous systems share is that in order to appropriately control them and prevent mission failure, they must be able to quickly estimate their internal state and the state of the world. A natural representation of many real world systems is to describe them in terms of a mixture of continuous and discrete variables. Unfortunately, hybrid estimation is typically intractable due to the large space of possible assignments to the discrete variables. In this thesis, we investigate how to incorporate conflict directed techniques from the consistency-based, model-based diagnosis community into a hybrid framework that is no longer purely consistency based. We introduce a novel search algorithm, A&#8727; with Bounding Conflicts, that uses conflicts to not only record infeasiblilities, but also learn where in the search space the heuristic function provided to the A&#8727; search is weak (possibly due to heavy to moderate sensor or process noise). Additionally, we describe a hybrid state estimation algorithm that uses this new search to perform estimation on hybrid discrete/continuous systems.",,73 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-30T23:46:03Z,,,,,,,,,,,,,,,,2013,948
Patrick Winston,"Finlayson, Mark Alan",2013-11-01T17:18:16Z,2013-11-01T17:18:16Z,2013-11-01,http://hdl.handle.net/1721.1/81949,,Code for Java Libraries for Accessing the Princeton Wordnet: Comparison and Evaluation,"This archive contains the code and data for running the evaluations described in: Finlayson, Mark Alan (2014) ""Java Libraries for Accessing the Princeton Wordnet: comparison and Evaluation"" in Proceedings of the 7th Global Wordnet Conference (GWC 2014). Tartu, Estonia, 25-29 January 2014. The archive contains five Eclipse projects (compatible with Eclipse 3.8.0) that may be imported directly into an Eclipse workspace. You will need a Java 1.4, 1.5, and 1.6 JRE to run all the code in the archive. Paper abstract: Java is a popular programming language for natural language processing. I compare and evaluate 12 Java libraries designed to access the information in the original Princeton Wordnet databases. From this comparison emerges a set of decision criteria that will enable a user to pick the library most suited to their purposes. I identify five deciding features: (1) availability of similarity metrics; (2) support for editing; (3) availability via Maven; (4) compatibility with retired Java versions; and (5) support for Enterprise Java. I also provide a comparison of other features of each library, the information exposed by each API, and the versions of Wordnet each library supports, and I evaluate each library for the speed of various retrieval operations. In the case that the user's application does not require one of the deciding features, I show that my library, JWI, the MIT Java Wordnet Interface, is the highest-performance, widest-coverage, easiest-to-use library available.",,,,,,Genesis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,2013-11-01T17:18:17Z,,,,,,,,,,,,,,,,2013,943
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2013-07-31T18:30:05Z,2013-07-31T18:30:05Z,2013-09-01,http://hdl.handle.net/1721.1/79746,MIT-CSAIL-TR-2013-017,Jigsaw: Scalable Software-Defined Caches (Extended Version),"Shared last-level caches, widely used in chip-multiprocessors (CMPs), face two fundamental limitations. First, the latency and energy of shared caches degrade as the system scales up. Second, when multiple workloads share the CMP, they suffer from interference in shared cache accesses. Unfortunately, prior research addressing one issue either ignores or worsens the other: NUCA techniques reduce access latency but are prone to hotspots and interference, and cache partitioning techniques only provide isolation but do not reduce access latency. We present Jigsaw, a technique that jointly addresses the scalability and interference problems of shared caches. Hardware lets software define shares, collections of cache bank partitions that act as virtual caches, and map data to shares. Shares give software full control over both data placement and capacity allocation. Jigsaw implements efficient hardware support for share management, monitoring, and adaptation. We propose novel resource-management algorithms and use them to develop a system-level runtime that leverages Jigsaw to both maximize cache utilization and place data close to where it is used. We evaluate Jigsaw using extensive simulations of 16- and 64-core tiled CMPs. Jigsaw improves performance by up to 2.2x (18% avg) over a conventional shared cache, and significantly outperforms state-of-the-art NUCA and partitioning techniques.",,16 p.,,,"cache, memory, NUCA, partitioning, isolation, multicore, virtual memory",Computation Structures,This work was supported in part by DARPA PERFECT contract HR0011-13-2-0005 and Quanta Computer.,Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,2013-07-31T18:30:05Z,,,,,,,,,,,,,,,,2013,937
Saman Amarasinghe,"Ansel, Jason; Kamil, Shoaib; Veeramachaneni, Kalyan; O'Reilly, Una-May; Amarasinghe, Saman",2013-11-01T20:30:03Z,2013-11-01T20:30:03Z,2013-11-01,http://hdl.handle.net/1721.1/81958,MIT-CSAIL-TR-2013-026,OpenTuner: An Extensible Framework for Program Autotuning,"Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 6 distinct projects and 14 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.",,13 p.,,,,Computer Architecture,"This work is partially supported by DOE award DE-SC0005288 and DOD DARPA award HR0011-10-9-0009.  This research used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.",,,,,,,,,,,,2013-11-01T20:30:03Z,,,,,,,,,,,,,,,,2013,942
Nancy Lynch,"Cornejo, Alejandro; Lynch, Nancy; Sastry, Srikanth",2013-10-10T18:15:04Z,2013-10-10T18:15:04Z,2013-10-10,http://hdl.handle.net/1721.1/81371,MIT-CSAIL-TR-2013-025,Asynchronous Failure Detectors,"Failure detectors -- oracles that provide information about process crashes -- are an important abstraction for crash tolerance in distributed systems. The generality of failure-detector theory, while providing great expressiveness, poses significant challenges in developing a robust hierarchy of failure detectors. We address some of these challenges by proposing (1) a variant of failure detectors called asynchronous failure detectors and (2) an associated modeling framework. Unlike the traditional failure-detector framework, our framework eschews real-time completely. We show that asynchronous failure detectors are sufficiently expressive to include several popular failure detectors including, but not limited to, the canonical Chandra-Toueg failure detectors, Sigma and other quorum failure detectors, Omega, anti-Omega, Omega^k, and Psi_k. Additionally, asynchronous failure detectors satisfy many desirable properties: they are self-implementable, guarantee that stronger asynchronous failure-detectors solve harder problems, and ensure that their outputs encode no information other than the set of crashed processes. We introduce the notion of a failure detector being representative for a problem to capture the idea that some problems encode the same information about process crashes as their weakest failure detectors do. We show that a large class of problems, called bounded problems, do not have representative failure detectors. Finally, we use the asynchronous failure-detector framework to show how sufficiently strong AFDs circumvent the impossibility of consensus in asynchronous systems.",,47 p.,,,,Theory of Computation,,,,This report supersedes MIT-CSAIL-TR-2013-002.,,,,,,,http://hdl.handle.net/1721.1/76716,,2013-10-10T18:15:05Z,,,,,,,,,,,,,,,,2013,941
Dina Katabi,"Adib, Fadel; Kabelac, Zach; Katabi, Dina; Miller, Robert C.",2013-12-11T18:30:03Z,2013-12-11T18:30:03Z,2013-12-11,http://hdl.handle.net/1721.1/82913,MIT-CSAIL-TR-2013-030,3D Tracking via Body Radio Reflections,"This paper introduces WiTrack, a system that tracks the 3D motion of a user from the radio signals reflected off her body. It works even if the person is occluded from the WiTrack device or in a different room. WiTrack does not require the user to carry any wireless device, yet its accuracy exceeds current RF localization systems, which require the user to hold a transceiver. Empirical measurements with a WiTrack prototype show that, on average, it localizes the center of a human body to within 10 to 13 cm in the x and y dimensions, and 21 cm in the z dimension. It also provides coarse tracking of body parts, identifying the direction of a pointing hand with a median of 11.2 degrees. WiTrack bridges a gap between RF-based localization systems which locate a user through walls and occlusions, and human-computer interaction systems like WiTrack, which can track a user without instrumenting her body, but require the user to stay within the direct line of sight of the device.",,13 p.,,,Seeing Through Walls; 3D Motion Tracking; Wireless Signals; Body Reflections,Networks & Mobile Systems,,,,,,,,,,,,,2013-12-11T18:30:03Z,,,,,,,,,,,,,,,,2013,947
Tomaso Poggio,"Poggio, Tomaso; Mutch, Jim; Anselmi, Fabio; Tacchetti, Andrea; Rosasco, Lorenzo; Leibo, Joel Z.",2013-08-12T02:30:12Z,2013-08-12T02:30:12Z,2013-08-06,http://hdl.handle.net/1721.1/79828,MIT-CSAIL-TR-2013-019; CBCL-313,Does invariant recognition predict tuning of neurons in sensory cortex?,"Tuning properties of simple cells in cortical V1 can be described in terms of a ""universal shape"" characterized by parameter values which hold across different species. This puzzling set of findings begs for a general explanation grounded on an evolutionarily important computational function of the visual cortex. We ask here whether these properties are predicted by the hypothesis that the goal of the ventral stream is to compute for each image a ""signature"" vector which is invariant to geometric transformations, with the the additional assumption that the mechanism for continuously learning and maintaining invariance consists of the memory storage of a sequence of neural images of a few objects undergoing transformations (such as translation, scale changes and rotation) via Hebbian synapses. For V1 simple cells the simplest version of this hypothesis is the online Oja rule which implies that the tuning of neurons converges to the eigenvectors of the covariance of their input. Starting with a set of dendritic fields spanning a range of sizes, simulations supported by a direct mathematical analysis show that the solution of the associated ""cortical equation"" provides a set of Gabor-like wavelets with parameter values that are in broad agreement with the physiology data. We show however that the simple version of the Hebbian assumption does not predict all the physiological properties. The same theoretical framework also provides predictions about the tuning of cells in V4 and in the face patch AL which are in qualitative agreement with physiology data.",,10 p.,,,,,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,2013-08-12T02:30:12Z,,,,,,,,,,,,,,,,2013,932
Tomaso Poggio,"Ni, Yuzhao; Frogner, Charles A.; Poggio, Tomaso A.",2013-09-19T22:30:06Z,2013-09-19T22:30:06Z,2013-09-19,http://hdl.handle.net/1721.1/80815,MIT-CSAIL-TR-2013-023; CBCL-314,Mouse Behavior Recognition with The Wisdom of Crowd,"In this thesis, we designed and implemented a crowdsourcing system to annotatemouse behaviors in videos; this involves the development of a novel clip-based video labeling tools, that is more efficient than traditional labeling tools in crowdsourcing platform, as well as the design of probabilistic inference algorithms that predict the true labels and the workers' expertise from multiple workers' responses. Our algorithms are shown to perform better than majority vote heuristic. We also carried out extensive experiments to determine the effectiveness of our labeling tool, inference algorithms and the overall system.",,69 p.,,,crowdsourcing; video labeling; human computation; mouse phenotyping; action recognition,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,2013-09-19T22:30:06Z,,,,,,,,,,,,,,,,2013,939
Hari Balakrishnan,"LaCurts, Katrina; Deng, Shuo; Balakrishnan, Hari",2013-02-28T17:15:10Z,2013-02-28T17:15:10Z,2013-02-12,http://hdl.handle.net/1721.1/77238,MIT-CSAIL-TR-2013-003,A Plan for Optimizing Network-Intensive Cloud Applications,"A significant and growing number of applications deployed on cloud infrastructures are network-intensive. These applications are frequently bottlenecked by the speed of network connections between the machines on which they are deployed. Due to the complexity and size of cloud networks, such applications often run slowly or have unpredictable completion times and/or throughput, both of which can result in increased cost to the customer. In this paper, we argue that cloud customers should be able to express the demands and objectives of their applications. We outline an architecture that allows for this type of expression, and distributes applications within the cloud network such that the application's objectives are met. We discuss some of the key questions that need to be addressed to implement the architecture, as well as the interactions between optimizations done by clients and by cloud providers. We also present preliminary results that indicate that these types of systems are feasible and improve performance.",,7 p.,,,,Networks & Mobile Systems,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2013,919
Nancy Lynch,"Cornejo, Alejandro; Lynch, Nancy; Sastry, Srikanth",2013-02-01T20:00:04Z,2013-02-01T20:00:04Z,2013-01-30,http://hdl.handle.net/1721.1/76716,MIT-CSAIL-TR-2013-002,Asynchronous Failure Detectors,"Failure detectors -- oracles that provide information about process crashes -- are an important abstraction for crash tolerance in distributed systems. The generality of failure-detector theory, while providing great expressiveness, poses significant challenges in developing a robust hierarchy of failure detectors. We address some of these challenges by proposing (1) a variant of failure detectors called asynchronous failure detectors and (2) an associated modeling framework. Unlike the traditional failure-detector framework, our framework eschews real-time completely. We show that asynchronous failure detectors are sufficiently expressive to include several popular failure detectors including, but not limited to, the canonical Chandra-Toueg failure detectors, Sigma and other quorum failure detectors, Omega, anti-Omega, Omega^k, and Psi_k. Additionally, asynchronous failure detectors satisfy many desirable properties: they are self-implementable, guarantee that stronger asynchronous failure-detectors solve harder problems, and ensure that their outputs encode no information other than the set of crashed processes. We introduce the notion of a failure detector being representative for a problem to capture the idea that some problems encode the same information about process crashes as their weakest failure detectors do. We show that a large class of problems, called bounded problems, do not have representative failure detectors. Finally, we use the asynchronous failure-detector framework to show how sufficiently strong AFDs circumvent the impossibility of consensus in asynchronous systems.",,46 p.,,,"Asynchronous System, Fault-Tolerance, I/O Automata",Theory of Computation,,,,This report is superseded by MIT-CSAIL-TR-2013-025.,,,,,http://hdl.handle.net/1721.1/81371,,,,,,,,,,,,,,,,,,,,2013,917
Regina Barzilay,"Kushman, Nate; Adib, Fadel; Katabi, Dina; Barzilay, Regina",2013-09-10T21:30:03Z,2013-09-10T21:30:03Z,2013-09-10,http://hdl.handle.net/1721.1/80380,MIT-CSAIL-TR-2013-022,Harvesting Application Information for Industry-Scale Relational Schema Matching,"Consider the problem of migrating a company's CRM or ERP database from one application to another, or integrating two such databases as a result of a merger. This problem requires matching two large relational schemas with hundreds and sometimes thousands of fields. Further, the correct match is likely complex: rather than a simple one-to-one alignment, some fields in the source database may map to multiple fields in the target database, and others may have no equivalent fields in the target database. Despite major advances in schema matching, fully automated solutions to large relational schema matching problems are still elusive. This paper focuses on improving the accuracy of automated large relational schema matching. Our key insight is the observation that modern database applications have a rich user interface that typically exhibits more consistency across applications than the underlying schemas. We associate UI widgets in the application with the underlying database fields on which they operate and demonstrate that this association delivers new information useful for matching large and complex relational schemas. Additionally, we show how to formalize the schema matching problem as a quadratic program, and solve it efficiently using standard optimization and machine learning techniques. We evaluate our approach on real-world CRM applications with hundreds of fields and show that it improves the accuracy by a factor of 2-4x.",,12 p.,,,,Natural Language Processing,,,,,,,,,,,,,2013-09-10T21:30:03Z,,,,,,,,,,,,,,,,2013,938
Gerald Sussman,"Panchekha, Pavel; Brodsky, Micah Z. (Micah Zev)",2013-10-09T17:30:04Z,2013-10-09T17:30:04Z,2013-10-08,http://hdl.handle.net/1721.1/81365,MIT-CSAIL-TR-2013-024,Distributed Shared State with History Maintenance,"Shared mutable state is challenging to maintain in a distributed environment. We develop a technique, based on the Operational Transform, that guides independent agents into producing consistent states through inconsistent but equivalent histories of operations. Our technique, history maintenance, extends and streamlines the Operational Transform for general distributed systems. We describe how to use history maintenance to create eventually-consistent, strongly-consistent, and hybrid systems whose correctness is easy to reason about.",,21 p.,,,eventual consistency; distributed systems; operational transform,Mathematics and Computation,,,,,,,,,,,,,2013-10-09T17:30:04Z,,,,,,,,,,,,,,,,2013,940
Gerald Sussman,"Evans, Isaac; Lynch, Joseph",2013-06-03T23:30:05Z,2013-06-03T23:30:05Z,2013-05-24,http://hdl.handle.net/1721.1/79057,MIT-CSAIL-TR-2013-010,Organon: A Symbolic Constraint Framework & Solver,"Organon is an open source system for expressing and solving complex symbolic constraints between generic entities. Our design avoids restricting the programmer s ability to phrase constraints; Organon acts purely as a framework that defines and holds together the key concepts of forms, constraints, and solvers. It has three main components: (1) Forms: Abstract representations of the entities to be constrained. (2) Constraints: Functions that symbolically express requirements on the relationships between forms as well as provide information a solver can use to improve the constraint s satisfaction. (3) Solvers: Functions which inspect instantiations of forms and manipulate them in an attempt to satisfy a set of objective constraints.",,33 p.,,,scheme; propagator; exponential solver; annealing solver,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,2013-06-03T23:30:06Z,,,,,,,,,,,,,,,,2013,925
Silvio Micali,"Chiesa, Alessandro; Micali, Silvio; Zhu, Zeyuan Allen",2013-12-03T20:45:05Z,2013-12-03T20:45:05Z,2013-12-03,http://hdl.handle.net/1721.1/82632,MIT-CSAIL-TR-2013-029,Bridging Utility Maximization and Regret Minimization,"We relate the strategies obtained by (1) utility maximizers who use regret to refine their set of undominated strategies, and (2) regret minimizers who use weak domination to refine their sets of regret-minimizing strategies.",,9 p.,,,,Theory of Computation,,,,,,,,,,,,,2013-12-03T20:45:05Z,,,,,,,,,,,,,,,,2013,946
Brian Williams,"Bush, Lawrence A. M.",2018-01-30T23:45:58Z,2018-01-30T23:45:58Z,2013-08-22,http://hdl.handle.net/1721.1/113363,MIT-CSAIL-TR-2018-003,Decision Uncertainty Minimization and Autonomous Information Gathering,"Over the past several decades, technologies for remote sensing and exploration have be- come increasingly powerful but continue to face limitations in the areas of information gathering and analysis. These limitations affect technologies that use autonomous agents, which are devices that can make routine decisions independent of operator instructions. Bandwidth and other communications limitation require that autonomous differentiate between relevant and irrelevant information in a computationally efficient manner.This thesis presents a novel approach to this problem by framing it as an adaptive sensing problem. Adaptive sensing allows agents to modify their information collection strategies in response to the information gathered in real time. We developed and tested optimization algorithms that apply information guides to Monte Carlo planners. Information guides provide a mechanism by which the algorithms may blend online (realtime) and offline (previously simulated) planning in order to incorporate uncertainty into the decision- making process. This greatly reduces computational operations as well as decisional and communications overhead. We begin by introducing a 3-level hierarchy that visualizes adaptive sensing at synoptic (global), mesoscale (intermediate) and microscale (close-up) levels (a spatial hierarchy). We then introduce new algorithms for decision uncertainty minimization (DUM) and representational uncertainty minimization (RUM). Finally, we demonstrate the utility of this approach to real-world sensing problems, including bathymetric mapping and disaster relief. We also examine its potential in space exploration tasks by describing its use in a hypothetical aerial exploration of Mars. Our ultimate goal is to facilitate future large-scale missions to extraterrestrial objects for the purposes of scientific advancement and human exploration.",,310 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,Brian Williams; Model-based Embedded and Robotic Systems,,,,,,,2018-01-30T23:45:58Z,,,,,,,,,,,,,,,,2013,936
Sam Madden,"Taft, Rebecca; Vartak, Manasi; Satish, Nadathur Rajagopalan; Sundaram, Narayanan; Madden, Samuel; Stonebraker, Michael",2013-11-20T17:00:05Z,2013-11-20T17:00:05Z,2013-11-19,http://hdl.handle.net/1721.1/82517,MIT-CSAIL-TR-2013-028,GenBase: A Complex Analytics Genomics Benchmark,"This paper introduces a new benchmark, designed to test database management system (DBMS) performance on a mix of data management tasks (joins, filters, etc.) and complex analytics (regression, singular value decomposition, etc.) Such mixed workloads are prevalent in a number of application areas, including most science workloads and web analytics. As a specific use case, we have chosen genomics data for our benchmark, and have constructed a collection of typical tasks in this area. In addition to being representative of a mixed data management and analytics workload, this benchmark is also meant to scale to large dataset sizes and multiple nodes across a cluster. Besides presenting this benchmark, we have run it on a variety of storage systems including traditional row stores, newer column stores, Hadoop, and an array DBMS. We present performance numbers on all systems on single and multiple nodes, and show that performance differs by orders of magnitude between the various solutions. In addition, we demonstrate that most platforms have scalability issues. We also test offloading the analytics onto a coprocessor. The intent of this benchmark is to focus research interest in this area; to this end, all of our data, data generators, and scripts are available on our web site.",,12 p.,,,,Database,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,2013-11-20T17:00:05Z,,,,,,,,,,,,,,,,2013,945
Leslie Kaelbling,"Jordan, Matthew; Perez, Alejandro",2013-08-16T18:30:03Z,2013-08-16T18:30:03Z,2013-08-15,http://hdl.handle.net/1721.1/79884,MIT-CSAIL-TR-2013-021,Optimal Bidirectional Rapidly-Exploring Random Trees,"In this paper we present a simple, computationally-efficient, two-tree variant of the RRT* algorithm along with several heuristics.",,12 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution-NonCommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,,,,,,,,,,2013-08-16T18:30:03Z,,,,,,,,,,,,,,,,2013,935
Martin Rinard,"Long, Fan; Sidiroglou-Douskos, Stelios; Kim, Deokhwan; Rinard, Martin",2013-08-12T02:30:08Z,2013-08-12T02:30:08Z,2013-08-06,http://hdl.handle.net/1721.1/79827,MIT-CSAIL-TR-2013-018,Sound Input Filter Generation for Integer Overflow Errors,"We present a system, SIFT, for generating input filters that nullify integer overflow errors associated with critical program sites such as memory allocation or block copy sites. SIFT uses a static program analysis to generate filters that discard inputs that may trigger integer overflow errors in the computations of the sizes of allocated memory blocks or the number of copied bytes in block copy operations. The generated filters are sound   if an input passes the filter, it will not trigger an integer overflow error for any analyzed site. Our results show that SIFT successfully analyzes (and therefore generates sound input filters for) 52 out of 58 memory allocation and block memory copy sites in analyzed input processing modules from five applications (VLC, Dillo, Swfdec, Swftools, and GIMP). These nullified errors include six known integer overflow vulnerabilities. Our results also show that applying these filters to 62895 real-world inputs produces no false positives. The analysis and filter generation times are all less than a second.",,20 p.,,,Static Analysis; Integer Overflow; Filter Generation,Computer Architecture,,,,,,,,,,,,,2013-08-12T02:30:08Z,,,,,,,,,,,,,,,,2013,933
Leslie Kaelbling,"Glover, Jared; Kaelbling, Leslie Pack",2013-03-29T21:30:06Z,2013-03-29T21:30:06Z,2013-03-27,http://hdl.handle.net/1721.1/78248,MIT-CSAIL-TR-2013-005,Tracking 3-D Rotations with the Quaternion Bingham Filter,"A deterministic method for sequential estimation of 3-D rotations is presented. The Bingham distribution is used to represent uncertainty directly on the unit quaternion hypersphere. Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions. Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic. We present two versions of the QBF, suitable for tracking the state of first- and second-order rotating dynamical systems.",,13 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2013,921
Tomaso Poggio,"Kim, Heejung; Wohlwend, Jeremy; Leibo, Joel Z.; Poggio, Tomaso",2013-06-20T17:00:04Z,2013-06-20T17:00:04Z,2013-06-18,http://hdl.handle.net/1721.1/79354,MIT-CSAIL-TR-2013-013; CBCL-312,Body-form and body-pose recognition with a hierarchical model of the ventral stream,"When learning to recognize a novel body shape, e.g., a panda bear, we are not misled by changes in its pose. A ""jumping panda bear"" is readily recognized, despite having no prior visual experience with the conjunction of these concepts. Likewise, a novel pose can be estimated in an invariant way, with respect to the actor's body shape. These body and pose recognition tasks require invariance to non-generic transformations that previous models of the ventral stream do not have. We show that the addition of biologically plausible, class-specific mechanisms associating previously-viewed actors in a range of poses enables a hierarchical model of object recognition to account for this human capability. These associations could be acquired in an unsupervised manner from past experience.",,10 p.,,,Ventral stream; Modularity; Computational neuroscience; HMAX,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,2013-06-20T17:00:05Z,,,,,,,,,,,,,,,,2013,928
Joshua Tenenbaum,"Wingate, David; Diuk, Carlos; O'Donnell, Timothy; Tenenbaum, Joshua; Gershman, Samuel",2013-04-18T00:45:04Z,2013-04-18T00:45:04Z,2013-04-12,http://hdl.handle.net/1721.1/78573,MIT-CSAIL-TR-2013-007,Compositional Policy Priors,"This paper describes a probabilistic framework for incorporating structured inductive biases into reinforcement learning. These inductive biases arise from policy priors, probability distributions over optimal policies. Borrowing recent ideas from computational linguistics and Bayesian nonparametrics, we define several families of policy priors that express compositional, abstract structure in a domain. Compositionality is expressed using probabilistic context-free grammars, enabling a compact representation of hierarchically organized sub-tasks. Useful sequences of sub-tasks can be cached and reused by extending the grammars nonparametrically using Fragment Grammars. We present Monte Carlo methods for performing inference, and show how structured policy priors lead to substantially faster learning in complex domains compared to methods without inductive biases.",,17 p.,,,,Computational Cognitive Science,"This work was supported by AFOSR FA9550-07-1-0075 and ONR
N00014-07-1-0937. SJG was supported by a Graduate Research Fellowship from the NSF.",,,,,,,,,,,,,,,,,,,,,,,,,,,,2013,922
Martin Rinard,"Achour, Sara; Rinard, Martin",2014-08-19T21:00:06Z,2014-08-19T21:00:06Z,2014-08-19,http://hdl.handle.net/1721.1/88926,MIT-CSAIL-TR-2014-016,Energy-Efficient Approximate Computation in Topaz,"We present Topaz, a new task-based language for computations that execute on approximate computing platforms that may occasionally produce arbitrarily inaccurate results. The Topaz implementation maps approximate tasks onto the approximate machine and integrates the approximate results into the main computation, deploying a novel outlier detection and reliable reexecution mechanism to prevent unacceptably inaccurate results from corrupting the overall computation. Topaz therefore provides the developers of approximate hardware with substantial freedom in producing designs with little or no precision or accuracy guarantees. Experimental results from our set of benchmark applications demonstrate the effectiveness of Topaz and the Topaz implementation in enabling developers to productively exploit emerging approximate hardware platforms.",,41 p.,,,,Computer Architecture,,,,,,,,,,,,,2014-08-19T21:00:07Z,,,,,,,,,,,,,,,,2014,967
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Piselli, Paolo; Rinard, Martin",2014-10-22T21:30:11Z,2014-10-22T21:30:11Z,2014-09-30,http://hdl.handle.net/1721.1/91149,MIT-CSAIL-TR-2014-025,Automatic Error Elimination by Multi-Application Code Transfer,"We present pDNA, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",,15 p.,,,automatic patching; software self-healing,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,MIT-CSAIL-TR-2014-026,http://hdl.handle.net/1721.1/91150,,,2014-10-22T21:30:12Z,,,,,,,,,,,,,,,,2014,971
Dina Katabi,"Abari, Omid; Rahul, Hariharan; Katabi, Dina",2014-04-28T18:30:03Z,2014-04-28T18:30:03Z,2014-04-27,http://hdl.handle.net/1721.1/86298,MIT-CSAIL-TR-2014-010,One Clock to Rule Them All: A Primitive for Distributed Wireless Protocols at the Physical Layer,"Implementing distributed wireless protocols at the physical layer today is challenging because different nodes have different clocks, each of which has slightly different frequencies. This causes the nodes to have frequency offset relative to each other, as a result of which transmitted signals from these nodes do not combine in a predictable manner over time. Past work tackles this challenge and builds distributed PHY layer systems by attempting to address the effects of the frequency offset and compensating for it in the transmitted signals. In this paper, we address this challenge by addressing the root cause - the different clocks with different frequencies on the different nodes. We present AirClock, a new wireless coordination primitive that enables multiple nodes to act as if they are driven by a single clock that they receive wirelessly over the air. AirClock presents a synchronized abstraction to the physical layer, and hence enables direct implementation of diverse kinds of distributed PHY protocols. We illustrate AirClock's versatility by using it to build three different systems: distributed MIMO, distributed rate adaptation for wireless sensors, and pilotless OFDM, and show that they can provide significant performance benefits over today's systems.",,14 p.,,,Clock Synchronization; Wireless; Distributed MIMO; Distributed Rate Adaptation; Frequency Synchronization; Wireless Sensor Networks,Networks & Mobile Systems,,,,,,,,,,,,,2014-04-28T18:30:03Z,,,,,,,,,,,,,,,,2014,960
Daniel Jackson,"Milicevic, Aleksandar; Near, Joseph P.; Kang, Eunsuk; Jackson, Daniel",2014-09-03T18:15:05Z,2014-09-03T18:15:05Z,2014-09-02,http://hdl.handle.net/1721.1/89157,MIT-CSAIL-TR-2014-018,Alloy*: A Higher-Order Relational Constraint Solver,"The last decade has seen a dramatic growth in the use of constraint solvers as a computational mechanism, not only for analysis and synthesis of software, but also at runtime. Solvers are available for a variety of logics but are generally restricted to first-order formulas. Some tasks, however, most notably those involving synthesis, are inherently higher order; these are typically handled by embedding a first-order solver (such as a SAT or SMT solver) in a domain-specific algorithm. Using strategies similar to those used in such algorithms, we show how to extend a first-order solver (in this case Kodkod, a model finder for relational logic used as the engine of the Alloy Analyzer) so that it can handle quantifications over higher-order structures. The resulting solver is sufficiently general that it can be applied to a range of problems; it is higher order, so that it can be applied directly, without embedding in another algorithm; and it performs well enough to be competitive with specialized tools on standard benchmarks. Although the approach is demonstrated for a particular relational logic, the principles behind it could be applied to other first-order solvers. Just as the identification of first-order solvers as reusable backends advanced the performance of specialized tools and simplified their architecture, factoring out higher-ordersolvers may bring similar benefits to a new class of tools.",,15 p.,,,,Software Design,,,,,,,,,,,,,2014-09-03T18:15:05Z,,,,,,,,,,,,,,,,2014,969
Saman Amarasinghe,"Ding, Yufei; Ansel, Jason; Veeramachaneni, Kalyan; Shen, Xipeng; O'Reilly, Una-May; Amarasinghe, Saman",2014-06-23T21:45:03Z,2014-06-23T21:45:03Z,2014-06-23,http://hdl.handle.net/1721.1/88083,MIT-CSAIL-TR-2014-014,Autotuning Algorithmic Choice for Input Sensitivity,"Empirical autotuning is increasingly being used in many domains to achieve optimized performance in a variety of different execution environments. A daunting challenge faced by such autotuners is input sensitivity, where the best autotuned configuration may vary with different input sets. In this paper, we propose a two level solution that: first, clusters to find input sets that are similar in input feature space; then, uses an evolutionary autotuner to build an optimized program for each of these clusters; and, finally, builds an adaptive overhead aware classifier which assigns each input to a specific input optimized program. Our approach addresses the complex trade-off between using expensive features, to accurately characterize an input, and cheaper features, which can be computed with less overhead. Experimental results show that by adapting to different inputs one can obtain up to a 3x speedup over using a single configuration for all inputs.",,14 p.,,,,Computer Architecture,,,,,,,,,,,,,2014-06-23T21:45:03Z,,,,,,,,,,,,,,,,2014,964
Nickolai Zeldovich,"Boyd-Wickizer, Silas; Kaashoek, M. Frans; Morris, Robert; Zeldovich, Nickolai",2014-09-16T19:30:05Z,2014-09-16T19:30:05Z,2014-09-16,http://hdl.handle.net/1721.1/89653,MIT-CSAIL-TR-2014-019,OpLog: a library for scaling update-heavy data structures,"Existing techniques (e.g., RCU) can achieve good multi-core scaling for read-mostly data, but for update-heavy data structures only special-purpose techniques exist. This paper presents OpLog, a general-purpose library supporting good scalability for update-heavy data structures. OpLog achieves scalability by logging each update in a low-contention per-core log; it combines logs only when required by a read to the data structure. OpLog achieves generality by logging operations without having to understand them, to ease application to existing data structures. OpLog can further increase performance if the programmer indicates which operations can be combined in the logs. An evaluation shows how to apply OpLog to three update-heavy Linux kernel data structures. Measurements on a 48-core AMD server show that the result significantly improves the performance of the Apache web server and the Exim mail server under certain workloads.",,13 p.,,,,Parallel and Distributed Operating Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-09-16T19:30:05Z,,,,,,,,,,,,,,,,2014,970
Brian Williams,"Wang, David; Williams, Brian C.",2014-10-24T18:15:04Z,2014-10-24T18:15:04Z,2014-10-24,http://hdl.handle.net/1721.1/91170,MIT-CSAIL-TR-2014-027,tBurton: A Divide and Conquer Temporal Planner,"Planning for and controlling a network of interacting devices requires a planner that accounts for the automatic timed transitions of devices while meeting deadlines and achieving durative goals. For example, a planner for an imaging satellite with a camera intolerant of exhaust would need to determine that opening a valve causes a chain reaction that ignites the engine, and thus needs to shield its camera. While planners exist that support deadlines and durative goals, currently, no planners can handle automatic timed transitions. We present tBurton, a temporal planner that supports these features while additionally producing a temporally least-commitment plan. tBurton uses a divide and conquer approach: dividing the problem using causal-graph decomposition and conquering each factor with heuristic forward search. The `sub-plans' from each factor are unified in a conflict directed search, guided by the causal graph structure. We describe why tBurton is fast and efficient and present its efficacy on benchmarks from the International Planning Competition.",,7 p.,,,timed automata; timed concurrent automata; temporal planning; simple temporal network,Model-based Embedded and Robotic Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-10-24T18:15:04Z,,,,,,,,,,,,,,,,2014,976
Patrick Winston,"Finlayson, Mark A.; Halverson, Jeffry R.; Corman, Steven R.",2014-03-22T16:45:08Z,2014-03-22T16:45:08Z,2014-03-22,http://hdl.handle.net/1721.1/85893,,The N2 Corpus v1.0,"The N2 Corpus (Narrative Networks Corpus) comprises 100 story texts (42,480 words) relevant to Islamist Extremism, drawn from religious stories, online material, and promotional magazines. The corpus has been annotated for 14 different layers of syntax and semantics. This v1.0 version is missing 33 texts that will be added in later versions. The corpus is described in: Mark A. Finlayson, Jeffry R. Halverson, and Steven R. Corman (2014) ""The N2 Corpus: A semantically annotated collection of Islamist extremist stories"", Proceedings of the 9th Language Resources and Evaluation Conference (LREC), Reykjavik, Iceland.",,,,,,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-03-22T16:45:08Z,,,,,,,,,,,,,MIT CSAIL,,,2014,953
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",2014-10-02T21:45:09Z,2014-10-02T21:45:09Z,2014-10-02,http://hdl.handle.net/1721.1/90561,MIT-CSAIL-TR-2014-021,Automatic Error Elimination by Multi-Application Code Transfer,"We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of CP to transfer code across applications to eliminate otherwise fatal integer and buffer over- flow errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to eliminate software errors via the successful transfer of correct code across applications.",,16 p.,,,automatic program repair,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2014-10-02T21:45:09Z,,,,,,,,,,,,,,,,2014,974
Armando Solar-Lezama,"Rose, Eva",2014-10-02T21:45:04Z,2014-10-02T21:45:04Z,2014-10-01,http://hdl.handle.net/1721.1/90560,MIT-CSAIL-TR-2014-020,Constraint Generation for the Jeeves Privacy Language,"Our goal is to present a completed, semantic formalization of the Jeeves privacy language evaluation engine, based on the original Jeeves constraint semantics defined by Yang et al at POPL12, but sufficiently strong to support a first complete implementation thereof. Specifically, we present and implement a syntactically and semantically completed concrete syntax for Jeeves that meets the example criteria given in the paper. We also present and implement the associated translation to J, but here formulated by a completed and decompositional operational semantic formulation. Finally, we present an enhanced and decompositional, non-substitutional operational semantic formulation and implementation of the J evaluation engine (the dynamic semantics) with privacy constraints. In particular, we show how implementing the constraints can be defined as a monad, and evaluation can be defined as monadic operation on the constraint environment. The implementations are all completed in Haskell, utilizing its almost one-to-one capability to transparently reflect the underlying semantic reasoning when formalized this way. In practice, we have applied the ""literate"" program facility of Haskell to this report, a feature that enables the source LATEX to also serve as the source code for the implementation (skipping the report-parts as comment regions). The implementation is published as a github project.",,56 p.,,,,Computer-Aided Programming,,,,,,,,,,,,,2014-10-02T21:45:04Z,,,,,,,,,,,,,,,,2014,972
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",2014-10-22T21:30:14Z,2014-10-22T21:30:14Z,2014-10-02,http://hdl.handle.net/1721.1/91150,MIT-CSAIL-TR-2014-026,Automatic Error Elimination by Multi-Application Code Transfer,"We present pDNA, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",,16 p.,,,automatic program repair,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,MIT-CSAIL-TR-2014-025; MIT-CSAIL-TR-2014-024,,2014-10-22T21:30:14Z,,,,,,,,,,,,,,,,2014,973
Hari Balakrishnan,"LaCurts, Katrina; Mogul, Jeffrey C.; Balakrishnan, Hari; Turner, Yoshio",2014-03-31T20:15:06Z,2014-03-31T20:15:06Z,2014-03-24,http://hdl.handle.net/1721.1/85975,MIT-CSAIL-TR-2014-004,Cicada: Predictive Guarantees for Cloud Network Bandwidth,"In cloud-computing systems, network-bandwidth guarantees have been shown to improve predictability of application performance and cost. Most previous work on cloud-bandwidth guarantees has assumed that cloud tenants know what bandwidth guarantees they want. However, application bandwidth demands can be complex and time-varying, and many tenants might lack sufficient information to request a bandwidth guarantee that is well-matched to their needs. A tenant's lack of accurate knowledge about its future bandwidth demands can lead to over-provisioning (and thus reduced cost-efficiency) or under-provisioning (and thus poor user experience in latency-sensitive user-facing applications). We analyze traffic traces gathered over six months from an HP Cloud Services datacenter, finding that application bandwidth consumption is both time-varying and spatially inhomogeneous. This variability makes it hard to predict requirements. To solve this problem, we develop a prediction algorithm usable by a cloud provider to suggest an appropriate bandwidth guarantee to a tenant. The key idea in the prediction algorithm is to treat a set of previously observed traffic matrices as ""experts"" and learn online the best weighted linear combination of these experts to make its prediction. With tenant VM placement using these predictive guarantees, we find that the inter-rack network utilization in certain datacenter topologies can be more than doubled.",,13 p.,,,networking; machine learning; traffic prediction,Networks & Mobile Systems,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2014-03-31T20:15:06Z,,,,,,,,,,,,,,,,2014,954
Seth Teller,"Fallon, Maurice; Kuindersma, Scott; Karumanchi, Sisir; Antone, Matthew; Schneider, Toby; Dai, Hongkai; Perez D'Arpino, Claudia; Deits, Robin; DiCicco, Matt; Fourie, Dehann; Koolen, Twan; Marion, Pat; Posa, Michael; Valenzuela, Andres; Yu, Kuan-Ting; Shah, Julie; Iagnemma, Karl; Tedrake, Russ; Teller, Seth",2014-03-17T21:30:06Z,2014-03-17T21:30:06Z,2014-03-16,http://hdl.handle.net/1721.1/85690,MIT-CSAIL-TR-2014-003,An Architecture for Online Affordance-based Perception and Whole-body Planning,"The DARPA Robotics Challenge Trials held in December 2013 provided a landmark demonstration of dexterous mobile robots executing a variety of tasks aided by a remote human operator using only data from the robot's sensor suite transmitted over a constrained, field-realistic communications link. We describe the design considerations, architecture, implementation and performance of the software that Team MIT developed to command and control an Atlas humanoid robot. Our design emphasized human interaction with an efficient motion planner, where operators expressed desired robot actions in terms of affordances fit using perception and manipulated in a custom user interface. We highlight several important lessons we learned while developing our system on a highly compressed schedule.",,29 p.,,,,"Robotics, Vision & Sensor Networks",,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-03-17T21:30:06Z,,,,,,,,,,,,,,,,2014,952
Boris Katz,"Borchardt, Gary; Katz, Boris; Nguyen, Hong-Linh; Felshin, Sue; Senne, Ken; Wang, Andy",2014-10-08T20:45:02Z,2014-10-08T20:45:02Z,2014-10-08,http://hdl.handle.net/1721.1/90812,MIT-CSAIL-TR-2014-022,An Analyst's Assistant for the Interpretation of Vehicle Track Data,"This report describes the Analyst's Assistant, a software system for language-interactive, collaborative user-system interpretation of events, specifically targeting vehicle events that can be recognized on the basis of vehicle track data. The Analyst's Assistant utilizes language not only as a means of interaction, but also as a basis for internal representation of scene information, background knowledge, and results of interpretation. Building on this basis, the system demonstrates emerging intelligent systems techniques related to event recognition, summarization of events, partitioning of subtasks between user and system, and handling of language and graphical references to scene entities during interactive analysis.",,73 p.,,,,Infolab,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2014-10-08T20:45:03Z,,,,,,,,,,,,,,,,2014,975
Sam Madden,"Cheung, Alvin; Madden, Samuel; Solar-Lezama, Armando",2014-04-14T23:00:03Z,2014-04-14T23:00:03Z,2014-04-14,http://hdl.handle.net/1721.1/86173,MIT-CSAIL-TR-2014-006,Sloth: Being Lazy is a Virtue (When Issuing Database Queries),"Many web applications store persistent data in databases. During execution, such applications spend a significant amount of time communicating with the database for retrieval and storing of persistent data over the network. These network round trips represent a significant fraction of the overall execution time for many applications and as a result increase their latency. While there has been prior work that aims to eliminate round trips by batching queries, they are limited by 1) a requirement that developers manually identify batching opportunities, or 2) the fact that they employ static program analysis techniques that cannot exploit many opportunities for batching. In this paper, we present Sloth, a new system that extends traditional lazy evaluation to expose query batching opportunities during application execution, even across loops, branches, and method boundaries. We evaluated Sloth using over 100 benchmarks from two large-scale open-source applications, and achieved up to a 3x reduction in page load time by delaying computation.",,21 p.,,,network round trips; application optimization; database applications; compilers; lazy evaluation,Database,,,,,,,,,,,,,2014-04-14T23:00:03Z,,,,,,,,,,,,,,,,2014,956
Seth Teller,"Park, Jun-geun; Teller, Seth",2014-08-26T20:30:04Z,2014-08-26T20:30:04Z,2014-08-26,http://hdl.handle.net/1721.1/89075,MIT-CSAIL-TR-2014-017,Motion Compatibility for Indoor Localization,"Indoor localization -- a device's ability to determine its location within an extended indoor environment -- is a fundamental enabling capability for mobile context-aware applications. Many proposed applications assume localization information from GPS, or from WiFi access points. However, GPS fails indoors and in urban canyons, and current WiFi-based methods require an expensive, and manually intensive, mapping, calibration, and configuration process performed by skilled technicians to bring the system online for end users. We describe a method that estimates indoor location with respect to a prior map consisting of a set of 2D floorplans linked through horizontal and vertical adjacencies. Our main contribution is the notion of ""path compatibility,"" in which the sequential output of a classifier of inertial data producing low-level motion estimates (standing still, walking straight, going upstairs, turning left etc.) is examined for agreement with the prior map. Path compatibility is encoded in an HMM-based matching model, from which the method recovers the user s location trajectory from the low-level motion estimates. To recognize user motions, we present a motion labeling algorithm, extracting fine-grained user motions from sensor data of handheld mobile devices. We propose ""feature templates,"" which allows the motion classifier to learn the optimal window size for a specific combination of a motion and a sensor feature function. We show that, using only proprioceptive data of the quality typically available on a modern smartphone, our motion labeling algorithm classifies user motions with 94.5% accuracy, and our trajectory matching algorithm can recover the user's location to within 5 meters on average after one minute of movements from an unknown starting location. Prior information, such as a known starting floor, further decreases the time required to obtain precise location estimate.",,14 p.,,,Indoor localization; Inertial sensing; Motion classification; Trajectory matching; Sensor fusion; Route networks; Conditional random fields; Hidden Markov models,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,2014-08-26T20:30:04Z,,,,,,,,,,,,,,,,2014,968
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",2014-06-09T20:15:07Z,2014-06-09T20:15:07Z,2014-06-09,http://hdl.handle.net/1721.1/87710,MIT-CSAIL-TR-2014-012,Possibilistic Beliefs and Higher-Level Rationality,"We consider rationality and rationalizability for normal-form games of incomplete information in which the players have possibilistic beliefs about their opponents. In this setting, we prove that the strategies compatible with the players being level-k rational coincide with the strategies surviving a natural k-step iterated elimination procedure. We view the latter strategies as the (level-k) rationalizable ones in our possibilistic setting.",,10 p.,,,,Theory of Computation,,,,,,,,,,,,,2014-06-09T20:15:07Z,,,,,,,,,,,,,,,,2014,963
Daniel Jackson,"Near, Joseph P.; Jackson, Daniel",2014-04-24T19:30:05Z,2014-04-24T19:30:05Z,2014-04-22,http://hdl.handle.net/1721.1/86235,MIT-CSAIL-TR-2014-007,Symbolic Execution for (Almost) Free: Hijacking an Existing Implementation to Perform Symbolic Execution,"Symbolic execution of a language is traditionally achieved by replacing the language s interpreter with an entirely new interpreter. This may be an unnecessary burden, and it is tempting instead to try to use as much of the existing interpret infrastructure as possible, both for handling aspects of the computation that are not symbolic, and for propagating symbolic ones. This approach was used to implement Rubicon, a bounded verification system for Ruby on Rails web applications, in less than 1000 lines of Ruby code. Rubicon uses symbolic execution to derive verification conditions from Rails applications and an off-the-shelf solver to check them. Despite its small size, Rubicon has been used to find previously unknown bugs in open-source Rails applications. The key idea is to encode symbolic values and operations in a library written in the target language itself, overriding only a small part of the standard interpreter. We formalize this approach, showing that replacing a few key operators with symbolic versions in a standard interpreter gives the same effect as replacing the entire interpreter with a symbolic one.",,12 p.,,,symbolic execution; web applications; security; verification,Software Design,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2014-04-24T19:30:05Z,,,,,,,,,,,,,,,,2014,957
,"Feizi, Soheil; Duffy, Ken; Kellis, Manolis; Medard, Muriel",2014-12-04T18:30:07Z,2014-12-04T18:30:07Z,2014-12-02,http://hdl.handle.net/1721.1/92031,,Network Infusion to Infer Information Sources in Networks,"Several models exist for diffusion of signals across biological, social, or engineered networks. However, the inverse problem of identifying the source of such propagated information appears more difficult even in the presence of multiple network snapshots, and especially for the single-snapshot case, given the many alternative, often similar, progression of diffusion that may lead to the same observed snapshots. Mathematically, this problem can be undertaken using a diffusion kernel that represents diffusion processes in a given network, but computing this kernel is computationally challenging in general. Here, we propose a path-based network diffusion kernel which considers edge-disjoint shortest paths among pairs of nodes in the network and can be computed efficiently for both homogeneous and heterogeneous continuous-time diffusion models. We use this network diffusion kernel to solve the inverse diffusion problem, which we term Network Infusion (NI), using both likelihood maximization and error minimization. The minimum error NI algorithm is based on an asymmetric Hamming premetric function and can balance between false positive and false negative error types. We apply this framework for both single-source and multi-source diffusion, for both single-snapshot and multi-snapshot observations, and using both uninformative and informative prior probabilities for candidate source nodes. We also provide proofs that under a standard susceptible-infected diffusion model, (1) the maximum-likelihood NI is mean-field optimal for tree structures or sufficiently sparse Erdos-Renyi graphs, (2) the minimum-error algorithm is mean-field optimal for regular tree structures, and (3) for sufficiently-distant sources, the multi-source solution is mean-field optimal in the regular tree structure. Moreover, we provide techniques to learn diffusion model parameters such as observation times. We apply NI to several synthetic networks and compare its performance to centrality-based and distance-based methods for Erdos-Renyi graphs, power-law networks, symmetric and asymmetric grids. Moreover, we use NI in two real-world applications. First, we identify the news sources for 3,553 stories in the Digg social news network, and validate our results based on annotated information, that was not provided to our algorithm. Second, we use NI to identify infusion hubs of human diseases, defined as gene candidates that can explain the connectivity pattern of disease-related genes in the human regulatory network. NI identifies infusion hubs of several human diseases including T1D, Parkinson, MS, SLE, Psoriasis and Schizophrenia. We show that, the inferred infusion hubs are biologically relevant and often not identifiable using the raw p-values.",,45 p.,,,,,,Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,Manolis Kellis; Computational Biology (Kellis),,,,,,,2014-12-04T18:30:07Z,,,,,,,,,,,,,MIT CSAIL,,,2014,977
Frans Kaashoek,"Beckmann, Nathan Z.; Gruenwald III, Charles; Johnson, Christopher R.; Kasture, Harshad; Sironi, Filippo; Agarwal, Anant; Kaashoek, M. Frans; Zeldovich, Nickolai",2014-01-29T19:30:05Z,2014-01-29T19:30:05Z,2014-01-28,http://hdl.handle.net/1721.1/84608,MIT-CSAIL-TR-2014-002,PIKA: A Network Service for Multikernel Operating Systems,"PIKA is a network stack designed for multikernel operating systems that target potential future architectures lacking cache-coherent shared memory but supporting message passing. PIKA splits the network stack into several servers that communicate using a low-overhead message passing layer. A key challenge faced by PIKA is the maintenance of shared state, such as a single accept queue and load balance information. PIKA addresses this challenge using a speculative 3-way handshake for connection acceptance, and a new distributed load balancing scheme for spreading connections. A PIKA prototype achieves competitive performance, excellent scalability, and low service times under load imbalance on commodity hardware. Finally, we demonstrate that splitting network stack processing by function across separate cores is a net loss on commodity hardware, and we describe conditions under which it may be advantageous.",,14  p.,,,Operating Systems; Multi-kernel; Micro-kernel; Scalability; Networking,Parallel and Distributed Operating Systems,,,,,,,,,,,,,2014-01-29T19:30:05Z,,,,,,,,,,,,,,,,2014,951
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Piselli, Paolo; Rinard, Martin",2014-10-22T21:30:07Z,2014-10-22T21:30:07Z,2014-08-11,http://hdl.handle.net/1721.1/91148,MIT-CSAIL-TR-2014-024,Automatic Error Elimination by Multi-Application Code Transfer,"We present pDNA, a system for automatically transferring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using three donor applications to eliminate seven errors in four recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer overflow errors at critical memory allocation sites. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",,14 p.,,,Automatic Program Repair,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,MIT-CSAIL-TR-2014-026,http://hdl.handle.net/1721.1/91150,,,2014-10-22T21:30:07Z,,,,,,,,,,,,,,,,2014,966
Martin Rinard,"Misailovic, Sasa; Carbin, Michael; Achour, Sara; Qi, Zichao; Rinard, Martin",2014-01-09T23:45:05Z,2014-01-09T23:45:05Z,2014-01-09,http://hdl.handle.net/1721.1/83843,MIT-CSAIL-TR-2014-001,Reliability-Aware Optimization of Approximate Computational Kernels with Rely,"Emerging high-performance architectures are anticipated to contain unreliable components (e.g., ALUs) that offer low power consumption at the expense of soft errors. Some applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors and can therefore trade accuracy of their results for reduced energy consumption by utilizing these unreliable hardware components. We present and evaluate a technique for reliability-aware optimization of approximate computational kernel implementations. Our technique takes a standard implementation of a computation and automatically replaces some of its arithmetic operations with unreliable versions that consume less power, but may produce incorrect results with some probability. Our technique works with a developer-provided specification of the required reliability of a computation -- the probability that it returns the correct result -- and produces an unreliable implementation that satisfies that specification. We evaluate our approach on five applications from the image processing, numerical analysis, and financial analysis domains and demonstrate how our technique enables automatic exploration of the trade-off between the reliability of a computation and its performance.",,11 p.,,,,Computer Architecture,,,,,,,,,,,,,2014-01-09T23:45:06Z,,,,,,,,,,,,,,,,2014,950
Nancy Lynch,"Cadambe, Viveck R.; Lynch, Nancy; Medard, Muriel; Musial, Peter",2014-08-06T18:00:06Z,2014-08-06T18:00:06Z,2014-08-01,http://hdl.handle.net/1721.1/88551,MIT-CSAIL-TR-2014-015,A Coded Shared Atomic Memory Algorithm for Message Passing Architectures,"This paper considers the communication and storage costs of emulating atomic (linearizable) multi-writer multi-reader shared memory in distributed message-passing systems. The paper contains three main contributions: (1) We present a atomic shared-memory emulation algorithm that we call Coded Atomic Storage (CAS). This algorithm uses erasure coding methods. In a storage system with 'N' servers that is resilient to 'f' server failures, we show that the communication cost of CAS is N/(N-2f) . The storage cost of CAS is unbounded. (2) We present a modification of the CAS algorithm known as CAS with Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer 'd' and has a bounded storage cost. We show that in every execution where the number of write operations that are concurrent with a read operation is no bigger than 'd', the CASGC algorithm with parameter 'd' satisfies atomicity and liveness. We explicitly characterize the storage cost of CASGC, and show that it has the same communication cost as CASGC. (3) We describe an algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS) algorithm that achieves a smaller communication cost than CAS and CASGC. In particular, CCOAS incurs read and write communication costs of N/(N-f) measured in terms of number of object values. We also discuss drawbacks of CCOAS as compared with CAS and CASGC.",,28 p.,,,,Theory of Computation,,,,,,,,,,,,,2014-08-06T18:00:06Z,,,,,,,,,,,,,,,,2014,965
Patrick Winston,"Finlayson, Mark Alan",2014-12-30T21:45:10Z,2014-12-30T21:45:10Z,2014-12-30,http://hdl.handle.net/1721.1/92563,,"Supplementary Materials for ""A Survey of Corpora in Computational and Cognitive Narrative Science""","This archive contains supplementary materials for the article titled ""A Survey of Corpora in Computational and Cognitive Narrative Science"" by Mark A. Finlayson, published in the journal *Sprache und Datenverarbeitung*. The archive contains two files. The first file is the raw bibliographic data of the survey, containing 2600+ citations. The second file is a spreadsheet with the coded features of each corpus, plus the analyses that underlie sections 3 & 4 of the paper.",,1172839 bytes,,,,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-12-30T21:45:10Z,,,,,,,,,,,,,,,,2014,979
,"Gombolay, Matthew; Golen, Toni; Shah, Neel; Shah, Julie",2014-12-16T22:00:06Z,2014-12-16T22:00:06Z,2014-12-16,http://hdl.handle.net/1721.1/92354,,Queueing Theory Analysis of Labor & Delivery at a Tertiary Care Center,"Labor and Delivery is a complex clinical service requiring the support of highly trained healthcare professionals from Obstetrics, Anesthesiology, and Neonatology and the access to a finite set of valuable resources. In the United States, the rate of cesarean sections on labor floors is approximately twice as high as considered appropriate for patient care. We analyze one month of data from a Boston-area hospital to assess how well the labor and delivery process can be modelled with tools from queueing theory. We find that the labor and delivery process is highly amenable to analysis under queueing theory models. We also investigate the problem of high cesarean section rates and the potential effects of resource utilization of lowering the rate of cesarean section.",,10 p.,,,Obstetrics; Healthcare; Queueing Theory; Operations Research,,,Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,Julie A Shah; Interactive Robotics Group,,,,,,,2014-12-16T22:00:07Z,,,,,,,,,,,,,MIT CSAIL,,,2014,978
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-02-02T22:00:04Z,2015-02-02T22:00:04Z,2015-02-02,http://hdl.handle.net/1721.1/93255,,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems (Supplementary Material),"We analyze reported patches for three prior generate-and-validate patch generation systems (GenProg, RSRepair, and AE). Because of experimental error, the majority of the reported patches violate the basic principle behind the design of these systems -- they do not produce correct outputs even for the inputs in the test suite used to validate the patches. We also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. We also present Kali, a generate-and-validate patch generation system that simply deletes functionality. Working with a simpler and more effectively focused search space, Kali produces more correct patches and at least as many patches that produce correct outputs for the inputs in the validation test suite as prior GenProg, RSRepair, and AE systems.",,,,,,Computer Architecture,,,,,,,,,,http://hdl.handle.net/1721.1/94337,,,2015-02-02T22:00:04Z,,Main paper: http://hdl.handle.net/1721.1/94337,,,,,,,,,,,,,,2015,982
Karen Sollins; Danny Weitzner,"Bruce, Elizabeth; Sollins, Karen; Vernon, Mona; Weitzner, Danny",2015-10-02T15:45:04Z,2015-10-02T15:45:04Z,2015-10-01,http://hdl.handle.net/1721.1/99127,MIT-CSAIL-TR-2015-030,Big Data Privacy Scenarios,"This paper is the first in a series on privacy in Big Data. As an outgrowth of a series of workshops on the topic, the Big Data Privacy Working Group undertook a study of a series of use scenarios to highlight the challenges to privacy that arise in the Big Data arena. This is a report on those scenarios. The deeper question explored by this exercise is what is distinctive about privacy in the context of Big Data. In addition, we discuss an initial list of issues for privacy that derive specifically from the nature of Big Data. These derive from observations across the real world scenarios and use cases explored in this project as well as wider reading and discussions:* Scale: The sheer size of the datasets leads to challenges in creating, managing and applying privacy policies.* Diversity: The increased likelihood of more and more diverse participants in Big Data collection, management, and use, leads to differing agendas and objectives. By nature, this is likely to lead to contradictory agendas and objectives.* Integration: With increased data management technologies (e.g. cloud services, data lakes, and so forth), integration across datasets, with new and often surprising opportunities for cross-product inferences, will also come new  information  about individuals and their behaviors.* Impact on secondary participants: Because many pieces of information are reflective of not only the targeted subject, but secondary, often unattended, participants, the inferences and resulting information will increasingly be reflective of other people, not originally considered as the subject of privacy concerns and approaches.* Need for emergent policies for emergent information: As inferences over merged data sets occur, emergent information or understanding will occur. Although each unique data set may have existing privacy policies and enforcement mechanisms, it is not clear that it is possible to develop the requisite and appropriate emerged privacy policies and appropriate enforcement of them automatically.",,53 p.,,,Big Data; Use scenarios; Privacy,Advanced Network Architecture; Decentralized Information Group,,Creative Commons Attribution-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nd/4.0/,,,,,,,,,,2015-10-02T15:45:04Z,,,,,,,,,,,,,,,,2015,1010
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-02-11T19:45:04Z,2015-02-11T19:45:04Z,2015-02-10,http://hdl.handle.net/1721.1/94337,MIT-CSAIL-TR-2015-003,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,"We analyze reported patches for three prior generate-and-validate patch generation systems (GenProg, RSRepair, and AE). Because of experimental error, the majority of the reported patches violate the basic principle behind the design of these systems -- they do not produce correct outputs even for the inputs in the test suite used to validate the patches. We also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. We also present Kali, a generate-and-validate patch generation system that simply deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many plausible patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks.",,13 p.,,,Automatic Program Repair; Patch Analysis; Functionality Elimination,Computer Architecture,,,,,,,,,,http://hdl.handle.net/1721.1/93255,,See supplementary material at http://hdl.handle.net/1721.1/93255,2015-02-11T19:45:04Z,,,,,,,,,,,,,,,,2015,983
Martin Rinard,"Long, Fan; Rinard, Martin",2015-05-26T22:00:02Z,2015-05-26T22:00:02Z,2015-05-26,http://hdl.handle.net/1721.1/97088,MIT-CSAIL-TR-2015-019,Prophet: Automatic Patch Generation via Learning from Successful Human Patches,"We present Prophet, a novel patch generation system that learns a probabilistic model over candidate patches from a large code database that contains many past successful human patches. It defines the probabilistic model as the combination of a distribution over program points based on error localization algorithms and a parameterized log-linear distribution over modification operations. It then learns the model parameters via maximum log-likelihood, which identifies important characteristics of the successful human patches. For a new defect, Prophet generates a search space that contains many candidate patches, applies the learned model to prioritize those potentially correct patches that are consistent with the identified successful patch characteristics, and then validates the candidate patches with a user supplied test suite.",,7 p.,,,,Program Analysis,,,,,,,,,,,,,2015-05-26T22:00:02Z,,,,,,,,,,,,,,,,2015,1001
Daniel Weitzner,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",2015-07-07T02:15:02Z,2015-07-07T02:15:02Z,2015-07-06,http://hdl.handle.net/1721.1/97690,MIT-CSAIL-TR-2015-026,Keys Under Doormats: Mandating insecurity by requiring government access to all data and communications,"Twenty years ago, law enforcement organizations lobbied to require data and communication services to engineer their products to guarantee law enforcement access to all data. After lengthy debate and vigorous predictions of enforcement channels going dark, these attempts to regulate the emerging Internet were abandoned. In the intervening years, innovation on the Internet flourished, and law enforcement agencies found new and more effective means of accessing vastly larger quantities of data. Today we are again hearing calls for regulation to mandate the provision of exceptional access mechanisms. In this report, a group of computer scientists and security experts, many of whom participated in a 1997 study of these same topics, has convened to explore the likely effects of imposing extraordinary access mandates. We have found that the damage that could be caused by law enforcement exceptional access requirements would be even greater today than it would have been 20 years ago. In the wake of the growing economic and social cost of the fundamental insecurity of today's Internet environment, any proposals that alter the security dynamics online should be approached with caution. Exceptional access would force Internet system developers to reverse forward secrecy design practices that seek to minimize the impact on user privacy when systems are breached. The complexity of today's Internet environment, with millions of apps and globally connected services, means that new law enforcement requirements are likely to introduce unanticipated, hard to detect security flaws. Beyond these and other technical vulnerabilities, the prospect of globally deployed exceptional access systems raises difficult problems about how such an environment would be governed and how to ensure that such systems would respect human rights and the rule of law.",,34 p.,,,,Decentralized Information Group,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-07-07T16:15:15Z,,,,,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",,,,,,,,,,,2015,1006
Manolis Kellis,"Feizi, Soheil; Makhdoumi, Ali; Duffy, Ken; Kellis, Manolis; Medard, Muriel",2015-09-23T19:00:07Z,2015-09-23T19:00:07Z,2015-09-21,http://hdl.handle.net/1721.1/98878,MIT-CSAIL-TR-2015-028,Network Maximal Correlation,"Identifying nonlinear relationships in large datasets is a daunting task particularly when the form of the nonlinearity is unknown. Here, we introduce Network Maximal Correlation (NMC) as a fundamental measure to capture nonlinear associations in networks without the knowledge of underlying nonlinearity shapes. NMC infers, possibly nonlinear, transformations of variables with zero means and unit variances by maximizing total nonlinear correlation over the underlying network. For the case of having two variables, NMC is equivalent to the standard Maximal Correlation. We characterize a solution of the NMC optimization using geometric properties of Hilbert spaces for both discrete and jointly Gaussian variables. For discrete random variables, we show that the NMC optimization is an instance of the Maximum Correlation Problem and provide necessary conditions for its global optimal solution. Moreover, we propose an efficient algorithm based on Alternating Conditional Expectation (ACE) which converges to a local NMC optimum. For this algorithm, we provide guidelines for choosing appropriate starting points to jump out of local maximizers. We also propose a distributed algorithm to compute a 1-$\epsilon$ approximation of the NMC value for large and dense graphs using graph partitioning. For jointly Gaussian variables, under some conditions, we show that the NMC optimization can be simplified to a Max-Cut problem, where we provide conditions under which an NMC solution can be computed exactly. Under some general conditions, we show that NMC can infer the underlying graphical model for functions of latent jointly Gaussian variables. These functions are unknown, bijective, and can be nonlinear. This result broadens the family of continuous distributions whose graphical models can be characterized efficiently. We illustrate the robustness of NMC in real world applications by showing its continuity with respect to small perturbations of joint distributions. We also show that sample NMC (NMC computed using empirical distributions) converges exponentially fast to the true NMC value. Finally, we apply NMC to different cancer datasets including breast, kidney and liver cancers, and show that NMC infers gene modules that are significantly associated with survival times of individuals while they are not detected using linear association measures.",,48 p.,,,,Computational Biology (Kellis),,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-09-23T19:00:07Z,,,,,,,,,,,,,,,,2015,1008
Saman Amarasinghe,"Kjolstad, Fredrik; Kamil, Shoaib; Ragan-Kelley, Jonathan; Levin, David I.W.; Sueda, Shinjiro; Chen, Desai; Vouga, Etienne; Kaufman, Danny M.; Kanwar, Gurtej; Matusik, Wojciech; Amarasinghe, Saman",2015-05-26T19:15:03Z,2015-05-26T19:15:03Z,2015-05-26,http://hdl.handle.net/1721.1/97075,MIT-CSAIL-TR-2015-017,Simit: A Language for Physical Simulation,"Using existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. The alternative is to prototype simulations in a high-level language like Matlab, thereby sacrificing performance. The Matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. However, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. Translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and the machine. High-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude. In this paper, we present Simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph, and as a set of global vectors, matrices and tensors depending on what is convenient at any given time. Simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. Using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. We demonstrate that Simit is easy to use: a Simit program is typically shorter than a Matlab program; that it is high-performance: a Simit program running sequentially on a CPU performs comparably to hand-optimized simulations; and that it is portable: Simit programs can be compiled for GPUs with no change to the program, delivering 5-25x speedups over our optimized CPU code.",,17 p.,,,"Graphs, Matrices, Tensors, Simulation",Computer Architecture,,,,,,,,,,,,,2015-05-26T19:15:04Z,,,,,,,,,,,,,,,,2015,1002
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2015-12-21T19:00:15Z,2015-12-21T19:00:15Z,2015-12-19,http://hdl.handle.net/1721.1/100465,MIT-CSAIL-TR-2015-034,Bridging Theory and Practice in Cache Replacement,"Much prior work has studied processor cache replacement policies, but a large gap remains between theory and practice. The optimal policy (MIN) requires unobtainable knowledge of the future, and prior theoretically-grounded policies use reference models that do not match real programs. Meanwhile, practical policies are designed empirically. Lacking a strong theoretical foundation, they do not make the best use of the information available to them. This paper bridges theory and practice. We propose that practical policies should replace lines based on their economic value added (EVA), the difference of their expected hits from the average. We use Markov decision processes to show that EVA is optimal under some reasonable simplifications. We present an inexpensive, practical implementation of EVA and evaluate it exhaustively over many cache sizes. EVA outperforms prior practical policies and saves area at iso-performance. These results show that formalizing cache replacement yields practical benefits.",,14 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:15Z,,,,,,,,,,,,,,,,2015,1016
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-05-26T23:00:02Z,2015-05-26T23:00:02Z,2015-05-26,http://hdl.handle.net/1721.1/97089,MIT-CSAIL-TR-2015-020,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,"We analyze reported patches for three existing generate-and-validate patch generation systems (GenProg, RSRepair, and AE). The basic principle behind generate-and-validate systems is to accept only plausible patches that produce correct outputs for all inputs in the test suite used to validate the patches. Because of errors in the patch evaluation infrastructure, the majority of the reported patches are not plausible --- they do not produce correct outputs even for the inputs in the validation test suite. The overwhelming majority of the reported patches are not correct and are equivalent to a single modification that simply deletes functionality. Observed negative effects include the introduction of security vulnerabilities and the elimination of desirable standard functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",,24 p.,,,,Program Analysis,,,,,,,,,,,,,2015-05-26T23:00:02Z,,,,,,,,,,,,,,,,2015,999
Julie A Shah,"Gombolay, Matthew C.",2015-07-06T22:15:05Z,2015-07-06T22:15:05Z,2015-07-02,http://hdl.handle.net/1721.1/97689,MIT-CSAIL-TR-2015-025,PhD Thesis Proposal: Human-Machine Collaborative Optimization via Apprenticeship Scheduling,"Resource optimization in health care, manufacturing, and military operations requires the careful choreography of people and equipment to effectively fulfill the responsibilities of the profession. However, resource optimization is a computationally challenging problem, and poorly utilizing resources can have drastic consequences. Within these professions, there are human domain experts who are able to learn from experience to develop strategies, heuristics, and rules-of-thumb to effectively utilize the resources at their disposal. Manually codifying these heuristics within a computational tool is a laborious process and leaves much to be desired. Even with a codified set of heuristics, it is not clear how to best insert an autonomous decision-support system into the human decision-making process. The aim of this thesis is to develop an autonomous computational method for learning domain-expert heuristics from demonstration that can support the human decision-making process. We propose a new framework, called apprenticeship scheduling, which learns and embeds these heuristics within a scalable resource optimization algorithm for real-time decision-support. Our initial investigation, comprised of developing scalable methods for scheduling and studying shared control in human-machine collaborative resource optimization, inspires the development of our apprenticeship scheduling approach. We present a promising, initial prototype for learning heuristics from demonstration and outline a plan for our continuing work.",,24 pages,,,,Interactive Robotics Group,,,,,,,,,,,,,2015-07-06T22:15:05Z,,,,,,,,,,,,,,,,2015,1005
Michael Stonebraker,"Battle, Leilani; Chang, Remco; Stonebraker, Michael",2015-10-19T20:15:05Z,2015-10-19T20:15:05Z,2015-10-19,http://hdl.handle.net/1721.1/99361,MIT-CSAIL-TR-2015-031,Dynamic Prefetching of Data Tiles for Interactive Visualization,"In this paper, we present ForeCache, a general-purpose tool for exploratory browsing of large datasets. ForeCache utilizes a client-server architecture, where the user interacts with a lightweight client-side interface to browse datasets, and the data to be browsed is retrieved from a DBMS running on a back-end server. We assume a detail-on-demand browsing paradigm, and optimize the back-end support for this paradigm by inserting a separate middleware layer in front of the DBMS. To improve response times, the middleware layer fetches data ahead of the user as she explores a dataset. We consider two different mechanisms for prefetching: (a) learning what to fetch from the user's recent movements, and (b) using data characteristics (e.g., histograms) to find data similar to what the user has viewed in the past. We incorporate these mechanisms into a single prediction engine that adjusts its prediction strategies over time, based on changes in the user's behavior. We evaluated our prediction engine with a user study, and found that our dynamic prefetching strategy provides: (1) significant improvements in overall latency when compared with non-prefetching systems (430% improvement); and (2) substantial improvements in both prediction accuracy (25% improvement) and latency (88% improvement) relative to existing prefetching techniques.",,13 p.,,,visualization; interactive exploration; databases,Database,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2015-10-19T20:15:05Z,,,,,,,,,,,,,,,,2015,1011
Nancy Lynch,"Lynch, Nancy; Newport, Calvin",2015-05-18T20:00:06Z,2015-05-18T20:00:06Z,2015-05-18,http://hdl.handle.net/1721.1/97014,MIT-CSAIL-TR-2015-016,A (Truly) Local Broadcast Layer for Unreliable Radio Networks,"In this paper, we implement an efficient local broadcast service for the dual graph model, which describes communication in a radio network with both reliable and unreliable links. Our local broadcast service offers probabilistic latency guarantees for: (1) message delivery to all reliable neighbors (i.e., neighbors connected by reliable links), and (2) receiving some message when one or more reliable neighbors are broadcasting. This service significantly simplifies the design and analysis of algorithms for the otherwise challenging dual graph model. To this end, we also note that our solution can be interpreted as an implementation of the abstract MAC layer specification---therefore translating the growing corpus of algorithmic results studied on top of this layer to the dual graph model. At the core of our service is a seed agreement routine which enables nodes in the network to achieve ""good enough"" coordination to overcome the difficulties of unpredictable link behavior. Because this routine has potential application to other problems in this setting, we capture it with a formal specification---simplifying its reuse in other algorithms. Finally, we note that in a break from much work on distributed radio network algorithms, our problem definitions (including error bounds), implementation, and analysis do not depend on global network parameters such as the network size, a goal which required new analysis techniques. We argue that breaking the dependence of these algorithms on global parameters makes more sense and aligns better with the rise of ubiquitous computing, where devices will be increasingly working locally in an otherwise massive network. Our push for locality, in other words, is a contribution independent of the specific radio network model and problem studied here.",,27 p.,,,,Theory of Computation,,,,,,,,,,,,,2015-05-18T20:00:07Z,,,,,,,,,,,,,,,,2015,997
Hari Balakrishnan,"Chen, Tiffany Yu-Han; Sivaraman, Anirudh; Das, Somak; Ravindranath, Lenin; Balakrishnan, Hari",2015-09-25T15:45:09Z,2015-09-25T15:45:09Z,2015-09-24,http://hdl.handle.net/1721.1/98905,MIT-CSAIL-TR-2015-029,Designing a Context-Sensitive Context Detection Service for Mobile Devices,"This paper describes the design, implementation, and evaluation of Amoeba, a context-sensitive context detection service for mobile devices. Amoeba exports an API that allows a client to express interest in one or more context types (activity, indoor/outdoor, and entry/exit to/from named regions), subscribe to specific modes within each context (e.g., ""walking"" or ""running"", but no other activity), and specify a response latency (i.e., how often the client is notified). Each context has a detector that returns its estimate of the mode. The detectors take both the desired subscriptions and the current context detection into account, adjusting both the types of sensors and the sampling rates to achieve high accuracy and low energy consumption. We have implemented Amoeba on Android. Experiments with Amoeba on 45+ hours of data show that our activity detector achieves an accuracy between 92% and 99%, outperforming previous proposals like UCLA* (59%), EEMSS (82%) and SociableSense (72%), while consuming 4 to 6× less energy.",,12 p.,,,context detection; context sensing; activity recognition; indoor detection; geofence; sensors; mobile sensing; energy,Networks & Mobile Systems,,,,,,,,,,,,,2015-09-25T15:45:09Z,,,,,,,,,,,,,,,,2015,1009
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2015-12-21T19:00:09Z,2015-12-21T19:00:09Z,2015-12-19,http://hdl.handle.net/1721.1/100464,MIT-CSAIL-TR-2015-033,Cache Calculus: Modeling Caches through Differential Equations,"Caches are critical to performance, yet their behavior is hard to understand and model. In particular, prior work does not provide closed-form solutions of cache performance, i.e. simple expressions for the miss rate of a specific access pattern. Existing cache models instead use numerical methods that, unlike closed-form solutions, are computationally expensive and yield limited insight. We present cache calculus, a technique that models cache behavior as a system of ordinary differential equations, letting standard calculus techniques find simple and accurate solutions of cache performance for common access patterns.",,4 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:10Z,,,,,,,,,,,,,,,,2015,1015
Leslie Kaelbling,"Zewdie, Dawit H.; Konidaris, George",2015-11-30T19:30:04Z,2015-11-30T19:30:04Z,2015-11-24,http://hdl.handle.net/1721.1/100053,MIT-CSAIL-TR-2015-032,Representation Discovery for Kernel-Based Reinforcement Learning,"Recent years have seen increased interest in non-parametric reinforcement learning. There are now practical kernel-based algorithms for approximating value functions; however, kernel regression requires that the underlying function being approximated be smooth on its domain. Few problems of interest satisfy this requirement in their natural representation. In this paper we define Value-Consistent Pseudometric (VCPM), the distance function corresponding to a transformation of the domain into a space where the target function is maximally smooth and thus well-approximated by kernel regression. We then present DKBRL, an iterative batch RL algorithm interleaving steps of Kernel-Based Reinforcement Learning and distance metric adjustment. We evaluate its performance on Acrobot and PinBall, continuous-space reinforcement learning domains with discontinuous value functions.",,16 p.,,,Metric learning,Learning and Intelligent Systems,,Creative Commons Attribution-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-sa/4.0/,,,,,,,,,,2015-11-30T19:30:04Z,,,,,,,,,,,,,,,,2015,1012
Martin Rinard,"Long, Fan; Rinard, Martin",2015-07-14T15:45:04Z,2015-07-14T15:45:04Z,2015-07-13,http://hdl.handle.net/1721.1/97735,MIT-CSAIL-TR-2015-027,Prophet: Automatic Patch Generation via Learning from Successful Patches,"We present Prophet, a novel patch generation system that learns a probabilistic model over candidate patches from a database of past successful patches. Prophet defines the probabilistic model as the combination of a distribution over program points based on defect localization algorithms and a parametrized log-linear distribution over modification operations. It then learns the model parameters via maximum log-likelihood, which identifies important characteristics of the previous successful patches in the database. For a new defect, Prophet generates a search space that contains many candidate patches, applies the learned model to prioritize those potentially correct patches that are consistent with the identified successful patch characteristics, and then validates the candidate patches with a user supplied test suite. The experimental results indicate that these techniques enable Prophet to generate correct patches for 15 out of 69 real-world defects in eight open source projects. The previous state of the art generate and validate system, which uses a set of hand-code heuristics to prioritize the search, generates correct patches for 11 of these same 69 defects.",,13 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-07-14T15:45:05Z,,,,,,,,,,,,,,,,2015,1007
Martin Rinard,"Long, Fan; Qi, Zichao; Achour, Sara; Rinard, Martin",2015-02-12T21:00:03Z,2015-02-12T21:00:03Z,2015-02-12,http://hdl.handle.net/1721.1/94520,MIT-CSAIL-TR-2015-004,Automatic Program Repair with Condition Synthesis and Compound Mutations,"We present PCR, a new automatic patch generation system. PCR uses a new condition synthesis technique to efficiently discover logical expressions that generate desired control- flow transfer patterns. Presented with a set of test cases, PCR deploys condition synthesis to find and repair incorrect if conditions that cause the application to produce the wrong result for one or more of the test cases. PCR also leverages condition synthesis to obtain a set of compound modifications that generate a rich, productive, and tractable search space of candidate patches. We evaluate PCR on a set of 105 defects from the GenProg benchmark set. For 40 of these defects, PCR generates plausible patches (patches that generate correct outputs for all inputs in the test suite used to validate the patch). For 12 of these defects, PCR generates correct patches that are functionally equivalent to developer patches that appear in subsequent versions. For comparison purposes, GenProg generates plausible patches for only 18 defects and correct patches for only 2 defects. AE generates plausible patches for only 27 defects and correct patches for only 3 defects.",,14 p.,,,,Computer Architecture,,,,,,,,,,,,,2015-02-12T21:00:03Z,,,,,,,,,,,,,,,,2015,984
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Rinard, Martin",2015-04-15T21:30:04Z,2015-04-15T21:30:04Z,2015-04-15,http://hdl.handle.net/1721.1/96625,MIT-CSAIL-TR-2015-013,Automatic Error Elimination by Horizontal Code Transfer Across Multiple Applications,"We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications.",,15 p.,,,,Program Analysis,,,,,,,,,,,,,2015-04-15T21:30:04Z,,,,,,,,,,,,,,,,2015,994
Boris Katz,"Borchardt, Gary C.",2015-03-31T22:15:06Z,2015-03-31T22:15:06Z,2015-03-30,http://hdl.handle.net/1721.1/96300,MIT-CSAIL-TR-2015-009,A Suite of Techniques for Describing Activity in Terms of Events,"This report presents a set of software techniques that support the tasks of event recognition, summarization of event sequences, explanation of recognized events, explanation of non-recognized events, prediction of event completions, and question answering by leveraging language-encoded human knowledge of what typically happens during various types of events. The techniques operate on sequences of timestamped, three-dimensional positions and contacts for humans, body parts, and objects, provided by a Microsoft Kinect sensor plus associated software. Appendices describe 64 activity sequences used for development and testing of the techniques and 102 event models created as part of the effort.",,89 p.,,,,Infolab,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-03-31T22:15:06Z,,,,,,,,,,,,,,,,2015,990
Daniel Sanchez,"Beckmann, Nathan; Tsai, Po-An; Sanchez, Daniel",2015-12-21T19:00:18Z,2015-12-21T19:00:18Z,2015-12-19,http://hdl.handle.net/1721.1/100466,MIT-CSAIL-TR-2015-035,Jenga: Harnessing Heterogeneous Memories through Reconfigurable Cache Hierarchies,"Conventional memory systems are organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit a wide range of applications, because working sets settle at the smallest (and fastest) level they fit in. However, rigid hierarchies also cause significant overheads, because each level adds latency and energy even when it does not capture the working set. In emerging systems with heterogeneous memory technologies such as stacked DRAM, these overheads often limit performance and efficiency. We propose Jenga, a reconfigurable cache hierarchy that avoids these pathologies and approaches the performance of a hierarchy optimized for each application. Jenga monitors application behavior and dynamically builds virtual cache hierarchies out of heterogeneous, distributed cache banks. Jenga uses simple hardware support and a novel software runtime to configure virtual cache hierarchies. On a 36-core CMP with a 1 GB stacked-DRAM cache, Jenga outperforms a combination of state-of-the-art techniques by 10% on average and by up to 36%, and does so while saving energy, improving system-wide energy-delay product by 29% on average and by up to 96%.",,12 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:18Z,,,,,,,,,,,,,,,,2015,1014
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-05-29T20:45:03Z,2015-05-29T20:45:03Z,2015-05-29,http://hdl.handle.net/1721.1/97130,MIT-CSAIL-TR-2015-021,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,"We analyze reported patches for three existing generate-and-validate patch generation systems (GenProg, RSRepair, and AE). The basic principle behind generate-and-validate systems is to accept only plausible patches that produce correct outputs for all inputs in the test suite used to validate the patches. Because of errors in the patch evaluation infrastructure, the majority of the reported patches are not plausible -- they do not produce correct outputs even for the inputs in the validation test suite. The overwhelming majority of the reported patches are not correct and are equivalent to a single modification that simply deletes functionality. Observed negative effects include the introduction of security vulnerabilities and the elimination of desirable standard functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",,24 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-05-29T20:45:03Z,,,,,,,,,,,,,,,,2015,1003
Martin Rinard,"Stanley-Marbell, Phillip; Rinard, Martin",2015-06-04T17:00:11Z,2015-06-04T17:00:11Z,2015-06-04,http://hdl.handle.net/1721.1/97180,MIT-CSAIL-TR-2015-022,Value-Deviation-Bounded Serial Data Encoding for Energy-Efficient Approximate Communication,"Transferring data between ICs accounts for a growing proportion of system power in wearable and mobile systems. Reducing signal transitions reduces the dynamic power dissipated in this data transfer, but traditional approaches cannot be applied when the transfer interfaces are serial buses. To address this challenge, we present a family of optimal value-deviation-bounded approximate serial encoders (VDBS encoders) that significantly reduce signal transitions (and hence, dynamic power) for bit-serial communication interfaces. When the data in transfer are from sensors, VDBS encoding enables a tradeoff between power efficiency and application fidelity, by exploiting the tolerance of many of the typical algorithms consuming sensor data to deviations in values. We derive analytic formulations for the family of VDBS encoders and introduce an efficient algorithm that performs close to the Pareto-optimal encoders. We evaluate the algorithm in two applications: Encoding data between a camera and processor in a text-recognition system, and between an accelerometer and processor in a pedometer system. For the text recognizer, the algorithm reduces signal transitions by 55% on average, while maintaining OCR accuracy at over 90% for previously-correctly-recognized text. For the pedometer, the algorithm reduces signal transitions by an average of 54% in exchange for step count errors of under 5%.",,20 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-06-04T17:00:11Z,,,,,,,,,,,,,,,,2015,1004
Patrick Winston,"Finlayson, Mark Alan",2015-12-03T16:30:05Z,2015-12-03T16:30:05Z,2015-12-02,http://hdl.handle.net/1721.1/100054,,"Supplementary materials for ""ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory""","This archive contains the supplementary material for the journal article ""ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory"", published in the Journal of Digital Scholarship in the Humanities (DSH), ca. 2016.The archive contains several different types of files. First, it contains the annotation guides that were used to train the annotators. The guides are numbered to match the team numbers in Table 6. Included here are not only detailed guides for some layers, as produced by the original developers of the specification, but also our synopsis guides for each layer, which were used as a reference and further training material for the annotators. Also of interest are the general annotator and adjudicator training guides, which outline the general procedures followed by the teams when conducting annotation. Those who are organizing their own annotation projects may find this material useful.Second, the archive contains a comprehensive manifest, in Excel spreadsheet format, listing the word counts, sources, types, and titles (in both Russian and English) of all the texts that are part of the corpus. Finally, the archive contains the actual corpus data files, in Story Workbench format, an XML-encoded stand-off annotation scheme. The scheme is described in the file format specification file, also included in the archive. These files can be parsed with the aid of any normal XML reading software, or can be loaded and edited easily with the Story Workbench annotation tool, also freely available.",,8341 KiB,,,,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,Patrick Winston; Genesis,,,,,,,2015-12-03T16:30:05Z,,,,,,,,,,,,,,,,2015,1013
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",2015-05-26T21:30:02Z,2015-05-26T21:30:02Z,2015-05-26,http://hdl.handle.net/1721.1/97087,MIT-CSAIL-TR-2015-018,Automatic Discovery and Patching of Buffer and Integer Overflow Errors,"We present Targeted Automatic Patching (TAP), an automatic buffer and integer overflow discovery and patching system. Starting with an application and a seed input that the application processes correctly, TAP dynamically analyzes the execution of the application to locate target memory allocation sites and statements that access dynamically or statically allocated blocks of memory. It then uses targeted error-discovery techniques to automatically generate inputs that trigger integer and/or buffer overflows at the target sites. When it discovers a buffer or integer overflow error, TAP automatically matches and applies patch templates to generate patches that eliminate the error. Our experimental results show that TAP successfully discovers and patches two buffer and six integer overflow errors in six real-world applications.",,10 p.,,,,Program Analysis,,,,,,,,,,,,,2015-05-26T21:30:03Z,,,,,,,,,,,,,,,,2015,1000
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-05-21T21:00:09Z,2015-05-21T21:00:09Z,2015-05-21,http://hdl.handle.net/1721.1/97051,,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems (Supplementary Material),"We analyze reported patches for three prior generate-and-validate patch generation systems (GenProg, RSRepair, and AE). Because of errors in the patch evaluation infrastructure, the majority of the reported patches violate the basic principle behind the design of these systems   they do not produce correct outputs even for the inputs in the test suite used to validate the patches. We also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",,13152246 bytes,,,Automatic Repair; Patch Analysis; Function Elimination,Computer Architecture,,,,,,,,,,,,,2015-05-21T21:00:09Z,,,,,,,,,,,,,,,,2015,998
