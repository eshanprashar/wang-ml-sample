dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.other,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.format.mimetype,dc.language.iso,dc.relation.ispartofseries,dc.title,dc.description,dc.subject,dc.relation.replaces,dc.relation.uri,dc.identifier.citation
Michael Ernst,"Kiezun, Adam; Ernst, Michael D.; Tip, Frank; Fuhrer, Robert M.",Program Analysis,2006-09-07T20:30:25Z,2006-09-07T20:30:25Z,2006-09-05,MIT-CSAIL-TR-2006-061,http://hdl.handle.net/1721.1/33965,"Type safety and expressiveness of many existing Java libraries and theirclient applications would improve, if the libraries were upgraded to definegeneric classes.  Efficient and accurate tools exist to assist clientapplications to use generics libraries, but so far the libraries themselvesmust be parameterized manually, which is a tedious, time-consuming, anderror-prone task.  We present a type-constraint-based algorithm forconverting non-generic libraries to add type parameters.  The algorithmhandles the full Java language and preserves backward compatibility, thusmaking it safe for existing clients.  Among other features, it is capableof inferring wildcard types and introducing type parameters formutually-dependent classes.  We have implemented the algorithm as a fullyautomatic refactoring in Eclipse.We evaluated our work in two ways.  First, our tool parameterized code thatwas lacking type parameters.  We contacted the developers of several ofthese applications, and in all cases where we received a response, theyconfirmed that the resulting parameterizations were correct and useful.Second, to better quantify its effectiveness, our tool parameterizedclasses from already-generic libraries, and we compared the results tothose that were created by the libraries' authors.  Our tool performed therefactoring accurately -- in 87% of cases the results were as good as thosecreated manually by a human expert, in 9% of cases the tool results werebetter, and in 4% of cases the tool results were worse.",11 p.; 268513 bytes; 1340485 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Refactoring for parameterizing Java classes,,,,,
Michael Ernst,"Tschantz, Matthew S.",Program Analysis,2006-09-07T18:51:29Z,2006-09-07T18:51:29Z,2006-09-05,MIT-CSAIL-TR-2006-059,http://hdl.handle.net/1721.1/33963,"This paper describes a programming language, Javari, that is capable of expressing and enforcing immutability constraints.  The specific constraint expressed is that the abstract state of the object to which an immutable reference refers cannot be modified using that reference.  The abstract state is (part of) the transitively reachable state: that is, the state of the object and all state reachable from it by following references.  The type system permits explicitly excluding fields from the abstract state of an object.  For a statically type-safe language, the type system guarantees reference immutability.The type system is distinguishes the notions of assignability and mutability; integrates with Java's generic types and with multi-dimensional arrays; provides a mutability polymorphism approach to avoiding code duplication; and has type-safe support for reflection and serialization.  This paper describes a core calculus including formal type rules for the language.Additionally, this paper describes a type inference algorithm that can be used convert existing Java programs to Javari.  Experimental results from a prototype implementation of the algorithm are presented.",133 p.; 1217863 bytes; 4225617 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Javari: Adding Reference Immutability to Java,MEng thesis,assignable; languages; mutable; readonly; type system; verification,,,
Nancy Lynch,"Canetti,, Ran; Cheung,, Ling; Kaynar,, Dilsun; Liskov,, Moses; Lynch,, Nancy; Pereira,, Olivier; Segala, Roberto",Theory of Computation,2006-09-07T20:14:22Z,2006-09-07T20:14:22Z,2006-09-05,MIT-CSAIL-TR-2006-060,http://hdl.handle.net/1721.1/33964,"Modeling frameworks such as Probabilistic I/O Automata (PIOA) andMarkov Decision Processes permit both probabilistic andnondeterministic choices.  In order to use such frameworks to express claims about probabilities of events, one needs mechanisms for resolving nondeterministic choices.  For PIOAs, nondeterministic choices have traditionally been resolved by schedulers that have perfect information about the past execution.  However, such schedulers are too powerful for certain settings, such as cryptographic protocol analysis, where information must sometimes be hidden. Here, we propose a new, less powerful nondeterminism-resolutionmechanism for PIOAs, consisting of tasks and local schedulers.Tasks are equivalence classes of system actions that are scheduled byoblivious, global task sequences.  Local schedulers resolve nondeterminism within system components, based on local information only.  The resulting task-PIOA framework yields simple notions of external behavior and implementation, and supports simple compositionality results.We also define a new kind of simulation relation, and show it to besound for proving implementation.  We illustrate the potential of the task-PIOA    framework by outlining its use in verifying an Oblivious Transfer protocol.",34 p.; 2343188 bytes; 369503 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Task-Structured Probabilistic I/O Automata,,,http://hdl.handle.net/1721.1/32525,http://hdl.handle.net/1721.1/32525,
Tomaso Poggio,"Caponnetto, Andrea; Yao, Yuan",Center for Biological and Computational Learning (CBCL),2006-09-29T18:36:45Z,2006-09-29T18:36:45Z,2006-09-10,MIT-CSAIL-TR-2006-063; CBCL-265,http://hdl.handle.net/1721.1/34217,"We consider learning algorithms induced by regularization methods in the regression setting.  We show that previously obtained error bounds for these algorithms using a-priori choices of the regularization parameter, can be attained using a suitable a-posteriori choice based on validation.  In particular, these results prove adaptation of the rate of convergence of the estimators to the minimax rate induced by the ""effective dimension"" of the problem.  We also show universal consistency for theses class methods.",19 p.; 963649 bytes; 819523 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Adaptation for Regularization Operators in Learning Theory,,"optimal rates, Learning, regularization methods, adaptation, cross-validation",,,
Tomaso Poggio,"Caponnetto, Andrea",Center for Biological and Computational Learning (CBCL),2006-09-29T18:36:42Z,2006-09-29T18:36:42Z,2006-09-10,MIT-CSAIL-TR-2006-062; CBCL-264,http://hdl.handle.net/1721.1/34216,"We develop some new error bounds for learning algorithms induced by regularization methods in the regression setting.  The ""hardness"" of the problem is characterized in terms of the parameters r and s, the first related to the ""complexity"" of the target function, the second connected to the effective dimension of the marginal probability measure over the input space.  We show, extending previous results, that by a suitable choice of the regularization parameter as a function of the number of the available examples, it is possible attain the optimal minimax rates of convergence for the expected squared loss of the estimators, over the family of priors fulfilling the constraint r + s > 1/2.  The setting considers both labelled and unlabelled examples, the latter being crucial for the optimality results on the priors in the range r < 1/2.",16 p.; 776374 bytes; 738421 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Optimal Rates for Regularization Operators in Learning Theory,,"optimal rates, regularized least-squares algorithm, regularization methods, adaptation",,,
Srini Devadas,"Sarmenta, Luis F. G.; van Dijk, Marten; O'Donnell, Charles W.; Rhodes, Jonathan; Devadas, Srinivas",Computation Structures,2006-09-11T22:20:24Z,2006-09-11T22:20:24Z,2006-09-11,MIT-CSAIL-TR-2006-064,http://hdl.handle.net/1721.1/33966,"A trusted monotonic counter is a valuable primitive thatenables a wide variety of highly scalable offlineand decentralized applications that would otherwise be prone to replay attacks, including offline payment, e-wallets, virtual trusted storage, and digital rights management (DRM).In this paper, we show how one can implement a very large number of virtual monotonic counters on an untrusted machine with a Trusted Platform Module (TPM) or similar device, without relying on a trusted OS.  We first present a log-based scheme that can be implemented with the current version of the TPM (1.2) and used incertain applications.We then show how the addition of a few simple features tothe TPM makes it possible to implement a hash-tree-based schemethat not only offers improved performance and scalability compared to the log-based scheme, but also makes it possible to implement count-limited objects (or ``clobs'' for short) -- i.e., encrypted keys, data, and other objectsthat can only be used when an associated virtual monotonic counter is within a certain range.Such count-limited objects include n-time use keys, n-out-of-m data blobs,n-copy migratable objects, and other variants, which have many potential uses in digital rights management (DRM), digital cash, digital voting, itinerant computing,and other application areas.",18 p.; 430350 bytes; 694048 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Virtual Monotonic Counters and Count-Limited Objects using a TPM without a Trusted OS (Extended Version),,trusted storage; key delegation; stored-value; e-wallet; smartcard; memory integrity checking; certified execution,,,A shorter version of this paper will appear in the 1st ACM CCS Workshop on Scalable Trusted Computing (STC'06).
Michael Ernst,"Artzi, Shay; Ernst, Michael D.; Glasser, David; Kiezun, Adam",Program Analysis,2006-09-18T17:55:18Z,2006-09-18T17:55:18Z,2006-09-17,MIT-CSAIL-TR-2006-065,http://hdl.handle.net/1721.1/33968,"Knowing which method parameters may be mutated during a method'sexecution is useful for many software engineering tasks.  We presentan approach to discovering parameter immutability, in which severallightweight, scalable analyses are combined in stages, with each stagerefining the overall result.  The resulting analysis is scalable andcombines the strengths of its component analyses.  As one of thecomponent analyses, we present a novel, dynamic mutability analysisand show how its results can be improved by random input generation.Experimental results on programs of up to 185 kLOC demonstrate that,compared to previous approaches, our approach increases both scalabilityand overall accuracy.",10 p.; 176363 bytes; 1038435 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combined static and dynamic mutability analysis,,immutability; mutability; side effect analysis; purity; pointer analysis; dynamic analysis; mutation,,,
Krste Asanovic,"Tseng, Jessica H.; Asanovic, Krste",Computer Architecture,2006-09-25T16:01:50Z,2006-09-25T16:01:50Z,2006-09-18,MIT-CSAIL-TR-2006-066,http://hdl.handle.net/1721.1/34012,"RingScalar is a complexity-effective microarchitecture for out-of-order superscalar processors, that reduces the area, latency, and power of all major structures in the instruction flow.  The design divides an N-way superscalar into N columns connected in a unidirectional ring, where each column contains a portion of the instruction window, a bank of the register file, and an ALU.  The design exploits the fact that most decoded instructions are waiting on just one operand to use only a single tag per issue window entry, and to restrict instruction wakeup and value bypass to only communicate with the neighboring column.  Detailed simulations of four-issue single-threaded machines running SPECint2000 show that RingScalar has IPC only 13% lower than an idealized superscalar, while providing large reductions in area, power, and circuit latency.",14 p.; 1561908 bytes; 957204 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,RingScalar: A Complexity-Effective Out-of-Order Superscalar Microarchitecture,,,,,
Saman Amarasinghe,"Zhao, Qin; Rabbah, Rodric; Amarasinghe, Saman; Rudolph, Larry; Wong, Weng-Fai",Computer Architecture,2006-09-25T21:21:43Z,2006-09-25T21:21:43Z,2006-09-25,MIT-CSAIL-TR-2006-067,http://hdl.handle.net/1721.1/34013,"Modern memory systems play a critical role in the performance ofapplications, but a detailed understanding of the application behaviorin the memory system is not trivial to attain. It requires timeconsuming simulations of the memory hierarchy using long traces, andoften using detailed modeling. It is increasingly possible to accesshardware performance counters to measure events in the memory system,but the measurements remain coarse grained, better suited forperformance summaries than providing instruction level feedback. Theavailability of a low cost, online, and accurate methodology forderiving fine-grained memory behavior profiles can prove extremelyuseful for runtime analysis and optimization of programs.This paper presents a new methodology for Ubiquitous MemoryIntrospection (UMI). It is an online and lightweight mini-simulationmethodology that focuses on simulating short memory access tracesrecorded from frequently executed code regions. The simulations arefast and can provide profiling results at varying granularities, downto that of a single instruction or address. UMI naturally complementsruntime optimizations techniques and enables new opportunities formemory specific optimizations.In this paper, we present a prototype implementation of a runtimesystem implementing UMI. The prototype is readily deployed oncommodity processors, requires no user intervention, and can operatewith stripped binaries and legacy software. The prototype operateswith an average runtime overhead of 20% but this slowdown is only 6%slower than a state of the art binary instrumentation tool.  We used32 benchmarks, including the full suite of SPEC2000 benchmarks, forour evaluation. We show that the mini-simulation results accuratelyreflect the cache performance of two existing memory systems, anIntel Pentium~4 and an AMD Athlon MP (K7) processor. We alsodemonstrate that low level profiling information from the onlinesimulation can serve to identify high-miss rate load instructions with a77% rate of accuracy compared to full offline simulations thatrequired days to complete. The online profiling results are used atruntime to implement a simple software prefetching strategy thatachieves a speedup greater than 60% in the best case.",23 p.; 278689 bytes; 1358949 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Ubiquitous Memory Introspection (Preliminary Manuscript),,Performance Monitoring; Online Simulation; Runtime Optimization; Cache Modelling; Memory Hierarchy,,,
Daniel Jackson,"Torlak, Emina; Jackson, Daniel",Software Design,2006-09-29T21:46:41Z,2006-09-29T21:46:41Z,2006-09-29,MIT-CSAIL-TR-2006-068,http://hdl.handle.net/1721.1/34218,"The key design challenges in the construction of a SAT-based relational engine are described, and novel techniques are proposed to address them.  An efficient engine must have a mechanism for specifying partial solutions, an effective symmetry detection and breaking scheme, and an economical translation from relational to boolean logic.  These desiderata are addressed with three new techniques: a symmetry detection algorithm that works in the presence of partial solutions, a sparse-matrix representation of relations, and a compact representation of boolean formulas inspired by boolean expression diagrams and reduced boolean circuits.  The presented techniques have been implemented and evaluated, with promising results.",11 p.; 444636 bytes; 1204261 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Design of a Relational Engine,,,,,
Nancy Lynch,"Konwar, K.; Musial, P.M.; Nicolau, N.C.; Shvartsman., A.A.",Theory of Computation,2006-10-17T16:04:56Z,2006-10-17T16:04:56Z,2006-10-12,MIT-CSAIL-TR-2006-070,http://hdl.handle.net/1721.1/34249,"Developing middleware services for dynamic distributed systems, e.g., ad-hoc networks, is a challenging task given that suchservices must deal with communicating devices that may join and leave the system, and fail or experience arbitrary delays. Algorithmsdeveloped for static settings are often not usable in dynamic settings because they rely on (logical) all-to-all connectivityor assume underlying routing protocols, which may be unfeasible in highly dynamic settings. This paper explores the indirectlearning approach to information dissemination within a dynamic distributed data service. The indirect learning scheme is usedto improve the liveness of the atomic read/write object service in the settings with uncertain connectivity. The service is formallyproved to be correct, i.e., the atomicity of the objects is guaranteed in all executions. Conditional analysis of the performanceof the new service is presented. This analysis has the potential of being generalized to other similar dynamic algorithms. Underthe assumption that the network is connected, and assuming reasonable timing conditions, the bounds on the duration of theread/write operations of the new service are calculated. Finally, the paper proposes a deployment strategy where indirect learningleads to an improvement in communication costs relative to a previous solution.",21 p.; 482192 bytes; 1655060 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Implementing Atomic Data through Indirect Learning in Dynamic Network,,,,,
Patrick Winston,"Finlayson, Mark Alan; Winston, Patrick Henry",Genesis,2006-11-07T15:36:52Z,2006-11-07T15:36:52Z,2006-11-07,MIT-CSAIL-TR-2006-071,http://hdl.handle.net/1721.1/34635,"Analogical reasoning has been implicated in many important cognitive processes, such as learning, categorization, planning, and understanding natural language. Therefore, to obtain a full understanding of these processes, we must come to a better understanding of how people reason by analogy. Analogical reasoning is thought to occur in at least three stages: retrieval of a source description from memory upon presentation of a target description, mapping of the source description to the target description, and transfer of relationships from source description to target description. Here we examine the first stage, the retrieval of relevant sources from long-term memory for their use in analogical reasoning. Specifically we ask: what can people retrieve from long-term memory, and how do they do it?Psychological experiments show that subjects display two sorts of retrieval patterns when reasoning by analogy: a novice pattern and an expert pattern. Novice-like subjects are more likely to recall superficiallysimilar descriptions that are not helpful for reasoning by analogy. Conversely, expert-like subjects are more likely to recall structurally-related descriptions that are useful for further analogical reasoning. Previous computational models of the retrieval stage have only attempted to model novice-like retrieval. We introduce a computational model that can demonstrate both novice-like and expert-like retrieval with the same mechanism. The parameter of the model that is varied to produce these two types of retrieval is the average size of the features used to identify matches in memory. We find that, in agreement with an intuition from the work of Ullman and co-workers regarding the use of features in visual classification (Ullman, Vidal-Naquet,& Sali, 2002), that features of an intermediate size are most useful for analogical retrieval.We conducted two computational experiments on our own dataset of fourteen formally described stories, which showed that our model gives the strongest analogical retrieval, and is most expert-like, when it uses features that are on average of intermediate size. We conducted a third computational experiment on the Karla the Hawk dataset which showed a modest effect consistent with our predictions. Because our model and Ullman’s work both rely on intermediate-sized features to perform recognition-like tasks, we take both as supporting what we call the Goldilocks hypothesis: that on the average those features that are maximally useful for recognition are neither too small nor too large, neither too simple nor too complex, but rather are in the middle, of intermediate size and complexity.",34 p.; 2808906 bytes; 859593 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Analogical Retrieval via Intermediate Features: The Goldilocks Hypothesis,,analogical access; precedent retrieval; intermediate features; symbolic computational modelling; mere-appearance; literally-similar,,,
Martin Rinard,"Bouillaguet, Charles; Kuncak, Viktor; Wies, Thomas; Zee, Karen; Rinard, Martin",Computer Architecture,2006-11-09T15:26:55Z,2006-11-09T15:26:55Z,2006-11-09,MIT-CSAIL-TR-2006-072,http://hdl.handle.net/1721.1/34874,"This paper presents our integration of efficient  resolution-based theorem provers into the Jahob data  structure verification system.  Our experimental results  show that this approach enables Jahob to automatically  verify the correctness of a range of complex dynamically  instantiable data structures, including data structures  such as hash tables and search trees, without the need for  interactive theorem proving or techniques tailored to  individual data structures.  Our primary technical results include: (1) a translation  from higher-order logic to first-order logic that enables  the application of resolution-based theorem provers and  (2) a proof that eliminating type (sort) information in  formulas is both sound and complete, even in the presence  of a generic equality operator.  Our  experimental results show that the elimination of   type information dramatically decreases the time required  to prove the resulting formulas.  These techniques enabled us to verify complex correctness  properties of Java programs such as a mutable set  implemented as an imperative linked list, a finite map  implemented as a functional ordered tree, a hash table  with a mutable array, and a simple library system example  that uses these container data structures.  Our system  verifies (in a matter of minutes) that data structure  operations correctly update the finite map, that they  preserve data structure invariants (such as ordering of  elements, membership in appropriate hash table buckets, or  relationships between sets and relations), and that there  are no run-time errors such as null dereferences or array  out of bounds accesses.",32 p.; 397902 bytes; 1759318 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Using First-Order Theorem Provers in the Jahob Data Structure Verification System,,program verification; shape analysis; multisorted logic,,,"Short version to appear in VMCAI'07, Nice, January 2007"
Fredo Durand,"Paris, Sylvain; Durand, Fredo",Computer Graphics,2006-11-13T18:32:46Z,2006-11-13T18:32:46Z,2006-11-09,MIT-CSAIL-TR-2006-073,http://hdl.handle.net/1721.1/34876,"The bilateral filter is a nonlinear filter that smoothes a signal while preserving strong edges. It has demonstrated great effectiveness for a variety of problems in computer vision and computer graphics, and fast versions have been proposed. Unfortunately, little is known about the accuracy of such accelerations. In this paper, we propose a new signal-processing analysis of the bilateral filter which complements the recent studies that analyzed it as a PDE or as a robust statistical estimator. The key to our analysis is to express the filter in a higher-dimensional space where the signal intensity is added to the original domain dimensions. Importantly, this signal-processing perspective allows us to develop a novel bilateral filtering acceleration using downsampling in space and intensity.  This affords a principled expression of accuracy in terms of bandwidth and sampling. The bilateral filter can be expressed as linear convolutions in this augmented space followed by two simple nonlinearities. This allows us to derive criteria for downsampling the key operations and achieving important acceleration of the bilateral filter. We show that, for the same running time, our method is more accurate than previous acceleration techniques. Typically, we are able to process a 2~megapixel image using our acceleration technique in less than a second, and have the result be visually similar to the exact computation that takes several tens of minutes. The acceleration is most effective with large spatial kernels. Furthermore, this approach extends naturally to color images and cross bilateral filtering.",38 p.; 11564290 bytes; 21570337 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Fast Approximation of the Bilateral Filter using a Signal Processing Approach,,color image processing; cross bilateral filter; edge-preserving filter,,,
Hari Balakrishnan,"Jung, Jaeyeon; Milito, Rodolfo A.; Paxson, Vern",Networks & Mobile Systems,2006-11-13T18:32:38Z,2006-11-13T18:32:38Z,2006-11-10,MIT-CSAIL-TR-2006-074,http://hdl.handle.net/1721.1/34875,"We present two light-weight worm detection algorithms thatoffer significant advantages over fixed-threshold methods.The first algorithm, RBS (rate-based sequential hypothesis testing)aims at the large class of worms that attempts to quickly propagate, thusexhibiting abnormal levels of the rate at which hosts initiateconnections to new destinations. The foundation of RBS derives fromthe theory of sequential hypothesis testing, the use of which fordetecting randomly scanning hosts was first introduced by our previouswork with the TRW (Threshold Random Walk) scan detection algorithm. The sequential hypothesistesting methodology enables engineering the detectors to meet falsepositives and false negatives targets, rather than triggering whenfixed thresholds are crossed. In this sense, the detectors that weintroduce are truly adaptive.We then introduce RBS+TRW, an algorithm that combines fan-out rate (RBS)and probability of failure (TRW) of connections to new destinations.RBS+TRW provides a unified framework that at one end acts as a pure RBSand at the other end as pure TRW, and extends RBS's power in detectingworms that scan randomly selected IP addresses.",17 p.; 400578 bytes; 1658364 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On the Adaptive Real-Time Detection of Fast-Propagating Network Worms,,,,,
Trevor Darrell,"Morency, Louis-Philippe",Vision,2006-11-17T11:12:55Z,2006-11-17T11:12:55Z,2006-11-15,MIT-CSAIL-TR-2006-075,http://hdl.handle.net/1721.1/34893,"During face-to-face conversation, people use visual feedback (e.g.,head and eye gesture) to communicate relevant information and tosynchronize rhythm between participants. When recognizing visualfeedback, people often rely on more than their visual perception.For instance, knowledge about the current topic and from previousutterances help guide the recognition of nonverbal cues. The goal ofthis thesis is to augment computer interfaces with the ability toperceive visual feedback gestures and to enable the exploitation ofcontextual information from the current interaction state to improvevisual feedback recognition.We introduce the concept of visual feedback anticipationwhere contextual knowledge from an interactive system (e.g. lastspoken utterance from the robot or system events from the GUIinterface) is analyzed online to anticipate visual feedback from ahuman participant and improve visual feedback recognition. Ourmulti-modal framework for context-based visual feedback recognitionwas successfully tested on conversational and non-embodiedinterfaces for head and eye gesture recognition.We also introduce Frame-based Hidden-state Conditional RandomField model, a new discriminative model for visual gesturerecognition which can model the sub-structure of a gesture sequence,learn the dynamics between gesture labels, and can be directlyapplied to label unsegmented sequences. The FHCRF model outperformsprevious approaches (i.e. HMM, SVM and CRF) for visual gesturerecognition and can efficiently learn relevant contextualinformation necessary for visual feedback anticipation.A real-time visual feedback recognition library for interactiveinterfaces (called Watson) was developed to recognize head gaze,head gestures, and eye gaze using the images from a monocular orstereo camera and the context information from the interactivesystem. Watson was downloaded by more then 70 researchers around theworld and was successfully used by MERL, USC, NTT, MIT Media Lab andmany other research groups.",195 p.; 5912220 bytes; 20231190 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Context-based Visual Feedback Recognition,PhD thesis,,,,
Michael Ernst,"McCamant, Stephen; Ernst, Michael D.",Program Analysis,2006-11-17T11:12:32Z,2006-11-17T11:12:32Z,2006-11-17,MIT-CSAIL-TR-2006-076,http://hdl.handle.net/1721.1/34892,"We present a new approach for tracking programs' use of data througharbitrary calculations, to determine how much information about secretinputs is revealed by public outputs.  Using a fine-grained dynamicbit-tracking analysis, the technique measures the information revealedduring a particular execution.  The technique accounts for indirectflows, e.g. via branches and pointer operations.  Two kinds ofuntrusted annotation improve the precision of the analysis.  Animplementation of the technique based on dynamic binary translation isdemonstrated on real C, C++, and Objective C programs of up to half amillion lines of code.  In case studies, the tool checked multiplesecurity policies, including one that was violated by a previouslyunknown bug.",18 p.; 450616 bytes; 1216950 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Quantitative Information-Flow Tracking for C and Related Languages,,Confidentiality; Privacy; Information disclosure; Tainting; Implicit flows; Valgrind; Memcheck; OpenSSH,,,
Sam Madden,"Abadi, Daniel J.; Myers, Daniel S.; DeWitt, David J.; Madden, Samuel R.",Database,2006-11-28T19:34:39Z,2006-11-28T19:34:39Z,2006-11-27,MIT-CSAIL-TR-2006-078,http://hdl.handle.net/1721.1/34929,"There has been renewed interest in column-oriented database architectures in recent years. For read-mostly query workloads such as those found in data warehouse and decision support applications, ``column-stores'' have been shown to perform particularly well relative to ``row-stores.'' In order for column-stores to be readily adopted as a replacement for row-stores, however, they must present the same interface to client applications as do row stores, which implies that they must output row-store-style tuples.Thus, the input columns stored on disk must be converted to rows at some point in the query plan, but the optimal point at which to do the conversion is not obvious. This problem can be considered as the opposite of the projection problem in row-store systems: while row-stores need to determine where in query plans to place projection operators to make tuples narrower, column-stores need to determine when to combine single-column projections into wider tuples. This paper describes a variety of strategies for tuple construction and intermediate result representations and provides a systematic evaluation of these strategies.",13 p.; 952582 bytes; 2497163 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Materialization Strategies in a Column-Oriented DBMS,,,,,Extension of Publication (of the same title) in the Proceedings of ICDE 2007
Sam Madden,"Gil, Thomer M.; Madden, Samuel",Database,2006-11-27T14:39:40Z,2006-11-27T14:39:40Z,2006-11-27,MIT-CSAIL-TR-2006-077,http://hdl.handle.net/1721.1/34916,"In this paper, we present the design of Scoop, a system for indexing and querying stored data in sensor networks. Scoop works by collecting statistics about the rate of queries and distribution of sensor readings over a sensor network, and uses those statistics to build an index that tells nodes where in the network to store their readings. Using this index, a user’s queries over that stored data can be answered efficiently, without &#64258;ooding those queries throughout the network. This approach offers a substantial advantage over other solutions that either store all data externally on a basestation (requiring every reading to be collected from all nodes), or that store all data locally on the node that produced it (requiring queries to be &#64258;ooded throughout the network). Our results, in fact, show that Scoop offers a factor of four improvement over existing techniques in a real implementation on a 64-node mote-based sensor network. These results also show that Scoop is able to efficciently adapt to changes in the distribution and rates of data and queries.",10 p.; 334459 bytes; 1062863 bytes,application/pdf; application/postscript,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Scoop: An Adaptive Indexing Scheme for Stored Data in Sensor Networks,,database,,,
Howard Shrobe,"Tzanov, Velin K.",AIRE,2006-12-05T20:42:13Z,2006-12-05T20:42:13Z,2006-12-05,MIT-CSAIL-TR-2006-079,http://hdl.handle.net/1721.1/34943,"The main goal of this thesis is to demonstrate the applicability of the distributed systems paradigm to robotic systems. This goal is accomplished by presenting two solutions to the Distributed Area Search problem: organizing a team of robots to collaborate in the task of searching through an area. The first solution is designed for unreliable robots equipped with a reliable GPS-style localization system. This solution demonstrates the efficiency and fault-tolerance of this type of distributed robotic systems, as well as their applicability to the real world. We present a theoretically near-optimal algorithm for solving Distributed Area Search under this setting, and we also present an implementation of our algorithm on an actual system, consisting of twelve robots. The second solution is designed for a completely autonomous system, without the aid of any centralized subsystem. It demonstrates how a distributed robotic system can solve a problem that is practically unsolvable for a single-robot system.",111 p.; 90018576 bytes; 2179689 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Distributed Area Search with a Team of Robots,MEng thesis,robotics; theory; distributed systems,,,
