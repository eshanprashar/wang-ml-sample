dc.contributor.advisor,dc.contributor.author,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.relation.ispartofseries,dc.title,dc.description.abstract,dc.identifier.other,dc.format.extent,dc.format.mimetype,dc.language.iso,dc.subject,dc.contributor.other,dc.description.sponsorship,dc.rights,dc.rights.uri,dc.description,dc.identifier.citation,dc.contributor,dc.relation.isversionof,dc.description.degree,dc.relation.isreplacedby,dc.relation.uri,dc.relation.replaces,dc.relation,dc.date.updated,dc.identifier,dc.relation.isreferencedby,dc.contributor.department,dc.type,dspace.orderedauthors,dc.date.submitted,dc.contributor.editor,dc.coverage.spatial,dc.coverage.temporal,eprint.grantNumber,dc.relation.requires,dc.language.rfc3066,dc.publisher,dc.relation.haspart,dc.identifier.oclc
"Szolovits, Peter","Tsien, Christine L.",2023-03-29T15:33:14Z,2023-03-29T15:33:14Z,2000,https://hdl.handle.net/1721.1/149913,MIT-LCS-TR-809,TrendFinder: Automated Detection of Alarmable Trends,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Micali, Silvio",2023-03-29T14:40:47Z,2023-03-29T14:40:47Z,2000,https://hdl.handle.net/1721.1/149277,MIT-LCS-TM-577,Copmutationally Sound Proofs,"This paper puts forward a new notion of a proof based on computational complexity and explores its implications for computation at large. Computationally sound proofs provide, in a novel and meaningful framework, answer to old and new questions in complexity theory. In particular, given a random oracle or a new complexity assumption, they enable us to 1. prove that verifying is easier than deciding for all theorems; 2. provides a quite effective way to prove membership in computationally hard languages (such as C-NP-complete ones); and 3. show that every computation possesses a short certificate vouching its correctness. FInally, if a special type of computationally sound proof exists, we show that Blum's notion of program checking can be meaningfully broadened so as to prove that NP-complete languages are checkable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bar-Joseph, Ziv; Keidar, Idit; Anker, Tal; Lynch, Nancy A.",2023-03-29T15:32:34Z,2023-03-29T15:32:34Z,2000-01,https://hdl.handle.net/1721.1/149906,MIT-LCS-TR-796,QoS Preserving Totally Ordered Multicast,This paper studies the Quality of Service (QoS) guarantees of totally ordered multicast algorithms. The paper shows that totally ordered multicast can coexist with guaranteed predictable delays in certain network models. The paper considers two reservatio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Amarasinghe, Saman; Agarwal, Anant","Barua, Rajeev",2023-03-29T15:32:37Z,2023-03-29T15:32:37Z,2000-01,https://hdl.handle.net/1721.1/149907,MIT-LCS-TR-799,Maps:  A Compiler-Managed Memory System for Software-Exposed Architectures,"Microprocessors must exploit both instruction-level parallelism (ILP) and memory parallelism for high performance.  Sophisticated techniques for ILP have boosted the ability of modern-day microprocessors to exploit ILP when available. Unfortunately, impro",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Katabi, Dina; Bazzi, Issam; Yang, Xiaowei",2023-03-29T14:41:46Z,2023-03-29T14:41:46Z,2000-03,https://hdl.handle.net/1721.1/149295,MIT-LCS-TM-604,An Information Theoretic Approach for Shared Bottleneck Inference Based on End-to-end Measurements,"Recent years have marked a growing interest in studying Internet path characteristics. However, most of the currently available tools to an end system to perform such measurements are slow inaccurate and generate an excessive amount of probing traffic. This paper introduces entropy as a novel and efficient metric for discovering Internet path characteristics based on data collected by an end system. In particular, the paper presents an entropy-based technique that enables an end system to cluster flows it receives according to their shared bottleneck. Our mechanism relies solely on information extracted from the packets' inter-arrivals at the receiver. It does not generate any probing traffic and can use data extracted from both TCP and UDP flows. Moreover, it requires only a small number of packets from each flow, which makes it useful for short-lived flows. We report the result of running the algorithm on simulated data and Internet traffic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bruering, Derek; Devabhaktuni, Srikrishna; Amarasinghe, Saman",2023-03-29T14:41:50Z,2023-03-29T14:41:50Z,2000-04,https://hdl.handle.net/1721.1/149296,MIT-LCS-TM-606,Softspec:  Software-based Speculative Parallelism,"We present Softspec, a technique for parallelizing sequential applications using only simple software mechanisms, requiring no complex program analysis or hardware support.  Softspec parallelizes loops whose memory references are stride-predictable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taylor, Michael Bedford; Lee, Walter; Frank, Matthew; Amarasinghe, Saman; Agarwal, Anant",2023-03-29T14:42:44Z,2023-03-29T14:42:44Z,2000-04,https://hdl.handle.net/1721.1/149317,MIT-LCS-TM-628,How to Build Scalable On-Chip ILP Networks for a Decentralized Architecture,"The era of billion transistors-on-a-chip is creating a completely different set of design constraints, forcing radically new microprocessor archiecture designs. This paper examines a few of the possible microarchitectures that are capable of obtaining scalable ILP performance. First, we observe that the network that interconnects the processing elements is the critical design point in the microarchitecture. Next, we characterize four fundamental properties that have to be satisfied by the interconnection network. Next, we provide case studies of two different networks that satisfy these properties. Finally, a detailed evaluation of these networks is presented to highlight the scalability and performance of these microarchitectures. We show that by using compile time information, we can build simpler networks and use them efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bansal, Deepak; Balakrishnan, Hari",2023-03-29T15:33:06Z,2023-03-29T15:33:06Z,2000-05,https://hdl.handle.net/1721.1/149910,MIT-LCS-TR-806,TCP-friendly Congestion Control for Real-time Streaming Applications,"This paper introduces and analyzes a class of nonlinear congestion control algorithms called binomial algorithms, motivated in part by the needs of streaming audio and video applications for which a drastic reduction in transmission rate upon congestion i",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bruening, Derek; Chapin, John",2023-03-29T14:41:53Z,2023-03-29T14:41:53Z,2000-05,https://hdl.handle.net/1721.1/149297,MIT-LCS-TM-607,Systematic Testing of Multithreaded Programs,"We present a practical testing algorithm called ExitBlock that systematically and deterministically finds program errors resulting from unintended timing dependencies.  ExitBlock executes a program or a portion of a program on a given input multiple times, enumerating meaningful schedules in order to cover all program behaviors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Heinz, Ernst",2023-03-29T14:41:55Z,2023-03-29T14:41:55Z,2000-05,https://hdl.handle.net/1721.1/149298,MIT-LCS-TM-608,A New Self-Play Experiment in Computer Chess,"This paper presents the results of a new self-play experiment in computer chess. It is the _x000C_rst such experiment ever to feature search depths beyond 9 plies and thousands of games for every single match. Overall, we executed 17,150 self-play games (1,050{3,000 per match) in one \\calibration"" match and seven \\depth X+1 , X"" handicap matches at _x000C_xed iteration depths ranging from 5{12 plies. For the experiment to be realistic and independently repeatable, we relied on a state-of-the-art commercial contestant: Fritz 6 , one of the strongest modern chess pro- grams available. The main result of our new experimentis thatit shows the existence of diminishing returns for additional search in computer chess self-play by Fritz 6 with 95% statistical con_x000C_dence. The dimin- ishing returns manifest themselves by declining rates of won games and reversely increasing rates of drawn games for the deeper searching pro- gram versions. The rate of lost games, however, remains quite steady for the whole depth range of 5{12 plies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Andersen, David; Bansal, Deccuk; Curtis, Dorothy; Seshan, Srinivasan; Balakrishnan, Hari",2023-03-29T15:33:11Z,2023-03-29T15:33:11Z,2000-05,https://hdl.handle.net/1721.1/149912,MIT-LCS-TR-808,System Support for Bandwidth Management and Content Adaptation in Internet Applications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Riesenhuber, Maximilian; Poggio, Tomaso",2004-10-20T20:50:13Z,2004-10-20T20:50:13Z,2000-05-01,http://hdl.handle.net/1721.1/7222,AIM-1682; CBCL-185,"The Individual is Nothing, the Class Everything: Psychophysics and Modeling of Recognition in Obect Classes","Most psychophysical studies of object recognition have focussed on the recognition and representation of individual objects subjects had previously explicitely been trained on. Correspondingly, modeling studies have often employed a 'grandmother'-type representation where the objects to be recognized were represented by individual units. However, objects in the natural world are commonly members of a class containing a number of visually similar objects, such as faces, for which physiology studies have provided support for a representation based on a sparse population code, which permits generalization from the learned exemplars to novel objects of that class. In this paper, we present results from psychophysical and modeling studies intended to investigate object recognition in natural ('continuous') object classes. In two experiments, subjects were trained to perform subordinate level discrimination in a continuous object class - images of computer-rendered cars - created using a 3D morphing system. By comparing the recognition performance of trained and untrained subjects we could estimate the effects of viewpoint-specific training and infer properties of the object class-specific representation learned as a result of training. We then compared the experimental findings to simulations, building on our recently presented HMAX model of object recognition in cortex, to investigate the computational properties of a population-based object class representation as outlined above. We find experimental evidence, supported by modeling results, that training builds a viewpoint- and class-specific representation that supplements a pre-existing repre-sentation with lower shape discriminability but possibly greater viewpoint invariance.",AIM-1682; CBCL-185,4110034 bytes; 1392514 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Papageorgiou, Constantine P.",2004-10-01T13:59:58Z,2004-10-01T13:59:58Z,2000-05-01,http://hdl.handle.net/1721.1/5566,AITR-1685; CBCL-186,A Trainable System for Object Detection in Images and Video Sequences,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",AITR-1685; CBCL-186,128 p.; 72537763 bytes; 15910731 bytes,application/postscript; application/pdf,en_US,AI; MIT; Artificial Intelligence; object detection; pattern recognition; people detection; face detection; car detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Heisele, Bernd; Poggio, Tomaso; Pontil, Massimiliano",2004-10-20T21:03:29Z,2004-10-20T21:03:29Z,2000-05-01,http://hdl.handle.net/1721.1/7229,AIM-1687; CBCL-187,Face Detection in Still Gray Images,"We present a trainable system for detecting frontal and near-frontal views of faces in still gray images using Support Vector Machines (SVMs). We first consider the problem of detecting the whole face pattern by a single SVM classifer. In this context we compare different types of image features, present and evaluate a new method for reducing the number of features and discuss practical issues concerning the parameterization of SVMs and the selection of training data. The second part of the paper describes a component-based method for face detection consisting of a two-level hierarchy of SVM classifers. On the first level, component classifers independently detect components of a face, such as the eyes, the nose, and the mouth. On the second level, a single classifer checks if the geometrical configuration of the detected components in the image matches a geometrical model of a face.",AIM-1687; CBCL-187,6267853 bytes; 482304 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Evgeniou, Theodoros; Pontil, Massimiliano",2004-10-20T20:48:37Z,2004-10-20T20:48:37Z,2000-05-01,http://hdl.handle.net/1721.1/7169,AIM-1681; CBCL-184,A Note on the Generalization Performance of Kernel Classifiers with Margin,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,AIM-1681; CBCL-184,9 p.; 1149066 bytes; 253797 bytes,application/postscript; application/pdf,en_US,AI; MIT; Artificial Intelligence; missing data; mixture models; statistical learning; EM algorithm; neural networks; kernel classifiers; Support Vector Machine; regularization networks; statistical learning theory; V-gamma dimension.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Nakajima, Chikahito; Pontil, Massimiliano; Heisele, Bernd; Poggio, Tomaso",2004-10-20T21:03:31Z,2004-10-20T21:03:31Z,2000-06-01,http://hdl.handle.net/1721.1/7230,AIM-1688; CBCL-188,People Recognition in Image Sequences by Supervised Learning,We describe a system that learns from examples to recognize people in images taken indoors. Images of people are represented by color-based and shape-based features. Recognition is carried out through combinations of Support Vector Machine classifiers (SVMs). Different types of multiclass strategies based on SVMs are explored and compared to k-Nearest Neighbors classifiers (kNNs). The system works in real time and shows high performance rates for people recognition throughout one day.,AIM-1688; CBCL-188,4611797 bytes; 373760 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shrestha, Govinda",2023-03-29T15:32:24Z,2023-03-29T15:32:24Z,2000-07,https://hdl.handle.net/1721.1/149903,MIT-LCS-TR-793,Information Technology Use in Developing Countries,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Micali, Silvio; Reyzin, Leonid",2023-03-29T14:41:58Z,2023-03-29T14:41:58Z,2000-08,https://hdl.handle.net/1721.1/149299,MIT-LCS-TM-609,Concurrent/Resettable Zero-Knowledge Protocols for NP in the Public Key Model,"We propose a four-round protocol for concurrent and resettable zero-knowledge arguments for any langauge in NP, assuming the verifier has a pre-registered public-key. We also propose a three-round protocol with an additional timing assumption.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kohler, Eddie; Chen, Benjie; Kaashoek, M. Frans; Morris, Robert T.; Poletto, Massimiliano",2023-03-29T15:33:47Z,2023-03-29T15:33:47Z,2000-08,https://hdl.handle.net/1721.1/149915,MIT-LCS-TR-812,Programming Language Techniques for Modular Router Configurations,"This paper applies programming language techniques to a high-level system description, both to optimize the system and to prove useful properties about it. The system in question is Click, a modular software router framework. Click routers are built from components called elements. Elements are written in C++, but the user creates a configuration using a simple, declarative data flow language. This language is amenable to data flow analysis and other conventional programming language techniques. Applied to a router configuration, these techniques have high-level results---for example, optimizing the router or verifying its high-level properties. This paper describes several programming language techniques that have been useful in practice, including optimization tools that remove virtual function calls from router definitions and remove redundant parts of adjacent routers. We also present performance results for an extensively optimized standards-compliant IP router. On conventional PC hardware, this router can forward up to 456,000 64-byte packets per second.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Riesenhuber, Maximilian; Poggio, Tomaso",2004-10-20T21:03:32Z,2004-10-20T21:03:32Z,2000-08-07,http://hdl.handle.net/1721.1/7231,AIM-1695; CBCL-190,Computational Models of Object Recognition in Cortex: A Review,"Understanding how biological visual systems perform object recognition is one of the ultimate goals in computational neuroscience. Among the biological models of recognition the main distinctions are between feedforward and feedback and between object-centered and view-centered. From a computational viewpoint the different recognition tasks - for instance categorization and identification - are very similar, representing different trade-offs between specificity and invariance. Thus the different tasks do not strictly require different classes of models. The focus of the review is on feedforward, view-based models that are supported by psychophysical and physiological data.",AIM-1695; CBCL-190,683319 bytes; 124521 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Liskov, Barbara H.; Morris, Robert T.","Ajmani, Sameer",2023-03-29T15:35:38Z,2023-03-29T15:35:38Z,2000-09,https://hdl.handle.net/1721.1/149948,MIT-LCS-TR-846,A Trusted Execution Platform for Multiparty Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kumar, Vinay; Poggio, Tomaso",2004-10-20T21:04:37Z,2004-10-20T21:04:37Z,2000-09-01,http://hdl.handle.net/1721.1/7264,AIM-1696; CBCL-191,Learning-Based Approach to Estimation of Morphable Model Parameters,"We describe the key role played by partial  evaluation in the Supercomputing Toolkit, a  parallel computing system for scientific  applications that effectively exploits the vast  amount of parallelism exposed by partial  evaluation. The Supercomputing Toolkit  parallel processor and its associated partial  evaluation-based compiler have been used  extensively by scientists at MIT, and have  made possible recent results in astrophysics  showing that the motion of the planets in our  solar system is chaotically unstable.",AIM-1696; CBCL-191,1037544 bytes; 218112 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Serre, Thomas; Heisele, Bernd; Mukherjee, Sayan; Poggio, Tomaso",2004-10-20T21:03:34Z,2004-10-20T21:03:34Z,2000-09-01,http://hdl.handle.net/1721.1/7232,AIM-1697; CBCL-192,Feature Selection for Face Detection,We present a new method to select features for a face detection system using Support Vector Machines (SVMs). In the first step we reduce the dimensionality of the input space by projecting the data into a subset of eigenvectors. The dimension of the subset is determined by a classification criterion based on minimizing a bound on the expected error probability of an SVM. In the second step we select features from the SVM feature space by removing those that have low contributions to the decision function of the SVM.,AIM-1697; CBCL-192,7211022 bytes; 1034240 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Snoeren, Alex C.; Andersen, David G.; Balakrishnan, Hari",2023-03-29T15:33:50Z,2023-03-29T15:33:50Z,2000-11,https://hdl.handle.net/1721.1/149916,MIT-LCS-TR-813,Fine-Grained Failover Using Connection Migration,"This paper presents a set of techniques for providing fine-grained failover of long-running connections across a distributed collection of replica servers, and is especially useful for fault-tolerant and load-balanced delivery of streaming media and telephony sessions. Our system achieves connection-level failover across both local- and wide-area server replication, without requiring a front-end transport- or application-layer switch. Our approach is enabled by the recently-developed end-to-end ``connection migration'' mechanism for transport protocols such as TCP, combined with a soft-state session synchronization protocol between replica servers.   The end result is a robust, fast, and fine-grained server failover mechanism that is transparent to both the client and server applications. We describe the details of our design and Linux implementation, as well as experiments with our implementation that show that this approach to failover is an attractive way to engineer robust systems for distributing long-running streams; connections suffer relatively low performance degradation even when server redirection occurs every few seconds, and overhead is negligible when compared to standard techniques. In particular, we observe the performance impact of migrating TCP connections depends on the length of time between migration and the most recent loss-recovery event.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ingols, Kyle; Keidar, Idit",2023-03-29T14:42:03Z,2023-03-29T14:42:03Z,2000-11,https://hdl.handle.net/1721.1/149301,MIT-LCS-TM-611,Availability Study of Dynamic Voting Algorithms,"Fault tolerant distributed systems often select a primary component to allow a subset of the processes to function when failures occur. The dynamic voting paradigm defines rules for selecting the primary component adaptively: when a partition occurs, if a majority of the previous primary component is connected, a new and possibly smaller primary is chosen. Several studies have shown that dynamic voting leads to more available solutions than other paradigms for maintaining a primary component. However, these studies have assumed that every attempt made by the algorithm to form a new primary component terminates successfully. Unfortunately, in real systems, this is not always the case: a change in connectivity can interrupt the algorithm whiel it is still attempting to form a new primary component; in such cases, algorithms typically block until processes can resolve the outcome of the interrupted attempt. This paper uses simulations to evaluate the effect of interruptions on the availability of dynamic voting algorithms. We study four dynamic voting algorithms, and identify two important characteristics that impact an algorithm's availability in runs with frequent connectivity changes. First, we show that the number of communication rounds exchanged in an algorithm plays a significant role in the availability achieved, especially in the degradation of availability as connectivity changes become more frequent. Second, we show that the number of processes that need to be present in order to resolve past attempts impacts the availability, especially during long runs with numerous connectivity changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Fekete, Alan; Keidar, Idit",2023-03-29T14:42:00Z,2023-03-29T14:42:00Z,2000-11,https://hdl.handle.net/1721.1/149300,MIT-LCS-TM-610,A General Framework for Highly Available Services based on Group Communication,"We present a general framework for building highly available services. The framework uses group communication to coordinate a collection of servers. Our framework is configurable, in that one can adjust parameters such as the number of servers and the extent to which they are synchronized. We analyze the scenarios that can lead to the service availability being temporarily comprised, and we discuss the tradeoffs that govern the choice of parameters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Thies, William F.; Viven, Frederic; Sheldon, Jeffery W.; Amarasinghe, Saman",2023-03-29T14:42:05Z,2023-03-29T14:42:05Z,2000-11,https://hdl.handle.net/1721.1/149302,MIT-LCS-TM-613,A Unified Framework for Schedule and Storage Optimization,"We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find the best storage mapping for a given schedule, the best schedule for a given storage mapping, and the best storage mapping that is valid for all legal schedules. Our techniques combines affine scheduling techniques with occupancy vector analysis, and incorporates general affine dependencies across statements and loop nests. We formulate the constraints imposed by the data dependencies and the storage mapping as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the best occupancy vector. We consider out method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shrestha, Govinda; Amarasinghe, Saman",2023-03-29T15:33:59Z,2023-03-29T15:33:59Z,2000-11,https://hdl.handle.net/1721.1/149918,MIT-LCS-TR-815,Perspectives on the Use of the Internet in Sri Lanka,"The survey examines the use of computers and the Internet in Sri Lanka from the perspective of the Internet Service Provider (ISP) members. It attempts to describe the general nature of IT use in terms of the availability, access, familiarity and general conditions associated with using computers and the Internet in the country.  The survey was conducted in July 1999. Questionnaires were e-mailed to 9448 ISP members in Sri Lanka, using e-mail addresses available to us at that time. Altogether, 560 members completed and returned questionnaires via e-mail to MIT's Laboratory for Computer Science.  Descriptive analysis of both quantitative and qualitative data was then conducted.    Major quantitative findings include:  *Over 60% of the respondents were members of their respective ISPs for two or less years, and over half had first used a computer sometime during the 1990-99 period. *Sixty-two percent of the respondents had sent 10 or more e-mails per week over the past six (or less) months, and 52% had received 15 or more e-mails per week during the same period. *Nearly half of the respondents used a computer at home, and 48% indicated 33.6K as the baud rate to connect their ISPs. *Seventy-eight percent of the respondents spent 1-9 hours per week sending and receiving e-mails, and a large majority (68%) spent 1-9 hours surfing the Web. *A majority of the respondents were positive about conditions in the workplace, such as the number and quality of opportunities for training and skill development, the quality of telecommunications facilities, and the quality and reliability of Internet connections. *An overwhelming majority of the respondents indicated that ISP subscriber fees, computer hardware and software costs, and telecommunications charges were generally high. *Most respondents were generally positive about 1) the quality of access to the Internet, 2) the quality of access to e-mails, Web pages and other Internet-based features, and 3) various benefits of Internet access. *Seventy-one percent of the respondents were male; nearly half were younger than 35, and a large majority were educated (with at least a high school diploma.)  Private company employees and people in business comprised over half of the respondents.  Major qualitative findings include: * It is crucially important to have faster access to information, increased communication at low costs, online-education and training, and increased efficiency in business, professional and organizational activities. * Matters of considerable concern include the low bandwidth, the high telecommunications charges, the low quality of Internet services, and the lack of organized information and databases. * Greatly needed is a raising of awareness, a change in the current regulatory environment, an open government, and a set of local information resources to support commerce.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Antone, Matthew E.; Teller, Seth",2023-03-29T15:33:52Z,2023-03-29T15:33:52Z,2000-12,https://hdl.handle.net/1721.1/149917,MIT-LCS-TR-814,Automatic Recovery of Camera Positions in Urban Scenes,"Accurate camera calibration is crucial to the reconstruction of three-dimensional geometry and the recovery of photometric scene properties. Calibration involves the determination of intrinsic parameters (e.g. focal length, principal point, and radial lens distortion) and extrinsic parameters (orientation and position).  In urban scenes and other environments containing sufficient geometric structure, it is possible to decouple extrinsic calibration into rotational and translational components that can be treated separately, simplifying the registration problem. Here we present such a decoupled formulation and describe methods for automatically recovering the positions of a large set of cameras given intrinsic calibration, relative rotations, and approximate positions.  Our algorithm first estimates the directions of translation (up to an unknown scale factor) between adjacent camera pairs using point features but without requiring explicit correspondence between them. This technique combines the robustness and simplicity of a Hough transform with the accuracy of Monte Carlo expectation maximization. We then find a set of distances between the pairs that produces globally-consistent camera positions. Novel uncertainty formulations and match plausibility criteria improve reliability and accuracy.  We assess our system's performance using both synthetic data and a large set of real panoramic imagery. The system produces camera positions accurate to within 5 centimeters in image networks extending over hundreds of meters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Liskov, Barbara H.","Ahmed, Sarah",2023-03-29T15:35:45Z,2023-03-29T15:35:45Z,2001-01,https://hdl.handle.net/1721.1/149951,MIT-LCS-TR-849,A Scalable Byzantine Fault Tolerant Secure Domain Name Service,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rinard, Martin; Kuncak, Viktor",2023-03-29T15:34:01Z,2023-03-29T15:34:01Z,2001-01,https://hdl.handle.net/1721.1/149919,MIT-LCS-TR-816,"Object Models, Heaps and Interpretations",This paper explores the use of object models for specifying verifiable heap invariants.  We define a simple language based on sets and relations and illustrate its use through examples.  We give formal semantics of the laguage by translation into predicate calculus and interpretation of predicates in terms of objects and references in the program heap.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Castro, Miguel",2023-03-29T15:34:03Z,2023-03-29T15:34:03Z,2001-01,https://hdl.handle.net/1721.1/149920,MIT-LCS-TR-817,Practical Byzantine Fault Tolerance,"Our growing reliance on online services accessible on the Internet demands highly-available systemsthat provide correct service without interruptions. Byzantine faults such as software bugs, operatormistakes, and malicious attacks are the major cause of service interruptions. This thesis describesa new replication algorithm, BFT, that can be used to build highly-available systems that tolerateByzantine faults. It shows, for the first time, how to build Byzantine-fault-tolerant systems that canbe used in practice to implement real services because they do not rely on unrealistic assumptionsand they perform well. BFT works in asynchronous environments like the Internet, it incorporatesmechanisms to defend against Byzantine-faulty clients, and it recovers replicas proactively. Therecovery mechanism allows the algorithm to tolerate any number of faults over the lifetime of thesystem provided fewer than 1=3 of the replicas become faulty within a small windowof vulnerability.The window may increase under a denial-of-service attack but the algorithm can detect and respondto such attacks and it can also detect when the state of a replica is corrupted by an attacker.BFT has been implemented as a generic program library with a simple interface. The BFTlibrary provides a complete solution to the problem of building real services that tolerate Byzantinefaults. We used the library to implement the first Byzantine-fault-tolerant NFS file system, BFS. TheBFT library and BFS perform well because the library incorporates several important optimizations.The most important optimization is the use of symmetric cryptography to authenticate messages.Public-key cryptography, which was the major bottleneck in previous systems, is used only toexchange the symmetric keys. The performance results show that BFS performs 2% faster to 24%slower than production implementations of the NFS protocol that are not replicated. Therefore, webelieve that the BFT library can be used to build practical systems that tolerate Byzantine faults.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Koile, Kimberle",2004-10-20T20:28:12Z,2004-10-20T20:28:12Z,2001-01-01,http://hdl.handle.net/1721.1/7072,AITR-2001-001,The Architect's Collaborator: Toward Intelligent Tools for Conceptual Design,"In early stages of architectural design, as in  other design domains, the language used is often very abstract. In architectural design, for  example, architects and their clients use experiential terms such as ""private"" or ""open""  to describe spaces. If we are to build programs that can help designers during this  early-stage design, we must give those programs the capability to deal with concepts  on the level of such abstractions. The work reported in this thesis sought to do that,  focusing on two key questions: How are  abstract terms such as ""private"" and ""open"" translated  into physical form? How might one build a tool to assist designers with this process? The Architect's Collaborator (TAC) was built to  explore these issues. It is a design assistant that supports iterative design refinement, and  that represents and reasons about how experiential qualities are manifested in  physical form. Given a starting design and a  set of design goals, TAC explores the space of  possible designs in search of solutions that  satisfy the goals. It employs a strategy we've called  dependency-directed redesign: it evaluates a design with respect to a set of goals, then  uses an explanation of the evaluation to guide proposal and refinement of repair  suggestions; it then carries out the repair  suggestions to create new designs. A series of experiments was run to study  TAC's behavior. Issues of control structure,  goal set size, goal order, and modification operator  capabilities were explored. In addition, TAC's use as a design assistant was studied  in an experiment using a house in the  process of being redesigned. TAC's use as an  analysis tool was studied in an experiment  using Frank Lloyd Wright's Prairie houses.",AITR-2001-001,20962265 bytes; 1471552 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Alvira, Mariano; Rifkin, Ryan",2004-10-20T20:50:07Z,2004-10-20T20:50:07Z,2001-01-01,http://hdl.handle.net/1721.1/7219,AIM-2001-004; CBCL-193,An Empirical Comparison of SNoW and SVMs for Face Detection,"Impressive claims have been made for the performance of the SNoW algorithm on face detection tasks by Yang et. al. [7]. In particular, by looking at both their results and those of Heisele et. al. [3], one could infer that the SNoW system performed substantially better than an SVM-based system, even when the SVM used a polynomial kernel and the SNoW system used a particularly simplistic 'primitive' linear representation. We evaluated the two approaches in a controlled experiment, looking directly at performance on a simple, fixed-sized test set, isolating out 'infrastructure' issues related to detecting faces at various scales in large images. We found that SNoW performed about as well as linear SVMs, and substantially worse than polynomial SVMs.",AIM-2001-004; CBCL-193,1232391 bytes; 319169 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Darrell, T.; Demirdjian, D.; Checka, N.; Felzenswalb, P.",2004-10-04T14:37:37Z,2004-10-04T14:37:37Z,2001-02-01,http://hdl.handle.net/1721.1/6075,AIM-2001-001,Plan-view Trajectory Estimation with Dense Stereo Background Models,"In a known environment, objects may be tracked in multiple views using a set of back-ground models. Stereo-based models can be illumination-invariant, but often have undefined values which inevitably lead to foreground classification errors. We derive dense stereo models for object tracking using long-term, extended dynamic-range imagery, and by detecting and interpolating uniform but unoccluded planar regions. Foreground points are detected quickly in new images using pruned disparity search. We adopt a 'late-segmentation' strategy, using an integrated plan-view density representation. Foreground points are segmented into object regions only when a trajectory is finally estimated, using a dynamic programming-based method. Object entry and exit are optimally determined and are not restricted to special spatial zones.",AIM-2001-001,5522496 bytes; 672260 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Stoica, Ion; Morris, Robert T.; Karger, David R.; Kaashoek, M. Frans; Balakrishnan, Hari",2023-03-29T15:34:10Z,2023-03-29T15:34:10Z,2001-03,https://hdl.handle.net/1721.1/149922,MIT-LCS-TR-819,Chord: A scalable peer-to-peer lookup service for Internet applications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Fu, Kevin; Sit, Emil; Smith, Kendra; Feamster, Nick",2023-03-29T15:34:08Z,2023-03-29T15:34:08Z,2001-03,https://hdl.handle.net/1721.1/149921,MIT-LCS-TR-818,Client Authentication on the Web,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sadr, Javid; Sinha, Pawan",2004-10-20T20:50:11Z,2004-10-20T20:50:11Z,2001-03-01,http://hdl.handle.net/1721.1/7221,AIM-2001-006; CBCL-196,Exploring Object Perception with Random Image Structure Evolution,"We have developed a technique called RISE (Random Image Structure Evolution), by which one may systematically sample continuous paths in a high-dimensional image space. A basic RISE sequence depicts the evolution of an object's image from a random field, along with the reverse sequence which depicts the transformation of this image back into randomness. The processing steps are designed to ensure that important low-level image attributes such as the frequency spectrum and luminance are held constant throughout a RISE sequence. Experiments based on the RISE paradigm can be used to address some key open issues in object perception. These include determining the neural substrates underlying object perception, the role of prior knowledge and expectation in object perception, and the developmental changes in object perception skills from infancy to adulthood.",AIM-2001-006; CBCL-196,14196504 bytes; 1545031 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shelton, Christian R.",2004-10-20T20:50:06Z,2004-10-20T20:50:06Z,2001-03-20,http://hdl.handle.net/1721.1/7218,AIM-2001-002; CBCL-194,Policy Improvement for POMDPs Using Normalized Importance Sampling,"We present a new method for estimating the expected return of a POMDP from experience. The estimator does not assume any knowle ge of the POMDP and allows the experience to be gathered with an arbitrary set of policies. The return is estimated for any new policy of the POMDP. We motivate the estimator from function-approximation and importance sampling points-of-view and derive its theoretical properties. Although the estimator is biased, it has low variance and the bias is often irrelevant when the estimator is used for pair-wise comparisons.We conclude by extending the estimator to policies with memory and compare its performance in a greedy search algorithm to the REINFORCE algorithm showing an order of magnitude reduction in the number of trials required.",AIM-2001-002; CBCL-194,4576001 bytes; 768071 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liskov, Moses; Lysyanskeya, Anna; Micali, Silvio; Reyzin, Leonid; Smith, Adam",2023-03-29T14:42:11Z,2023-03-29T14:42:11Z,2001-04,https://hdl.handle.net/1721.1/149304,MIT-LCS-TM-615,Mutually Independent Commitment,"We describe a new kind of commitment scheme in which two parties commit to values in a commitment stage, at the end of which we are assured that the values they have committed to cannot be correlated to one another. We call this new primitive mutually independent commitments. We present three mutually independent commitment schemes which handle single bit commitments, and which are computationally hiding and perfecting binding.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Itkis, Gene; Reyzin, Leonid",2023-03-29T14:42:08Z,2023-03-29T14:42:08Z,2001-04,https://hdl.handle.net/1721.1/149303,MIT-LCS-TM-614,Forward-Secure Signatures with Optimal Signing and Verifying,"Ordinary digital signatures have an inherent weakness: if the secret key is leaked, then all signatures, even the ones generated before the leak, are no longer trustworthy. Forward-secure digital signatures were recently proposed to address this weakness: they ensure that past signatures remain secure even if the current secret key is leaked. We propose the first forward-secure signature scheme for which both signing and verifying are as efficient as for one of the most efficient ordinary signature schemes (Guillou-Quisquater): each requiring just two modular exponentiations with a short exponent. All previously proposed forward-secure signature schemes took significantly longer to sign and verify than ordinary signature schemees. Our scheme requires only fractional increases to the sizes of keys and signatures, and no additional public storage. Like the underlying Guillou-Quisquater scheme, our scheme is provably secure in the random oracle model.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lepinski, Matthew; Micali, Silvio",2023-03-29T14:42:14Z,2023-03-29T14:42:14Z,2001-04,https://hdl.handle.net/1721.1/149305,MIT-LCS-TM-616,Three Round Zero-Knowledge Using a Proof of Knowledge Assumption,We provide a proof of knowledge assumption that allows us to construct a three round zero-knowledge proof system for any language in NP.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Meuleau, Nicolas; Peshkin, Leonid; Kim, Kee-Eung",2004-10-04T14:37:39Z,2004-10-04T14:37:39Z,2001-04-03,http://hdl.handle.net/1721.1/6076,AIM-2001-003,Exploration in Gradient-Based Reinforcement Learning,"Gradient-based policy search is an alternative to value-function-based methods for reinforcement learning in non-Markovian domains. One apparent drawback of policy search is its requirement that all actions be 'on-policy'; that is, that there be no explicit exploration. In this paper, we provide a method for using importance sampling to allow any well-behaved directed exploration policy during learning. We show both theoretically and experimentally that using this method can achieve dramatic performance improvements.",AIM-2001-003,5594043 bytes; 516972 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chan, Nicholas Tung; Shelton, Christian",2004-10-20T20:50:09Z,2004-10-20T20:50:09Z,2001-04-17,http://hdl.handle.net/1721.1/7220,AIM-2001-005; CBCL-195,An Electronic Market-Maker,"This paper presents an adaptive learning model for market-making under the reinforcement learning framework. Reinforcement learning is a learning technique in which agents aim to maximize the long-term accumulated rewards. No knowledge of the market environment, such as the order arrival or price process, is assumed. Instead, the agent learns from real-time market experience and develops explicit market-making strategies, achieving multiple objectives including the maximizing of profits and minimization of the bid-ask spread. The simulation results show initial success in bringing learning techniques to building market-making algorithms.",AIM-2001-005; CBCL-195,2620276 bytes; 480221 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Arkoudas, Konstantine",2004-10-04T14:37:40Z,2004-10-04T14:37:40Z,2001-04-30,http://hdl.handle.net/1721.1/6077,AIM-2001-007,Certified Computation,"This paper introduces the notion of certified computation. A certified computation does not only produce a result r, but also a correctness certificate, which is a formal proof that r is correct. This can greatly enhance the credibility of the result: if we trust the axioms and inference rules that are used in the certificate,then we can be assured that r is correct. In effect,we obtain a trust reduction: we no longer have to trust the entire computation; we only have to trust the certificate. Typically, the reasoning used in the certificate is much simpler and easier to trust than the entire computation. Certified computation has two main applications: as a software engineering discipline, it can be used to increase the reliability of our code; and as a framework for cooperative computation, it can be used whenever a code consumer executes an algorithm obtained from an untrusted agent and needs to be convinced that the generated results are correct. We propose DPLs (Denotational Proof Languages)as a uniform platform for certified computation. DPLs enforce a sharp separation between logic and control and over versatile mechanicms for constructing certificates. We use Athena as a concrete DPL to illustrate our ideas, and we present two examples of certified computation, giving full working code in both cases.",AIM-2001-007,1923011 bytes; 286231 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rodrigues, Rodrigo",2023-03-29T15:35:48Z,2023-03-29T15:35:48Z,2001-05,https://hdl.handle.net/1721.1/149952,MIT-LCS-TR-850,Combining Abstraction with Byzantine Fault-Tolerance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Mui, Lik; Mohtashemi, Mojdeh; Ang, Cheewee; Szolovits, Peter; Halberstadt, Ari",2023-03-29T14:42:16Z,2023-03-29T14:42:16Z,2001-05,https://hdl.handle.net/1721.1/149306,MIT-LCS-TM-617,Ratings in Distributed Systems: A Bayesian Approach,"For distributed systems at large and e-commerce systems in particular, ratings play an increasingly important role. Rating confer reputation measures about sources. This paper reports our formalization of the rating process. This paper argues that rating shuold be context- and individual- dependent quantities. In contrast to existing rating systems in many e-commerce or developer sites, our approach makes use of personalized and contextualized ratings for assessing source reputation. Our approach is based on a Bayesian probabilistic framework.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Katabi, Dina; Handley, Mark",2023-03-29T15:34:14Z,2023-03-29T15:34:14Z,2001-05,https://hdl.handle.net/1721.1/149923,MIT-LCS-TR-820,Using precise feedback for controlling congestion in the Internet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"De Couto, Douglas S.J.; Morris, Robert T.",2023-03-29T15:34:27Z,2023-03-29T15:34:27Z,2001-05,https://hdl.handle.net/1721.1/149927,MIT-LCS-TR-824,Location Proxies and Intermediate Node Forwarding for Practical Geographic Forwarding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ajmani, Sameer; Morris, Robert T.; Liskov, Barbara H.",2023-03-29T15:35:41Z,2023-03-29T15:35:41Z,2001-05,https://hdl.handle.net/1721.1/149949,MIT-LCS-TR-847,A Trusted Third-Party Computation Service,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Szolovits, Peter","Koh, Waikit",2023-03-29T15:34:59Z,2023-03-29T15:34:59Z,2001-05,https://hdl.handle.net/1721.1/149933,MIT-LCS-TR-830,An Information-Theoretic Approach to Interest Making,"The Internet has brought a new meaning to the term communities. Geography is no longer a barrier to international communications. However, the paradigm of meeting new interesting people remains entrenched in traditional means; meeting new interesting people on the Internet still relies on chance and contacts. This thesis explores a new approach towards matching users in online communities in an effective fashion.  Instead of using the conventional feature vector scheme to profile users, each user is represented by a personalized concept hierarchy (or an ontology) that is learnt from the user's behavior in the system. Each concept hierarchy is then interpreted within the Information Theory framework as a probabilistic decision tree. The matching algorithm uses the Kullback-Leiber distance as a measure of deviation between two probabilistic decision trees. Thus, in an online community, where a personalized concept hierarchy represents each user, the Kullback-Leiber distance imposes a full- order rank on the level of similarity of all the users with respect to a particular user in question.  The validity and utility of the proposed scheme of matching users is then applied in a set of simulations, using the feature-vector-overlap measure as a baseline. The results of the simulations show that the Kullback Leiber distance, when used in conjunction with the concept hierarchy, is more robust to noise and is able to make a stronger and more distinctive classification of users into similar groups in comparison to the conventional keyword-overlap scheme. A graphical agent system that relies upon the ontology-based interest matching algorithm, called the Collaborative Sanctioning Network, is also described in this thesis.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tanudjaja, Francisco; Mui, Lik",2023-03-29T14:42:18Z,2023-03-29T14:42:18Z,2001-05,https://hdl.handle.net/1721.1/149307,MIT-LCS-TM-618,Persona: A Contextualized and Personalized Web Search,"Recent advances in graph-based search techniques derived from Kleinberg's work [1] have been impressive. This paper further improves the graph-based search algorithm in two dimensions. Firstly, variants of Kleinberg's techniques do not take into account the semantics of the query string nor of the nodes being searched. As a result, polysemy of query words cannot be resolved. This paper presents an interactive query scheme utilizing the simple web ontology provided by the Open Directory Project to resolve meanings of a user query. Secondly, we extend a recently proposed personalized version of the Kleinberg algorithm [3]. Simulation results are presented to illustrate the sensitivity of our technique. We outline the implementation of our algorithm in the Persona personalized web search system.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Keidar, Idit; Rajsbaum, Sergio",2023-03-29T15:34:18Z,2023-03-29T15:34:18Z,2001-05,https://hdl.handle.net/1721.1/149924,MIT-LCS-TR-821,On the Cost of Fault-Tolerant Consensus When There Are No Faults - A Tutorial,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Felzenszwalb, Pedro F.",2004-10-20T20:28:15Z,2004-10-20T20:28:15Z,2001-05-01,http://hdl.handle.net/1721.1/7073,AITR-2001-002,Object Recognition with Pictorial Structures,"This thesis presents a statistical framework for object recognition. The framework is motivated by the pictorial structure models introduced by Fischler and Elschlager nearly 30 years ago. The basic idea is to model an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. The problem of detecting an object in an image and the problem of learning an object model using training examples are naturally formulated under a statistical approach. We present efficient algorithms to solve these problems in our framework. We demonstrate our techniques by training models to represent faces and human bodies. The models are then used to locate the corresponding objects in novel images.",AITR-2001-002,15588217 bytes; 1282972 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sezgin, Tevfik Metin",2004-10-20T20:28:30Z,2004-10-20T20:28:30Z,2001-05-01,http://hdl.handle.net/1721.1/7077,AITR-2001-009,Feature Point Detection and Curve Approximation for Early Processing of Freehand Sketches,"Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for  feature detection in strokes and apply this technique using two approaches to  signal processing, one using simple average based thresholding and a second  using scale space.",AITR-2001-009,82 p.; 10553461 bytes; 5067939 bytes,application/postscript; application/pdf,en_US,AI; Feature Point Detection; Curve Approximation; Freehand Sketching,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Banks, Jessica",2004-10-20T20:28:07Z,2004-10-20T20:28:07Z,2001-05-01,http://hdl.handle.net/1721.1/7070,AITR-2001-005,Design and Control of an Anthropomorphic Robotic Finger with Multi-point Tactile Sensation,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",AITR-2001-005,88 p.; 17699541 bytes; 1837341 bytes,application/postscript; application/pdf,en_US,AI; tactile sensation; finger; robot; anthropomorphic; skin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ucko, Aaron Mark",2004-10-20T20:28:09Z,2004-10-20T20:28:09Z,2001-05-01,http://hdl.handle.net/1721.1/7071,AITR-2001-006,Predicate Dispatching in the Common Lisp Object System,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",AITR-2001-006,74 p.; 2463955 bytes; 977046 bytes,application/postscript; application/pdf,en_US,AI; predicate dispatching; Common Lisp; CLOS; Weyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demirdjian, D.; Darrell, T.",2004-10-04T14:37:43Z,2004-10-04T14:37:43Z,2001-05-07,http://hdl.handle.net/1721.1/6079,AIM-2001-009,Motion Estimation from Disparity Images,"A new method for 3D rigid motion estimation from stereo is proposed in this paper. The appealing feature of this method is that it directly uses the disparity images obtained from stereo matching. We assume that the stereo rig has parallel cameras and show, in that case, the geometric and topological properties of the disparity images. Then we introduce a rigid transformation (called d-motion) that maps two disparity images of a rigidly moving object. We show how it is related to the Euclidean rigid motion and a motion estimation algorithm is derived. We show with experiments that our approach is simple and more accurate than standard approaches.",AIM-2001-009,1035214 bytes; 158055 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rahimi, A.; Morency, L.-P.; Darrell, T.",2004-10-04T14:37:42Z,2004-10-04T14:37:42Z,2001-05-07,http://hdl.handle.net/1721.1/6078,AIM-2001-008,Reducing Drift in Parametric Motion Tracking,"We develop a class of differential motion trackers that automatically stabilize when in finite domains. Most differ-ential trackers compute motion only relative to one previous frame, accumulating errors indefinitely. We estimate pose changes between a set of past frames, and develop a probabilistic framework for integrating those estimates. We use an approximation to the posterior distribution of pose changes as an uncertainty model for parametric motion in order to help arbitrate the use of multiple base frames. We demonstrate this framework on a simple 2D translational tracker and a 3D, 6-degree of freedom tracker.",AIM-2001-008,8757672 bytes; 1663085 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ho, Purdy",2004-10-20T20:48:40Z,2004-10-20T20:48:40Z,2001-05-31,http://hdl.handle.net/1721.1/7171,AIM-2001-010; CBCL-197,Rotation Invariant Real-time Face Detection and Recognition System,"In this report, a face recognition system that is capable of detecting and recognizing frontal and rotated faces was developed. Two face recognition methods focusing on the aspect of pose invariance are presented and evaluated - the whole face approach and the component-based approach. The main challenge of this project is to develop a system that is able to identify faces under different viewing angles in realtime. The development of such a system will enhance the capability and robustness of current face recognition technology.  The whole-face approach recognizes faces by classifying a single feature vector consisting of the gray values of the whole face image. The component-based approach  first locates the facial components and extracts them. These components are normalized and combined into a single feature vector for classification. The Support Vector Machine (SVM) is used as the classifier for both approaches. Extensive tests with respect to the robustness against pose changes are performed on a  database that includes faces rotated up to about 40 degrees in depth. The component-based approach clearly outperforms the whole-face approach on all tests. Although this approach isproven to be more reliable, it is still too slow for real-time applications. That is the reason why a real-time face recognition system using the whole-face approach is implemented to recognize people in color video sequences.",AIM-2001-010; CBCL-197,24 p.; 12501066 bytes; 896203 bytes,application/postscript; application/pdf,en_US,AI; vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Nagpal, Radhika",2004-10-20T20:28:28Z,2004-10-20T20:28:28Z,2001-06-01,http://hdl.handle.net/1721.1/7076,AITR-2001-008,Programmable Self-Assembly: Constructing Global Shape using Biologically-inspire,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",AITR-2001-008,118 p.; 27221557 bytes; 1541086 bytes,application/postscript; application/pdf,en_US,AI; self-organisation; multi agent; developmental biology; amorphous computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Lee, Walter; Amarasinghe, Saman","Frank, Matthew; Lee, Walter; Amarasinghe, Saman",2023-03-29T14:42:21Z,2023-03-29T14:42:21Z,2001-07,https://hdl.handle.net/1721.1/149308,MIT-LCS-TM-619,A Software Framework for Supporting General Purpose Applications on RAW Computation Fabrics,"This paper presents SUDS (Software Un-Do Systems), a data speculation system for Raw processors. SUDS manages specultation in software. Thekey to managing speculation in software is to use the compiler to minimize the number of data items that need to be managed in runtime. Managing speculation in software enables Raw processors to achieve good performance on integer applications without sacrificing chip area for speculation hardware. This additional area can instead be devoted to additional computer resources, improving the performance of dense matrix and media applications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Alvira, Mariano; Paris, Jim; Rifkin, Ryan",2004-10-20T21:03:36Z,2004-10-20T21:03:36Z,2001-07-01,http://hdl.handle.net/1721.1/7234,AIM-2001-012; CBCL-199,The Audiomomma Music Recommendation System,"We design and implement a system that recommends musicians to listeners. The basic idea is to keep track of what artists a user listens to, to find other users with similar tastes, and to recommend other artists that these similar listeners enjoy. The system utilizes a client-server architecture, a web-based interface, and an SQL database to store and process information. We describe Audiomomma-0.3, a proof-of-concept implementation of the above ideas.",AIM-2001-012; CBCL-199,10 p.; 2186561 bytes; 257129 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chan, Nicholas T.; Dahan, Ely; Lo, Andrew W.; Poggio, Tomaso",2004-10-20T21:03:35Z,2004-10-20T21:03:35Z,2001-07-01,http://hdl.handle.net/1721.1/7233,AIM-2001-013; CBCL-200,Experimental Markets for Product Concepts,"Market prices are well known to efficiently collect and aggregate diverse information regarding the value of commodities and assets. The role of markets has been particularly suitable to pricing financial securities. This article provides an alternative application of the pricing mechanism to marketing research - using pseudo-securities markets to measure preferences over new product concepts. Surveys, focus groups, concept tests and conjoint studies are methods traditionally used to measure individual and aggregate preferences. Unfortunately, these methods can be biased, costly and time-consuming to conduct. The present research is motivated by the desire to efficiently measure preferences and more accurately predict new product success, based on the efficiency and incentive-compatibility of security trading markets. The article describes a novel market research method, pro-vides insight into why the method should work, and compares the results of several trading experiments against other methodologies such as concept testing and conjoint analysis.",AIM-2001-013; CBCL-200,3069806 bytes; 287156 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Russell, Richard; Sinha, Pawan",2004-10-20T21:03:39Z,2004-10-20T21:03:39Z,2001-07-01,http://hdl.handle.net/1721.1/7235,AIM-2001-014; CBCL-201,Perceptually-based Comparison of Image Similarity Metrics,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",AIM-2001-014; CBCL-201,13 p.; 9714300 bytes; 2612761 bytes,application/postscript; application/pdf,en_US,AI; Image matching; vector quantization; Minkowski metric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:41Z,2004-10-20T21:03:41Z,2001-07-25,http://hdl.handle.net/1721.1/7236,AIM-2001-015; CBCL-202,Recognizing Indoor Scenes,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,AIM-2001-015; CBCL-202,17 p.; 14931961 bytes; 3219314 bytes,application/postscript; application/pdf,en_US,AI; Scene classification; Navigation; scene representation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Lam, Patrick; Rinard, Martin",2023-03-29T15:34:21Z,2023-03-29T15:34:21Z,2001-08,https://hdl.handle.net/1721.1/149925,MIT-LCS-TR-822,Roles Are Really Great!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Thies, William F.; Karczmarek, Michael; Amarasinghe, Saman",2023-03-29T14:42:24Z,2023-03-29T14:42:24Z,2001-08,https://hdl.handle.net/1721.1/149309,MIT-LCS-TM-620,StreaMIT: A Language for Streaming Applications,"We characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly important. The StreaMIT language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreaMIT compiler aims to improve the performance of streaming applications via stream-specific analyses and optimizations. In this paper, we motivate, describe and justify the language features of StreaMIT, which include: a structured model of streams, a messaging system for control, a re-initialization mechanism, and a natural textual syntax. We also present a means of reasoning about time in terms of ""information flow"": a concept that we believe is fundamental to the streaming domain. Using this concept, we give a formal semantics for StreaMIT's messaging system, as well as a simple algorithm for detecting deadlock and buffer overlow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Zollei, Lilla",2004-10-20T20:28:33Z,2004-10-20T20:28:33Z,2001-08-01,http://hdl.handle.net/1721.1/7078,AITR-2002-001,2D-3D Rigid-Body Registration of X-Ray Fluoroscopy and CT Images,"The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation.   Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",AITR-2002-001,128 p.; 21043480 bytes; 1712245 bytes,application/postscript; application/pdf,en_US,AI; registration; medical imaging,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shelton, Christian Robert",2004-10-01T14:00:04Z,2004-10-01T14:00:04Z,2001-08-01,http://hdl.handle.net/1721.1/5568,AITR-2001-003; CBCL-204,Importance Sampling for Reinforcement Learning with Multiple Objectives,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",AITR-2001-003; CBCL-204,108 p.; 10551422 bytes; 1268632 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; RL; importance sampling; estimation; market-making,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sinha, Pawan; Torralba, Antonio",2004-10-20T21:03:43Z,2004-10-20T21:03:43Z,2001-08-01,http://hdl.handle.net/1721.1/7237,AIM-2001-017; CBCL-203,Role of Low-level Mechanisms in Brightness Perception,"Brightness judgments are a key part of the primate brain's visual analysis of the environment. There is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. However, it is unclear precisely how a region's context influences its perceived brightness. Recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. This work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. Here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. We find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. These results point to the important role of low-level mechanisms in determining brightness percepts.",AIM-2001-017; CBCL-203,17 p.; 5391865 bytes; 331759 bytes,application/postscript; application/pdf,en_US,AI; brightness perception; perceptual organization; local mechanisms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2004-10-04T14:37:46Z,2004-10-04T14:37:46Z,2001-08-13,http://hdl.handle.net/1721.1/6082,AIM-2001-016,An Algorithm for Bootstrapping Communications,I present an algorithm which allows two agents to generate a simple language based only on observations of a shared environment. Vocabulary and roles for the language are learned in linear time. Communication is robust and degrades gradually as complexity increases. Dissimilar modes of experience will lead to a shared kernel vocabulary.,AIM-2001-016,35 p.; 6622296 bytes; 264031 bytes,application/postscript; application/pdf,en_US,AI; Adaptive Learning Hash-coding communication architecture algorithm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yeo, Gene; Poggio, Tomaso",2004-10-20T21:03:45Z,2004-10-20T21:03:45Z,2001-08-25,http://hdl.handle.net/1721.1/7238,AIM-2001-018; CBCL-206,Multiclass Classification of SRBCTs,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",AIM-2001-018; CBCL-206,17 p.; 6552074 bytes; 816114 bytes,application/postscript; application/pdf,en_US,AI; multiclass; SVM; feature selection; SRBCT; tumors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tan, Godfrey; Mui, Allen; Guttag, John V.; Balakrishnan, Hari",2023-03-29T15:34:32Z,2023-03-29T15:34:32Z,2001-09,https://hdl.handle.net/1721.1/149929,MIT-LCS-TR-826,Forming Scatternets from Bluetooth Personal Area Networks,"There is increasing interest in wireless ad hoc networks built from portable devices equipped with short-range wireless network interfaces. This paper addresses issues related to internetworking such networks to form larger ÔøΩscatternets.ÔøΩ  Within the constraints imposed by the emerging standard Bluetooth link layer and MAC protocol, we describe an efficient online topology formation algorithm, called TSF (Tree Scatternet Formation) to build scatternets. TSF connects nodes in a tree structure that simplifies packet routing and scheduling. The design allows nodes to arrive and leave arbitrarily, incrementally building the topology and healing partitions when they occur. We present simulation results that show that TSF has low tree formation latency and also generates an efficient topology for forwarding packets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lynch, Nancy A.; Segala, Roberto; Vaandrager, Frits",2023-03-29T15:34:34Z,2023-03-29T15:34:34Z,2001-09,https://hdl.handle.net/1721.1/149930,MIT-LCS-TR-827a,Hybrid I/O Automata*,"Hybrid systems are systems that exhibit a combination of discrete and continuous behavior. Typical hybrid systems include computer components, which operate in discrete program steps, and real-world components, whose behavior over time intervals evolves according to physical constraints. Important examples of hybrid systems include automated transportation systems, robotics systems, process control systems, systems of embedded devices, and mobile computing systems. Such systems can be very complex, and very difficult to describe and analyze. This paper presents the Hybrid Input/Output Automaton (HIOA) modeling framework, a basic mathematical framework to support description and analysis of hybrid systems. An important feature of this model is its support for decomposing hybrid system descriptions. In particular, the framework includes a notion of external behavior for a hybrid I/O automaton, which captures its discrete and continuous interactions with its environment. The framework also defines what it means for one HIOA to implement another, based on an inclusion relationship between their external behavior sets, and defines a notion of simulation, which provides a sufficient condition for demonstrating implementation relationships. The framework also includes a composition operation for HIOAs, which respects external behavior, and a notion of receptiveness, which implies that an HIOA does not block the passage of time. The framework is intended to support analysis methods from both computer science and control theory. This work is a simplification of an earlier version of the HIOA model [49, 50]. The main simplification in the new model is a clearer separation between the mechanisms used to model discrete and continuous interaction between components. In particular, the new model removes the dual use of external variables for discrete and continuous interaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Teller, Seth",2023-03-29T15:34:30Z,2023-03-29T15:34:30Z,2001-09,https://hdl.handle.net/1721.1/149928,MIT-LCS-TR-825,"Scalable, Controlled Imagery Capture in Urban Environments","We describe the design considerations underlying a system for scalable, automated capture of precisely controlled imagery in urban scenes. The system operates for architectural scenes in which, from every camera position, some  two vanishing points are visible. It has been used to capture thousands of controlled images in outdoor environments spanning hundreds of meters. The proposed system architecture forms the foundation for a future, fully robotic outdoor mapping capability for urban areas, analogous to existing, satellite-based robotic mapping systems which acquire images and models of natural terrain.  Four key ideas distinguish our approach from other methods. First, our sensor acquires georeferencing metadata with every image, enabling related images to be efficiently identified and registered. Second, the sensor acquires omni-directional images; we show strong experimental evidence that such images are fundamentally more powerful observations than conventional (narrow-FOV) images. Third, the system uses a probabilistic, projective error formulation to account for uncertainty. By treating measurement error in an appropriate depth-free framework, and by deferring decisions about camera calibration and scene structure until many noisy observations can be fused, the system achieves superior robustness and accuracy. Fourth, the system's computational requirements scale linearly in the input size, the area of the acquisition region, and the size of the output model. This is in contrast to most previous methods, which either assume constant-size inputs or exhibit quadratic running time (or worse) asymptotically. These attributes enable the system to operate in a regime of scale and physical extent which is unachievable by any other method, whether manual or automated. Consequently, it can acquire the most complex calibrated terrestrial image sets in existence, while operating faster thanany existing manual or algorithmic method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rennie, Jason D. M.",2004-10-20T20:28:16Z,2004-10-20T20:28:16Z,2001-09-01,http://hdl.handle.net/1721.1/7074,AITR-2001-004,Improving Multi-class Text Classification with Naive Bayes,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",AITR-2001-004,49 p.; 2017370 bytes; 687421 bytes,application/postscript; application/pdf,en_US,AI; naive bayes; text; classification; feature selection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hong, Won",2004-10-20T20:28:24Z,2004-10-20T20:28:24Z,2001-09-01,http://hdl.handle.net/1721.1/7075,AITR-2001-007,"Modeling, Estimation, and Control of Robot-Soil Interactions","This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",AITR-2001-007,225 p.; 66603884 bytes; 4629577 bytes,application/postscript; application/pdf,en_US,AI; Robotics; Soil Modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bryson, Joanna J.",2004-10-20T20:29:10Z,2004-10-20T20:29:10Z,2001-09-01,http://hdl.handle.net/1721.1/7080,AITR-2002-003,Intelligence by Design: Principles of Modularity and Coordination for Engineerin,"All intelligence relies on search --- for  example, the search for an intelligent agent's  next action. Search is only likely to succeed in resource-bounded agents if they have already  been biased towards finding the right answer.  In artificial agents, the primary source of bias is engineering.   This dissertation describes an approach,  Behavior-Oriented Design (BOD) for  engineering complex agents. A complex agent  is one that must arbitrate between potentially  conflicting goals or behaviors.  Behavior-oriented design builds on work in  behavior-based and hybrid architectures for agents, and the object  oriented approach to software engineering.   The primary contributions of this dissertation  are:     1.The BOD architecture: a modular  architecture with each module providing  specialized representations to facilitate  learning.    This includes one pre-specified module  and representation for action selection or  behavior arbitration. The specialized    representation underlying BOD action  selection is Parallel-rooted, Ordered,  Slip-stack Hierarchical (POSH) reactive plans.     2.The BOD development process: an  iterative process that alternately scales the  agent's capabilities then optimizes the agent  for    simplicity, exploiting tradeoffs between the  component representations. This ongoing  process for controlling complexity not only    provides bias for the behaving agent, but  also facilitates its maintenance and  extendibility.   The secondary contributions of this  dissertation include two implementations of  POSH action selection, a procedure for  identifying useful idioms in agent architectures and  using them to distribute knowledge across  agent paradigms, several examples of  applying BOD idioms to established architectures, an  analysis and comparison of the attributes and  design trends of a large number of agent architectures, a comparison of biological  (particularly mammalian) intelligence to  artificial agent architectures, a novel model of primate transitive inference, and many other  examples of BOD agents and BOD  development.",AITR-2002-003,232 p.; 4544378 bytes; 1027952 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:49Z,2004-10-20T21:03:49Z,2001-09-01,http://hdl.handle.net/1721.1/7239,AIM-2001-020; CBCL-205,Contextual Priming for Object Detection,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",AIM-2001-020; CBCL-205,27 p.; 40187890 bytes; 5238575 bytes,application/postscript; application/pdf,en_US,AI; context; image statistics; Bayesian reasoning; recognition; focus of attention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dror, Ron O.; Adelson, Edward H.; Willsky, Alan S.",2004-10-08T20:36:32Z,2004-10-08T20:36:32Z,2001-09-01,http://hdl.handle.net/1721.1/6656,AIM-2001-023,Surface Reflectance Estimation and Natural Illumination Statistics,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",AIM-2001-023,22 p.; 7750699 bytes; 706071 bytes,application/postscript; application/pdf,en_US,AI; reflectance; lighting; BRDF; surface; illumination statistics; natural images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lee, Lily",2004-10-08T20:36:33Z,2004-10-08T20:36:33Z,2001-09-01,http://hdl.handle.net/1721.1/6657,AIM-2001-019,Gait Dynamics for Recognition and Classification,"This paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. Our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. We claim that our gait dynamics representation is rich enough for the task of recognition and classification. The use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. In addition, preliminary results are shown on gender classification using our gait dynamics features.",AIM-2001-019,12 p.; 1128480 bytes; 92054 bytes,application/postscript; application/pdf,en_US,AI; gait; recognition; gender classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Miller, Erik G.; Tieu, Kinh; Stauffer, Chris P.",2004-10-08T20:36:37Z,2004-10-08T20:36:37Z,2001-09-01,http://hdl.handle.net/1721.1/6659,AIM-2001-021,Learning Object-Independent Modes of Variation with Feature Flow Fields,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",AIM-2001-021,9 p.; 8233900 bytes; 814636 bytes,application/postscript; application/pdf,en_US,AI; Invariance; Optical Flow; Color Constancy; Object Recognition; image manifold,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yu, Angela J.; Giese, Martin A.; Poggio, Tomaso A.",2004-10-20T21:03:51Z,2004-10-20T21:03:51Z,2001-09-01,http://hdl.handle.net/1721.1/7240,AIM-2001-022; CBCL-207,Biologically Plausible Neural Circuits for Realization of Maximum Operations,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",AIM-2001-022; CBCL-207,28 p.; 2197042 bytes; 930880 bytes,application/postscript; application/pdf,en_US,AI; maximum operation; invariance; recurrent inhibition; shunting inhibition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taycher, Leonid; Darrell, Trevor",2004-10-08T20:36:35Z,2004-10-08T20:36:35Z,2001-09-01,http://hdl.handle.net/1721.1/6658,AIM-2001-024,Range Segmentation Using Visibility Constraints,"Visibility constraints can aid the segmentation of foreground objects observed with multiple range images. In our approach, points are defined as foreground if they can be determined to occlude some {em empty space} in the scene. We present an efficient algorithm to estimate foreground points in each range view using explicit epipolar search. In cases where the background pattern is stationary, we show how visibility constraints from other views can generate virtual background values at points with no valid depth in the primary view. We demonstrate the performance of both algorithms for detecting people in indoor office environments.",AIM-2001-024,10 p.; 15686301 bytes; 1574798 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Arkoudas, Konstantine",2004-10-08T20:36:40Z,2004-10-08T20:36:40Z,2001-10-05,http://hdl.handle.net/1721.1/6661,AIM-2001-025,Type-alpha DPLs,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",AIM-2001-025,27 p.; 1766438 bytes; 815435 bytes,application/postscript; application/pdf,en_US,AI; Deduction; formal proofs; semantics; proof checking; soundness; logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Arkoudas, Konstantine",2004-10-08T20:36:42Z,2004-10-08T20:36:42Z,2001-10-16,http://hdl.handle.net/1721.1/6662,AIM-2001-027,Type-omega DPLs,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",AIM-2001-027,62 p.; 3794425 bytes; 787916 bytes,application/postscript; application/pdf,en_US,AI; deduction; computation; proof search; soundness; logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rennie, Jason D. M.; Rifkin, Ryan",2004-10-20T21:03:52Z,2004-10-20T21:03:52Z,2001-10-16,http://hdl.handle.net/1721.1/7241,AIM-2001-026; CBCL-210,Improving Multiclass Text Classification with the Support Vector Machine,"We compare Naive Bayes and Support Vector Machines on the task of multiclass text classification. Using a variety of approaches to combine the underlying binary classifiers, we find that SVMs substantially outperform Naive Bayes. We present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets. We develop a new indicator of binary performance to show that the SVM's lower multiclass error is a result of its improved binary performance. Furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties.",AIM-2001-026; CBCL-210,14 p.; 1240992 bytes; 1091543 bytes,application/postscript; application/pdf,en_US,AI; text classification; support vector machine; multiclass classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Fleming, Roland W.; Dror, Ron O.; Adelson, Edward H.",2004-10-08T20:36:44Z,2004-10-08T20:36:44Z,2001-10-21,http://hdl.handle.net/1721.1/6663,AIM-2001-032,How do Humans Determine Reflectance Properties under Unknown Illumination?,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",AIM-2001-032,9 p.; 7609556 bytes; 945959 bytes,application/postscript; application/pdf,en_US,AI; illumination; reflectance; natural image statistics; human vision; BRDF,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dror, Ron O.; Edward H. Adelson,; Willsky, Alan S.",2004-10-08T20:36:53Z,2004-10-08T20:36:53Z,2001-10-21,http://hdl.handle.net/1721.1/6664,AIM-2001-033,Recognition of Surface Reflectance Properties from a Single Image under Unknown Real-World Illumination,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",AIM-2001-033,9 p.; 5961528 bytes; 831200 bytes,application/postscript; application/pdf,en_US,AI; illumination; reflectance; computer vision; geometry; natural image statistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Larsen, Samuel; Witchel, Emmett; Amarasinghe, Saman",2023-03-29T14:42:27Z,2023-03-29T14:42:27Z,2001-11,https://hdl.handle.net/1721.1/149310,MIT-LCS-TM-621,Techniques for Increasing and Detecting Memory Alignment,"Memory alignment is an important property in memory system performance. Extraction of alignment information at compile-time enables the possibility for new classes of program optimization. In this paper, we present methods for increasing and detecting the alignment of memory references in a program. Our transformations and analyses do not require interprocedural analysis and introduce almost no overhead. As a result, they can be incorporated into real compilation systems. On average, our techniques are able to achieve a five-fold increase in the number of dynamically aligned memory references. We are then able to detect 94% of these operations. This success is invaluable in providing performance gains in a range of different areas. When alignment information is incorporated into a vectorizing compiler, we can increase the performance of a G4 AltiVec processor by more than a factor of two. Using the same methods, we are able to reduce energy consumption in a data cache by as much as 35%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ostrovsky, Yuri; Cavanagh, Patrick; Sinha, Pawan",2004-10-20T21:03:57Z,2004-10-20T21:03:57Z,2001-11-05,http://hdl.handle.net/1721.1/7243,AIM-2001-029; CBCL-209,Perceiving Illumination Inconsistencies in Scenes,"The human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. Here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. Contrary to predictions from previous studies [Enns and Rensink, 1990; Sun and Perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. Our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. Specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",AIM-2001-029; CBCL-209,13 p.; 3418249 bytes; 947913 bytes,application/postscript; application/pdf,en_US,AI; Illumination; natural scene perception; lighting direction; pop-out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:55Z,2004-10-20T21:03:55Z,2001-11-05,http://hdl.handle.net/1721.1/7242,AIM-2001-028; CBCL-208,Detecting Faces in Impoverished Images,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",AIM-2001-028; CBCL-208,14 p.; 20987363 bytes; 1810477 bytes,application/postscript; application/pdf,en_US,AI; Face detection; image resolution; contrast negation; vertical inversion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Corduneanu, Adrian; Jaakkola, Tommi",2004-10-08T20:37:18Z,2004-10-08T20:37:18Z,2001-11-08,http://hdl.handle.net/1721.1/6679,AIM-2001-030,Stable Mixing of Complete and Incomplete Information,"An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n^3) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",AIM-2001-030,9 p.; 1207127 bytes; 733599 bytes,application/postscript; application/pdf,en_US,AI; semi-supervised learning; incomplete data; EM; stable estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Arkoudas, Konstantine",2004-10-08T20:37:19Z,2004-10-08T20:37:19Z,2001-11-13,http://hdl.handle.net/1721.1/6680,AIM-2001-031,Simplifying transformations for type-alpha certificates,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",AIM-2001-031,45 p.; 2306816 bytes; 532283 bytes,application/postscript; application/pdf,en_US,AI; deduction; proofs; simplifiation; proof optimization; deduction complexity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wang, Karen",2023-03-29T15:34:42Z,2023-03-29T15:34:42Z,2001-12,https://hdl.handle.net/1721.1/149932,MIT-LCS-TR-829,2RegionRED: a Congestion Control Mechanism for the High Speed Internet,This thesis proposes a new Active Queue Management (AQM) scheme called 2RegionRED.  It is superior to the classic Random Early Detection (RED) algorithm in that there is an intuitive way to set its parameters and it is self-tuning.  Its design is motivated by an original principle to sustain the smallest queue possible while still allowing for maximum link utilization.  2RegionRED uses the number of competing TCPs as its measure of load.  However it does not keep an explicit count.  The result is a novel algorithm that adjusts the drop rate according to two regions of operation: that requiring less than and greater than one drop per round-trip time (RTT).  This thesis also analyzes methods for measuring the persistent queue and proposes the ABSMIN method.  Simulations of 2RegionRED using ABSMIN reveal some difficulties and insights.  Basic comparisons to the Adaptive RED and Flow Proportional Queuing (FPQ) adaptive algorithms are also demonstrated through simulation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Katabi, Dina; Blake, Charles",2023-03-29T15:34:40Z,2023-03-29T15:34:40Z,2001-12,https://hdl.handle.net/1721.1/149931,MIT-LCS-TR-828,Inferring Congestion Sharing and Path Characteristics from Packet Interarrival Times,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Oliva, Aude",2004-10-20T21:04:45Z,2004-10-20T21:04:45Z,2001-12-01,http://hdl.handle.net/1721.1/7267,AIM-2001-036; CBCL-213,Global Depth Perception from Familiar Scene Structure,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",AIM-2001-036; CBCL-213,22 p.; 40226611 bytes; 7425856 bytes,application/postscript; application/pdf,en_US,AI; depth; monocular; scale selection; natural images; scene recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Riesenhuber, Maximilian",2004-10-20T21:04:38Z,2004-10-20T21:04:38Z,2001-12-10,http://hdl.handle.net/1721.1/7265,AIM-2001-034; CBCL-211,"Generalization over contrast and mirror reversal, but not figure-ground reversal, in an ""edge-based","Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",AIM-2001-034; CBCL-211,3 p.; 798352 bytes; 91696 bytes,application/postscript; application/pdf,en_US,AI; AI; computational neuroscience; object recognition; macaque; IT; invariance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yip, Andrew; Sinha, Pawan",2004-10-20T21:04:40Z,2004-10-20T21:04:40Z,2001-12-13,http://hdl.handle.net/1721.1/7266,AIM-2001-035; CBCL-212,Role of color in face recognition,"One of the key challenges in face perception lies in determining the contribution of different cues to face identification. In this study, we focus on the role of color cues. Although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. Here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. Under such conditions, recognition performance with color images is significantly better than that with grayscale images. Our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",AIM-2001-035; CBCL-212,12 p.; 1469164 bytes; 237772 bytes,application/postscript; application/pdf,en_US,AI; Face recognition; color; low-resolution; grayscale,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Heo, Seongmoo; Barr, Kenneth; Hampton, Mark; Asanovi_, Krste",2023-03-29T15:35:04Z,2023-03-29T15:35:04Z,2002-01,https://hdl.handle.net/1721.1/149935,MIT-LCS-TR-832,Fine-Grain Dynamic Leakage Reduction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Heo, Seongmoo; Asanovi_, Krste",2023-03-29T15:35:02Z,2023-03-29T15:35:02Z,2002-01,https://hdl.handle.net/1721.1/149934,MIT-LCS-TR-831,Leakage-Biased Domino Circuits for Dynamic Fine-Grain Leakage Reduction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2004-10-20T20:29:08Z,2004-10-20T20:29:08Z,2002-01-01,http://hdl.handle.net/1721.1/7079,AITR-2002-002,Generating Communications Systems Through Shared Context,"In a distributed model of intelligence, peer components need to communicate with one another. I present a system which enables two agents connected by a thick twisted bundle of wires to bootstrap a simple communication system from observations of a shared environment. The agents learn a large vocabulary of symbols, as well as inflections on those symbols which allow thematic role-frames to be transmitted. Language acquisition time is rapid and linear in the number of symbols and inflections. The final communication system is robust and performance degrades gradually in the face of problems.",AITR-2002-002,58 p.; 7876447 bytes; 588901 bytes,application/postscript; application/pdf,en_US,AI; distributed amorphous human intelligence genesis robust communication network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Darrell, Trevor; Checka, Neal; Oh, Alice; Morency, Louis-Philippe",2004-10-08T20:37:23Z,2004-10-08T20:37:23Z,2002-01-01,http://hdl.handle.net/1721.1/6682,AIM-2002-001,Exploring Vision-Based Interfaces: How to Use Your Head in Dual Pointing Tasks,"The utility of vision-based face tracking for dual pointing tasks is evaluated. We first describe a 3-D face tracking technique based on real-time parametric motion-stereo, which is non-invasive, robust, and self-initialized. The tracker provides a real-time estimate of a ?frontal face ray? whose intersection with the display surface plane is used as a second stream of input for scrolling or pointing, in paral-lel with hand input. We evaluated the performance of com-bined head/hand input on a box selection and coloring task: users selected boxes with one pointer and colors with a second pointer, or performed both tasks with a single pointer. We found that performance with head and one hand was intermediate between single hand performance and dual hand performance. Our results are consistent with previously reported dual hand conflict in symmetric pointing tasks, and suggest that a head-based input stream should be used for asymmetric control.",AIM-2002-001,1 p.; 1612360 bytes; 298580 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Freeman, William T.; Zhang, Hao",2004-10-08T20:37:26Z,2004-10-08T20:37:26Z,2002-01-10,http://hdl.handle.net/1721.1/6683,AIM-2002-002,Shape-Time Photography,"We introduce a new method to describe, in a single image, changes in  shape over time. We acquire both range and image information with a  stationary stereo camera. From the pictures taken, we display a  composite image consisting of the image data from the  surface closest to the camera at every pixel. This reveals the 3-d  relationships over time by easy-to-interpret occlusion relationships  in the composite image. We call the composite a shape-time  photograph.   Small errors in depth measurements cause artifacts in the shape-time  images. We correct most of these using a Markov network to estimate  the most probable front surface, taking into account the depth  measurements, their uncertainties, and layer continuity assumptions.",AIM-2002-002,6 p.; 6494953 bytes; 11283819 bytes,application/postscript; application/pdf,en_US,AI; video summarization; stereo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Zee, Karen; Rinard, Martin",2023-03-29T15:35:09Z,2023-03-29T15:35:09Z,2002-02,https://hdl.handle.net/1721.1/149937,MIT-LCS-TR-834,Write Barrier Removal by Static Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Matusik, Wojciech; Buehler, Chris; McMillan, Leonard; Gortler, Steven J.",2023-03-29T14:42:33Z,2023-03-29T14:42:33Z,2002-02,https://hdl.handle.net/1721.1/149312,MIT-LCS-TM-623,An Efficient Visual Hull Computation Algorithm,"In this paper we describe an efficient algorithm for computing the visual hull of an object. This problem is equivalent to computing the intersection of generalized cones. The naïve visual hull computation algorithm requires intersecting 3D polyhedra. We exploit the special structure of generalized cone polyhedra and show how to reduce this computation to a set of intersections in 2D. Moreover, we describe how the 2D intersections can be carried out efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Matusik, Wojciech; Buehler, Chris; McMillan, Leonard",2023-03-29T14:42:35Z,2023-03-29T14:42:35Z,2002-02,https://hdl.handle.net/1721.1/149313,MIT-LCS-TM-624,Efficient View-Dependent Sampling of Visual Hulls,"In this paper we present an efficient algorithm for sampling visual hulls. Our algorithm computers exact points and normals on the surface of visual hull instead of a more traditional volumetric representation. The main feature that distinguishes our algorithm from previous ones is that it allows for sampling along arbitrary viewing rays with no loss of efficiency. Using this property, we adaptively sample visual hulls to minimize the number of samples needed to attain a given fidelity. In our experiments, the number of samples can typically be reduced by an order of magnitude, resulting in a corresponding performance increase over previous algorithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kiriansky, Vladimir; Bruening, Derek; Amarasinghe, Saman",2023-03-29T14:42:37Z,2023-03-29T14:42:37Z,2002-02,https://hdl.handle.net/1721.1/149314,MIT-LCS-TM-625,Secure Execution Via Program Shepherding,"We introduce program shepherding, a method for monitoring control flow transfers during program execution to enforce a security policy. Shepherding ensures that malicious code masquerading as data is never executed, thwarting a large class of security attacks. Shepherding can also enforce entry points as the only way to execute shared library code. Furthermore, shepherding guarantees that sandboxing checks around any type of program operation will never be bypassed. We have implemented these capabilities efficiently in a runtime system with minimal or no performance penalties. This system operates on unmodified native binaries, requires no special hardware or operating system support, and runs on existing IA-32 machines.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Katabi, Dina; Blake, Charles",2023-03-29T14:42:39Z,2023-03-29T14:42:39Z,2002-02,https://hdl.handle.net/1721.1/149315,MIT-LCS-TM-626,A Note on the Stability Requirements of Adaptive Virtual Queue,Choosing the correct value for the parameters of an Active Queue Management (AQM) scheme is a well-known hard problem. The Adaptive Virtual Queue (AVQ) attempts at solving this problem by using stability requirements to devise a rule for setting its parameter. This memo shows that the AVQ rule for setting its parameter is impractical for many real-life situations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lynch, Nancy A.; Segala, Roberto; Vaandrager, Frits",2023-03-30T15:32:47Z,2023-03-29T15:34:34Z; 2023-03-30T15:32:47Z,2002-02,https://hdl.handle.net/1721.1/149930.2,MIT-LCS-TR-827b,Hybrid I/O Automata*,"Hybrid systems are systems that exhibit a combination of discrete and continuous behavior. Typical hybrid systems include computer components, which operate in discrete program steps, and real-world components, whose behavior over time intervals evolves according to physical constraints. Important examples of hybrid systems include automated transportation systems, robotics systems, process control systems, systems of embedded devices, and mobile computing systems. Such systems can be very complex, and very difficult to describe and analyze. This paper presents the Hybrid Input/Output Automaton (HIOA) modeling framework, a basic mathematical framework to support description and analysis of hybrid systems. An important feature of this model is its support for decomposing hybrid system descriptions. In particular, the framework includes a notion of external behavior for a hybrid I/O automaton, which captures its discrete and continuous interactions with its environment. The framework also defines what it means for one HIOA to implement another, based on an inclusion relationship between their external behavior sets, and defines a notion of simulation, which provides a sufficient condition for demonstrating implementation relationships. The framework also includes a composition operation for HIOAs, which respects external behavior, and a notion of receptiveness, which implies that an HIOA does not block the passage of time. The framework is intended to support analysis methods from both computer science and control theory. This work is a simplification of an earlier version of the HIOA model [49, 50]. The main simplification in the new model is a clearer separation between the mechanisms used to model discrete and continuous interaction between components. In particular, the new model removes the dual use of external variables for discrete and continuous interaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Thies, William F.; Karczmarek, Michael; Gordon, Michael; Maze, David; Wong, Jeremy; Hoffmann, Henry; Brown, Matthew; Amarasinghe, Saman",2023-03-29T14:42:29Z,2023-03-29T14:42:29Z,2002-02,https://hdl.handle.net/1721.1/149311,MIT-LCS-TM-622,StreamIT: A Complier for Streaming Applications,"Streaming programs represent an increasingly important and widespread class of applications that holds unprecedented opportunitie sfor high-impact compiler technology. Unlike sequential programs with obscured dependence information and complex communication patterns, a stream program is naturally written as a set of concurrent filters with regular steady-state communication. The StreamIt language aims to provide a natural, high-level syntax that improves programmer productivity in the streaming domain. At the same time, the language imposes a hierarchical structure on the stream graph that enables novel representations and optimizations within the StreamIt compiler. We define the ""stream dependence function,"" a fundamental relationship between the input channels of two filters in a stream graph. We also describe a suite of stream optimizations, a denotational semantics for validating these optimizations, and a novel phased scheduling algorithm for stream graphs. In addition, we have implemented a prototype of the StreamIt optimizing compiler that is showing promising results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"DeCouto, Douglas S.J.; Aguayo, Daniel; Chambers, Benjamin A.; Morris, Robert",2023-03-29T15:35:12Z,2023-03-29T15:35:12Z,2002-03,https://hdl.handle.net/1721.1/149938,MIT-LCS-TR-836,Effects of Loss Rate on Ad Hoc Wireless Routing,"This paper uses measurements from two deployed wireless ad hoc networks to illustrate the effects of link loss rates on routing protocol performance. Measurements of these networks show that the radio links between the majority of nodes have substantial loss rates. These loss rates are high enough to prevent existing ad hoc routing protocols from using the links. Link-level retransmission can mask high loss rates, at the cost of substantial decreases in throughput. Simulations, driven by the observed loss rates, show that the shortest paths chosen by existing routing protocols tend to find routes with much less capacity than is available along the best route. Based on these observations, we present a routing metric intended to allow routing protocols to find good routes in wireless ad hoc networks. The metric is the expected total number of transmissions required to deliver a packet along a route. This metric favors routes with high throughput and low total impact on spectrum. It is expected to perform better than existing techniques that eliminate links based on loss rate thresholds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Boyapati, Chandrasekhar; Lee, Robert; Rinard, Martin",2023-03-29T15:35:19Z,2023-03-29T15:35:19Z,2002-03,https://hdl.handle.net/1721.1/149941,MIT-LCS-TR-839,A Type System for Preventing Data Races and Deadlocks in Java Programs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gordon, Michael; Thies, William; Karczmarek, Michael; Wong, Jeremy; Hoffmann, Henry; Maze, David; Amarasinghe, Saman",2023-03-29T14:42:42Z,2023-03-29T14:42:42Z,2002-03,https://hdl.handle.net/1721.1/149316,MIT-LCS-TM-627,A Stream Compiler for Communication-Exposed Architectures,"With the increasing miniturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, iWarp, SmartMemories). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures. In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, much analysis is needed to adapt a stream program to a parallel stream processor. We describe fission and fusion transformations that can be used to adjust the granularity of a stream graph, a layout algorithm for mapping a stream graph to a given network topology, and a scheduling algorithm for generating a fine-grained static communication pattern for each computational element. We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing optimizations. Using the cycle-accurate Raw simulator, we demonstrate that these optimizations can improve performance by up to 145%. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chen, Benjie; Morris, Robert T.",2023-03-29T15:35:15Z,2023-03-29T15:35:15Z,2002-03,https://hdl.handle.net/1721.1/149939,MIT-LCS-TR-837,L+: Scalable Landmark Routing and Address Lookup for Multi-hop Wireless Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demaine, Erik D.; Hajiaghayi, Mohammad Taghi; Thilikos, Dimitrios M.",2023-03-29T15:35:17Z,2023-03-29T15:35:17Z,2002-03,https://hdl.handle.net/1721.1/149940,MIT-LCS-TR-838,"Exponential Speedup of Fixed Parameter Algorithms K_{3,3}-minor-free or K_5-minor-free Graphs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Poggio, Tomaso; Rifkin, Ryan; Mukherjee, Sayan; Rakhlin, Alex",2004-10-20T21:04:57Z,2004-10-20T21:04:57Z,2002-03-01,http://hdl.handle.net/1721.1/7268,AIM-2002-003; CBCL-214,Bagging Regularizes,"Intuitively, we expect that averaging --- or  bagging --- different regressors with low correlation should  smooth their behavior and be somewhat similar to regularization. In this  note we make this intuition precise. Using an almost classical  definition of stability, we prove that a certain form of averaging  provides generalization bounds with a rate of convergence of the  same order as Tikhonov regularization --- similar to fashionable RKHS-based learning algorithms.",AIM-2002-003; CBCL-214,7 p.; 906324 bytes; 285651 bytes,application/postscript; application/pdf,en_US,AI; Bagging; stability; regularization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sollins, Karen R.",2008-10-29T18:30:09Z,2008-10-29T18:30:09Z,2002-03-01,http://hdl.handle.net/1721.1/42898,MIT-CSAIL-TR-2008-064,Recursively invoking Linnaeus: A Taxonomy for Naming Systems,"Naming is a central element of a distributed or network system design. Appropriate design choices are central. This paper explores a taxonomy of naming systems, and engineering tradeoffs as an aid to the namespace designer. The three orthogonal components of the taxonomy are the characteristics of the namespace itself, name assignment, and name resolution. Within each of these, we explore a number of distinct characteristics. The position of this paper is that engineering design of naming systems should be informed by the possibilities and tradeoffs that those possibilities represent. The paper includes a review of a sampling of naming system designs that reflect different choices within the taxonomy and discussion about why those choices were made.",,11 p.,,,Identification; Namespace management; Namespace definition,Advanced Network Architecture,"This effort was sponsored by the Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-00-2-0553.",Creative Commons Attribution-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
,"Knoblich, Ulf; Riesenhuber, Maximilan",2004-10-20T21:04:59Z,2004-10-20T21:04:59Z,2002-03-15,http://hdl.handle.net/1721.1/7269,AIM-2002-004; CBCL-215,Stimulus Simplification and Object Representation: A Modeling Study,"Tsunoda et al. (2001) recently studied the  nature of object representation in monkey  inferotemporal cortex using a combination of  optical imaging and extracellular recordings.  In particular, they examined IT neuron  responses to complex natural objects and  ""simplified"" versions thereof. In that study, in  42% of the cases, optical imaging revealed a  decrease in the number of activation patches  in IT as stimuli were ""simplified"". However, in  58% of the cases, ""simplification"" of the  stimuli actually led to the appearance of  additional activation patches in IT. Based on  these results, the authors propose a scheme  in which an object is represented by  combinations of active and inactive columns  coding for individual features.  We examine the patterns of activation caused  by the same stimuli as used by Tsunoda et al.  in our model of object recognition in cortex  (Riesenhuber 99). We find that object-tuned  units can show a pattern of appearance and  disappearance of features identical to the  experiment. Thus, the data of Tsunoda et al.  appear to be in quantitative agreement with a  simple object-based representation in which  an object's identity is coded by its similarities  to reference objects. Moreover, the agreement  of simulations and experiment suggests that  the simplification procedure used by Tsunoda  (2001) is not necessarily an accurate method  to determine neuronal tuning.",AIM-2002-004; CBCL-215,7 p.; 1367647 bytes; 641994 bytes,application/postscript; application/pdf,en_US,AI; computational neuroscience object recognition representation simplification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sullivan, Gregory T.",2004-10-08T20:37:46Z,2004-10-08T20:37:46Z,2002-03-22,http://hdl.handle.net/1721.1/6686,AIM-2002-005,"Advanced Programming Language Features for Executable Design Patterns ""Better Patterns Through Reflection","The Design Patterns book [GOF95] presents  24 time-tested patterns that consistently appear in well-designed  software systems. Each pattern is presented with a description of the  design problem the pattern addresses, as well as sample  implementation code and design considerations. This paper explores how the  patterns from the ""Gang of Four'', or ""GOF'' book, as it is often called,  appear when similar problems are addressed using a dynamic,  higher-order, object-oriented programming language. Some of the  patterns disappear -- that is, they are supported directly by language features,  some patterns are simpler or have a different focus, and some are  essentially unchanged.",AIM-2002-005,45 p.; 1734113 bytes; 322829 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bar-Joseph, Ziv; Keidar, Idit; Lynch, Nancy A.",2023-03-29T15:35:22Z,2023-03-29T15:35:22Z,2002-04,https://hdl.handle.net/1721.1/149942,MIT-LCS-TR-840,Early-Delivery Dynamic Atomic Broadcast,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Boyapati, Chandrasekhar",2023-03-29T15:35:28Z,2023-03-29T15:35:28Z,2002-04,https://hdl.handle.net/1721.1/149944,MIT-LCS-TR-842,Towards An Extensible Virtual Machine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Finney, Sarah; Gardiol, Natalia H.; Kaelbling, Leslie Pack; Oates, Tim",2004-10-08T20:37:45Z,2004-10-08T20:37:45Z,2002-04-10,http://hdl.handle.net/1721.1/6685,AIM-2002-006,Learning with Deictic Representation,"Most reinforcement learning methods operate  on propositional representations of the world state. Such  representations are often intractably large and generalize poorly. Using  a deictic representation is believed to be a viable  alternative: they promise generalization while allowing the use of  existing reinforcement-learning methods. Yet, there  are few experiments on learning with deictic representations reported  in the literature. In this paper we explore the effectiveness of two  forms of deictic representation and a naive propositional  representation in a simple blocks-world domain. We find,  empirically, that the deictic representations actually worsen performance.  We conclude with a discussion of possible causes of these  results and strategies for more effective learning in domains with objects.",AIM-2002-006,41 p.; 5712208 bytes; 1294450 bytes,application/postscript; application/pdf,en_US,AI; Reinforcement Learning; Partial Observability; Representations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Knoblich, Ulf; Freedman, David J.; Riesenhuber, Maximilian",2004-10-20T21:05:01Z,2004-10-20T21:05:01Z,2002-04-18,http://hdl.handle.net/1721.1/7270,AIM-2002-007; CBCL-216,Categorization in IT and PFC: Model and Experiments,"In a recent experiment, Freedman et al.  recorded from inferotemporal (IT) and prefrontal cortices (PFC) of monkeys  performing a ""cat/dog"" categorization task (Freedman 2001 and  Freedman, Riesenhuber, Poggio, Miller 2001). In this paper we analyze the tuning properties of view-tuned  units in our HMAX model of object recognition in cortex (Riesenhuber  1999) using the same paradigm and stimuli  as in the experiment. We then compare the simulation results to the monkey  inferotemporal neuron population data. We find that view-tuned  model IT units that were trained without any explicit category  information can show category-related tuning as observed in the  experiment. This suggests that the tuning properties of experimental IT  neurons might primarily be shaped by bottom-up stimulus-space  statistics, with little influence of top-down task-specific  information. The population of experimental PFC neurons, on the other hand,  shows tuning properties that cannot be explained just by stimulus  tuning. These analyses are compatible with a model of object recognition  in cortex (Riesenhuber 2000)  in which a population of shape-tuned  neurons provides a general basis for neurons tuned to  different recognition tasks.",AIM-2002-007; CBCL-216,11 p.; 1497623 bytes; 678374 bytes,application/postscript; application/pdf,en_US,AI; categorization IT PFC computational neuroscience model HMAX,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gassend, Blaise; Clarke, Dwaine; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:35:07Z,2023-03-29T15:35:07Z,2002-05,https://hdl.handle.net/1721.1/149936,MIT-LCS-TR-833,Silicon Physical Unknown Functions and Secure Smartcards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ne Win, Toh; Ernst, Michael D.",2023-03-29T15:35:26Z,2023-03-29T15:35:26Z,2002-05,https://hdl.handle.net/1721.1/149943,MIT-LCS-TR-841,Verifying Distributed Algorithms via Dynamic Analysis and Theorem Proving,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ma, Albert; Asanovi_, Krste",2023-03-29T15:35:33Z,2023-03-29T15:35:33Z,2002-05,https://hdl.handle.net/1721.1/149946,MIT-LCS-TR-844,A Double-Pulsed Set-Conditional-Reset Flip-Flop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Steinbach, Carl",2004-10-20T20:29:42Z,2004-10-20T20:29:42Z,2002-05-01,http://hdl.handle.net/1721.1/7093,AITR-2002-007,A Reinforcement-Learning Approach to Power Management,"We describe an adaptive, mid-level approach  to the wireless device power management problem. Our approach  is based on reinforcement learning, a machine learning  framework for autonomous agents. We describe how our  framework can be applied to the power management problem in both  infrastructure and ad~hoc wireless networks. From this thesis we conclude that  mid-level power management policies can outperform low-level policies and  are more convenient to implement than high-level policies. We also  conclude that power management policies need to adapt to the  user and network, and that a mid-level power management framework  based on reinforcement learning fulfills these requirements.",AITR-2002-007,41 p.; 8457203 bytes; 989455 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; power management; wireless networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Huang, Andrew ""bunnie""",2004-10-08T20:38:06Z,2004-10-08T20:38:06Z,2002-05-26,http://hdl.handle.net/1721.1/6694,AIM-2002-008,Keeping Secrets in Hardware: the Microsoft Xbox(TM) Case Study,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",AIM-2002-008,15 p.; 837733 bytes; 527464 bytes,application/postscript; application/pdf,en_US,AI; Tamper-resistant hardware; Microsoft Xbox; Cryptography; Privacy; Public Key Algos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Harder, Michael",2023-03-29T15:35:43Z,2023-03-29T15:35:43Z,2002-06,https://hdl.handle.net/1721.1/149950,MIT-LCS-TR-848,Improving Test Suites via Generated Specifications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liskov, Barbara H.; Moh, Chuang-Hue; Richman, Steven; Shrira, Liuba; Chueng, Yin; Boyapati, Chandrasekhar",2023-03-29T15:35:50Z,2023-03-29T15:35:50Z,2002-06,https://hdl.handle.net/1721.1/149953,MIT-LCS-TR-851,Safe Lazy Software Upgrades in Object-Oriented Databases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Boyapati, Chandrasekhar; Lee, Robert; Rinard, Martin",2023-03-29T15:35:55Z,2023-03-29T15:35:55Z,2002-06,https://hdl.handle.net/1721.1/149955,MIT-LCS-TR-853,Safe Runtime Downcasts With Ownership Types,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gassend, Blaise; Clarke, Dwaine; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:35:57Z,2023-03-29T15:35:57Z,2002-06,https://hdl.handle.net/1721.1/149956,MIT-LCS-TR-854,Delay-Based Circuit Authentication With Application to Key Cards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gassend, Blaise; Clarke, Dwaine; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:35:36Z,2023-03-29T15:35:36Z,2002-06,https://hdl.handle.net/1721.1/149947,MIT-LCS-TR-845,Controlled Physical Unknown Functions: Applications to Secure Smartcards and Certified Execution,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Nimmer, Jeremy",2023-03-29T15:35:52Z,2023-03-29T15:35:52Z,2002-06,https://hdl.handle.net/1721.1/149954,MIT-LCS-TR-852,Automatic Generation and Checking of Program Specifications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Huang, Andrew ""bunnie""",2004-10-20T20:29:51Z,2004-10-20T20:29:51Z,2002-06-01,http://hdl.handle.net/1721.1/7096,AITR-2002-006,ADAM: A Decentralized Parallel Computer Architecture Featuring Fast Thread and Data Migration and a Uniform Hardware Abstraction,"The furious pace of Moore's Law is driving  computer architecture into a realm where the the speed of light is the  dominant factor in system latencies. The number of clock cycles to span  a chip are increasing, while the number of bits that can be accessed  within a clock cycle is decreasing. Hence, it is becoming more  difficult to hide latency. One alternative solution is to reduce latency by  migrating threads and data, but the overhead of existing  implementations has previously made migration an unserviceable solution so  far.  I present an architecture, implementation, and  mechanisms that reduces the overhead of migration to the point where  migration is a viable supplement to other latency hiding  mechanisms, such as multithreading. The architecture is abstract,  and presents programmers with a simple, uniform fine-grained  multithreaded parallel programming model with implicit memory management. In  other words, the spatial nature and implementation details (such as  the number of processors) of a parallel machine are entirely hidden from  the programmer. Compiler writers are  encouraged to devise programming languages for the machine that guide a  programmer to express their ideas in terms of objects, since objects exhibit  an inherent physical locality of data and code. The machine  implementation can then leverage this locality to automatically distribute  data and threads across the physical machine by using a set of  high performance migration mechanisms.  An implementation of this architecture could  migrate a null thread in 66 cycles -- over a factor of 1000 improvement  over previous work. Performance also scales well; the time  required to move a typical thread is only 4 to 5 times that of a null  thread. Data migration performance is similar, and scales  linearly with data block size. Since the performance of the migration  mechanism is on par with that of an L2 cache, the implementation  simulated in my work has no data caches and relies instead on  multithreading and the migration mechanism to hide and reduce access  latencies.",AITR-2002-006,299 p.; 13404896 bytes; 2307234 bytes,application/postscript; application/pdf,en_US,AI; HPC parallel computer architecture queues fault tolerance programmability ADAM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Brown, Jeremy Hanford",2004-10-20T20:06:13Z,2004-10-20T20:06:13Z,2002-06-01,http://hdl.handle.net/1721.1/6910,AITR-2002-005,"Sparsely Faceted Arrays: A Mechanism Supporting Parallel Allocation, Communication, and Garbage Collection","Conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. In this  thesis, I introduce sparsely faceted arrays (SFAs), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. Sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the Cray T3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. A sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. By providing simple semantics and  making efficient use of memory, SFAs enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. I present example applications which use  SFAs, and describe and evaluate simple hardware mechanisms for  implementing SFAs.  Keeping track of which nodes have allocated  facets for a particular SFA is an important task that suggests the  need for automatic memory management, including garbage collection.  To address this need, I first argue that conventional tracing  techniques such as mark/sweep and copying GC are inherently unscalable in  parallel systems. I then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. I also discuss opportunities  for hardware support of this garbage collection strategy.  I have implemented a high-level hardware/OS  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. I describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of SFAs and SFA-aware garbage collection. Simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",AITR-2002-005,115 p.; 3145524 bytes; 677754 bytes,application/postscript; application/pdf,en_US,AI; sparsely faceted arrays; shared memory; garbage collection; data structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kim, Adlar J.; Shelton, Christian R.",2004-10-20T21:05:02Z,2004-10-20T21:05:02Z,2002-06-01,http://hdl.handle.net/1721.1/7271,AIM-2002-009; CBCL-217,Modeling Stock Order Flows and Learning Market-Making from Data,"Stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. In this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  The sequence of buys and sells for a  particular stock, the order flow, we model as an Input-Output Hidden Markov  Model fit to historical data. When combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. Our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. We  demonstrate learning results for two separate real stocks.",AIM-2002-009; CBCL-217,7 p.; 2119856 bytes; 1370177 bytes,application/postscript; application/pdf,en_US,AI; input/output HMM; market-making; reinforcement learning; stock order flow model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"III, Teodoro Arvizo",2004-10-20T20:29:40Z,2004-10-20T20:29:40Z,2002-06-01,http://hdl.handle.net/1721.1/7092,AITR-2002-004,A Virtual Machine for a Type-omega Denotational Proof Language,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",AITR-2002-004,106 p.; 2935187 bytes; 816842 bytes,application/postscript; application/pdf,en_US,AI; virtual machine; SECD; SECD machine; denotational proof language; Athena,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Boyapati, Chandrasekhar; Liskov, Barbara H.; Shrira, Liuba",2023-03-29T15:36:02Z,2023-03-29T15:36:02Z,2002-07,https://hdl.handle.net/1721.1/149958,MIT-LCS-TR-858,Ownership Types and Safe Lazy Upgrades in Object-Oriented Databases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gassend, Blaise; Suh, G. Edward; Clarke, Dwaine; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:36:00Z,2023-03-29T15:36:00Z,2002-07,https://hdl.handle.net/1721.1/149957,MIT-LCS-TR-857,Caches and Merkle Trees for Efficient Memory Authentication,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kaynar, Dilsun Kirh; Chefter, Anna; Dean, Laura; Garland, Stephen J.; Lynch, Nancy A.; Ne Win, Toh; Ramírez-Robredo, Antonio",2023-03-29T15:35:31Z,2023-03-29T15:35:31Z,2002-07,https://hdl.handle.net/1721.1/149945,MIT-LCS-TR-843,The IOA Simulator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taylor, Michael Bedford; Lee, Walter; Amarasinghe, Saman; Agarwal, Anant",2023-03-29T15:36:04Z,2023-03-29T15:36:04Z,2002-07,https://hdl.handle.net/1721.1/149959,MIT-LCS-TR-859,Scalar Operand Networks: On-chip interconnect for ILP in Partitioned Architechures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Strumpen, Volker; Krishnamurthy, Arvind",2023-03-29T14:42:47Z,2023-03-29T14:42:47Z,2002-07,https://hdl.handle.net/1721.1/149318,MIT-LCS-TM-629,A Collision Model for Randomized Routing In Fat-Tree Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Werfel, Justin",2004-10-08T20:38:09Z,2004-10-08T20:38:09Z,2002-07-01,http://hdl.handle.net/1721.1/6696,AIM-2002-010,Implementing Universal Computation in an Evolutionary System,"Evolutionary algorithms are a common tool in  engineering and in the study of natural  evolution. Here we take their use in a new  direction by showing how they can be made to  implement a universal computer. We  consider populations of individuals with  genes whose values are the variables of  interest. By allowing them to interact with one  another in a specified environment with  limited resources, we demonstrate the ability  to construct any arbitrary logic circuit. We  explore models based on the limits of small  and large populations, and show examples of  such a system in action, implementing a  simple logic circuit.",AIM-2002-010,17 p.; 3942374 bytes; 1153028 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Clarke, Dwaine; Gassend, Blaise; Suh, G. Edward; van Dijk, Marten; Devadas, Srinivas",2023-03-29T14:42:58Z,2023-03-29T14:42:58Z,2002-08,https://hdl.handle.net/1721.1/149320,MIT-LCS-TM-631,Offline Authentication of Untrusted Storage,"We extend the offline memory correctness checking scheme presented by Blum et. al [BEG+91], by using incremental cryptography, to detect attacks by an active adversary. We also introduce a hybrid o_ine-online checking scheme designed for untrusted storages in file systems and databases. Previous work [GSC+02] [FKM00] [MVS00] describe systems in which Merkle trees are used to verify the authenticity of data stored on untrusted storage. The Merkle trees [Mer79] are used to check, after each operation, whether the storage performed correctly. The offline and hybrid checkers are designed for checking sequences of operations on an untrusted storage and, in the common case, require only a constant overhead on the number of accesses to the storage, as compared to the logarithmic overhead incurred by online Merkle tree schemes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Aftab, Omar",2023-03-29T15:36:38Z,2023-03-29T15:36:38Z,2002-08,https://hdl.handle.net/1721.1/149972,MIT-LCS-TR-876b,Economic Mechanisms for Efficient Wireless Coexistence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Thies, William; Lin, Jasper; Amarasinghe, Saman",2023-03-29T14:42:53Z,2023-03-29T14:42:53Z,2002-08,https://hdl.handle.net/1721.1/149319,MIT-LCS-TM-630,Phased Computation Graphs in the Polyhedral Model,"We present a translation scheme that allows a broad class of dataflow graphs to be considered under the optimization framework of the polyhedral model. The input to our analysis is a Phased Computation Graph, which we define as a generalization of the most widely used dataflow representations, including synchronous dataflow, cyclo-static dataflow, and computation graphs. The output of our analysis is a System of Affine Recurrence Equations (SARE) that exactly captures the data dependencies between the nodes of the original graph. Using the SARE representation, one can apply many techniques from the scientific community that are new to the DSP domain. For example, we propose simple optimizations such as node splitting, decimation propagation, and stead-state invariant code motion that leverage the fine-grained dependence information of the SARE to perform novel transformations on a stream graph. We also propose ways in which the polyhedral model can offer new approaches to classic problems of the DSP community, such as minimizing buffer size, code size, and optimizing the schedule.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Schneider, Robert; Riesenhuber, Maximilian",2004-10-20T20:48:51Z,2004-10-20T20:48:51Z,2002-08-01,http://hdl.handle.net/1721.1/7178,AIM-2002-011; CBCL-218,A Detailed Look at Scale and Translation Invariance in a Hierarchical Neural Model of Visual Object Recognition,"The HMAX model has recently been proposed  by Riesenhuber & Poggio as a  hierarchical  model of position- and size-invariant object  recognition in visual cortex. It has also turned  out to model successfully a number of other  properties of the ventral visual stream (the  visual pathway thought to be crucial for object  recognition in cortex), and particularly of (view-tuned) neurons in macaque inferotemporal  cortex, the brain area at the top of the ventral  stream. The original modeling study only  used ``paperclip'' stimuli, as in the  corresponding physiology experiment, and did  not explore systematically how model units'  invariance properties depended on model  parameters. In this study, we aimed at a  deeper understanding of the inner workings of  HMAX and its performance for various  parameter settings and ``natural'' stimulus  classes. We examined HMAX responses for  different stimulus sizes and positions  systematically and found a dependence of  model units' responses on stimulus position  for which a quantitative description is offered.  Interestingly, we find that scale invariance  properties of hierarchical neural models are  not independent of stimulus class, as  opposed to translation invariance, even  though both are affine transformations within  the image plane.",AIM-2002-011; CBCL-218,12 p.; 2137337 bytes; 1062341 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Giese, Martin Alexander; Poggio, Tomaso",2004-10-20T21:05:04Z,2004-10-20T21:05:04Z,2002-08-01,http://hdl.handle.net/1721.1/7272,AIM-2002-012; CBCL-219,Biologically Plausible Neural Model for the Recognition of Biological Motion and Actions,"The visual recognition of complex movements  and actions is crucial for communication and  survival in many species. Remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. In recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  However, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  This paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. The  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. The  model makes specific predictions that  motivate new experiments.",AIM-2002-012; CBCL-219,26 p.; 3562724 bytes; 2540946 bytes,application/postscript; application/pdf,en_US,AI; biological motion; action recognition; visual pathways; hierarchical processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Giese, M.A.; Xie, X.",2004-10-20T21:05:06Z,2004-10-20T21:05:06Z,2002-08-01,http://hdl.handle.net/1721.1/7273,AIM-2002-013; CBCL-220,Exact Solution of the Nonlinear Dynamics of Recurrent Neural Mechanisms for Direction Selectivity,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",AIM-2002-013; CBCL-220,7 p.; 2554351 bytes; 1165357 bytes,application/postscript; application/pdf,en_US,AI; direction; visual cortex; nonlinear dynamics; lurching waves; stability; recurre,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hajiaghayi, Mohammad Taghi; Mahdian, Mohammad; Mirrokni, Vahab S.",2023-03-29T15:36:07Z,2023-03-29T15:36:07Z,2002-09,https://hdl.handle.net/1721.1/149960,MIT-LCS-TR-864,The Facility Location Problem with Concave Cost Functions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Raman, Sanjay; Clarke, Dwaine; Burnside, Matt; Devadas, Srinivas; Rivest, Ronald L.",2023-03-29T15:36:22Z,2023-03-29T15:36:22Z,2002-09,https://hdl.handle.net/1721.1/149966,MIT-LCS-TR-870,Access-Controlled Resource Discovery for Pervasive Networks,"Networks of the future will be characterized by a variety of computational devices that display a level of dynamism not seen in traditional wired networks. Because of the dynamic nature of these networks, resource discovery is one of the fundamental problems that must be faced. While resource discovery systems are not a novel concept, securing these systems in an efficient and scalable way is challenging. This paper describes the design and implementation of an architecture for access-controlled resource discovery. This system achieves this goal by integrating access control with the Intentional Naming System (INS), a resource discovery and service location system. The integration is scalable, efficient, and fits well within a proxy-based security framework designed for dynamic networks. We provide performance experiments that show how our solution outperforms existing schemes. The result is a system that provides secure, access-controlled resource discovery that can scale to large numbers of resources and users",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Steck, Harald; Jaakkola, Tommi S.",2004-10-08T20:38:20Z,2004-10-08T20:38:20Z,2002-09-01,http://hdl.handle.net/1721.1/6702,AIM-2002-014,On the Dirichlet Prior and Bayesian Regularization,"A common objective in learning a model from  data is to recover its network structure, while the model  parameters are of minor interest. For example, we may wish to recover  regulatory networks from high-throughput data sources. In this paper  we examine how Bayesian regularization using a Dirichlet prior over the  model parameters affects the learned model structure in a  domain with discrete variables. Surprisingly, a weak prior in the  sense of smaller equivalent sample size leads to a strong  regularization of the model structure (sparse graph) given a sufficiently  large data set. In particular, the empty graph is obtained in the  limit of a vanishing strength of prior belief. This is  diametrically opposite to what one may expect in this limit, namely the  complete graph from an (unregularized) maximum likelihood estimate.  Since the prior affects the parameters as expected, the prior strength  balances a ""trade-off"" between regularizing the parameters or the  structure of the model. We demonstrate the benefits of optimizing this  trade-off in the sense of predictive accuracy.",AIM-2002-014,11 p.; 3152389 bytes; 1414851 bytes,application/postscript; application/pdf,en_US,AI; Regularization; Dirichlet Prior,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Freeman, William T.; Torralba, Antonio",2004-10-08T20:38:34Z,2004-10-08T20:38:34Z,2002-09-01,http://hdl.handle.net/1721.1/6704,AIM-2002-016,Shape Recipes: Scene Representations that Refer to the Image,"The goal of low-level vision is to estimate an  underlying scene, given an observed image. Real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. We propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. Shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. We describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) Shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",AIM-2002-016,12 p.; 2606902 bytes; 1497926 bytes,application/postscript; application/pdf,en_US,AI; scene representation; shape; stereo; shape recipes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tappen, Marshall F.; Freeman, William T.; Adelson, Edward H.",2004-10-08T20:38:21Z,2004-10-08T20:38:21Z,2002-09-01,http://hdl.handle.net/1721.1/6703,AIM-2002-015,Recovering Intrinsic Images from a Single Image,"We present an algorithm that uses multiple  cues to recover shading and reflectance intrinsic images from a single  image. Using both color information and a classifier trained to  recognize gray-scale patterns, each image derivative is classified as being  caused by shading or a change in the surface's reflectance.  Generalized Belief Propagation is then used to propagate information from  areas where the correct classification is clear to areas where it is  ambiguous. We also show results on real images.",AIM-2002-015,12 p.; 1591784 bytes; 1186163 bytes,application/postscript; application/pdf,en_US,AI; intrinisic images; reflectance estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kumar, Vinay P.",2004-10-01T14:00:07Z,2004-10-01T14:00:07Z,2002-09-01,http://hdl.handle.net/1721.1/5569,AITR-2002-008; CBCL-221,Towards Man-Machine Interfaces: Combining Top-down Constraints with Bottom-up Learning in Facial Analysis,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",AITR-2002-008; CBCL-221,68 p.; 21293042 bytes; 2473001 bytes,application/postscript; application/pdf,en_US,AI; Facial Expression Recognition; Pose Estimation; Viseme Recognition; SVM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demaine, Erik D.; Hohenberger, Susan; Liben-Nowell, David",2023-03-29T15:36:10Z,2023-03-29T15:36:10Z,2002-10,https://hdl.handle.net/1721.1/149961,MIT-LCS-TR-865,"Tetris is Hard, Even to Approximate",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Livadas, Carolos; Keidar, Idit",2023-03-29T15:36:15Z,2023-03-29T15:36:15Z,2002-10,https://hdl.handle.net/1721.1/149963,MIT-LCS-TR-867,The Case for Exploiting Packet Loss Locality in Multicast Loss Recovery,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tan, Godfrey",2023-03-29T15:36:12Z,2023-03-29T15:36:12Z,2002-10,https://hdl.handle.net/1721.1/149962,MIT-LCS-TR-866,Blueware: Bluetooth Simulator for ns,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Batten, Christopher; Barr, Kenneth; Saraf, Arvind; Trepetin, Stanley",2023-03-29T14:43:01Z,2023-03-29T14:43:01Z,2002-10,https://hdl.handle.net/1721.1/149321,MIT-LCS-TM-632,pStore: A Secure Peer-to-Peer Backup System,"In an effort to combine research in peer-to-peer systems with techniques for incremental backup systems, we propose pStore: a secure distributed backup system based on an adaptive peer-to-peer network. pStore exploits unused personal hard drive space attached to the Internet to provide the distributed redundancy needed for reliable and effective data backup. Experiments on a 30 node network show that 95% of the files in a 13 MB dataset can be retrieved even when 7 of the nodes have failed. On top of this reliability, pStore includes support for file encryption, versioning, and secure sharing. Its custom versioning system permits arbitrary version retrieval similar to CVS. pStore provides this functionality at less than 10% of the network bandwidth and requires 85% less storage capacity than simpler local tape backup schemes for a representative workload.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dror, Ron O.",2004-10-20T20:29:53Z,2004-10-20T20:29:53Z,2002-10-01,http://hdl.handle.net/1721.1/7097,AITR-2002-009,Surface Reflectance Recognition and Real-World Illumination Statistics,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",AITR-2002-009,195 p.; 7366082 bytes; 3656634 bytes,application/postscript; application/pdf,en_US,AI; illumination; reflectance; natural image statistics; vision; materials,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Van Eepoel, John M.",2004-10-20T20:29:56Z,2004-10-20T20:29:56Z,2002-10-22,http://hdl.handle.net/1721.1/7098,AITR-2002-010,Achieving Real-Time Mode Estimation through Offline Compilation,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",AITR-2002-010,321 p.; 20495512 bytes; 7253655 bytes,application/postscript; application/pdf,en_US,AI; mode estimation; compilation; model-based; reasoning; autonomy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Clarke, Dwaine; Gassend, Blaise; Suh, G. Edward; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:36:25Z,2023-03-29T15:36:25Z,2002-11,https://hdl.handle.net/1721.1/149967,MIT-LCS-TR-871,Offline Integrity Checking of Untrusted Storage,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"De Prisco, Roberto; Fekete, Alan; Lynch, Nancy A.; Shvartsman, Alexander A.",2023-03-29T15:36:30Z,2023-03-29T15:36:30Z,2002-11,https://hdl.handle.net/1721.1/149969,MIT-LCS-TR-873,A Dynamic Primary View Group Communication Service,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Salcianu, Alexandru; Boyapati, Chandrasekhar; Beebee, William S., Jr.; Rinard, Martin",2023-03-29T15:36:20Z,2023-03-29T15:36:20Z,2002-11,https://hdl.handle.net/1721.1/149965,MIT-LCS-TR-869,A Type System for Safe Region-Based Memory Management in Real-Time Java,"The Real-Time Specification for Java (RTSJ) allows a program to create real-time threads with hard real time constraints. Real-time threads use immortal memory and region-based memory management to avoid unbounded pauses caused by interference from the garbage collector. The RTSJ uses runtime checks to ensure that deleting a region does not create dangling references and that real-time threads do not access references to objects allocated in the garbage-collected heap. This paper presents a static type system that guarantees that these runtime checks will never fail for well-typed programs. Our type system therefore 1) provides an important safety guarantee for real-time programs and 2) makes it possible to eliminate the runtime checks and their associated overhead. Our system also makes several contributions over previous work on region types. For object-oriented programs, it combines region types and ownership types in a unified type system framework. For multithreaded programs, it allows long-lived threads to share objects without using the heap and without having memory leaks. For real-time programs, it ensures that real-time threads do not interfere with the garbage collector. We have implemented several programs in our system. Our experience indicates that our type system is sufficiently expressive and requires little programming overhead. We also ran these programs on our RTSJ platform. Our experiments show that eliminating the RTSJ runtime checks using a static type system can significantly decrease the execution time of a real-time program.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Livadas, Carolos; Lynch, Nancy A.",2023-03-29T15:36:17Z,2023-03-29T15:36:17Z,2002-11,https://hdl.handle.net/1721.1/149964,MIT-LCS-TR-868,A Formal Venture into Reliable Multicast Territory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Suh, G. Edward; Clarke, Dwaine; Gassend, Blaise; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:36:27Z,2023-03-29T15:36:27Z,2002-11,https://hdl.handle.net/1721.1/149968,MIT-LCS-TR-872,Hardware Mechanisms for Memory Integrity Checking,"Memory integrity verification is a useful primitive when implementing  secure processors that are resistant to attacks on hardware components.  This paper proposes new hardware schemes to verify the integrity of  untrusted external memory using a very small amount of trusted on-chip  storage. Our schemes maintain incremental multiset hashes of all memory  reads and writes at run-time, and can verify a {\\em sequence} of memory  operations at a later time. We study the advantages and disadvantages of  the two new schemes and two existing integrity checking schemes, MACs  and hash trees, when implemented in hardware in a microprocessor.  Simulations show that the new schemes outperform existing schemes of  equivalent functionality when integrity verification is infrequent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sussman, Gerald Jay; Wisdom, Jack",2004-10-08T20:38:38Z,2004-10-08T20:38:38Z,2002-11-01,http://hdl.handle.net/1721.1/6707,AIM-2002-018,The Role of Programming in the Formulation of Ideas,"Classical mechanics is deceptively simple. It  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. To address this problem we  use computational techniques to communicate a deeper  understanding of Classical Mechanics. Computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. Expressing the methods in a computer language forces them to be  unambiguous and computationally effective. The task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. Also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",AIM-2002-018,18 p.; 1180238 bytes; 786910 bytes,application/postscript; application/pdf,en_US,AI; Education; Mechanics; Functional Programming; Symbolic Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wisdom, Jack",2004-10-08T20:38:37Z,2004-10-08T20:38:37Z,2002-11-01,http://hdl.handle.net/1721.1/6706,AIM-2002-017,Swimming in Space-Time,"Cyclic changes in the shape of a quasi-rigid  body on a curved manifold can lead to net translation and/or  rotation of the body in the manifold. Presuming space-time is a  curved manifold as portrayed by general relativity, translation in space can  be accomplished simply by cyclic changes in the shape of a body,  without any thrust or external forces.",AIM-2002-017,32 p.; 6370543 bytes; 473755 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lam, Patrick; Rinard, Martin",2023-03-29T15:36:33Z,2023-03-29T15:36:33Z,2002-12,https://hdl.handle.net/1721.1/149970,MIT-LCS-TR-874,A Type System and Analysis for the Automatic Extraction and Enforcement of Design Information,"We present a new type system and associated type checker, analysis, and model extraction algorithms for automatically extracting models that capture aspects of the design of the program. Our type system enables the developer to place a _token_ on each object; this token serves as the object's representative during the analysis and model extraction. The polymorphism in our type system enables the use of general-purpose classes whose instances may serve different purposes in the computation; programmers may also hide the details of internal data structures by placing the same token on all of the objects in these data structures.  Our combined type system and analysis provide the model extraction algorithms with sound heap aliasing information. Our algorithms can therefore extract both structural models that characterize object referencing relationships and behavioral models that capture indirect interactions mediated by objects in the heap. Previous approaches, in contrast, in the absence of aliasing information, have focused on control-flow interactions that take place at procedure call boundaries. We have implemented our type checker, analysis, and model extraction algorithms and used them to produce design models. Our experience indicates that it is straightforward to produce the token annotations and that the extracted models provide useful insight into the structure and behavior of the program.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demsky, Brian; Rinard, Martin",2023-03-29T15:36:35Z,2023-03-29T15:36:35Z,2002-12,https://hdl.handle.net/1721.1/149971,MIT-LCS-TR-875,Automatic Detection and Repair of Errors in Data Structures,"We present a system that accepts a specification of key data structure constraints, then dynamically detects and repairs violations of these constraints. Our experience using our system indicates that the specifications are relatively easy to develop once one understands the data structures. Furthermore, for our set of benchmark applications, our system can effectively repair errors to deliver consistent data structures that allow the program to continue to operate successfully within its designed operating envelope.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Attie, Paul C.; Lynch, Nancy A.; Rajsbaum, Sergio",2023-03-29T15:36:40Z,2023-03-29T15:36:40Z,2002-12,https://hdl.handle.net/1721.1/149973,MIT-LCS-TR-877,Boosting Fault-Tolerance in Asynchronous Message Passing Systems is Impossible,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Mirrokni, Vahab S.; Lee, Walter; Karger, David; Amarasinghe, Saman",2023-03-29T14:43:03Z,2023-03-29T14:43:03Z,2002-12,https://hdl.handle.net/1721.1/149322,MIT-LCS-TM-635,A Theoretical and Practical Approach to Instruction Scheduling on Spatial Architectures,"This paper studies the problem of instruction assignment and scheduling on spatial architectures. Spatial architectures are architectures whose resources are organized in clusters, with non-zero communication delays between the clusters. On these architectures, instruction scheduling include both space scheduling, where instructions are mapped to clusters, and the traditional time scheduling. This paper considers the problem from both the theoretical and practical perspectives. It presents two integer linear program formulations with known performance bounds. We also present an 8-approximation algorithm for constant m and constant communication delays. Then, we introduce three heuristic algorithms based on list scheduling. Then we study a layer partitioning method. Our final algorithm is a combination of layer partitioning and the third heuristic. Two of the better algorithms are evaluated on the Raw machine. Results show that they are competitive with previously published results; for scientfici codes, our heuristics can perform an average of 25% better.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sudderth, Erik B.; Ihler, Alexander T.; Freeman, William T.; Willsky, Alan S.",2004-10-04T14:15:28Z,2004-10-04T14:15:28Z,2002-12-01,http://hdl.handle.net/1721.1/5932,AIM-2002-020,Nonparametric Belief Propagation and Facial Appearance Estimation,"In many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. Stochastic methods such as particle filters  provide an appealing alternative. However, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. Drawing on ideas from regularized particle  filters and belief propagation (BP), this paper develops a  nonparametric belief propagation (NBP) algorithm applicable to  general graphs. Each NBP iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. The algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. Thus, NBP  extends particle filtering methods to the more general vision  problems that graphical models can describe. We apply the NBP  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",AIM-2002-020,10 p.; 3701870 bytes; 2537534 bytes,application/postscript; application/pdf,en_US,AI; graphical model; belief propagation; nonparametric inference; vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2004-10-04T14:15:29Z,2004-10-04T14:15:29Z,2002-12-01,http://hdl.handle.net/1721.1/5933,AIM-2002-021,Leaderless Distributed Hierarchy Formation,"I present a system for robust leaderless  organization of an amorphous network into hierarchical clusters. This  system, which assumes that nodes are spatially embedded and can only  talk to neighbors within a given radius, scales to networks of arbitrary  size and converges rapidly. The amount of data stored at each  node is logarithmic in the diameter of the network, and the hierarchical  structure produces an addressing scheme such that there is an  invertible relation between distance and address for any pair of nodes.  The system adapts automatically to stopping failures, network  partition, and reorganization.",AIM-2002-021,27 p.; 7370490 bytes; 1660395 bytes,application/postscript; application/pdf,en_US,AI; amorphous computing hierarchy leaderless distributed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bouvrie, Jake V.",2004-10-08T20:38:35Z,2004-10-08T20:38:35Z,2002-12-01,http://hdl.handle.net/1721.1/6705,AIM-2002-022,Multiple Resolution Image Classification,"Binary image classifiction is a problem that  has received much attention  in recent years. In this paper we evaluate a  selection of popular  techniques in an effort to find a feature set/ classifier combination which  generalizes well to full resolution image data.  We then apply that system  to images at one-half through one-sixteenth  resolution, and consider the  corresponding error rates. In addition, we  further observe generalization  performance as it depends on the number of  training images, and lastly,  compare the system's best error rates to that  of a human performing an  identical classification task given teh same  set of test images.",AIM-2002-022,1054982 bytes; 824527 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Mukherjee, Sayan; Niyogi, Partha; Poggio, Tomaso; Rifkin, Ryan",2004-08-31T18:12:01Z,2004-08-31T18:12:01Z,2002-12-01,http://hdl.handle.net/1721.3/5507,AIM-2002-024; CBCL-223,Statistical Learning: Stability is Sufficient for Generalization and Necessary and Sufficient for Consistency of Empirical Risk Minimization,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",AIM-2002-024; CBCL-223,24 p.; 1854466 bytes; 400508 bytes,application/postscript; application/pdf,en_US,AI; Theory of Learning; Great Discoveries; Consistency; ERM; Stability,,,,,revised July 2003,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Freeman, William T.",2004-10-08T20:38:07Z,2004-10-08T20:38:07Z,2002-12-01,http://hdl.handle.net/1721.1/6695,AIM-2002-019,Properties and Applications of Shape Recipes,"In low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. The  approach proposed here is to let the image itself bear as much of the  representational burden as possible. In many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. The scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. We illustrate the use of this  representation for encoding shape information. We show how  this representation has appealing properties such as locality and  slow variation across space and scale. These properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",AIM-2002-019,9 p.; 4798019 bytes; 3236270 bytes,application/postscript; application/pdf,en_US,AI; shape from X; scene representation; shape recipes; stereo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Perez-Breva, Luis; Yoshimi, Osamu",2004-10-20T20:48:55Z,2004-10-20T20:48:55Z,2002-12-01,http://hdl.handle.net/1721.1/7181,AIM-2002-023; CBCL-222,Model Selection in Summary Evaluation,"A difficulty in the design of automated text  summarization   algorithms is in the objective evaluation.  Viewing summarization   as a tradeoff between length and  information content, we introduce   a technique based on a hierarchy of  classifiers to rank, through   model selection, different summarization  methods. This summary   evaluation technique allows for broader  comparison of   summarization methods than the traditional  techniques of summary   evaluation. We present an empirical study  of two simple, albeit   widely used, summarization methods that  shows the different usages   of this automated task-based evaluation  system and confirms the   results obtained with human-based  evaluation methods over smaller   corpora.",AIM-2002-023; CBCL-222,1739841 bytes; 1972183 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grossman, J.P.",2004-10-20T20:00:24Z,2004-10-20T20:00:24Z,2002-12-05,http://hdl.handle.net/1721.1/6828,AITR-2002-011,Design and Evaluation of the Hamal Parallel Computer,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",AITR-2002-011,186 p.; 14854547 bytes; 6844439 bytes,application/postscript; application/pdf,en_US,AI; parallel; network; simulation; hashing; multithreading; synchronization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Newton, Ryan; Beal, Jacob",2006-03-01T19:47:25Z,2006-03-01T19:47:25Z,2002-12-10,http://hdl.handle.net/1721.1/31221,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Amorphous Infrastructure for Language Implementation,"We propose a method for the robust implementation of simple graphical automataon an amorphous computer. This infrastructure is applied to the implementationof purely functional programming languages. Specifically, it is usedin conjunction with data-flow techniques to implement a toy language homologousto recurrence equations, exploiting control-flow parallelism through paralleloperand evaluation. Also, data parallelism is explored in a separate implementation,in which a simple mark-up syntax enables Scheme programs to performspatially-distributed tree-walking without modifying their semantics. This additionenables an idiomatically expressed interpreter to be trivially instrumented,producing a spatially distributed universal machine, and once again achievingcontrol flow parallelism in the interpreted language.",MIT-CSAIL-TR-2006-015,20 p.; 21433070 bytes; 757210 bytes,application/postscript; application/pdf,en_US,,Mathematics and Computation,,,,,6.978 Final Project,,,,,,,,,,,,,,,,,,,,,,,
,"Suh, G. Edward; Clarke, Dwaine; Gassend, Blaise; van Dijk, Marten; Devadas, Srinivas",2023-03-30T15:51:48Z,2023-03-29T15:36:51Z; 2023-03-30T15:51:48Z,2003,https://hdl.handle.net/1721.1/149977.2,MIT-LCS-TR-883a,AEGIS: Architecture for Tamper-Evident and Tamper-Resistant Processing,"We describe the architecture for a single-chip AEGIS processor which can be used to build computing systems secure against both physical and software attacks. Our architecture assumes that all components external to the processor, such as memory, are untrusted. We show two different implementations. In the first case, the core functionality of the operating system is trusted and implemented in a security kernel. We also describe a variant implementation assuming an untrusted operating system. AEGIS provides users with tamper-evident, authenticated environments in which any physical or software tampering by an adversary is guaranteed to be detected, and private and authenticated tamper-resistant environments where additionally the adversary is unable to obtain any information about software or data by tampering with, or otherwise observing, system operation. AEGIS enables many applications, such as commercial grid computing, secure mobile agents, software licensing, and digital rights management. Preliminary simulation results indicate that the overhead of security mechanisms in AEGIS is reasonable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Suh, G. Edward; Clarke, Dwaine; Gassend, Blaise; van Dijk, Marten; Devadas, Srinivas",2023-03-30T15:53:37Z,2023-03-29T15:36:51Z; 2023-03-30T15:51:48Z; 2023-03-30T15:53:37Z,2003,https://hdl.handle.net/1721.1/149977.3,MIT-LCS-TR-883b,The AEGIS Processor Architecture for Tamper-Evident and Tamper-Resistant Processing,"We describe the architecture of the AEGIS processor which can be used to build computing systems secure against both physical and software attacks. AEGIS assumes that the operating system and all components external to it, such as memory, are untrusted. AEGIS provides tamper-evident, authenticated environments in which any physical or software tampering by the adversary is guaranteed to be detected, and private and authenticated, tamper-resistant environments where additionally the adversary is unable to obtain any information about software or data by tampering with, or otherwise observing, system operation. AEGIS enables many applications, such as commercial grid computing, software licensing, and digital rights management. We present a new encryption/decryption method that successfully hides a significant portion of encryption/decryption latency, in comparison to a conventional direct encryption scheme. Efficient memory encryption and integrity verification enable the implementation of a secure computing system with the only trusted component being a single-chip AEGIS CPU. Detailed simulation results indicate that the performance overhead of security mechanisms in AEGIS is reasonable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Suh, G. Edward; Clarke, Dwaine; Gassend, Blaise; van Dijk, Marten; Devadas, Srinivas",2023-03-30T16:31:13Z,2023-03-29T15:36:51Z; 2023-03-30T15:51:48Z; 2023-03-30T15:53:37Z; 2023-03-30T16:31:13Z,2003,https://hdl.handle.net/1721.1/149977.4,MIT-LCS-TR-883c;,The AEGIS Processor Architecture for Tamper-Evident and Tamper-Resistant Processing,"We describe the architecture for a single-chip AEGIS processor which can be used to build computing systems secure against both physical and software attacks. Our architecture assumes that all components external to the processor, such as memory, are untrusted. We show two different implementations. In the first case, the core functionality of the operating system is trusted and implemented in a security kernel. We also describe a variant implementation assuming an untrusted operating system. AEGIS provides users with  tamper-evident, authenticated environments in which any physical or software tampering by an adversary is guaranteed to be detected, and private and authenticated tamper-resistant environments where additionally the adversary is unable to obtain any information about software or data by tampering with, or otherwise observing, system operation. AEGIS enables many applications, such as commercial grid computing, secure mobile agents, software licensing, and digital rights management. We also present a new encryption/decryption method that successfully hides a significant portion of encryption/decryption latency, in comparison to a conventional direct encryption scheme. Efficient memory encryption and integrity verification enable the implementation of a secure computing system with the only trusted component being a single-chip AEGIS CPU. Preliminary simulation results indicate that the overhead of security mechanisms in AEGIS is reasonable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lynch, Nancy A.; Segala, Roberto; Vaandrager, Frits",2023-03-30T15:36:48Z,2023-03-29T15:34:34Z; 2023-03-30T15:32:47Z; 2023-03-30T15:36:48Z,2003-01,https://hdl.handle.net/1721.1/149930.3,MIT-LCS-TR-827c,Hybrid I/O Automata*,"Hybrid systems are systems that exhibit a combination of discrete and continuous behavior. Typical hybrid systems include computer components, which operate in discrete program steps, and real-world components, whose behavior over time intervals evolves according to physical constraints. Important examples of hybrid systems include automated transportation systems, robotics systems, process control systems, systems of embedded devices, and mobile computing systems. Such systems can be very complex, and very difficult to describe and analyze. This paper presents the Hybrid Input/Output Automaton (HIOA) modeling framework, a basic mathematical framework to support description and analysis of hybrid systems. An important feature of this model is its support for decomposing hybrid system descriptions. In particular, the framework includes a notion of external behavior for a hybrid I/O automaton, which captures its discrete and continuous interactions with its environment. The framework also defines what it means for one HIOA to implement another, based on an inclusion relationship between their external behavior sets, and defines a notion of simulation, which provides a sufficient condition for demonstrating implementation relationships. The framework also includes a composition operation for HIOAs, which respects external behavior, and a notion of receptiveness, which implies that an HIOA does not block the passage of time. The framework is intended to support analysis methods from both computer science and control theory. This work is a simplification of an earlier version of the HIOA model [49, 50]. The main simplification in the new model is a clearer separation between the mechanisms used to model discrete and continuous interaction between components. In particular, the new model removes the dual use of external variables for discrete and continuous interaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Rinard, Martin",2023-03-29T15:36:43Z,2023-03-29T15:36:43Z,2003-01,https://hdl.handle.net/1721.1/149974,MIT-LCS-TR-879,On the Theory of Structural Subtyping,"We show that the first-order theory of structural subtyping of non-recursive types is decidable.   Let Sigma be a language consisting of function symbols (representing type constructors) and C a decidable structure in the relational language L containing a binary relation <. C represents primitive types; < represents a subtype ordering.  We introduce the notion of Sigma-term-power of C, which generalizes the structure arising in structural subtyping.  The domain of the Sigma-term-power of C is the set of Sigma-terms over the set of elements of C.   We show that the decidability of the first-order theory of C implies the decidability of the first-order theory of the Sigma-term-power of C.  This result implies the decidability of the first-order theory of structural subtyping of non-recursive types.   Our decision procedure is based on quantifier elimination and makes use of quantifier elimination for term algebras and Feferman-Vaught construction for products of decidable structures.   We also explore connections between the theory of structural subtyping of recursive types and monadic second-order theory of tree-like structures.  In particular, we give an embedding of the monadic second-order theory of infinite binary tree into the first-order theory of structural subtyping of recursive types.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Suh, G. Edward; Clarke, Dwaine; Gassend, Blaise; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:36:51Z,2023-03-29T15:36:51Z,2003-01,https://hdl.handle.net/1721.1/149977,MIT-LCS-TR-883,The AEGIS Processor Architecture for Tamper-Evident and Private Tamper-Resistant Processing,"We describe the architecture of the AEGIS processor which can be used to build computing systems secure against both physical and software attacks. AEGIS assumes that the operating system and all components external to it, such as memory, are untrusted. AEGIS provides tamper-evident, authenticated environments in which any physical or software tampering by the adversary is guaranteed to be detected, and private and authenticated, tamper-resistant environments where additionally the adversary is unable to obtain any information about software or data by tampering with, or otherwise observing, system operation. AEGIS enables many applications, such as commercial grid computing, software licensing, and digital rights management. We present a new encryption/decryption method that successfully hides a significant portion of encryption/decryption latency, in comparison to a conventional direct encryption scheme. Efficient memory encryption and integrity verification enable the implementation of a secure computing system with the only trusted component being a single-chip AEGIS CPU. Detailed simulation results indicate that the performance overhead of security mechanisms in AEGIS is reasonable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kaminsky, Michael; Peterson, Eric; Fu, Kevin; Mazières, David; Kaashoek, M. Frans",2023-03-29T15:36:55Z,2023-03-29T15:36:55Z,2003-01,https://hdl.handle.net/1721.1/149978,MIT-LCS-TR-884,"REX: Secure, modular remote execution through file descriptor passing","The ubiquitous SSH package has demonstrated the importance of   secure remote login and execution.  This paper presents a new system,   REX, designed to provide remote login and execution in the context of   the SFS secure distributed file system.  REX departs from traditional   remote login design and is built around two main mechanisms---file   descriptor passing and a user agent process.        File descriptor passing allows REX to be split into several   smaller pieces; privileged code can run as its own process to   provide enhanced security guarantees.  REX also emulates secure file   descriptor passing over network connections, allowing users to build   extensions to REX outside of the core REX software.        REX uses and extends SFS's agent mechanism to provide a   transparent distributed computing environment to users.  The   agent stores private keys, server nicknames, and other per-user   configuration state; REX makes the SFS agent available to programs   that it executes on remote machines.        We have an implementation of REX and demonstrate that its   flexibility does not come at the cost of performance.  Initial REX   connections are comparable to those of SSH in speed, while subsequent   connections are much faster because REX exploits the SFS agent to   cache connection state to avoid costly public-key operations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Krashinsky, Ronny",2023-03-29T15:36:49Z,2023-03-29T15:36:49Z,2003-01,https://hdl.handle.net/1721.1/149976,MIT-LCS-TR-882,Efficient Web Browsing for Mobile Clients using HTTP Compression,"Efficient web browsing on mobile computers presents a unique challenge.  These machines are different from other classes of client computers since they have relatively low-bandwidth connections and they are battery-powered and therefore limited by their energy consumption.  However, they tend to interact with the same servers for the delivery of web content.  This project investigates optimizing the final critical link between a mobile client and a stationary base station by compressing HTTP request and response messages.  Using a split proxy design, compression of individual request messages reduces bandwidth by 26% to 34% across a variety of benchmark traces, and applying compression to response messages yields savings of 59% to 82% of the compressible data.  Higher compression rates are achieved by using streaming compression algorithms to compress the streams of request and response messages.  In this case, the bandwidth for requests sees an order of magnitude improvement, and the response stream obtains additional savings of 7% to 25% on top of the savings achieved with per-response compression.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Srebro, Nathan; Jaakkola, Tommi",2004-10-08T20:38:40Z,2004-10-08T20:38:40Z,2003-01-15,http://hdl.handle.net/1721.1/6708,AIM-2003-001,Generalized Low-Rank Approximations,"We study the frequent problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving {\\em weighted} low rank approximation problems, which, unlike simple matrix factorization problems, do not admit a closed form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low rank representation, and extend the formulation to non-Gaussian noise models such as classification (collaborative filtering).",AIM-2003-001,10 p.; 2061103 bytes; 911431 bytes,application/postscript; application/pdf,en_US,AI; svd pca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ostrovsky, Rafail; Rackoff, Charles; Smith, Adam",2023-03-29T15:36:58Z,2023-03-29T15:36:58Z,2003-02,https://hdl.handle.net/1721.1/149979,MIT-LCS-TR-887,Efficient Consistency Proofs on a Committed Database,"A consistent query protocol allows a database owner to publish a very short string c which commits her to a particular database D with special consistency property (i.e., given c, every allowable query has unique and well-defined answer with respect to D.)  Moreover, when a user makes a query, any server hosting the database can answer the query, and provide a very short proof P that the answer is well-defined, unique, and consistent with c (and hence with D).  One potential application of consistent query protocols is for guaranteeing the consistency of many replicated copies of D---the owner can publish c, and users can verify the consistency of a query to some copy of D by making sure P is consistent with c.  This strong guarantee holds even for owners who try to cheat, while creating c.  The task of consistent query protocols was originally proposed for membership queries by Micali and Rabin, and subsequently and independently, by Kilian. In this setting a server can prove to a client whether or not a given key is present or not in a database, based only on a short public commitment c.  We strengthen their results in several ways. For membership queries, we improve the communication complexity; more importantly, we provide protocols for more general types of queries and more general relational databases.  For example, we consider databases in which entries have several keys and where we allow range queries (e.g. we allow a client to ask for all entries within a certain age range and a certain salary range).   Towards this goal, we introduce query algorithms with certain inherent robustness properties---called data-robust algorithms---and show how this robustness can be achieved. In particular, we illustrate our general technique by constructing an efficient data-robust algorithm for proving consistency of orthogonal range queries (a particular case of a ``join''query).  The server's proof convinces the client not only that all the matching entries provided are in D, but also that no others are present.  Our guarantees hold even if the answer is the empty set.  In the case of one-dimensional range queries we also show a new data-hiding technique---called explicit hashing---which allows us to a execute consistent query protocol P and at the same time protect the privacy of all other information in the database efficiently. In particular, we avoid the NP reductions required in a generic zero-knowledge proof.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Devadas, Srinivas","Gassend, Blaise",2023-03-29T15:36:46Z,2023-03-29T15:36:46Z,2003-02,https://hdl.handle.net/1721.1/149975,MIT-LCS-TR-881,Physical Random Functions,"In general, secure protocols assume that participants are able to maintain secret key information. In practice, this assumption is often incorrect as an increasing number of devices are vulnerable to physical attacks.  Typical examples of vulnerable devices are smartcards and Automated Teller Machines.   To address this issue, Physical Random Functions are introduced. These are Random Functions that are physically tied to a particular device. To show that Physical Random Functions solve the initial problem, it must be shown that they can be made, and that it is possible to use them to provide secret keys for higher level protocols. Experiments with Field Programmable Gate Arrays are used to evaluate the feasibility of Physical Random Functions in silicon.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Adler, Aaron D.",2004-10-20T20:31:48Z,2004-10-20T20:31:48Z,2003-02-01,http://hdl.handle.net/1721.1/7103,AITR-2003-004,Segmentation and Alignment of Speech and Sketching in a Design Environment,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",AITR-2003-004,193 p.; 34430522 bytes; 46149955 bytes,application/postscript; application/pdf,en_US,AI; sketch; design; multimodal; disambiguation; segmentation; alignment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kim, Philip Mjong-Hyon Shin",2004-10-20T20:31:34Z,2004-10-20T20:31:34Z,2003-02-05,http://hdl.handle.net/1721.1/7099,AITR-2003-001,"Understanding Subsystems in Biology through Dimensionality Reduction, Graph Partitioning and Analytical Modeling","Biological systems exhibit rich and complex behavior through the orchestrated interplay of a large array of components. It is hypothesized that separable subsystems with some degree of functional autonomy exist; deciphering their independent behavior and functionality would greatly facilitate understanding the system as a whole. Discovering and analyzing such subsystems are hence pivotal problems in the quest to gain a quantitative understanding of complex biological systems.  In this work, using approaches from machine learning, physics and graph theory, methods for the identification and analysis of such subsystems were developed. A novel methodology, based on a recent machine learning algorithm known as non-negative matrix factorization (NMF), was developed to discover such subsystems in a set of large-scale gene expression data. This set of subsystems was then used to predict functional relationships between genes, and this approach was shown to score significantly higher than conventional methods when benchmarking them against existing databases. Moreover, a mathematical treatment was developed to treat simple network subsystems based only on their topology (independent of particular parameter values). Application to a problem of experimental interest demonstrated the need for extentions to the conventional model to fully explain the experimental data.  Finally, the notion of a subsystem was evaluated from a topological perspective. A number of different protein networks were examined to analyze their topological properties with respect to separability, seeking to find separable subsystems. These networks were shown to exhibit separability in a nonintuitive fashion, while the separable subsystems were of strong biological significance. It was demonstrated that the separability property found was not due to incomplete or biased data, but is likely to reflect biological structure.",AITR-2003-001,124 p.; 14826182 bytes; 3860263 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chklovski, Timothy",2004-10-20T20:31:36Z,2004-10-20T20:31:36Z,2003-02-12,http://hdl.handle.net/1721.1/7100,AITR-2003-002,Using Analogy to Acquire Commonsense Knowledge from Human Contributors,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",AITR-2003-002,173 p.; 4895337 bytes; 1809437 bytes,application/postscript; application/pdf,en_US,AI; knowledge acquisition; knowledge capture; analogy; natural language; reasoning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Peshkin, Leonid",2004-10-20T20:31:39Z,2004-10-20T20:31:39Z,2003-02-14,http://hdl.handle.net/1721.1/7101,AITR-2003-003,Reinforcement Learning by Policy Search,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",AITR-2003-003,144 p.; 26942112 bytes; 1735254 bytes,application/postscript; application/pdf,en_US,AI; POMDP; policy search; adaptive systems; reinforcement learning; adaptive behavior,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Steck, Harald; Jaakkola, Tommi S.",2004-10-08T20:38:42Z,2004-10-08T20:38:42Z,2003-02-25,http://hdl.handle.net/1721.1/6709,AIM-2003-002,(Semi-)Predictive Discretization During Model Selection,"In this paper, we present an approach to discretizing  multivariate continuous data while learning the  structure of a graphical model. We derive the joint  scoring function from the principle of predictive  accuracy, which inherently ensures the optimal trade-off between goodness of fit and model complexity  (including the number of discretization levels). Using  the so-called finest grid implied by the data, our scoring  function depends only on the number of data points in  the various discretization levels. Not only can it be  computed efficiently, but it is also independent of the  metric used in the continuous space. Our experiments  with gene expression data show that discretization  plays a crucial role regarding the resulting network  structure.",AIM-2003-002,15 p.; 4299414 bytes; 910469 bytes,application/postscript; application/pdf,en_US,AI; Discretization; Graphical models,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Geiger, Gadi; Ezzat, Tony; Poggio, Tomaso",2004-10-20T21:05:09Z,2004-10-20T21:05:09Z,2003-02-28,http://hdl.handle.net/1721.1/7275,AIM-2003-003; CBCL-224,Perceptual Evaluation of Video-Realistic Speech,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",AIM-2003-003; CBCL-224,17 p.; 1515741 bytes; 1358361 bytes,application/postscript; application/pdf,en_US,AI; visual speech; speech animation; face animation; image morphing; lip reading,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Donovan, Alan; Ernst, Michael D.",2023-03-29T15:37:03Z,2023-03-29T15:37:03Z,2003-03,https://hdl.handle.net/1721.1/149981,MIT-LCS-TR-889,Inference of Generic Types in Java,"Future versions of Java will include support for parametric polymorphism, or generic classes.  This will bring many benefits to Java programmers, not least because current Java practise makes heavy use of pseudo-generic classes.  Such classes (for example, those in package java.util) have logically generic specifications and documentation, but the type system cannot prove their patterns of use to be safe.   This work aims to solve the problem of automatic translation of Java source code into Generic Java (GJ) source code.  We present two algorithms that together can be used to translate automatically a Java source program into a semantically-equivalent GJ program with generic types.   The first algorithm infers a candidate generalisation for any class, based on the methods of that class in isolation.  The second algorithm analyses the whole program; it determines a precise parametric type for every value in the program.  Optionally, it also refines the generalisations produced by the first analysis as required by the patterns of use of those classes in client code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Zhao, Jianjun; Rinard, Martin",2023-03-29T15:37:09Z,2023-03-29T15:37:09Z,2003-03,https://hdl.handle.net/1721.1/149983,MIT-LCS-TR-891,System Dependence Graph Construction for Aspect-Oriented Programs,"We extend previous dependence-based representations called system dependence graphs (SDGs) to represent aspect-oriented programs and present an SDG construction algorithm. This algorithm first constructs a module dependence graph (MDG) for each piece of advice, introduction, and method in aspects and classes. It then uses existing techniques to connect the MDGs at call sites to form a partial SDG. Finally, it weaves the MDG for each piece of advice into the partial SDG for those methods whose behavior may be affected by the advice. The result is the complete SDG. Our SDGs capture the additional structure present in many aspect-oriented features such as join points, advice, introduction, aspects, and aspect inheritance, and various types of interactions between aspects and classes. They also correctly reflect the semantics of aspect-oriented concepts such as advice precedence, introduction scope, and aspect weaving. SDGs therefore provide a solid foundation for the further analysis of aspect-oriented programs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chen, Benjie; Gil, Thomer M.; Muthitacharoen, Athicha; Morris, Robert T.",2023-03-29T15:37:00Z,2023-03-29T15:37:00Z,2003-03,https://hdl.handle.net/1721.1/149980,MIT-LCS-TR-888,Building Data Structures on Untrusted Peer-to-Peer Storage with Per-participant Logs,"L* is a technique for building multi-user distributed data structures out of untrusted peer-to-peer distributed hash tables (DHTs). L* uses multiple logs, one log per participant, to store changes to the data structure. Each participant finds data by consulting all logs, but performs modifications by appending only to its own log. This dencentralized structure allows L* to maintain meta-data consistency without locking and to isolate users' changes from each other, an appropriate arrangement for unreliable users. Applications use L* to maintain consistent data structures. L* interleaves multiple logs deterministically so that decentralized clients can agree on the order of completed operations, even if those operations where issued concurrently. When the data structure is quiescent, L* guarantees that clients agree on the state of the data structure. L* optionally provides mutual exclusion for applications that need to ensure atomicity for multi-step operations. The Ivy file system, built on top of L*, demonstrates that L*'s consistency guarantees are useful and can be used and implemented efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gilbert, Seth; Lynch, Nancy A.; Shvartsman, Alexander A.",2023-03-29T15:37:05Z,2023-03-29T15:37:05Z,2003-03,https://hdl.handle.net/1721.1/149982,MIT-LCS-TR-890,RAMBO II: Rapidly Reconfigurable Atomic Memory for Dynamic Networks,"Future civilian rescue and military operations will depend on a complex system of communicating devices that can operate in highly dynamic environments. In order to present a consistent view of a complex world, these devices will need to maintain data objects with atomic (linearizable) read/write semantics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Henry, Hoffman; Strumpen, Volker; Agarwal, Anant",2023-03-29T14:43:07Z,2023-03-29T14:43:07Z,2003-03,https://hdl.handle.net/1721.1/149323,MIT-LCS-TM-636,Stream Algorithms and Architecture,"Wire-exposed, programmable microarchitectures including Trips [11]], Smart Memories [8], and Raw [13] offer an opportunity to schedule instruction execution and data movement explicitly. This paper proposes stream algorithms, which, along with a decoupled systolic architecture, provide an excellent match for the physical and technological constraints of single-chip tiles architectures. Stream algorithms enable programmed systolic computations for different problem sizes, without incurring the cost of memory accesses. To that end, we decouple memory accesses from computation and move the memory accesses off the critical path. By structuring computations in systolic phases, and deferring memory accesses to dedicated memory processors, stream algorithms can solve many regular problems with varying sizes on a constant-sized tiled array. Contrary to common sense, the compute efficiency of stream algorithms increases as we increase the number of processing elements. In particular, we show that the compute efficiency of stream algorithms can approach 100% asymptotically, that is for large numbers of processors and appropriate problem size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Jarudi, Izzat N.; Sinha, Pawan",2004-10-20T21:05:07Z,2004-10-20T21:05:07Z,2003-03-01,http://hdl.handle.net/1721.1/7274,AIM-2003-004; CBCL-225,Relative Contributions of Internal and External Features to Face Recognition,"The central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. Notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. Past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. The results obtained are  therefore most relevant for understanding the  identification of faces at close range. However, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. Here, we report experiments  designed to address this issue. Our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  While we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. These  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",AIM-2003-004; CBCL-225,12 p.; 1448956 bytes; 677551 bytes,application/postscript; application/pdf,en_US,AI; Face recognition; features; low resolution; degraded images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2004-10-08T20:38:43Z,2004-10-08T20:38:43Z,2003-03-17,http://hdl.handle.net/1721.1/6710,AIM-2003-007,Leveraging Learning and Language Via Communication Bootstrapping,"In a Communication Bootstrapping system, peer  components with different perceptual worlds invent symbols and syntax  based on correlations between their percepts. I propose that  Communication Bootstrapping can also be used to acquire functional  definitions of words and causal reasoning knowledge. I illustrate this  point with several examples, then sketch the architecture of a  system in progress which attempts to execute this task.",AIM-2003-007,13 p.; 2403922 bytes; 651138 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.; Rubin, Mark A.",2004-10-08T20:38:46Z,2004-10-08T20:38:46Z,2003-03-19,http://hdl.handle.net/1721.1/6711,AIM-2003-005,Context-Based Vision System for Place and Object Recognition,"While navigating in an environment, a vision system  has to be able to recognize where it is and what the  main objects in the scene are. In this paper we present  a context-based vision system for place and object  recognition. The goal is to identify familiar locations  (e.g., office 610, conference room 941, Main Street), to  categorize new environments (office, corridor, street)  and to use that information to provide contextual priors  for object recognition (e.g., table, chair, car, computer). We present a low-dimensional global image representation that provides  relevant information for place recognition and  categorization, and how such contextual information  introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60  locations (indoors and outdoors) and to suggest the  presence and locations of more than 20 different object  types. The algorithm has been integrated into a mobile system that provides real-time feedback to the  user.",AIM-2003-005,9 p.; 7141251 bytes; 2104025 bytes,application/postscript; application/pdf,en_US,AI; context-based vision; place recognition; object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Li, Ji; Liu, Haiyang; Sollins, Karen",2023-03-29T14:43:10Z,2023-03-29T14:43:10Z,2003-04,https://hdl.handle.net/1721.1/149324,MIT-LCS-TM-637,Scalable Packet Classification Using Bit Vector Aggregating and Folding,"Packet classification is a central function for a number of network applications, such as routing and firewalls. Most existing algorithms for packet classification scale poorly in either time or space when the database size grows. The scalable algorithm Aggregated Bit Vector (ABV) is an improvement on the Lucent bit vector scheme (BV), but has some limitations. Our algorithm, Aggregated and Folded Bit Vector (AFBV), seeks to reduce false matches while keeping the benefits of bit vector aggregation and avoiding rule rearrangement. It combines bit vector aggregation and folding to achieve this goal. Experiments showed that our algorithm outperforms both the BV and ABV schemes in synthetically generated databases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Priyantha, Nissanka B.; Balakrishnan, Hari; Demaine, Erik; Teller, Seth",2023-03-29T15:37:11Z,2023-03-29T15:37:11Z,2003-04,https://hdl.handle.net/1721.1/149984,MIT-LCS-TR-892,Anchor-free Distributed Localization in Sensor Networks,"Many sensor network applications require that each node's sensor stream be annotated with its physical location in some common coordinate system. Manual measurement and configuration methods for obtaining location don't scale and are error-prone, and equipping sensors with GPS is often expensive and does not work in indoor and urban deployments. Sensor networks can therefore benefit from a self-configuring method where nodes cooperate with each other, estimate local distances to their neighbors, and converge to a consistent coordinate assignment. This paper describes a fully decentralized algorithm called AFL (Anchor-Free Localization) where nodes start from a random initial coordinate assignment and converge to a consistent solution using only local node interactions. The key idea in AFL is fold-freedom, where nodes first configure into a topology that resembles a scaled and unfolded version of the true configuration, and then run a force-based relaxation procedure. We show using extensive simulations under a variety of network sizes, node densities, and distance estimation errors that our algorithm is superior to previously proposed methods that incrementally compute the coordinate of nodes in the network, in terms of its ability to computer correct coordinates under a wider variety of conditions and its robuestness to measurement errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hull, Bret; Jamieson, Kyle; Balakrishnan, Hari",2023-03-29T15:37:36Z,2023-03-29T15:37:36Z,2003-04,https://hdl.handle.net/1721.1/149994,MIT-LCS-TR-909,Bandwidth Management in Wireless Sensor Networks,"Wireless sensor networks are often used in monitoring and control applications, where software running on generalpurpose computers ÔøΩpullÔøΩ information from remote sensors and ÔøΩpushÔøΩ actuations into the network. The sensors themselves form a multihop wireless network communicatingwith one or more sensor access points (SAPs) that interface between application software and the sensor network. This paper addresses the problem of managing wireless network bandwidth and improving network capacity in a sensor network deployed as a shared infrastructure, concurrently used by different applications. Our bandwidth management architecture incorporates three ideas: first, we develop a simple rule system that allows applications and the network administrator to specify how traffic generated by sensors should be treated by the sensor network. Each rule is a function that maps a sensor data type and generated value to a transmission rate and a traffic class. Second, we show how using multiple SAPs and SAP selection method that considers packet loss probabilities, path load, and path lengths improves the capacity of the network and the performance of individual sensor streams. Third, we show that hopby- hop flow control, rather than end-to-end congestion control, is a better way to cope with the nature of sensor network traffic and avoids unnecessary packet losses that waste valuable wireless network bandwidth. Our experimental results from a 40-node indoor wireless sensor testbed show that these three techniques are simple to implement and allow scarce network bandwidth to be used efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Alvarado, Christine; Teevan, Jaime; Ackerman, Mark S.; Karger, David",2004-10-08T20:38:49Z,2004-10-08T20:38:49Z,2003-04-15,http://hdl.handle.net/1721.1/6713,AIM-2003-006,Surviving the Information Explosion: How People Find Their Electronic Information,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",AIM-2003-006,9 p.; 980296 bytes; 422112 bytes,application/postscript; application/pdf,en_US,AI; information seeking; search; orienteering; context; Semantic Web,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2004-10-08T20:38:47Z,2004-10-08T20:38:47Z,2003-04-15,http://hdl.handle.net/1721.1/6712,AIM-2003-011,Persistent Nodes for Reliable Memory in Geographically Local Networks,"A Persistent Node is a redundant distributed mechanism for storing a key/value pair reliably in a geographically local network. In this paper, I develop a method of establishing Persistent Nodes in an amorphous matrix. I address issues of construction, usage, atomicity guarantees and reliability in the face of stopping failures. Applications include routing, congestion control, and data storage in gigascale networks.",AIM-2003-011,19 p.; 5503051 bytes; 1849500 bytes,application/postscript; application/pdf,en_US,AI; robust atomic distributed amorphous,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen; Shakhnarovich, Gregory; Darrell, Trevor",2004-10-08T20:38:51Z,2004-10-08T20:38:51Z,2003-04-17,http://hdl.handle.net/1721.1/6714,AIM-2003-008,Inferring 3D Structure with a Statistical Image-Based Shape Model,"We present an image-based approach to infer 3D  structure parameters using a probabilistic ""shape+structure''  model. The 3D shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. Bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. We  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. Model matching and  parameter inference are done entirely in the image domain and require no  explicit 3D construction. Our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. Using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3D  locations of 19 joints on the body based on observed silhouette  contours from real images.",AIM-2003-008,17 p.; 6362014 bytes; 9371703 bytes,application/postscript; application/pdf,en_US,AI; 3D structure; statistical shape model; multi-view imagery; pose estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Christoudias, Chris Mario; Morency, Louis-Philippe; Darrell, Trevor",2004-10-08T20:38:54Z,2004-10-08T20:38:54Z,2003-04-18,http://hdl.handle.net/1721.1/6716,AIM-2003-010,Light Field Morphable Models,"Statistical shape and texture appearance models are  powerful image representations, but previously had  been restricted to 2D or simple 3D shapes. In this paper  we present a novel 3D morphable model based on  image-based rendering techniques, which can  represent complex lighting conditions, structures, and  surfaces. We describe how to construct a manifold of  the multi-view appearance of an object class using light  fields and show how to match a 2D image of an object  to a point on this manifold. In turn we use the  reconstructed light field to render novel views of the  object. Our technique overcomes the limitations of  polygon based appearance models and uses light  fields that are acquired in real-time.",AIM-2003-010,1375810 bytes; 716555 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shakhnarovich, Gregory; Viola, Paul; Darrell, Trevor",2004-10-08T20:38:53Z,2004-10-08T20:38:53Z,2003-04-18,http://hdl.handle.net/1721.1/6715,AIM-2003-009,Fast Pose Estimation with Parameter Sensitive Hashing,"Example-based methods are effective for parameter  estimation problems when the underlying system is simple or the  dimensionality of the input is low. For complex and high-dimensional  problems such as pose estimation, the number of required examples and the  computational complexity rapidly becme prohibitively high. We  introduce a new algorithm that learns a set of hashing functions that  efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for  locality-sensitive hashing, which finds approximate neighbors in time  sublinear in the number of examples. This method depends critically  on the choice of hash functions; we show how to find the set of hash  functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and  accurately estimate the articulated pose of human figures from a large  database of example images.",AIM-2003-009,12 p.; 5030222 bytes; 6836715 bytes,application/postscript; application/pdf,en_US,AI; parameter estimation; nearest neighbor; locally weighted learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Jamieson, Kyle; Balakrishnan, Hari; Tay, Y.C.",2023-03-29T15:37:15Z,2023-03-29T15:37:15Z,2003-05,https://hdl.handle.net/1721.1/149985,MIT-LCS-TR-894,Sift: A MAC Protocol for Event-Driven Wireless Sensor Networks,"Nodes in sensor networks often encounter spatially-correlated contention, where multiple nodes in the same neighborhood all sense an event they need to transmit information about. Furthermore, in many sensor network applications, it is sufficient if a subset of the nodes that observe the same event report it. We show that traditional carrier-sense multiple access (CSMA) protocols like 802.11 do not handle the first constraint adequately, and do not take advantage of the second property, leading to degraded latency and throughput as the network scales in size.   We present Sift, a medium access protocol for wireless sensor networks designed with the above observations in mind. Sift is a randomized CSMA protocol, but unlike previous protocols, does not use a time-varying contention window from which a node randomly picks a transmission slot. Rather, to reduce the latency for the delivery of event reports, Sift uses a fixed-size contention window and a carefully-chosen, non-uniform probability distribution of transmitting in each slot within the window. We show using simulations that Sift can offer up to a 7-fold latency reduction compared to 802.11 as the size of the sensor network scales up to 500 nodes. We then analytically prove bounds on the best latency achievable by a decentralized CSMA-based MAC protocol for sensor networks where one report of each event is enough, and show that Sift comes close to meeting this bound.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sollins, Karen R.","Li, Ji",2023-03-29T15:37:17Z,2023-03-29T15:37:17Z,2003-05,https://hdl.handle.net/1721.1/149986,MIT-LCS-TR-897,Improving Application-level Network Services with Regions,"The underlying premise of the Region Project is that the concept of a region should be a new architecture capability in networking. A region is an entity that encapsulates and implements scoping, grouping, subdividing, and crossing boundaries of sets of entities. It is a powerful tool for managing the increasingly complex demands on the Internet and its successors, and thus should be made into an explicit, first-class component of the network architecture. Autonomous Systems and peer-to-peer networks can be viewed as two simple forms of existing regions. In this work, we explore the utility of informing members in one region of the membership of those same entities in different regions. Specifically, we improve peer-to-peer networks with information derived from Autonomous Systems. This thesis makes three notable contributions. Firstly, we provide a general peer-to-peer simulation framework for different optimization schemes. Secondly, we achieve performance improvements in the lookup, caching and replication of peer-to-peer system. Finally, we enhance our overall understanding of regions through the simulation, as well as their utilities to improve system performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Clarke, Dwaine; Devadas, Srinivas; van Dijk, Marten; Gassend, Blaise; Suh, G. Edward",2023-03-29T15:37:19Z,2023-03-29T15:37:19Z,2003-05,https://hdl.handle.net/1721.1/149987,MIT-LCS-TR-899,Incremental Multiset Hash Functions and their Application to Memory Integrity Checking,"We introduce a new cryptographic tool: multiset hash functions. Unlike standard hash functions which take strings as input, multiset hash functions operate on multisets (or sets). They map multisets of arbitrary finite size to strings (hashes) of fixed length. They are incremental in that, when new members are added to the multiset, the hash can be updated in time proportional to the change. The functions may be multiset-collision resistant in that it is diÔøΩcult to find two multisets which produce the same hash, or just set-collision resistant in that it is diÔøΩcult to find a set and a multiset which produce the same hash. In particular, we introduce four multiset hash functions, each with its own advantages. MSet-XOR-Hash uses the XOR operation and is very eÔøΩcient; however, it uses a secret key and is only set-collision resistant. MSet-Add-Hash uses addition modulo a large integer and, thus, is slightly less eÔøΩcient than MSet-XOR-Hash; MSet-Add-Hash also uses a secret key but it is multiset-collision resistant. MSet-Mu-Hash uses finite field arithmetic and is not as eÔøΩcient as the other two hash functions; however, MSet-Mu-Hash is multiset-collision resistant, and unlike the other two hash functions, does not require a secret key. MSet-VAdd-Hash is more eÔøΩcient than MSet-Mu-Hash; it is also multiset-collision resistant, and does not use a secret key, but the hashes it produces are significantly longer than the hashes of the other functions. The proven security of MSet-XOR-Hash and MSet-Add-Hash is quantitative. We reduce the hardness of finding collisions to the hardness of breaking the underlying pseudorandom functions. The proven security of MSet-Mu-Hash is in the random oracle model and is based on the hardness of the discrete logarithm problem. The proven security of MSet-VAdd-Hash is also in the random oracle model and is based on the hardness of the worst-case shortest vector problem. We demonstrate how set-collision resistant multiset hash functions make an existing oÔøΩine memory integrity checker secure against active adversaries. We improve on this checker such that it can use smaller time stamps without increasing the frequency of checks. The improved checker uses multiset-collision resistant hash functions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demaine, Erik D.; Hajiaghayi, Mohammad Taghi",2023-03-29T15:37:24Z,2023-03-29T15:37:24Z,2003-05,https://hdl.handle.net/1721.1/149989,MIT-LCS-TR-903,Equivalence of Local Treewidth and Linear Local Treewidth and its Algorithmic Applications,"We solve an open problem posed by Eppstein in 1995 and re-enforced by Grohe concerning locally bounded treewidth in minor-closed families of graphs. A graph has bounded local treewidth if the subgraph induced by vertices within distance r of any vertex has treewidth bounded by a function of r (not n). Eppstein characterized minor-closed families of graphs with bounded local treewidth as precisely minor-closed families that minor-exclude an apex graph, where an apex graph has one vertex whose removal leaves a planar graph. In particular, Eppstein showed that all apex-minor-free graphs have bounded local treewidth, but his bound is doubly exponential in r, leaving open whether a tighter bound could be obtained.  We improve this doubly exponential bound to a linear bound, which is optimal. In particular, any minor-closed graph family with bounded local treewidth has linear local treewidth. Our bound generalizes previously known linear bounds for special classes of graphs proved by several authors.  As a consequence of our result, we obtain substantially faster polynomial-time approximation schemes for a broad class of problems in apex-minor-free graphs, improving the running time from 2^(2^(2^O(1/epsilon))) n^O(1) to 2^O(1/epsilon) n^O(1).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kiriansky, Vladimir; Bruening, Derek; Amarasinghe, Saman",2023-03-29T14:43:12Z,2023-03-29T14:43:12Z,2003-05,https://hdl.handle.net/1721.1/149325,MIT-LCS-TM-638,Execution Model Enforcement Via Program Shepherding,"Nearly all security attacks have one thing in common: they coerce the target program into performing actions that it was never intended to perform.  In short, they violate the program's execution model. The execution model encompasses the Application Binary Interface (ABI), higher-level specifications from the program's source programming language, and components specific to the program --- for example, which values a particular function pointer may take.  If this execution model were enforced, and only program actions that the programmer intended were allowed, a majority of current security holes would be closed.   In this paper, we employ program shepherding[26] to enforce a program's execution model.  Program shepherding monitors control flow in order to enforce a security policy.  We use static and dynamic analyses to automatically build a custom security policy for a target program which specifies the program's execution model.  We have implemented our analyses in the DynamoRIO [5] runtime code modification system.  The resulting system imposes minimal or no performance overhead, operates on unmodified native binaries, and requires no special hardware or operating system support.  Our static analyses require source code access but not recompilation.  The analysis process requires no user interaction, but is able to build a strict enough policy to prevent all deviations from the program's control flow graph and nearly all violations of the calling convention, greatly reducing the possibility of an unintended program action.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Moh, Chuang-Hue",2023-03-29T15:37:22Z,2023-03-29T15:37:22Z,2003-05,https://hdl.handle.net/1721.1/149988,MIT-LCS-TR-901,Snapshots in a Distributed Persistent Object Storage System,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Morency, Louis-Philippe",2004-10-20T20:31:42Z,2004-10-20T20:31:42Z,2003-05-01,http://hdl.handle.net/1721.1/7102,AITR-2003-006,Stereo-Based Head Pose Tracking Using Iterative Closest Point and Normal Flow Constraint,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",AITR-2003-006,60 p.; 5276045 bytes; 2896854 bytes,application/postscript; application/pdf,en_US,AI; Head pose estimation; Stereo processing; Cursor control; 3D model acquisition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shih, Lawrence; Karger, David",2004-10-08T20:38:58Z,2004-10-08T20:38:58Z,2003-05-01,http://hdl.handle.net/1721.1/6719,AIM-2003-013,Learning Classes Correlated to a Hierarchy,"Trees are a common way of organizing large amounts  of information by placing items with similar  characteristics near one another in the tree. We  introduce a classification problem where a given tree  structure gives us information on the best way to label  nearby elements. We suggest there are many practical  problems that fall under this domain. We propose a  way to map the classification problem onto a standard  Bayesian inference problem. We also give a fast,  specialized inference algorithm that incrementally  updates relevant probabilities. We apply this algorithm  to web-classification problems and show that our  algorithm empirically works well.",AIM-2003-013,1146195 bytes; 480357 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2004-10-08T20:38:56Z,2004-10-08T20:38:56Z,2003-05-01,http://hdl.handle.net/1721.1/6717,AIM-2003-012,A Robust Amorphous Hierarchy from Persistent Nodes,"For a very large network deployed in space with only nearby nodes able to talk to each other, we want to do tasks like robust routing and data storage. One way to organize the network is via a hierarchy, but hierarchies often have a few critical nodes whose death can disrupt organization over long distances. I address this with a system of distributed aggregates called Persistent Nodes, such that spatially local failures disrupt the hierarchy in an area proportional to the diameter of the failure. I describe and analyze this system, which has been implemented in simulation.",AIM-2003-012,12 p.; 3383342 bytes; 1922951 bytes,application/postscript; application/pdf,en_US,AI; amorphous distributed fault tolerant gigascale,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Martin, Martin C.",2004-10-08T20:38:57Z,2004-10-08T20:38:57Z,2003-05-01,http://hdl.handle.net/1721.1/6718,AIM-2003-014,The Essential Dynamics Algorithm: Essential Results,"This paper presents a novel algorithm for learning in a  class of stochastic Markov decision processes (MDPs)  with continuous state and action spaces that trades  speed for accuracy. A transform of the stochastic MDP  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. In this transformed MDP, the calculation of  values is greatly simplified. The online algorithm  estimates the model of the transformed MDP and  simultaneously does policy search against it. Bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. The algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic MDP, using less domain knowledge. All  code used in the experiments is available on the  project's web site.",AIM-2003-014,12 p.; 1085830 bytes; 303781 bytes,application/postscript; application/pdf,en_US,AI; Reinforcement learning; bicycle; policy search; markov decision processes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen",2004-10-20T20:31:53Z,2004-10-20T20:31:53Z,2003-05-22,http://hdl.handle.net/1721.1/7104,AITR-2003-007,A Statistical Image-Based Shape Model for Visual Hull Reconstruction and 3D Structure Inference,"We present a statistical image-based shape +  structure model for Bayesian visual hull reconstruction  and 3D structure inference. The 3D shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. Bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. We show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. The proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3D models under  various noise conditions are shown. We further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the Bayesian  reconstruction process. Model matching and parameter  inference are done entirely in the image domain and  require no explicit 3D construction. Our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. Using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3D locations of 19 joints on the  body based on observed silhouette contours from real images.",AITR-2003-007,60 p.; 14619811 bytes; 42799632 bytes,application/postscript; application/pdf,en_US,AI; visual hull; 3D structure; shape model; Bayesian inference,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wehowsky, Andreas F.",2004-10-20T20:31:59Z,2004-10-20T20:31:59Z,2003-05-30,http://hdl.handle.net/1721.1/7106,AITR-2003-012,Safe Distributed Coordination of Heterogeneous Robots through Dynamic Simple Temporal Networks,"Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman-Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",AITR-2003-012,95 p.; 3611933 bytes; 908879 bytes,application/postscript; application/pdf,en_US,AI; model-based autonomy; distributed planning; distributed constraint satisfaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demaine, Erik D.; Hajiaghayi, Mohammad Taghi",2023-03-29T15:37:26Z,2023-03-29T15:37:26Z,2003-06,https://hdl.handle.net/1721.1/149990,MIT-LCS-TR-904,Fixed Parameter Algorithms for Minor-Closed Graphs (of Locally Bounded Treewidth),"Frick and Grohe showed that for each property phi that is definable in first-order logic, and for each class of minor-closed graphs of locally bounded treewidth, there is an O(n^(1+epsilon))-time algorithm deciding whether a given graph has property phi. In this paper, we extend this result for fixed-parameter algorithms and show that any minor-closed [contraction-closed] bidimensional parameter which can be computed in polynomial time on graphs of bounded treewidth is also fixed-parameter tractable on general minor-closed graphs [minor-closed class of graphs of locally bounded treewidth].  These parameters include many domination and covering parameters such as vertex cover, feedback vertex set, dominating set, and clique-transversal set.  Our algorithm is very simple and its running time is explicit (in contrast to the work of Frick and Grohe).  Along the way, we obtain interesting combinatorial bounds between the aforementioned parameters and the treewidth of the graphs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demaine, Erik D.; Fomin, Fedor V.; Hajiaghayi, Mohammad Taghi; Thilikos, Dimitrios M.",2023-03-29T15:37:28Z,2023-03-29T15:37:28Z,2003-06,https://hdl.handle.net/1721.1/149991,MIT-LCS-TR-905,Subexponential Parameterized Algorithms on Graphs of Bounded Genus and H-minor-free Graphs,"We introduce a new framework for designing fixed-parameter algorithms with subexponential running time---2^O(sqrt k) n^O(1).  Our results apply to a broad family of graph problems, called bidimensional problems, which includes many domination and covering problems such as vertex cover, feedback vertex set, minimum maximal matching, dominating set, edge dominating set, clique-transversal set, and many others restricted to bounded genus graphs. Furthermore, it is fairly straightforward to prove that a problem is bidimensional.  In particular, our framework includes as special cases all previously known problems to have such subexponential algorithms.  Previously, these algorithms applied to planar graphs, single-crossing-minor-free graphs, and/or map graphs; we extend these results to apply to bounded-genus graphs as well.  In a parallel development of combinatorial results, we establish an upper bound on the treewidth (or branchwidth) of a bounded-genus graph that excludes some planar graph H as a minor.  This bound depends linearly on the size |V(H)| of the excluded graph H and the genus g(G) of the graph G, and applies and extends the graph-minors work of Robertson and Seymour.   Building on these results, we develop subexponential fixed-parameter algorithms for dominating set, vertex cover, and set cover in any class of graphs excluding a fixed graph H as a minor.  In particular, this general category of graphs includes planar graphs, bounded-genus graphs, single-crossing-minor-free graphs, and any class of graphs that is closed under taking minors. Specifically, the running time is 2^O(sqrt k) n^h, where h is a constant depending only on H, which is polynomial for k = O(log^2 n).  We introduce a general approach for developing algorithms on H-minor-free graphs, based on structural results about H-minor-free graphs at the heart of Robertson and Seymour's graph-minors work.  We believe this approach opens the way to further development on problems in H-minor-free graphs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Birka, Adrian",2023-03-29T15:37:34Z,2023-03-29T15:37:34Z,2003-06,https://hdl.handle.net/1721.1/149993,MIT-LCS-TR-908,Computer-Enforced Immutability for the Java Language,"This thesis presents the design, implementation, and evaluation of an extension to the Java language, ConstJava, that is capable of expressing immutability constraints and verifying them at compile time. The specific constraint expressed in ConstJava is that the transitive state of the object to which a given reference refers cannot be modified using that reference. In addition to the ability to specify and enforce this basic constraint, ConstJava includes several other features, such as mutable fields, immutable classes, templates, and the const cast operator, that make ConstJava a more useful language. The thesis evaluates the utility of ConstJava via experiments involving writing ConstJava code and converting Java code to ConstJava code. The evaluation of ConstJava shows that the language provides tangible benefits in early detection and correction of bugs that would otherwise be difficult to catch. There are also costs associated with the use of ConstJava. These are minimized by ConstJavaÔøΩs backward compatibility with Java, and by the high degree of inter-operability of the two languages, which allows for a less painful transition from Java to ConstJava. This technical report is a revision of the authorÔøΩs MasterÔøΩs thesis, which was advised by Prof. Michael D. Ernst.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Fitzpatrick, Paul",2004-10-20T20:31:57Z,2004-10-20T20:31:57Z,2003-06-01,http://hdl.handle.net/1721.1/7105,AITR-2003-008,From First Contact to Close Encounters: A Developmentally Deep Perceptual System for a Humanoid Robot,"This thesis presents a perceptual system for a  humanoid robot that integrates abilities such as object  localization and recognition with the deeper  developmental machinery required to forge those  competences out of raw physical experiences. It shows  that a robotic platform can build up and maintain a  system for object localization, segmentation, and  recognition, starting from very little. What the robot  starts with is a direct solution to achieving figure/ground  separation: it simply 'pokes around' in a region of visual  ambiguity and watches what happens. If the arm  passes through an area, that area is recognized as free  space. If the arm collides with an object, causing it to  move, the robot can use that motion to segment the  object from the background. Once the robot can  acquire reliable segmented views of objects, it learns  from them, and from then on recognizes and segments  those objects without further contact. Both low-level and  high-level visual features can also be learned in this  way, and examples are presented for both: orientation  detection and affordance recognition, respectively. The  motivation for this work is simple. Training on large  corpora of annotated real-world data has proven crucial  for creating robust solutions to perceptual problems  such as speech recognition and face detection. But the  powerful tools used during training of such systems are  typically stripped away at deployment. Ideally they  should remain, particularly for unstable tasks such as  object detection, where the set of objects needed in a  task tomorrow might be different from the set of objects  needed today. The key limiting factor is access to  training data, but as this thesis shows, that need not be  a problem on a robotic platform that can actively probe  its environment, and carry out experiments to resolve  ambiguity. This work is an instance of a general  approach to learning a new perceptual judgment:  find special situations in which the perceptual judgment  is easy and study these situations to find correlated  features that can be observed more generally.",AITR-2003-008,19935684 bytes; 7552671 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Das, Sanmay",2004-10-01T14:00:08Z,2004-10-01T14:00:08Z,2003-06-01,http://hdl.handle.net/1721.1/5570,AITR-2003-005; CBCL-226,Intelligent Market-Making in Artificial Financial Markets,"This thesis describes and evaluates a market-making  algorithm for setting prices in financial markets with asymmetric  information, and analyzes the properties of artificial markets in which the  algorithm is used. The core of our algorithm is a technique for  maintaining an online probability density estimate of the underlying  value of a stock. Previous theoretical work on market-making has  led to price-setting equations for which solutions cannot be  achieved in practice, whereas empirical work on algorithms for  market-making has focused on sets of heuristics and rules that lack  theoretical justification. The algorithm presented in this thesis is  theoretically justified by results in finance, and at the same time  flexible enough to be easily extended by incorporating modules for  dealing with considerations like portfolio risk and competition from  other market-makers. We analyze the performance of our  algorithm experimentally in artificial markets with different  parameter settings and find that many reasonable real-world properties  emerge. For example, the spread increases in response to  uncertainty about the true value of a stock, average spreads tend to be higher  in more volatile markets, and market-makers with lower  average spreads perform better in environments with multiple competitive market-makers. In addition, the time series data generated by simple  markets populated with market-makers using our algorithm replicate  properties of real-world financial time series, such as volatility  clustering and the fat-tailed nature of return distributions, without the  need to specify explicit models for opinion propagation and  herd behavior in the trading crowd.",AITR-2003-005; CBCL-226,49 p.; 3910312 bytes; 827445 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gajos, Krzysztof; Shrobe, Howard",2004-10-08T20:39:03Z,2004-10-08T20:39:03Z,2003-06-01,http://hdl.handle.net/1721.1/6721,AIM-2003-016,"Delegation, Arbitration and High-Level Service Discovery as Key Elements of a Software Infrastructure for Pervasive Computing","The dream of pervasive computing is slowly becoming  a reality. A number of projects around the world are constantly contributing ideas and solutions  that are bound to change the way we interact with our environments and with one another. An  essential component of the future is a software infrastructure that is capable of supporting interactions  on scales ranging from a single physical space to intercontinental collaborations. Such  infrastructure must help applications adapt to very diverse environments and must protect people's  privacy and respect their personal preferences. In this paper we indicate a number of limitations present  in the software infrastructures proposed so far (including our previous work). We then describe the  framework for building an infrastructure that satisfies the abovementioned criteria. This  framework hinges on the concepts of delegation, arbitration and high-level service discovery.  Components of our own implementation of such an infrastructure are presented.",AIM-2003-016,17 p.; 1039823 bytes; 325691 bytes,application/postscript; application/pdf,en_US,AI; pervasive computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Louie, Jennifer",2004-10-01T14:00:10Z,2004-10-01T14:00:10Z,2003-06-01,http://hdl.handle.net/1721.1/5571,AITR-2003-009; CBCL-227,A Biological Model of Object Recognition with Feature Learning,"Previous biological models of object recognition in  cortex have been evaluated using idealized scenes  and have hard-coded features, such as the HMAX  model by Riesenhuber and Poggio [10]. Because  HMAX uses the same set of features for all object  classes, it does not perform well in the task of detecting  a target object in clutter. This thesis presents a new  model that integrates learning of object-specific  features with the HMAX. The new model performs  better than the standard HMAX and comparably to a  computer vision system on face detection. Results from  experimenting with unsupervised learning of features  and the use of a biologically-plausible classifier are  presented.",AITR-2003-009; CBCL-227,4307593 bytes; 5073756 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rosen, Ezra",2004-10-01T14:00:11Z,2004-10-01T14:00:11Z,2003-06-05,http://hdl.handle.net/1721.1/5572,AITR-2003-010; CBCL-228,Face Representation in Cortex: Studies Using a Simple and Not So Special Model,"The face inversion effect has been widely documented  as an effect of the uniqueness of face processing. Using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. In simulations using HMAX, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. Using  many, sharply tuned units, an ``expert'' has a large inversion effect. On the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. As the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. Using the same representations as with  faces, we create experts and novices for cars. We also measure the  feasibility of a view-based model for recognition of rotated objects  using HMAX. Using faces, we show that transfer of learning to novel views is possible.  Given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. Although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",AITR-2003-010; CBCL-228,66 p.; 13121869 bytes; 3182779 bytes,application/postscript; application/pdf,en_US,AI; Face Recognition; Representation; Invariance; HMAX,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Koile, Kimberle; Tollmar, Konrad; Demirdjian, David; Shrobe, Howard; Darrell, Trevor",2004-10-08T20:39:02Z,2004-10-08T20:39:02Z,2003-06-10,http://hdl.handle.net/1721.1/6720,AIM-2003-015,Activity Zones for Context-Aware Computing,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",AIM-2003-015,12 p.; 17075202 bytes; 8771896 bytes,application/postscript; application/pdf,en_US,AI; context-aware; activity; intelligent environment; ubiquitous; 3d tracker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Monteleoni, Claire",2004-10-20T20:32:01Z,2004-10-20T20:32:01Z,2003-06-12,http://hdl.handle.net/1721.1/7107,AITR-2003-011,Online Learning of Non-stationary Sequences,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",AITR-2003-011,48 p.; 1815576 bytes; 911860 bytes,application/postscript; application/pdf,en_US,AI; online learning; relative loss bounds; switching dynamics; wireless; 802.11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Marjanovic, Matthew J.",2004-10-20T20:32:04Z,2004-10-20T20:32:04Z,2003-06-20,http://hdl.handle.net/1721.1/7108,AITR-2003-013,Teaching an Old Robot New Tricks: Learning Novel Tasks via Interaction with People and Things,"As AI has begun to reach out beyond its symbolic, objectivist roots into the embodied, experientialist realm, many projects are exploring different aspects of creating machines which interact with and respond to the world as humans do. Techniques for visual processing, object recognition, emotional response, gesture production and recognition, etc., are necessary components of a complete humanoid robot. However, most projects invariably concentrate on developing a few of these individual components, neglecting the issue of how all of these pieces would eventually fit together.  The focus of the work in this dissertation is on creating a framework into which such specific competencies can be embedded, in a way that they can interact with each other and build layers of new functionality. To be of any practical value, such a framework must satisfy the real-world constraints of functioning in real-time with noisy sensors and actuators. The humanoid robot Cog provides an unapologetically adequate platform from which to take on such a challenge.  This work makes three contributions to embodied AI. First, it offers a general-purpose architecture for developing behavior-based systems distributed over networks of PC's. Second, it provides a motor-control system that simulates several biological features which impact the development of motor behavior. Third, it develops a framework for a system which enables a robot to learn new behaviors via interacting with itself and the outside world. A few basic functional modules are built into this framework, enough to demonstrate the robot learning some very simple behaviors taught by a human trainer.  A primary motivation for this project is the notion that it is practically impossible to build an ""intelligent"" machine unless it is designed partly to build itself. This work is a proof-of-concept of such an approach to integrating multiple perceptual and motor systems into a complete learning agent.",AITR-2003-013,181 p.; 13057317 bytes; 13082678 bytes,application/postscript; application/pdf,en_US,AI; cog humanoid robot embodied learning phd thesis metaphor pancake reaching vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lee, Lily",2004-10-20T20:32:06Z,2004-10-20T20:32:06Z,2003-06-26,http://hdl.handle.net/1721.1/7109,AITR-2003-014,Gait Analysis for Classification,"This thesis describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. A suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. Despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. Each of the integration methods are investigated for their advantages and disadvantages. An improved gait representation is built based on our experiences with the initial set of gait representations. In addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.",AITR-2003-014,110 p.; 4040471 bytes; 994319 bytes,application/postscript; application/pdf,en_US,AI; gait recognition; gender classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Timoner, Samson",2004-10-20T20:32:09Z,2004-10-20T20:32:09Z,2003-07-04,http://hdl.handle.net/1721.1/7110,AITR-2003-015,Compact Representations for Fast Nonrigid Registration of Medical Images,"We develop efficient techniques for the non-rigid registration of medical images by using representations that adapt to the anatomy found in such images.   Images of anatomical structures typically have uniform intensity interiors and smooth boundaries. We create methods to represent such regions compactly using tetrahedra. Unlike voxel-based representations, tetrahedra can accurately describe the expected smooth surfaces of medical objects. Furthermore, the interior of such objects can be represented using a small number of tetrahedra. Rather than describing a medical object using tens of thousands of voxels, our representations generally contain only a few thousand elements.  Tetrahedra facilitate the creation of efficient non-rigid registration algorithms based on finite element methods (FEM). We create a fast, FEM-based method to non-rigidly register segmented anatomical structures from two subjects. Using our compact tetrahedral representations, this method generally requires less than one minute of processing time on a desktop PC.  We also create a novel method for the non-rigid registration of gray scale images. To facilitate a fast method, we create a tetrahedral representation of a displacement field that automatically adapts to both the anatomy in an image and to the displacement field. The resulting algorithm has a computational cost that is dominated by the number of nodes in the mesh (about 10,000), rather than the number of voxels in an image (nearly 10,000,000). For many non-rigid registration problems, we can find a transformation from one image to another in five minutes. This speed is important as it allows use of the algorithm during surgery.  We apply our algorithms to find correlations between the shape of anatomical structures and the presence of schizophrenia. We show that a study based on our representations outperforms studies based on other representations. We also use the results of our non-rigid registration algorithm as the basis of a segmentation algorithm. That algorithm also outperforms other methods in our tests, producing smoother segmentations and more accurately reproducing manual segmentations.",AITR-2003-015,183 p.; 19734335 bytes; 5610171 bytes,application/postscript; application/pdf,en_US,AI; non-rigid registration; medical image processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Timoner, Samson",2005-12-12T23:24:20Z,2005-12-12T23:24:20Z,2003-07-04,http://hdl.handle.net/1721.1/29830,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Compact Representations for Fast Nonrigid Registration of Medical Images,"We develop efficient techniques for the non-rigid registration of medical
images by using representations that adapt to the anatomy found in such
images.

 Images of anatomical structures typically have uniform intensity interiors
and smooth boundaries. We create methods to represent such regions
compactly using tetrahedra.  Unlike voxel-based representations, tetrahedra
can accurately describe the expected smooth surfaces of medical
objects. Furthermore, the interior of such objects can be represented using
a small number of tetrahedra. Rather than describing a medical object using
tens of thousands of voxels, our representations generally contain only a few
thousand elements.

Tetrahedra facilitate the creation of efficient non-rigid registration
algorithms based on finite element methods (FEM).  We create a fast,
FEM-based method to non-rigidly register segmented anatomical structures
from two subjects. Using our compact tetrahedral representations, this
method generally requires less than one minute of processing time on a desktop
PC.

We also create a novel method for the non-rigid registration of gray scale
images. To facilitate a fast method, we create a tetrahedral representation
of a displacement field that automatically adapts to both the anatomy in an
image and to the displacement field.  The resulting algorithm has a
computational cost that is dominated by the number of nodes in the mesh
(about 10,000), rather than the number of voxels in an image (nearly
10,000,000). For many non-rigid registration problems, we can find a
transformation from one image to another in five minutes. This speed is
important as it allows use of the algorithm during surgery.

We apply our algorithms to find correlations between the shape of
anatomical structures and the presence of schizophrenia. We show that a
study based on our representations outperforms studies based on other
representations. We also use the results of our non-rigid registration
algorithm as the basis of a segmentation algorithm. That algorithm also
outperforms other methods in our tests, producing smoother segmentations
and more accurately reproducing manual segmentations.",MIT-CSAIL-TR-2003-001; AITR-2003-015,183 p.; 160218641 bytes; 8166856 bytes,application/postscript; application/pdf,en_US,AI; non-rigid registration; medical image processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hajiaghayi, MohammadTaghi; Leighton, F. Thomson",2005-12-12T23:22:48Z,2005-12-12T23:22:48Z,2003-07-05,http://hdl.handle.net/1721.1/29829,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On the Max-Flow Min-Cut Ratio for Directed Multicommodity Flows,"We give a pure combinatorial problem whose solution determines max-flow
min-cut ratio for directed multicommodity flows. In addition, this
combinatorial problem has applications in improving the approximation  factor of Greedy algorithm for maximum edge disjoint path problem. More
precisely, our upper bound improves the approximation factor for this
problem to O(n^{3/4}). Finally, we demonstrate how even for very simple
graphs the aforementioned ratio might be very large.",MIT-CSAIL-TR-2003-002; MIT-LCS-TR-910,5 p.; 7867417 bytes; 389570 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Karger, David; Ruhl, Matthias",2005-12-12T23:24:43Z,2005-12-12T23:24:43Z,2003-07-16,http://hdl.handle.net/1721.1/29831,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New Algorithms for Load Balancing in Peer-to-Peer Systems,"Load balancing is a critical issue for the efficient operation of peer-to-peer networks. We give new protocols for several scenarios, whose provable performance guarantees are within a constant factor of optimal.

First, we give an improved version of consistent hashing, a scheme used for item to node assignments in the Chord system. In its original form, it required every network node to operate O(log n) virtual nodes to achieve a balanced load, causing a corresponding increase in space and bandwidth usage. Our protocol eliminates the necessity of virtual nodes while maintaining a balanced load. Improving on related protocols, our scheme allows for the deletion of nodes and admits a simpler analysis, since the assignments do not depend on the history of the network.

We then analyze a simple protocol for load sharing by movements of data from higher loaded to lower loaded nodes. This protocol can be extended to preserve the ordering of data items. As an application, we use the last protocol to give an efficient implementation of a distributed data structure for range searches on ordered data.",MIT-CSAIL-TR-2003-003; MIT-LCS-TR-911,5 p.; 8268446 bytes; 349665 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dodoo, Nii; Lin, Lee; Ernst, Michael D.",2005-12-19T22:47:18Z,2005-12-19T22:47:18Z,2003-07-21,http://hdl.handle.net/1721.1/30402,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Selecting Refining and Evaluating Properties for Program Analysis,"This research proposes and evaluates techniques for selectingpredicates for conditional program propertiesÂ—thatis, implications such as p ) q whose consequent must betrue whenever the predicate is true. Conditional propertiesare prevalent in recursive data structures, which behave differentlyin their base and recursive cases, in programs thatcontain branches, in programs that fail only on some inputs,and in many other situations. The experimental context ofthe research is dynamic detection of likely program invariants,but the ideas are applicable to other domains.Trying every possible predicate for conditional propertiesis computationally infeasible and yields too many undesirableproperties. This paper compares four policies forselecting predicates: procedure return analysis, code conditionals,clustering, and random selection. It also showshow to improve predicates via iterated analysis. An experimentalevaluation demonstrates that the techniques improveperformance on two tasks: statically proving the absence ofrun-time errors with a theorem-prover, and separating faultyfrom correct executions of erroneous programs.",MIT-CSAIL-TR-2003-005; MIT-LCS-TR-914,12 p.; 21668771 bytes; 858070 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Suh, G. Edward; Lee, Jaewook; Zhang, David; Devadas, Srinivas",2005-12-19T22:06:13Z,2005-12-19T22:06:13Z,2003-07-21,http://hdl.handle.net/1721.1/30396,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Secure Program Execution Via Dynamic Information Flow Tracking,"We present a simple architectural mechanism called dynamicinformation flow tracking that can significantly improve thesecurity of computing systems with negligible performanceoverhead. Dynamic information flow tracking protects programs against malicious software attacks by identifying spurious information flows from untrusted I/O and restrictingthe usage of the spurious information.Every security attack to take control of a program needsto transfer the programÂ’s control to malevolent code. Inour approach, the operating system identifies a set of inputchannels as spurious, and the processor tracks all information flows from those inputs. A broad range of attacks areeffectively defeated by checking the use of the spurious values as instructions and pointers.Our protection is transparent to users or application programmers; the executables can be used without any modification. Also, our scheme only incurs, on average, a memoryoverhead of 1.4% and a performance overhead of 1.1%.",MIT-CSAIL-TR-2003-004; MIT-LCS-TR-912,12 p.; 23224161 bytes; 957525 bytes,application/postscript; application/pdf,en_US,,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Attie, Paul C.; Lynch, Nancy A.",2005-12-19T23:43:21Z,2005-12-19T23:43:21Z,2003-07-26,http://hdl.handle.net/1721.1/30422,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Dynamic Input/Output Automata: A Formal Model for Dynamic Systems,"We present a mathematical state-machine model, the Dynamic I/O Automaton (DIOA) model, for defining and analyzing dynamic systems of interacting components. The systems we consider are dynamic in two senses: (1) components can be created and destroyed as computation proceeds, and (2) the events in which the components may participate may change. The new model admits a notion of external system behavior, based on sets of traces. It also features a parallel composition operator for dynamic systems, which respects external behavior, and a notion of simulation from one dynamic system to another, which can be used to prove that one system implements the other.",MIT-CSAIL-TR-2003-006; MIT-LCS-TR-902,42 p.; 44906891 bytes; 1796902 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Immorlica, Nicole; Mahdian, Mohammad",2005-12-19T22:49:33Z,2005-12-19T22:49:33Z,2003-07-28,http://hdl.handle.net/1721.1/30405,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Marriage, Honesty, and Stability","Many centralized two-sided markets form a matching between participantsby running a stable marriage algorithm. It is a well-knownfact that no matching mechanism based on a stable marriage algorithmcan guarantee truthfulness as a dominant strategy for participants.However, as we will show in this paper, in a probabilisticsetting where the preference lists of one side of the market are composedof only a constant (independent of the the size of the market)number of entries, each drawn from an arbitrary distribution, thenumber of participants that have more than one stable partner is vanishinglysmall. This proves (and generalizes) a conjecture of Rothand Peranson [23]. As a corollary of this result, we show that, withhigh probability, the truthful strategy is the best response for a givenplayer when the other players are truthful. We also analyze equilibriaof the deferred acceptance stable marriage game. We show thatthe game with complete information has an equilibrium in which a(1?o(1)) fraction of the strategies are truthful in expectation. In themore realistic setting of a game of incomplete information, we willshow that the set of truthful strategies form a (1+o(1))-approximateBayesian-Nash equilibrium. Our results have implications in manypractical settings and were inspired by the work of Roth and Peranson[23] on the National Residency Matching Program.",MIT-CSAIL-TR-2003-007; MIT-LCS-TR-913,12 p.; 22012898 bytes; 874678 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Felzenszwalb, Pedro F.",2004-10-20T20:32:11Z,2004-10-20T20:32:11Z,2003-08-08,http://hdl.handle.net/1721.1/7111,AITR-2003-016,Representation and Detection of Shapes in Images,"We present a set of techniques that can be used to represent and detect shapes in images. Our methods revolve around a particular shape representation based on the description of objects using triangulated polygons. This representation is similar to the medial axis transform and has important properties from a computational perspective. The first problem we consider is the detection of non-rigid objects in images using deformable models. We present an efficient algorithm to solve this problem in a wide range of situations, and show examples in both natural and medical images. We also consider the problem of learning an accurate non-rigid shape model for a class of objects from examples. We show how to learn good models while constraining them to the form required by the detection algorithm. Finally, we consider the problem of low-level image segmentation and grouping. We describe a stochastic grammar that generates arbitrary triangulated polygons while capturing Gestalt principles of shape regularity. This grammar is used as a prior model over random shapes in a low level algorithm that detects objects in images.",AITR-2003-016,80 p.; 6877524 bytes; 3132998 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Felzenszwalb, Pedro F.",2005-12-19T22:44:55Z,2005-12-19T22:44:55Z,2003-08-08,http://hdl.handle.net/1721.1/30400,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Representation and Detection of Shapes in Images,"We present a set of techniques that can be used to represent anddetect shapes in images.  Our methods revolve around a particularshape representation based on the description of objects usingtriangulated polygons.  This representation is similar to the medialaxis transform and has important properties from a computationalperspective.  The first problem we consider is the detection ofnon-rigid objects in images using deformable models.  We present anefficient algorithm to solve this problem in a wide range ofsituations, and show examples in both natural and medical images.  Wealso consider the problem of learning an accurate non-rigid shapemodel for a class of objects from examples.  We show how to learn goodmodels while constraining them to the form required by the detectionalgorithm.  Finally, we consider the problem of low-level imagesegmentation and grouping.  We describe a stochastic grammar thatgenerates arbitrary triangulated polygons while capturing Gestaltprinciples of shape regularity.  This grammar is used as a prior modelover random shapes in a low level algorithm that detects objects inimages.",MIT-CSAIL-TR-2003-008; AITR-2003-016,80 p.; 38103057 bytes; 1889641 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Livadas, Carolos; Lynch, Nancy A.",2005-12-19T23:14:28Z,2005-12-19T23:14:28Z,2003-08-11,http://hdl.handle.net/1721.1/30410,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Reliable Broadcast Scheme for Sensor Networks,"In this short technical report, we present a simple yet effective reliable broadcast protocol for sensor networks. This protocol disseminates packets throughout the sensor network by flooding and recovers from losses resulting from collisions by having hosts retransmit packets whenever they notice that their neighbors have fallen behind. Such retransmissions serve to flood the appropriate packets throughout the regions of the sensor network that did not receive the given packets as a result of prior flooding attempts.",MIT-CSAIL-TR-2003-010; MIT-LCS-TR-915,5 p.; 6233240 bytes; 285081 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2005-12-19T22:49:01Z,2005-12-19T22:49:01Z,2003-08-11,http://hdl.handle.net/1721.1/30404,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Near-Optimal Distributed Failure Circumscription,"Small failures should only disrupt a small part of a network.  One wayto do this is by marking the surrounding area as untrustworthy ---circumscribing the failure. This can be done with a distributedalgorithm using hierarchical clustering and neighbor relations, andthe resulting circumscription is near-optimal for convex failures.",MIT-CSAIL-TR-2003-009; AIM-2003-017,9 p.; 13236751 bytes; 840133 bytes,application/postscript; application/pdf,en_US,AI; amorphous distributed ad-hoc computing self-organizing stopping failure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2004-10-08T20:39:04Z,2004-10-08T20:39:04Z,2003-08-11,http://hdl.handle.net/1721.1/6722,AIM-2003-017,Near-Optimal Distributed Failure Circumscription,"Small failures should only disrupt a small part of a network. One way to do this is by marking the surrounding area as untrustworthy --- circumscribing the failure. This can be done with a distributed algorithm using hierarchical clustering and neighbor relations, and the resulting circumscription is near-optimal for convex failures.",AIM-2003-017,9 p.; 2144454 bytes; 705176 bytes,application/postscript; application/pdf,en_US,AI; amorphous distributed ad-hoc computing self-organizing stopping failure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Balas, Benjamin J.; Sinha, Pawan",2004-10-20T21:05:11Z,2004-10-20T21:05:11Z,2003-08-13,http://hdl.handle.net/1721.1/7276,AIM-2003-018; CBCL-229,Dissociated Dipoles: Image representation via non-local comparisons,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . We introduce the ""Dissociated Dipole,"" or ""Sticks"" operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",AIM-2003-018; CBCL-229,15 p.; 691165 bytes; 974071 bytes,application/postscript; application/pdf,en_US,AI; image representation; recognition; non-local filtering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Balas, Benjamin J.; Sinha, Pawan",2005-12-19T22:36:12Z,2005-12-19T22:36:12Z,2003-08-13,http://hdl.handle.net/1721.1/30398,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Dissociated Dipoles: Image representation via non-local comparisons,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filterÂ’s span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. .We introduce the Â“Dissociated Dipole,Â” or Â“SticksÂ” operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",MIT-CSAIL-TR-2003-011; AIM-2003-018; CBCL-229,15 p.; 18211324 bytes; 682491 bytes,application/postscript; application/pdf,en_US,AI; image representation; recognition; non-local filtering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Rinard, Martin",2005-12-19T23:09:23Z,2005-12-19T23:09:23Z,2003-08-22,http://hdl.handle.net/1721.1/30409,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On The Boolean Algebra of Shape Analysis Constraints,"Shape analysis is a promising technique for statically verifyingand extracting properties of programs that manipulatecomplex data structures. We introduce a new characterizationof constraints that arise in parametric shapeanalysis based on manipulation of three-valued structuresas dataflow facts.We identify an interesting syntactic class of first-orderlogic formulas that captures the meaning of three-valuedstructures under concretization. This class is broader thanpreviously introduced classes, allowing for a greater flexibilityin the formulation of shape analysis constraints inprogram annotations and internal analysis representations.Three-valued structures can be viewed as one possible normalform of the formulas in our class.Moreover, we characterize the meaning of three-valuedstructures under Â“tight concretizationÂ”. We show that theseemingly minor change from concretization to tight concretizationincreases the expressive power of three-valuedstructures in such a way that the resulting constraints areclosed under all boolean operations. We call the resultingconstraints boolean shape analysis constraints.The main technical contribution of this paper is a naturalsyntactic characterization of boolean shape analysis constraintsas arbitrary boolean combinations of first-order sentencesof certain form, and an algorithm for transformingsuch boolean combinations into the normal form that correspondsdirectly to three-valued structures.Our result holds in the presence of arbitrary shape analysisinstrumentation predicates. The result enables the reduction(without any approximation) of the entailment andthe equivalence of shape analysis constraints to the satisfiabilityof shape analysis constraints. When the satisfiabilityof the constraints is decidable, our result implies that theentailment and the equivalence of the constraints are alsodecidable, which enables the use of constraints in a compositionalshape analysis with a predictable behavior.",MIT-CSAIL-TR-2003-012; MIT-LCS-TR-916,18 p.; 30231553 bytes; 1304355 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shimizu, Hiroaki; Poggio, Tomaso",2005-12-19T22:32:54Z,2005-12-19T22:32:54Z,2003-08-27,http://hdl.handle.net/1721.1/30397,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Direction Estimation of Pedestrian from Images,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",MIT-CSAIL-TR-2003-013; AIM-2003-020; CBCL-230,11 p.; 12026063 bytes; 489901 bytes,application/postscript; application/pdf,en_US,AI; pedestrian; walking direction; classification; SVM; recognition; human motion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kaynar, Dilsun K.; Lynch, Nancy; Segala, Roberto; Vaandrager, Frits",2005-12-19T22:48:27Z,2005-12-19T22:48:27Z,2003-08-27,http://hdl.handle.net/1721.1/30403,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Theory of Timed I/O Automata,"Revised version -- November 23, 2004.This paper presents the Timed Input/Output Automaton (TIOA) modeling framework, a basic mathematical framework to support description and analysis of timed systems.",MIT-CSAIL-TR-2003-014; MIT-LCS-TR-917,130 p.; 112222505 bytes; 4311471 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shimizu, Hiroaki; Poggio, Tomaso",2004-10-20T21:05:12Z,2004-10-20T21:05:12Z,2003-08-27,http://hdl.handle.net/1721.1/7277,AIM-2003-020; CBCL-230,Direction Estimation of Pedestrian from Images,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",AIM-2003-020; CBCL-230,11 p.; 784806 bytes; 664353 bytes,application/postscript; application/pdf,en_US,AI; pedestrian; walking direction; classification; SVM; recognition; human motion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Mukherjee, Sayan; Golland, Polina; Panchenko, Dmitry",2005-12-19T23:02:49Z,2005-12-19T23:02:49Z,2003-08-28,http://hdl.handle.net/1721.1/30408,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Permutation Tests for Classification,"We introduce and explore an approach to estimating statisticalsignificance of classification accuracy, which is particularly usefulin scientific applications of machine learning where highdimensionality of the data and the small number of training examplesrender most standard convergence bounds too loose to yield ameaningful guarantee of the generalization ability of theclassifier. Instead, we estimate statistical significance of theobserved classification accuracy, or the likelihood of observing suchaccuracy by chance due to spurious correlations of thehigh-dimensional data patterns with the class labels in the giventraining set. We adopt permutation testing, a non-parametric techniquepreviously developed in classical statistics for hypothesis testing inthe generative setting (i.e., comparing two probabilitydistributions). We demonstrate the method on real examples fromneuroimaging studies and DNA microarray analysis and suggest atheoretical analysis of the procedure that relates the asymptoticbehavior of the test to the existing convergence bounds.",MIT-CSAIL-TR-2003-016; AIM-2003-019,22 p.; 22876548 bytes; 882217 bytes,application/postscript; application/pdf,en_US,AI; Classification; Permutation testing; Statistical significance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Mukherjee, Sayan; Golland, Polina; Panchenko, Dmitry",2004-10-08T20:39:06Z,2004-10-08T20:39:06Z,2003-08-28,http://hdl.handle.net/1721.1/6723,AIM-2003-019,Permutation Tests for Classification,"We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",AIM-2003-019,22 p.; 1135156 bytes; 662639 bytes,application/postscript; application/pdf,en_US,AI; Classification; Permutation testing; Statistical significance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Che, Austin",2005-12-19T22:50:24Z,2005-12-19T22:50:24Z,2003-08-31,http://hdl.handle.net/1721.1/30406,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Fluorescence Assay for Polymerase Arrival Rates,"To engineer complex synthetic biological systems will require modulardesign, assembly, and characterization strategies. The RNApolymerase arrival rate (PAR) is defined to be the rate that RNApolymerases arrive at a specified location on the DNA. Designing andcharacterizing biological modules in terms of RNA polymerase arrivalrates provides for many advantages in the construction and modeling ofbiological systems.PARMESAN is an in vitro method for measuring polymerase arrival ratesusing pyrrolo-dC, a fluorescent DNA base that can substitute forcytosine. Pyrrolo-dC shows a detectable fluorescence difference whenin single-stranded versus double-stranded DNA. During transcription,RNA polymerase separates the two strands of DNA, leading to a changein the fluorescence of pyrrolo-dC. By incorporating pyrrolo-dC atspecific locations in the DNA, fluorescence changes can be taken as adirect measurement of the polymerase arrival rate.",MIT-CSAIL-TR-2003-017; AITR-2003-017,112 p.; 113486485 bytes; 7023595 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Che, Austin",2004-10-20T20:32:21Z,2004-10-20T20:32:21Z,2003-08-31,http://hdl.handle.net/1721.1/7112,AITR-2003-017,Fluorescence Assay for Polymerase Arrival Rates,"To engineer complex synthetic biological systems will require modular design, assembly, and characterization strategies. The RNA polymerase arrival rate (PAR) is defined to be the rate that RNA polymerases arrive at a specified location on the DNA. Designing and characterizing biological modules in terms of RNA polymerase arrival rates provides for many advantages in the construction and modeling of biological systems.  PARMESAN is an in vitro method for measuring polymerase arrival rates using pyrrolo-dC, a fluorescent DNA base that can substitute for cytosine. Pyrrolo-dC shows a detectable fluorescence difference when in single-stranded versus double-stranded DNA. During transcription, RNA polymerase separates the two strands of DNA, leading to a change in the fluorescence of pyrrolo-dC. By incorporating pyrrolo-dC at specific locations in the DNA, fluorescence changes can be taken as a direct measurement of the polymerase arrival rate.",AITR-2003-017,112 p.; 92476964 bytes; 3362118 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ross, Michael G.; Kaelbling, Leslie Pack",2004-10-08T20:43:02Z,2004-10-08T20:43:02Z,2003-09-08,http://hdl.handle.net/1721.1/6730,AIM-2003-022,Learning object segmentation from video data,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",AIM-2003-022,15 p.; 2769288 bytes; 1654353 bytes,application/postscript; application/pdf,en_US,AI; learning; image segmentation; motion; Markov random field; belief propagation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kouh, Minjoon; Riesenhuber, Maximilian",2004-10-20T21:05:14Z,2004-10-20T21:05:14Z,2003-09-08,http://hdl.handle.net/1721.1/7278,AIM-2003-021; CBCL-231,Investigating shape representation in area V4 with HMAX: Orientation and Grating selectivities,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",AIM-2003-021; CBCL-231,14 p.; 2802887 bytes; 1234306 bytes,application/postscript; application/pdf,en_US,AI; Shape Tuning; Shape Representation; Features; HMAX; Visual Cortex; Gratings; V4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ross, Michael G.; Kaelbling, Leslie Pack",2005-12-19T22:46:46Z,2005-12-19T22:46:46Z,2003-09-08,http://hdl.handle.net/1721.1/30401,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning object segmentation from video data,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",MIT-CSAIL-TR-2003-018; AIM-2003-022,15 p.; 23365488 bytes; 1821447 bytes,application/postscript; application/pdf,en_US,AI; learning; image segmentation; motion; Markov random field; belief propagation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kouh, Minjoon; Riesenhuber, Maximilian",2005-12-20T22:01:32Z,2005-12-20T22:01:32Z,2003-09-08,http://hdl.handle.net/1721.1/30424,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Investigating shape representation in area V4 with HMAX: Orientation and Grating selectivities,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",MIT-CSAIL-TR-2003-019; AIM-2003-021; CBCL-231,14 p.; 20150764 bytes; 1258167 bytes,application/postscript; application/pdf,en_US,AI; Shape Tuning; Shape Representation; Features; HMAX; Visual Cortex; Gratings; V4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rodrigues, Rodrigo; Liskov, Barbara",2005-12-20T22:29:44Z,2005-12-20T22:29:44Z,2003-09-25,http://hdl.handle.net/1721.1/30425,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Correctness Proof for a Byzantine-Fault-Tolerant Read/Write Atomic Memory with Dynamic Replica Membership,We prove correctness of a Byzantine-fault-tolerant replication algorithm for a read/writeatomic memory that supports a dynamic replica set.,MIT-CSAIL-TR-2003-020; MIT-LCS-TR-920,10 p.; 13866415 bytes; 493836 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Marinov, Darko; Rodoicic, Rados",2005-12-22T12:00:00Z,2005-12-22T12:00:00Z,2003-10-09,http://hdl.handle.net/1721.1/30426,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Generating Trees of (Reducible) 1324-avoiding Permutations,"We consider permutations that avoid the pattern 1324. We give exact formulas for thenumber of reducible 1324-avoiding permutations and the number of {1324, 4132, 2413, 3241}-avoiding permutations. By studying the generating tree for all 1324-avoiding permutations,we obtain a recurrence formula for their number. A computer program provides data for thenumber of 1324-avoiding permutations of length up to 20.",MIT-CSAIL-TR-2003-021; MIT-LCS-TR-924,12 p.; 13383375 bytes; 567281 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liskov, Moses; Milcali, Silvio",2005-12-22T01:06:21Z,2005-12-22T01:06:21Z,2003-10-14,http://hdl.handle.net/1721.1/30428,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Updatable Zero-Knowledge Sets,"We build on the work of Micali, Rabin, and Killian [4] to introduce zero-knowledge sets and databases that may be updated in a desirable way. In particular, in order to make an update the owner of the set must publish a commitment to the update, and update the commitment to the set. The update should take time independent of the size of the set. In addition, the update should not leak which key was added (or removed), or what data is associated with that key. Furthermore, our update will be transparent in that those already possessing a proof of a particular key being present or absent should be able to update their proofs to obtain a valid proof relative to the updated set, except if their proof is relative to the element that was changed.",MIT-CSAIL-TR-2003-023; MIT-LCS-TM-640,6 p.; 6191398 bytes; 283726 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liskov, Moses",2005-12-22T12:00:00Z,2005-12-22T12:00:00Z,2003-10-14,http://hdl.handle.net/1721.1/30427,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Electronic Cash with Blind Deposits: How to Have No Spare Change,"Electronic cash schemes in which the bank authenticates many coins at once suffer from the problem that coins that are authenticated together can be linked to one another. Unfortunately, unless a user spends coins in a closely prescribed manner, different batches of coins (""wallets"") will be linked together in these schemes. This is illustrated by the problem of what a customer does with the ""spare change"" - an unusable small amount of money left in a wallet. We propose a new protocol to be used in e-cash schemes: blind deposits. In a blind deposit, a customer returns a coin to the bank without revealing the coin. We present a secure and efficient e-cash scheme with this added feature based on that of Liskov-Micali [LM01].",MIT-CSAIL-TR-2003-022; MIT-LCS-TM-639,13 p.; 12754502 bytes; 526739 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Strumpen, Volker; Hoffmann, Henry; Agarwal, Anant",2005-12-22T01:09:48Z,2005-12-22T01:09:48Z,2003-10-22,http://hdl.handle.net/1721.1/30429,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Stream Algorithm for the SVD,"We present a stream algorithm for the Singular-Value Decomposition (SVD) of anM X N matrix A. Our algorithm trades speed of numerical convergence for parallelism,and derives from a one-sided, cyclic-by-rows Hestenes SVD. Experimental results showthat we can create O(M) parallelism, at the expense of increasing the computationalwork by less than a factor of about 2. Our algorithm qualifes as a stream algorithmin that it requires no more than a small, bounded amount of local storage per processor and its compute efficiency approaches an optimal 100% asymptotically for largenumbers of processors and appropriate problem sizes.",MIT-CSAIL-TR-2003-024; MIT-LCS-TM-641,31 p.; 30567456 bytes; 1124918 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Rinard, Martin",2005-12-22T01:12:11Z,2005-12-22T01:12:11Z,2003-10-24,http://hdl.handle.net/1721.1/30430,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Role Logic,"We present role logic, a notation for describing propertiesof relational structures in shape analysis, databases, andknowledge bases.  We construct role logic using the ideas ofde Bruijn's notation for lambda calculus, an encoding offirst-order logic in lambda calculus, and a simple rule forimplicit arguments of unary and binary predicates.The unrestricted version of role logic has the expressivepower of first-order logic with transitive closure.  Using asyntactic restriction on role logic formulas, we identify anatural fragment RL^2 of role logic.  We show that the RL^2fragment has the same expressive power as two-variable logicwith counting C^2 and is therefore decidable.We present a translation of an imperative language into thedecidable fragment RL^2, which allows compositionalverification of programs that manipulate relationalstructures.  In addition, we show how RL^2 encodes booleanshape analysis constraints and an expressive descriptionlogic.",MIT-CSAIL-TR-2003-025; MIT-LCS-TR-925,20 p.; 26284070 bytes; 1140595 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Eisenstein, Jacob",2005-12-22T01:12:20Z,2005-12-22T01:12:20Z,2003-10-28,http://hdl.handle.net/1721.1/30431,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Evolving Robocode Tank Fighters,"In this paper, I describe the application of genetic programming to evolve a controller for a robotic tank in a simulated environment.The purpose is to explore how genetic techniques can best be applied to produce controllers based on subsumption and behavior oriented languages such as REX.  As part of my implementation, I developed TableRex, a modification of REX that can be expressed on a fixed-lengthgenome.  Using a fixed subsumption architecture of TableRex modules, I evolved robots that beat some of the most competitive hand-coded adversaries.",MIT-CSAIL-TR-2003-026; AIM-2003-023,24 p.; 21707085 bytes; 716288 bytes,application/postscript; application/pdf,en_US,AI; genetic programming; robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Eisenstein, Jacob",2004-10-08T20:43:03Z,2004-10-08T20:43:03Z,2003-10-28,http://hdl.handle.net/1721.1/6731,AIM-2003-023,Evolving Robocode Tank Fighters,"In this paper, I describe the application of genetic programming  to evolve a controller for a robotic tank in a simulated environment. The purpose is to explore how genetic techniques can best be applied  to produce controllers based on subsumption and behavior oriented  languages such as REX. As part of my implementation, I developed  TableRex, a modification of REX that can be expressed on a fixed-length genome. Using a fixed subsumption architecture of TableRex modules,  I evolved robots that beat some of the most competitive hand-coded  adversaries.",AIM-2003-023,24 p.; 963754 bytes; 653698 bytes,application/postscript; application/pdf,en_US,AI; genetic programming; robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ford, Bryan",2005-12-22T01:14:34Z,2005-12-22T01:14:34Z,2003-10-31,http://hdl.handle.net/1721.1/30432,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Scalable Internet Routing on Topology-Independent Node Identities,"Unmanaged Internet Protocol (UIP) is a fully selforganizingnetwork-layer protocol that implements scalableidentity-based routing. In contrast with addressbasedrouting protocols, which depend for scalability oncentralized hierarchical address management, UIP nodesuse a flat namespace of cryptographic node identifiers.Node identities can be created locally on demand andremain stable across network changes. Unlike locationindependentname services, the UIP routing protocol canstitch together many conventional address-based networkswith disjoint or discontinuous address domains, providingconnectivity between any pair of participating nodes evenwhen no underlying network provides direct connectivity.The UIP routing protocol works on networks with arbitrarytopologies and global traffic patterns, and requiresonlyO(log N) storage per node for routing state, enablingeven small, ubiquitous edge devices to act as ad-hoc selfconfiguringrouters. The protocol rapidly recovers fromnetwork partitions, bringing every node up-to-date in amulticast-based chain reaction of O(log N) depth. Simulationresults indicate that UIP finds routes that are onaverage within 2X  the length of the best possible route.",MIT-CSAIL-TR-2003-027; MIT-LCS-TR-926,15 p.; 25392418 bytes; 1209916 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demsky, Brian; Cadar, Cristian; Roy, Daniel; Rinard, Martin",2005-12-22T01:14:41Z,2005-12-22T01:14:41Z,2003-11-13,http://hdl.handle.net/1721.1/30433,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Efficient Specification-Assisted Error Localization and Correction,"We present a new error localization tool, Archie, that accepts aspecification of key data structure consistency constraints, then generatesan algorithm that checks if the data structures satisfy theconstraints. We also present a set of specification analyses and optimizationsthat (for our benchmark software system) improve theperformance of the generated checking algorithm by over a factorof 3,900 as compared with the initial interpreted implementation,enabling Archie to efficiently support interactive debugging.We evaluate ArchieÂ’s effectiveness by observing the actions oftwo developer populations (one using Archie, the other using standarderror localization techniques) as they attempted to localize andcorrect three errors in a benchmark software system. With Archie,the developers were able to localize each error in less than 10 minutesand correct each error in (usually much) less than 20 minutes.Without Archie, the developers were, with one exception, unableto locate each error after more than an hour of effort. These resultsillustrate ArchieÂ’s potential to substantially improve current errorlocalization and correction techniques.",MIT-CSAIL-TR-2003-028; MIT-LCS-TR-927,10 p.; 19768606 bytes; 820085 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hajiaghayi, MohammadTaghi; Sorkin, Gregory B.",2005-12-22T01:14:48Z,2005-12-22T01:14:48Z,2003-11-20,http://hdl.handle.net/1721.1/30434,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Satisfiability Threshold of Random 3-SAT Is at Least 3.52,We prove that a random 3-SAT instance with clause-to-variable densityless than 3.52 is satisfiable with high probability.The proof comes through an algorithm which selects (and sets) a variabledepending on its degree and that of its complement.,MIT-CSAIL-TR-2003-029; MIT-LCS-TR-929,8 p.; 6904092 bytes; 339435 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liben-Nowell, David; Vee, Erik; Zhu, An",2005-12-22T01:15:02Z,2005-12-22T01:15:02Z,2003-11-26,http://hdl.handle.net/1721.1/30435,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Finding Longest Increasing and Common Subsequences in Streaming Data,"In this paper, we present algorithms and lower bounds for the Longest Increasing Subsequence(LIS) and Longest Common Subsequence (LCS) problems in the data streaming model.",MIT-CSAIL-TR-2003-030; MIT-LCS-TR-931,15 p.; 19636061 bytes; 807700 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Morgenstern, Christian; Heisele, Bernd",2004-10-20T21:05:16Z,2004-10-20T21:05:16Z,2003-11-28,http://hdl.handle.net/1721.1/7279,AIM-2003-024; CBCL-232,Component based recognition of objects in an office environment,We present a component-based approach for recognizing objects under large pose changes. From a set of training images of a given object we extract a large number of components which are clustered based on the similarity of their image features and their locations within the object image. The cluster centers build an initial set of component templates from which we select a subset for the final recognizer. In experiments we evaluate different sizes and types of components and three standard techniques for component selection. The component classifiers are finally compared to global classifiers on a database of four objects.,AIM-2003-024; CBCL-232,12 p.; 3572823 bytes; 962401 bytes,application/postscript; application/pdf,en_US,AI; computer vision; object recognition; component object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Morgenstern, Christian; Heisele, Bernd",2005-12-22T01:15:11Z,2005-12-22T01:15:11Z,2003-11-28,http://hdl.handle.net/1721.1/30436,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Component based recognition of objects in an office environment,We present a component-based approach for recognizing objectsunder large pose changes. From a set of training images of a givenobject we extract a large number of components which are clusteredbased on the similarity of their image features and their locations withinthe object image. The cluster centers build an initial set of componenttemplates from which we select a subset for the final recognizer.In experiments we evaluate different sizes and types of components andthree standard techniques for component selection. The component classifiersare finally compared to global classifiers on a database of fourobjects.,MIT-CSAIL-TR-2003-031; AIM-2003-024; CBCL-232,12 p.; 20676042 bytes; 965767 bytes,application/postscript; application/pdf,en_US,AI; computer vision; object recognition; component object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chang, Yu-Han; Ho, Tracey; Kaelbling, Leslie Pack",2005-12-22T01:15:17Z,2005-12-22T01:15:17Z,2003-12-04,http://hdl.handle.net/1721.1/30437,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Mobilized ad-hoc networks:  A reinforcement learning approach,"Research in mobile ad-hoc networks has focused on situations in whichnodes have no control over their movements.  We investigate animportant but overlooked domain in which nodes do have controlover their movements.  Reinforcement learning methods can be used tocontrol both packet routing decisions and node mobility, dramaticallyimproving the connectivity of the network.  We first motivate theproblem by presenting theoretical bounds for the connectivityimprovement of partially mobile networks and then present superiorempirical results under a variety of different scenarios in which themobile nodes in our ad-hoc network are embedded with adaptive routingpolicies and learned movement policies.",MIT-CSAIL-TR-2003-032; AIM-2003-025,9 p.; 15523730 bytes; 577014 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; multi-agent learning; ad-hoc networking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chang, Yu-Han; Ho, Tracey; Kaelbling, Leslie Pack",2004-10-08T20:43:04Z,2004-10-08T20:43:04Z,2003-12-04,http://hdl.handle.net/1721.1/6732,AIM-2003-025,Mobilized ad-hoc networks: A reinforcement learning approach,"Research in mobile ad-hoc networks has focused on situations in which nodes have no control over their movements. We investigate an important but overlooked domain in which nodes do have control over their movements. Reinforcement learning methods can be used to control both packet routing decisions and node mobility, dramatically improving the connectivity of the network. We first motivate the problem by presenting theoretical bounds for the connectivity improvement of partially mobile networks and then present superior empirical results under a variety of different scenarios in which the mobile nodes in our ad-hoc network are embedded with adaptive routing policies and learned movement policies.",AIM-2003-025,9 p.; 771382 bytes; 1199447 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; multi-agent learning; ad-hoc networking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen; Darrell, Trevor",2005-12-22T01:15:24Z,2005-12-22T01:15:24Z,2003-12-05,http://hdl.handle.net/1721.1/30438,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Fast Contour Matching Using Approximate Earth Mover's Distance,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",MIT-CSAIL-TR-2003-033; AIM-2003-026,16 p.; 18655633 bytes; 2291372 bytes,application/postscript; application/pdf,en_US,AI; contour matching; shape matching; EMD; image retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen; Darrell, Trevor",2004-10-08T20:43:06Z,2004-10-08T20:43:06Z,2003-12-05,http://hdl.handle.net/1721.1/6733,AIM-2003-026,Fast Contour Matching Using Approximate Earth Mover's Distance,"Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",AIM-2003-026,16 p.; 7561935 bytes; 7530316 bytes,application/postscript; application/pdf,en_US,AI; contour matching; shape matching; EMD; image retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rodrigues, Rodrigo; Liskov, Barbara",2005-12-22T01:15:38Z,2005-12-22T01:15:38Z,2003-12-17,http://hdl.handle.net/1721.1/30440,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Rosebud: A Scalable Byzantine-Fault-Tolerant Storage Architecture,"This paper presents Rosebud, a new Byzantine faulttolerantstorage architecture designed to be highly scalableand deployable in the wide-area. To support massiveamounts of data, we need to partition the data among thenodes. To support long-lived operation, we need to allowthe set of nodes in the system to change. To our knowledge,we are the first to present a complete design and arunning implementation of Byzantine-fault-tolerant storagealgorithms for a large scale, dynamic membership.We deployed Rosebud in a wide area testbed and ran experimentsto evaluate its performance, and our experimentsshow that it performs well. We show that our storage algorithmsperform equivalently to highly optimized replicationalgorithms in the wide-area. We also show that performancedegradation is minor when the system reconfigures.",MIT-CSAIL-TR-2003-035; MIT-LCS-TR-932,14 p.; 28245099 bytes; 1025372 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob; Gilbert, Seth",2004-10-08T20:43:07Z,2004-10-08T20:43:07Z,2003-12-17,http://hdl.handle.net/1721.1/6734,AIM-2003-027,RamboNodes for the Metropolitan Ad Hoc Network,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",AIM-2003-027,22 p.; 1312502 bytes; 499111 bytes,application/postscript; application/pdf,en_US,AI; ad-hoc networks distributed algorithms atomic distributed shared memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob; Gilbert, Seth",2005-12-22T01:15:30Z,2005-12-22T01:15:30Z,2003-12-17,http://hdl.handle.net/1721.1/30439,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,RamboNodes for the Metropolitan Ad Hoc Network,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",MIT-CSAIL-TR-2003-034; AIM-2003-027,22 p.; 23886105 bytes; 803571 bytes,application/postscript; application/pdf,en_US,AI; ad-hoc networks distributed algorithms atomic distributed shared memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lam, Patrick; Kuncak, Viktor; Rinard, Martin",2005-12-22T01:15:47Z,2005-12-22T01:15:47Z,2003-12-18,http://hdl.handle.net/1721.1/30441,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Modular Pluggable Analyses Using Set Interfaces,"We present a technique that enables the focused applicationof multiple analyses to different modules in the same program. Our researchhas two goals: 1) to address the scalability limitations of preciseanalyses by focusing the analysis on only those parts of the program thatare relevant to the properties that the analysis is designed to verify, and2) to enable the application of specialized analyses that verify propertiesof specifc classes of data structures to programs that simultaneouslymanipulate several dfferent kinds of data structures.In our approach, each module encapsulates a data structure and usesmembership in abstract sets to characterize how objects participate inits data structure. Each analysis verifies that the implementation of themodule 1) preserves important internal data structure representationinvariants and 2) conforms to a specification that uses formulas in a setalgebra to characterize the effects of operations on the data structure.The analyses use the common set abstraction to 1) characterize howobjects participate in multiple data structures and to 2) enable the interanalysiscommunication required to verify properties that depend onmultiple modules analyzed by different analyses.We characterize the key soundness property that an analysis plugin mustsatisfy to successfully participate in our system and present several analysisplugins that satisfy this property: a flag plugin that analyzes modulesin which abstract set membership is determined by a flag  field in eachobject, and a graph types plugin that analyzes modules in which abstractset membership is determined by reachability properties of objects storedin tree-like data structures.",MIT-CSAIL-TR-2003-036; MIT-LCS-TR-933,34 p.; 31854752 bytes; 1275133 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Schneider, Robert; Riesenhuber, Maximilian",2004-10-20T21:05:18Z,2004-10-20T21:05:18Z,2004-01-14,http://hdl.handle.net/1721.1/7280,AIM-2004-004; CBCL-235,On the difficulty of feature-based attentional modulations in visual object recognition: A modeling study.,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45]. We modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",AIM-2004-004; CBCL-235,38 p.; 4871469 bytes; 1392271 bytes,application/postscript; application/pdf,en_US,AI; object recognition; attention; vision; modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Schneider, Robert; Riesenhuber, Maximilian",2005-12-22T01:18:52Z,2005-12-22T01:18:52Z,2004-01-14,http://hdl.handle.net/1721.1/30442,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On the difficulty of feature-based attentional modulations in visual object recognition: A modeling study.,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45].We modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",MIT-CSAIL-TR-2004-001; AIM-2004-004; CBCL-235,38 p.; 65171868 bytes; 2503743 bytes,application/postscript; application/pdf,en_US,AI; object recognition; attention; vision; modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rakhlin, Alexander; Panchenko, Dmitry; Mukherjee, Sayan",2005-12-22T01:18:58Z,2005-12-22T01:18:58Z,2004-01-27,http://hdl.handle.net/1721.1/30443,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Risk Bounds for Mixture Density Estimation,"In this paper we focus on the problem of estimating a boundeddensity using a finite combination of densities from a givenclass. We consider the Maximum Likelihood Procedure (MLE) and the greedy procedure described by Li and Barron. Approximation and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\frac{1}{\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",MIT-CSAIL-TR-2004-002; AIM-2004-001; CBCL-233,11 p.; 9297095 bytes; 498791 bytes,application/postscript; application/pdf,en_US,AI; density estimation; MLE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wolf, Lior; Amnon Shashua,; Mukherjee, Sayan",2004-10-20T21:05:21Z,2004-10-20T21:05:21Z,2004-01-27,http://hdl.handle.net/1721.1/7282,AIM-2004-002; CBCL-234,Selecting Relevant Genes with a Spectral Approach,"Array technologies have made it possible to record simultaneously the expression pattern of thousands of genes. A fundamental problem in the analysis of gene expression data is the identification of highly relevant genes that either discriminate between phenotypic labels or are important with respect to the cellular process studied in the experiment: for example cell cycle or heat shock in yeast experiments, chemical or genetic perturbations of mammalian cell lines, and genes involved in class discovery for human tumors. In this paper we focus on the task of unsupervised gene selection. The problem of selecting a small subset of genes is particularly challenging as the datasets involved are typically characterized by a very small sample size ?? the order of few tens of tissue samples ??d by a very large feature space as the number of genes tend to be in the high thousands. We propose a model independent approach which scores candidate gene selections using spectral properties of the candidate affinity matrix. The algorithm is very straightforward to implement yet contains a number of remarkable properties which guarantee consistent sparse selections. To illustrate the value of our approach we applied our algorithm on five different datasets. The first consists of time course data from four well studied Hematopoietic cell lines (HL-60, Jurkat, NB4, and U937). The other four datasets include three well studied treatment outcomes (large cell lymphoma, childhood medulloblastomas, breast tumors) and one unpublished dataset (lymph status). We compared our approach both with other unsupervised methods (SOM,PCA,GS) and with supervised methods (SNR,RMB,RFE). The results clearly show that our approach considerably outperforms all the other unsupervised approaches in our study, is competitive with supervised methods and in some case even outperforms supervised approaches.",AIM-2004-002; CBCL-234,2062939 bytes; 836436 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wolf, Lior; Shashua, Amnon; Mukherjee, Sayan",2005-12-22T01:19:04Z,2005-12-22T01:19:04Z,2004-01-27,http://hdl.handle.net/1721.1/30444,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Selecting Relevant Genes with a Spectral Approach,"Array technologies have made it possible to record simultaneouslythe expression pattern of thousands of genes. A fundamental problemin the analysis of gene expression data is the identification ofhighly relevant genes that either discriminate between phenotypiclabels or are important with respect to the cellular process studied inthe experiment: for example cell cycle or heat shock in yeast experiments,chemical or genetic perturbations of mammalian cell lines,and genes involved in class discovery for human tumors. In this paperwe focus on the task of unsupervised gene selection. The problemof selecting a small subset of genes is particularly challengingas the datasets involved are typically characterized by a very smallsample size Â— in the order of few tens of tissue samples Â— andby a very large feature space as the number of genes tend to bein the high thousands. We propose a model independent approachwhich scores candidate gene selections using spectral properties ofthe candidate affinity matrix. The algorithm is very straightforwardto implement yet contains a number of remarkable properties whichguarantee consistent sparse selections. To illustrate the value of ourapproach we applied our algorithm on five different datasets. Thefirst consists of time course data from four well studied Hematopoieticcell lines (HL-60, Jurkat, NB4, and U937). The other fourdatasets include three well studied treatment outcomes (large celllymphoma, childhood medulloblastomas, breast tumors) and oneunpublished dataset (lymph status). We compared our approachboth with other unsupervised methods (SOM,PCA,GS) and withsupervised methods (SNR,RMB,RFE). The results clearly showthat our approach considerably outperforms all the other unsupervisedapproaches in our study, is competitive with supervised methodsand in some case even outperforms supervised approaches.",MIT-CSAIL-TR-2004-003; AIM-2004-002; CBCL-234,0 p.; 12089662 bytes; 629163 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rakhlin, Alexander; Panchenko, Dmitry; Mukherjee, Sayan",2004-10-20T21:05:19Z,2004-10-20T21:05:19Z,2004-01-27,http://hdl.handle.net/1721.1/7281,AIM-2004-001; CBCL-233,Risk Bounds for Mixture Density Estimation,"In this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class. We consider the Maximum Likelihood Procedure (MLE) and  the greedy procedure described by Li and Barron. Approximation  and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\\frac{1}{\\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",AIM-2004-001; CBCL-233,11 p.; 1656004 bytes; 658609 bytes,application/postscript; application/pdf,en_US,AI; density estimation; MLE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen; Shakhnarovich, Gregory; Darrell, Trevor",2004-10-08T20:43:09Z,2004-10-08T20:43:09Z,2004-01-28,http://hdl.handle.net/1721.1/6735,AIM-2004-003,Virtual Visual Hulls: Example-Based 3D Shape Estimation from a Single Silhouette,"Recovering a volumetric model of a person, car, or other object of interest from a single snapshot would be useful for many computer graphics applications. 3D model estimation in general is hard, and currently requires active sensors, multiple views, or integration over time. For a known object class, however, 3D shape can be successfully inferred from a single snapshot. We present a method for generating a ``virtual visual hull''-- an estimate of the 3D shape of an object from a known class, given a single silhouette observed from an unknown viewpoint. For a given class, a large database of multi-view silhouette examples from calibrated, though possibly varied, camera rigs are collected. To infer a novel single view input silhouette's virtual visual hull, we search for 3D shapes in the database which are most consistent with the observed contour. The input is matched to component single views of the multi-view training examples. A set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. The 3D shape estimate for the input is then found by interpolating between the contours of these aligned views. When the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. We show results of our algorithm on real and synthetic images of people.",AIM-2004-003,25 p.; 7098694 bytes; 2050007 bytes,application/postscript; application/pdf,en_US,AI; visual hulls; silhouettes; nearest neighbors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen; Shakhnarovich, Gregory; Darrell, Trevor",2005-12-22T01:19:14Z,2005-12-22T01:19:14Z,2004-01-28,http://hdl.handle.net/1721.1/30445,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Virtual Visual Hulls: Example-Based 3D Shape Estimation from a Single Silhouette,"Recovering a volumetric model of a person, car, or other objectof interest from a single snapshot would be useful for many computergraphics applications.  3D model estimation in general is hard, andcurrently requires active sensors, multiple views, or integration overtime.  For a known object class, however, 3D shape can be successfullyinferred from a single snapshot.  We present a method for generating a``virtual visual hull''-- an estimate of the 3D shape of an objectfrom a known class, given a single silhouette observed from an unknownviewpoint.  For a given class, a large database of multi-viewsilhouette examples from calibrated, though possibly varied, camerarigs are collected.  To infer a novel single view input silhouette'svirtual visual hull, we search for 3D shapes in the database which aremost consistent with the observed contour.  The input is matched tocomponent single views of the multi-view training examples.  A set ofviewpoint-aligned virtual views are generated from the visual hullscorresponding to these examples.  The 3D shape estimate for the inputis then found by interpolating between the contours of these alignedviews.  When the underlying shape is ambiguous given a single viewsilhouette, we produce multiple visual hull hypotheses; if a sequenceof input images is available, a dynamic programming approach isapplied to find the maximum likelihood path through the feasiblehypotheses over time.  We show results of our algorithm on real andsynthetic images of people.",MIT-CSAIL-TR-2004-004; AIM-2004-003,25 p.; 47215444 bytes; 7506667 bytes,application/postscript; application/pdf,en_US,AI; visual hulls; silhouettes; nearest neighbors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rinard, Martin; Cadar, Cristian; Dumitran, Daniel; Roy, Daniel M.; Jr., William S. Beebee",2005-12-22T01:19:21Z,2005-12-22T01:19:21Z,2004-02-06,http://hdl.handle.net/1721.1/30446,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Enhancing Availability and Security Through Failure-Oblivious Computing,"We present a new technique, failure-oblivious computing,that enables programs to continue to execute through memoryerrors without memory corruption. Our safe compilerfor C inserts checks that dynamically detect invalid memoryaccesses. Instead of terminating the execution or throwingan exception, the generated code simply discards invalidwrites and manufactures values to return for invalid reads,enabling the program to continue its normal execution.We have applied failure-oblivious computing to a set ofwidely-used programs that are part of the Linux-based opensourceinteractive computing environment. Our results showthat our techniques 1) make these programs invulnerableto known security attacks that exploit memory errors, and2) enable the programs to continue to operate successfullyto service legitimate requests and satisfy the needs of theirusers even after attacks trigger their memory errors.",MIT-CSAIL-TR-2004-005; MIT-LCS-TR-935,10 p.; 21109742 bytes; 788444 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shrobe, Howard; Laddaga, Robert",2005-12-22T01:19:31Z,2005-12-22T01:19:31Z,2004-02-09,http://hdl.handle.net/1721.1/30447,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New Architectural Models for Visibly Controllable Computing: The Relevance of Dynamic Object Oriented Architecturesand Plan Based Computing Models,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or perhaps on how to ease the system's continued evolution.  But if we look at life cycle costs, then we must conclude that the important question is how to make a system easy to operate.  To do this we need to make it easy for the operators to see what's going on and to then manipulate the system so that it does what it is supposed to.  This is a radically different criterion for success.What makes a computer system visible and controllable?  This is a difficult question, but it's clear that today's modern operating systems with nearly 50 million source lines of code are neither.  Strikingly, the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's mainstream sytsems, but with only 1 Million lines of code.  This paper is a retrospective examination of the features of the Lisp Machine hardware and software system.  Our key claim is that by building the Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained.It is our hope that this is a lesson that can impact tomorrow's designs.  We also speculate on how the spirit of the Lisp Machine could be extended to include a comprehensive access control model and how new layers of abstraction could further enrich this model.",MIT-CSAIL-TR-2004-006; AIM-2004-005,52 p.; 54496871 bytes; 1580494 bytes,application/postscript; application/pdf,en_US,AI; Software Environments; Computer Archicture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shrobe, Howard; Laddaga, Robert",2004-10-22T20:14:43Z,2004-10-22T20:14:43Z,2004-02-09,http://hdl.handle.net/1721.1/7286,AIM-2004-005,New Architectural Models for Visibly Controllable Computing: The Relevance of Dynamic Object Oriented Architectures and Plan Based Computing Models,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or  perhaps on how to ease the system's continued evolution. But if we look at life cycle costs, then we  must conclude that the important question is how to make a system easy to operate. To do this we  need to make it easy for the operators to see what's going on and to then manipulate the system so  that it does what it is supposed to. This is a radically different criterion for success.  What makes a computer system visible and controllable? This is a difficult question, but it's clear that  today's modern operating systems with nearly 50 million source lines of code are neither. Strikingly,  the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's  mainstream sytsems, but with only 1 Million lines of code. This paper is a retrospective examination of  the features of the Lisp Machine hardware and software system. Our key claim is that by building the  Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained. It is our hope that this is a lesson that can impact tomorrow's designs. We also speculate on how the  spirit of the Lisp Machine could be extended to include a comprehensive access control model and how  new layers of abstraction could further enrich this model.",AIM-2004-005,52 p.; 2594625 bytes; 829436 bytes,application/postscript; application/pdf,en_US,AI; Software Environments; Computer Archicture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lynch, Nancy; Stoica, Ion",2005-12-22T01:19:39Z,2005-12-22T01:19:39Z,2004-02-19,http://hdl.handle.net/1721.1/30448,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,MultiChord: A Resilient Namespace Management Protocol,"MultiChord is a new variant of the Chord namespace management algorithm [7] that includes lightweight mechanismsfor accommodating a limited rate of change, specifically, process joins and failures. This paper describes thealgorithm formally and evaluates its performance, using both simulation and analysis. Our main result is that lookupsare provably correctÂ—that is, each lookup returns results that are consistent with a hypothetical ideal system that differsfrom the actual system only in entries corresponding to recent joins and failuresÂ—in the presence of a limited rateof change. In particular, if the number of joins and failures that occur during a given time interval in a given regionof system are bounded, then all lookups are correct. A second result is a guaranteed upper bound for the latency of alookup operation in the absence of any other lookups in the system. Finally, we establish a relationship between thedeterministic assumptions of bounded joins and failures and the probabilistic assumptions (which are often used tomodel large scale networks). In particular, we derive a lower bound for the mean time between two violations of thedeterministic assumptions in a steady state system where joins and failures are modeled by Poisson processes.",MIT-CSAIL-TR-2004-007; MIT-LCS-TR-936,32 p.; 40331651 bytes; 1860288 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dolev, Shlomi; Gilbert, Seth; Lynch, Nancy A.; Shvartsman, Alex A.; Welch, Jennifer L.",2005-12-22T01:19:48Z,2005-12-22T01:19:48Z,2004-02-25,http://hdl.handle.net/1721.1/30449,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,GeoQuorums: Implementing Atomic Memory in Mobile Ad Hoc Networks,"We present a new approach, the GeoQuorums approach, for implementing atomic read/write shared memoryin mobile ad hoc networks. Our approach is based on associating abstract atomic objects with certain geographiclocations. We assume the existence of focal points, geographic areas that are normally Â“populatedÂ” by mobile nodes.For example, a focal point may be a road junction, a scenic observation point, or a water resource in the desert. Mobilenodes that happen to populate a focal point participate in implementing a shared atomic object, using a replicated statemachine approach. These objects, which we call focal point objects, are then used to implement atomic read/writeoperations on a virtual shared object, using our new GeoQuorums algorithm. The GeoQuorums algorithm uses aquorum-based strategy in which each each quorum consists of a set of focal point objects. The quorums are used tomaintain the consistency of the shared memory and to tolerate limited failures of the focal point objects, caused bydepopulation of the corresponding geographic areas. We present a mechanism for changing the set of quorums onthe fly, thus improving efficiency. Overall, the new GeoQuorums algorithm efficiently implements read and writeoperations in a highly dynamic, mobile network.",MIT-CSAIL-TR-2004-008; MIT-LCS-TR-900a,43 p.; 54380882 bytes; 2282502 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dolev, Shlomi; Gilbert, Seth; Lynch, Nancy A.; Schiller, Elad; Shvarstman, Alex A.; Welch, Jennifer",2005-12-22T01:19:54Z,2005-12-22T01:19:54Z,2004-02-26,http://hdl.handle.net/1721.1/30450,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Virtual Mobile Nodes for Mobile Ad Hoc Networks,"One of the most significant challenges introduced by mobile networks is the difficulty in coping withthe unpredictable movement of mobile nodes. If, instead, the mobile nodes could be programmed totravel through the world in a predictable and useful manner, the task of designing algorithms for mobilenetworks would be significantly simplified. Alas, users of mobile devices in the real world are notamenable to following instructions as to where their devices may travel.While real mobile nodes may be disinclined to move as desired, we propose executing algorithmson virtual mobile nodes that move in a predetermined, predictable, manner through the real world. Inthis paper, we define the Virtual Mobile Node Abstraction, and present selected algorithms that takeadvantage of virtual mobile nodes to simply and efficiently perform complicated tasks in highly dynamic,unpredictable mobile ad hoc networks.We then present the Mobile Point Emulator, a new algorithm that implements robust virtual mobilenodes. This algorithm replicates the virtual node at a constantly changing set of real nodes, choosingnew replicas as the real nodes move in and out of the path of the virtual node. We claim that the MobilePoint algorithm correctly implements a virtual mobile node, and that it is robust as long as the virtualnode travels through well-populated areas of the network.",MIT-CSAIL-TR-2004-009; MIT-LCS-TR-937,17 p.; 21915881 bytes; 913204 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,Riesenhuber; Jarudi; Gilad; Sinha,2005-12-22T01:20:02Z,2005-12-22T01:20:02Z,2004-03-05,http://hdl.handle.net/1721.1/30451,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Face processing in humans is compatible with a simple shape-based model of vision,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",MIT-CSAIL-TR-2004-010; AIM-2004-006; CBCL-236,12 p.; 14255528 bytes; 840975 bytes,application/postscript; application/pdf,en_US,AI; object recognition; faces; psychophysics; inversion effect; neuroscience; comput,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,Riesenhuber; Jarudi; Gilad; Sinha,2004-10-20T21:05:22Z,2004-10-20T21:05:22Z,2004-03-05,http://hdl.handle.net/1721.1/7283,AIM-2004-006; CBCL-236,Face processing in humans is compatible with a simple shape-based model of vision,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",AIM-2004-006; CBCL-236,12 p.; 1595221 bytes; 690861 bytes,application/postscript; application/pdf,en_US,AI; object recognition; faces; psychophysics; inversion effect; neuroscience; comput,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Abadi, Daniel J.; Madden, Samuel R.",2005-12-22T01:20:12Z,2005-12-22T01:20:12Z,2004-03-22,http://hdl.handle.net/1721.1/30452,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"REED: Robust, Efficient Filtering and Event Detection in Sensor Networks","This paper presents an algorithm for handling many types of filters insensor networks that cannot be expressed using a simple predicate.Specifically, the action of the filter may be predicated on sensor produceddata where an entire table of sensor-data/result-value pairs are needed toresolve the filter. We describe and evaluate three algorithms that canperform these filters that take advantage of database distributed jointechniques. Our join-based algorithms are capable of running in verylimited amounts of RAM, can distribute the storage burden over groups ofnodes, and are tolerant to dropped packets and node failures. REED isthus suitable for a wide range of event-detection applications thattraditional sensor network database and data collection systems cannot beused to implement.",MIT-CSAIL-TR-2004-011; MIT-LCS-TR-939,14 p.; 29691355 bytes; 1109049 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Stephenson, Mark; Amarasinghe, Saman",2005-12-22T01:20:19Z,2005-12-22T01:20:19Z,2004-03-22,http://hdl.handle.net/1721.1/30453,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Predicting Unroll Factors Using Nearest Neighbors,"In order to deliver the promise of MooreÂ’s Law to the enduser, compilers must make decisions that are intimately tiedto a specific target architecture. As engineers add architecturalfeatures to increase performance, systems becomeharder to model, and thus, it becomes harder for a compilerto make effective decisions.Machine-learning techniques may be able to help compilerwriters model modern architectures. Because learning techniquescan effectively make sense of high dimensional spaces,they can be a valuable tool for clarifying and discerningcomplex decision boundaries. In our work we focus on loopunrolling, a well-known optimization for exposing instructionlevel parallelism. Using the Open Research Compileras a testbed, we demonstrate how one can use supervisedlearning techniques to model the appropriateness of loopunrolling.We use more than 1,100 loops Â— drawn from 46 benchmarksÂ— to train a simple learning algorithm to recognizewhen loop unrolling is advantageous. The resulting classifiercan predict with 88% accuracy whether a novel loop(i.e., one that was not in the training set) benefits fromloop unrolling. Furthermore, we can predict the optimal ornearly optimal unroll factor 74% of the time. We evaluatethe ramifications of these prediction accuracies using theOpen Research Compiler (ORC) and the Itanium r  2 architecture.The learned classifier yields a 6% speedup (overORCÂ’s unrolling heuristic) for SPEC benchmarks, and a 7%speedup on the remainder of our benchmarks. Because thelearning techniques we employ run very quickly, we wereable to exhaustively determine the four most salient loopcharacteristics for determining when unrolling is beneficial.",MIT-CSAIL-TR-2004-012; MIT-LCS-TR-938,9 p.; 15158625 bytes; 629381 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yokono, Jerry Jun; Poggio, Tomaso",2005-12-22T01:25:52Z,2005-12-22T01:25:52Z,2004-03-24,http://hdl.handle.net/1721.1/30454,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Evaluation of sets of oriented and non-oriented receptive fields as local descriptors,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",MIT-CSAIL-TR-2004-013; AIM-2004-007; CBCL-237,20 p.; 86081886 bytes; 22425671 bytes,application/postscript; application/pdf,en_US,"AI; local descriptor; steerable filter; Gaussian derivatives; selectivity,invariance",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yokono, Jerry Jun; Poggio, Tomaso",2004-10-20T21:05:24Z,2004-10-20T21:05:24Z,2004-03-24,http://hdl.handle.net/1721.1/7284,AIM-2004-007; CBCL-237,Evaluation of sets of oriented and non-oriented receptive fields as local descriptors,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",AIM-2004-007; CBCL-237,20 p.; 3426196 bytes; 1925439 bytes,application/postscript; application/pdf,en_US,AI; local descriptor; steerable filter; Gaussian derivatives; selectivity; invariance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Donovan, Alan; Kiezun, Adam; Tschantz, Matthew S.; Ernst, Michael D.",2005-12-22T01:26:03Z,2005-12-22T01:26:03Z,2004-03-30,http://hdl.handle.net/1721.1/30456,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Converting Java Programs to Use Generic Libraries,"Java 1.5 will include a type system (called JSR-14) that supports parametric polymorphism, or generic classes. This will bring many benefits to Java programmers, not least because current Java practice makes heavy use of logically-generic classes, including container classes.Translation of Java source code into semantically equivalent JSR-14 source code requires two steps: parameterization (adding type parameters to class definitions) and instantiation (adding the type arguments at each use of a parameterized class). Parameterization need be done only once for a class, whereas instantiation must be performed for each client, of which there are potentially many more. Therefore, this work focuses on the instantiation problem. We present a technique to determine sound and precise JSR-14 types at each use of a class for which a generic type specification is available. Our approach uses a precise and context-sensitive pointer analysis to determine possible types at allocation sites, and a set-constraint-based analysis (that incorporates guarded, or conditional, constraints) to choose consistent types for both allocation and declaration sites. The technique handles all features of the JSR-14 type system, notably the raw types that provide backward compatibility. We have implemented our analysis in a tool that automatically inserts type parameters into Java code, and we report its performance when applied to a number of real-world Java programs.",MIT-CSAIL-TR-2004-015; MIT-LCS-TR-940,20 p.; 39929459 bytes; 1693357 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"McCamant, Stephen; Ernst, Michael D.",2005-12-22T01:25:58Z,2005-12-22T01:25:58Z,2004-03-30,http://hdl.handle.net/1721.1/30455,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Predicting Problems Caused by Component Upgrades,"This report presents a new, automatic technique to assess whether replacing a component of a softwaresystem by a purportedly compatible component may change the behavior of the system. The techniqueoperates before integrating the new component into the system or running system tests, permitting quickerand cheaper identification of problems. It takes into account the systemÂ’s use of the component, becausea particular component upgrade may be desirable in one context but undesirable in another. No formalspecifications are required, permitting detection of problems due either to errors in the component or toerrors in the system. Both external and internal behaviors can be compared, enabling detection of problemsthat are not immediately reflected in the output.The technique generates an operational abstraction for the old component in the context of the system,and one for the new component in the context of its test suite. An operational abstraction is a set of programproperties that generalizes over observed run-time behavior. Modeling a system as divided into modules,and taking into account the control and data flow between the modules, we formulate a logical conditionto guarantee that the systemÂ’s behavior is preserved across a component replacement. If automated logicalcomparison indicates that the new component does not make all the guarantees that the old one did, thenthe upgrade may affect system behavior and should not be performed without further scrutiny.We describe a practical implementation of the technique, incorporating enhancements to handle nonlocalstate, non-determinism, and missing test suites, and to distinguish old from new incompatibilities. Weevaluate the implementation in case studies using real-world systems, including the Linux C library and 48Unix programs. Our implementation identified real incompatibilities among versions of the C library thataffected some of the programs, and it approved the upgrades for other programs that were unaffected by thechanges.This report is a revision of the first authorÂ’s MasterÂ’s thesis, submitted January 2004.",MIT-CSAIL-TR-2004-014; MIT-LCS-TR-941,51 p.; 58257912 bytes; 2025302 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Rinard, Martin",2005-12-22T01:26:10Z,2005-12-22T01:26:10Z,2004-04-06,http://hdl.handle.net/1721.1/30457,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Generalized Records and Spatial Conjunction in Role Logic,"We have previously introduced role logic as a notation fordescribing properties of relational structures in shapeanalysis, databases and knowledge bases.  A natural fragmentof role logic corresponds to two-variable logic withcounting and is therefore decidable.We show how to use role logic to describe open and closedrecords, as well the dual of records, inverse records.  Weobserve that the spatial conjunction operation of separationlogic naturally models record concatenation.  Moreover, weshow how to eliminate the spatial conjunction of formulas ofquantifier depth one in first-order logic with counting.  Asa result, allowing spatial conjunction of formulas ofquantifier depth one preserves the decidability oftwo-variable logic with counting.  This result applies totwo-variable role logic fragment as well.The resulting logic smoothly integrates type system andpredicate calculus notation and can be viewed as a naturalgeneralization of the notation for constraints arising inrole analysis and similar shape analysis approaches.",MIT-CSAIL-TR-2004-016; MIT-LCS-TR-942,30 p.; 29046951 bytes; 1214158 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Georgiou, Chryssis; Musial, Peter M.; Shvartsman, Alexander A.",2005-12-22T01:26:16Z,2005-12-22T01:26:16Z,2004-04-12,http://hdl.handle.net/1721.1/30458,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Long-Lived Rambo: Trading Knowledge for Communication,"Shareable data services providing consistency guarantees, such as atomicity (linearizability), make building distributedsystems easier. However, combining linearizability with efficiency in practical algorithms is difficult. A reconfigurablelinearizable data service, called Rambo, was developed by Lynch and Shvartsman. This service guarantees consistencyunder dynamic conditions involving asynchrony, message loss, node crashes, and new node arrivals. The specificationof the original algorithm is given at an abstract level aimed at concise presentation and formal reasoning aboutcorrectness. The algorithm propagates information by means of gossip messages. If the service is in use for along time, the size and the number of gossip messages may grow without bound. This paper presents a consistentdata service for long-lived objects that improves on Rambo in two ways: it includes an incremental communicationprotocol and a leave service. The new protocol takes advantage of the local knowledge, and carefully manages thesize of messages by removing redundant information, while the leave service allows the nodes to leave the systemgracefully. The new algorithm is formally proved correct by forward simulation using levels of abstraction. Anexperimental implementation of the system was developed for networks-of-workstations. The paper also includesselected analytical and preliminary empirical results that illustrate the advantages of the new algorithm.",MIT-CSAIL-TR-2004-017; MIT-LCS-TR-943,28 p.; 36030589 bytes; 1489589 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wentzlaff, David; Agarwal, Anant",2005-12-22T01:26:26Z,2005-12-22T01:26:26Z,2004-04-13,http://hdl.handle.net/1721.1/30459,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"A Quantitative Comparison of Reconfigurable, Tiled, and Conventional Architectures on Bit-level Computation","General purpose computing architectures are being called on to work on amore diverse application mix every day.  This has been fueled by the needfor reduced time to market and economies of scale that are the hallmarksof software on general purpose microprocessors.  As this application mixexpands, application domains such as bit-level computation, which hasprimarily been the domain of ASICs and FPGAs, will need to be effectivelyhandled by general purpose hardware.  Examples of bit-level applicationsinclude Ethernet framing, forward error correction encoding/decoding, andefficient state machine implementation.In this paper we compare how differing computational structures such asASICs, FPGAs, tiled architectures, and superscalar microprocessors areable to compete on bit-level communication applications.  A quantitativecomparison in terms of absolute performance and performance per area willbe presented.  These results show that although modest gains~(2-3x) inabsolute performance can be achieved when using FPGAs versus tunedmicroprocessor implementations, it is the significantly larger gains~(2-3orders of magnitude) that can be achieved in performance per area thatwill motivate work on supporting bit-level computation in a generalpurpose fashion in the future.",MIT-CSAIL-TR-2004-018; MIT-LCS-TR-944,11 p.; 16819251 bytes; 664884 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio",2004-10-08T20:43:12Z,2004-10-08T20:43:12Z,2004-04-14,http://hdl.handle.net/1721.1/6737,AIM-2004-009,Contextual Influences on Saliency,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",AIM-2004-009,12 p.; 2980182 bytes; 1698158 bytes,application/postscript; application/pdf,en_US,AI; Attention; context; saliency; scene recognition; object detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Katti, Sachin; Katabi, Dina; Kohler, Eddie; Strauss, Jacob",2005-12-22T01:27:11Z,2005-12-22T01:27:11Z,2004-04-14,http://hdl.handle.net/1721.1/30462,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"M&M: A Passive Toolkit for Measuring, Correlating, and Tracking Path Characteristics","This paper presents M&M, a passive measurement toolkitsuitable for large-scale studies of Internet path characteristics.The multiQ tool uses equally-spaced mode gaps in TCP flowsÂ’packet interarrival time distributions to detect multiple bottleneckcapacities and their relative order. Unlike previous tools,multiQ can discover up to three bottlenecks fromthe tcpdumptrace of a single flow, and can work with acknowledgment aswell as data interarrivals.We also describe the mystery tool, asimple TCP loss event, packet loss, and RTT analyzer designedto work in concert with multiQ. The M&M toolkit can measuresimple path properties; correlate different types of measurementof the same path, producing new kinds of results; andbecause M&M is passive, it can use publicly-available traces totrack the value of a measurement over multiple years.We validate our tools in depth using the RON overlay network[4], which provides more than 400 heterogeneous Internetpaths and detailed information about their characteristics.We compare multiQ with Nettimer and Pathrate, two othercapacity measurement tools, in the first wide-area, real-worldvalidation of capacity measurement techniques. Each tool accuratelydiscovers minimum capacities (85% of measurementsare within 10%of the true value); multiQ additionally discoversmultiple bottlenecks and their orderings. We also use ourtoolkit to perform several measurement studies using a reservoirof 375 million traced packets spanning the last two years.Among the results of these studies are that bottleneck capacityon our traced links has gone up by around an order ofmagnitudefrom 2002 to 2004, and that differences in levels of statisticalmultiplexing on 10 Mb/s and 100 Mb/s bottleneck links resultin flows over those links having similar fair-share bandwidths.",MIT-CSAIL-TR-2004-022; MIT-LCS-TR-945,13 p.; 25499604 bytes; 1134347 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2004-10-08T20:43:10Z,2004-10-08T20:43:10Z,2004-04-14,http://hdl.handle.net/1721.1/6736,AIM-2004-008,Sharing visual features for multiclass and multiview object detection,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",AIM-2004-008,17 p.; 4223512 bytes; 1537371 bytes,application/postscript; application/pdf,en_US,AI; Object detection; sharing features; feature selection; multiclass; Boosting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio",2005-12-22T01:26:54Z,2005-12-22T01:26:54Z,2004-04-14,http://hdl.handle.net/1721.1/30460,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Contextual Influences on Saliency,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",MIT-CSAIL-TR-2004-020; AIM-2004-009,12 p.; 18377938 bytes; 597436 bytes,application/postscript; application/pdf,en_US,AI; Attention; context; saliency; scene recognition; object detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Weinstein, Eugene; Steele, Kenneth; Agarwal, Anant; Glass, James",2005-12-22T01:27:05Z,2005-12-22T01:27:05Z,2004-04-14,http://hdl.handle.net/1721.1/30461,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A 1020-Node Modular Microphone Array and Beamformer for Intelligent Computing Spaces,"Ubiquitous computing environments are characterized by an unboundedamount of noise and crosstalk. In these environments, traditionalmethods of sound capture are insufficient, and array microphones areneeded in order to obtain a clean recording of desired speech. In thiswork, we have designed, implemented, and tested LOUD, a novel 1020-nodemicrophone array utilizing the Raw tile parallel processorarchitecture for computation. To the best of our knowledge,this is currently the largest microphone array in the world. We haveexplored the uses of the array within ubiquitous computing scenarios byimplementing an acoustic beamforming algorithm for sound sourceamplification in a noisy environment, and have obtained preliminaryresults demonstrating the efficacy of the array. From one to 1020microphones, we have shown a 13.7dB increase in peak SNR on arepresentative utterance, an 87.2% drop in word error rate withinterferer present, and an 89.6% drop in WER without an interferer.",MIT-CSAIL-TR-2004-021; MIT-LCS-TM-642,18 p.; 26946578 bytes; 901671 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2005-12-19T22:41:45Z,2005-12-19T22:41:45Z,2004-04-14,http://hdl.handle.net/1721.1/30399,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Sharing visual features for multiclass and multiview object detection,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",MIT-CSAIL-TR-2004-019; AIM-2004-008,17 p.; 24172096 bytes; 1434721 bytes,application/postscript; application/pdf,en_US,AI; Object detection; sharing features; feature selection; multiclass; Boosting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Perez-Breva, Luis",2005-12-22T01:27:18Z,2005-12-22T01:27:18Z,2004-04-21,http://hdl.handle.net/1721.1/30463,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Cascading Regularized Classifiers,"Among the various methods to combine classifiers, Boosting was originally thought as an stratagem to cascade pairs of classifiers through their disagreement. I recover the same idea from the work of Niyogi et al. to show how to loosen the requirement of weak learnability, central to Boosting, and introduce a new cascading stratagem. The paper concludes with an empirical study of an implementation of the cascade that, under assumptions that mirror the conditions imposed by Viola and Jones in [VJ01], has the property to preserve the generalization ability of boosting.",MIT-CSAIL-TR-2004-023; AIM-2004-028,8 p.; 8847621 bytes; 505102 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chockler, Gregory; Malkhi, Dahlia",2005-12-22T01:27:28Z,2005-12-22T01:27:28Z,2004-04-22,http://hdl.handle.net/1721.1/30464,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Light-Weight Leases for Storage-Centric Coordination,"We propose light-weight lease primitives to leverage fault-tolerant coordination among clients accessing a shared storage infrastructure (such as network attached disks or storage servers). In our approach, leases are implemented from the very shared data that they protect. That is, there is no global lease manager, there is a lease per data item (e.g., a file, a directory, a disk partition, etc.) or a collection thereof. Our lease primitives are useful for facillitating exculsive access to data in systems satisfying certain timeliness constraints. In addition, they can be utilized as a building block for implementing dependable services resilient to timing failures. In particular, we show a simple lease based solution for fault-tolerant Consensus which is a benchmark distributed coordination problem.",MIT-CSAIL-TR-2004-024; MIT-LCS-TR-934,26 p.; 28838794 bytes; 1106660 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yokono, Jerry Jun; Poggio, Tomaso",2004-10-20T21:05:26Z,2004-10-20T21:05:26Z,2004-04-27,http://hdl.handle.net/1721.1/7285,AIM-2004-010; CBCL-238,Rotation Invariant Object Recognition from One Training Example,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",AIM-2004-010; CBCL-238,15 p.; 5162833 bytes; 968095 bytes,application/postscript; application/pdf,en_US,AI; object recognition; local descriptor; rotation invariant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yokono, Jerry Jun; Poggio, Tomaso",2005-12-22T01:30:35Z,2005-12-22T01:30:35Z,2004-04-27,http://hdl.handle.net/1721.1/30465,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Rotation Invariant Object Recognition from One Training Example,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",MIT-CSAIL-TR-2004-025; AIM-2004-010; CBCL-238,15 p.; 38274547 bytes; 7811820 bytes,application/postscript; application/pdf,en_US,AI; object recognition; local descriptor; rotation invariant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Zollei, Lilla; Fisher, John; Wells, William",2004-10-08T20:43:13Z,2004-10-08T20:43:13Z,2004-04-28,http://hdl.handle.net/1721.1/6738,AIM-2004-011,A Unified Statistical and Information Theoretic Framework for Multi-modal Image Registration,"We formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  A unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. Additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. We analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. Among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",AIM-2004-011,21 p.; 2760680 bytes; 531001 bytes,application/postscript; application/pdf,en_US,AI; registration; information theory; unified framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Zollei, Lilla; Fisher, John; Wells, William",2005-12-22T01:30:39Z,2005-12-22T01:30:39Z,2004-04-28,http://hdl.handle.net/1721.1/30466,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Unified Statistical and Information Theoretic Framework for Multi-modal Image Registration,"We formulate and interpret several multi-modal registration methods inthe context of a unified statistical and information theoretic framework. A unified interpretation clarifies the implicit assumptionsof each method yielding a better understanding of their relativestrengths and weaknesses. Additionally, we discuss a generativestatistical model from which we derive a novel analysis tool, the""auto-information function"", as a means of assessing and exploiting thecommon spatial dependencies inherent in multi-modal imagery. Weanalytically derive useful properties of the ""auto-information"" aswell as verify them empirically on multi-modal imagery. Among theuseful aspects of the ""auto-information function"" is that it canbe computed from imaging modalities independently and it allows one todecompose the search space of registration problems.",MIT-CSAIL-TR-2004-026; AIM-2004-011,21 p.; 17309765 bytes; 765629 bytes,application/postscript; application/pdf,en_US,AI; registration; information theory; unified framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gupta, Anjali; Krohn, Maxwell; Walfish, Michael",2005-12-22T01:30:43Z,2005-12-22T01:30:43Z,2004-05-05,http://hdl.handle.net/1721.1/30467,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Can Basic ML Techniques Illuminate Rateless Erasure Codes?,"The recently developed rateless erasure codes are a near-optimal channel coding technique that guaranteeslow overhead and fast decoding. The underlying theory, and current implementations, of thesecodes assume that a network transmitter encodes according to a pre-specified probability distribution.In this report, we use basic Machine Learning techniques to try to understand what happens when thisassumption is false. We train several classes of models using certain features that describe the empiricaldistribution realized at a network receiver, and we investigate whether these models can Â“learnÂ” topredict whether a given encoding will require extra overhead. Our results are mixed.",MIT-CSAIL-TR-2004-027; MIT-LCS-TM-643,15 p.; 20087603 bytes; 791808 bytes,application/postscript; application/pdf,en_US,,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Arkoudas, Konstantine; Zee, Karen; Kuncak, Viktor; Rinard, Martin",2005-12-22T01:30:49Z,2005-12-22T01:30:49Z,2004-05-06,http://hdl.handle.net/1721.1/30468,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Verifying a File System Implementation,"We present a correctness proof for a basic file system implementation. This implementation contains key elements of standard Unix file systems such as inodes and fixed-size disk blocks. We prove the implementation correct by establishing a simulation relation between the specification of the file system (which models the file system as an abstract map from file names to sequences of bytes) and its implementation (which uses fixed-size disk blocks to store the contents of the files).We used the Athena proof checker to represent and validate our proof. Our experience indicates that Athena's use of block-structured natural deduction, support for structural induction and proof abstraction, and seamless connection with high-performance automated theorem provers were essential to our ability to successfully manage a proof of this size.",MIT-CSAIL-TR-2004-028; MIT-LCS-TR-946,31 p.; 26287249 bytes; 1119970 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sand, Peter; Teller, Seth",2005-12-22T01:30:57Z,2005-12-22T01:30:57Z,2004-05-11,http://hdl.handle.net/1721.1/30469,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Video Matching,"This paper describes a method for bringing two videos (recorded at different times) into spatiotemporal alignment, then comparing and combining corresponding pixels for applications such as background subtraction, compositing, and increasing dynamic range. We align a pair of videos by searching for frames that best match according to a robust image registration process. This process uses locally weighted regression to interpolate and extrapolate high-likelihood image correspondences, allowing new correspondences to be discovered and refined. Image regions that cannot be matched are detected and ignored, providing robustness to changes in scene content and lighting, which allows a variety of new applications.",MIT-CSAIL-TR-2004-029; MIT-LCS-TR-947,8 p.; 23656728 bytes; 1181560 bytes,application/postscript; application/pdf,en_US,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Feamster, Nick; Balakrishnan, Hari",2005-12-22T01:31:12Z,2005-12-22T01:31:12Z,2004-05-17,http://hdl.handle.net/1721.1/30471,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Verifying the Correctness of Wide-Area Internet Routing,"Several studies have shown that wide-area Internet routing is fragile, with failures occurring for a variety of reasons. Routing fragility is largely due to the flexible and powerful ways in which BGP can be configured to perform various tasks, which range from implementing the policies of commercial relationships to configuring backup paths. Configuring routers in an AS is like writing a distributed program, and BGP's flexible configuration and today's relatively low-level configuration languages make the process error-prone. The primary method used by operators to determine whether their complex configurations are correct is to try them out in operation.We believe that there is a need for a systematic approach to verifying router configurations before they are deployed. This paper develops a static analysis framework for configuration checking, and uses it in the design of rcc, a ``router configuration checker''. rcc takes as input a set of router configurations and flags anomalies and errors, based on a set of well-defined correctness conditions. We have used rcc to check BGP configurations from 9 operational networks, testing nearly 700 real-world router configurations in the process. Every network we analyzed had configuration errors, some of which were potentially serious and had previously gone unnoticed. Our analysis framework and results also suggest ways in which BGP and configuration languages should be improved. rcc has also been downloaded by 30 network operators to date.",MIT-CSAIL-TR-2004-031; MIT-LCS-TR-948,14 p.; 28764880 bytes; 1265172 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Salcianu, Alexandru; Rinard, Martin",2005-12-22T01:31:05Z,2005-12-22T01:31:05Z,2004-05-17,http://hdl.handle.net/1721.1/30470,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Combined Pointer and Purity Analysis for Java Programs,"We present a new method purity analysis for Java programs.A method is pure if it does not mutate any location that exists in the program state right before method invocation.Our analysis is built on top of a combined pointer and escape analysis for Java programs and is capable of determining that methods are pure even when the methods do heap mutation, provided that the mutation affects only objects created after the beginning of the method. Because our analysis extracts a precise representation of the region of the heap that each method may access, it is able to provide useful information even for methods with externally visible side effects. In particular, it can recognize read-only parameters (a parameter is read-only if the method does not mutate any objects transitively reachable from the parameter) and safe parameters (a parameter is safe if it is read-only and the method does not create any new externally visible paths in the heap to objects transitively reachable from the parameter). The analysis can also generate regular expressions that characterize the externally visible heap locations that the method mutates.We have implemented our analysis and used it to analyze several data structure implementations. Our results show that our analysis effectively recognize a variety of pure methods, including pure methods that allocate and mutate complex auxiliary data structures. Even if the methods are not pure, our analysis can provide information which may enable developers to usefully bound the potential side effects of the method.",MIT-CSAIL-TR-2004-030; MIT-LCS-TR-949,16 p.; 28889089 bytes; 1207932 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kennell, Jonathan",2004-10-20T20:32:23Z,2004-10-20T20:32:23Z,2004-05-18,http://hdl.handle.net/1721.1/7113,AITR-2004-002,Generative Temporal Planning with Complex Processes,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems. This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander. To support such a system, this thesis presents the Spock generative planner. To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes. This is in contrast to traditional planners, whose operators represent simple atomic or durative actions. Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes. RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators. Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution. Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts. This thesis describes the Spock algorithm in detail, along with example problems and test results.",AITR-2004-002,90 p.; 15726143 bytes; 1269432 bytes,application/postscript; application/pdf,en_US,"AI; planning ""temporal planning""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Stamatoiu, Oana L.",2004-10-20T20:32:25Z,2004-10-20T20:32:25Z,2004-05-18,http://hdl.handle.net/1721.1/7114,AITR-2004-001,Learning Commonsense Categorical Knowledge in a Thread Memory System,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",AITR-2004-001,96 p.; 6550712 bytes; 1993377 bytes,application/postscript; application/pdf,en_US,AI; learning; context; categorization; similarity; Bridge; thread memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kennell, Jonathan",2005-12-22T01:31:24Z,2005-12-22T01:31:24Z,2004-05-18,http://hdl.handle.net/1721.1/30472,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Generative Temporal Planning with Complex Processes,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems.  This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander.  To support such a system, this thesis presents the Spock generative planner.  To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes.  This is in contrast to traditional planners, whose operators represent simple atomic or durative actions.  Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes.  RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators.  Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution.  Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts.  This thesis describes the Spock algorithm in detail, along with example problems and test results.",MIT-CSAIL-TR-2004-032; AITR-2004-002,90 p.; 56421512 bytes; 2495752 bytes,application/postscript; application/pdf,en_US,"AI; planning ""temporal planning""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Jon, Doyle; Kohane, Isaac; Long, William; Szolovits, Peter",2005-12-19T23:26:14Z,2005-12-19T23:26:14Z,2004-05-18,http://hdl.handle.net/1721.1/30413,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"The Architecture of MAITA: A Tool for Monitoring, Analysis, and Interpretation","This report describes the aims, functions, and organization of the MAITAsystem for knowledge-based construction, adaptation, and control of networks of monitoringprocesses.",MIT-CSAIL-TR-2004-034; MIT-LCS-TR-951,68 p.; 67174302 bytes; 2280148 bytes,application/postscript; application/pdf,en_US,,Clinical Decision Making,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Stamatoiu, Oana L.",2005-12-22T01:31:32Z,2005-12-22T01:31:32Z,2004-05-18,http://hdl.handle.net/1721.1/30473,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning Commonsense Categorical Knowledge in a Thread Memory System,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",MIT-CSAIL-TR-2004-033; AITR-2004-001,96 p.; 68735708 bytes; 2432875 bytes,application/postscript; application/pdf,en_US,AI; learning; context; categorization; similarity; Bridge; thread memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Demsky, Brian; Rinard, Martin",2005-12-22T01:31:40Z,2005-12-22T01:31:40Z,2004-05-18,http://hdl.handle.net/1721.1/30474,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Data Structure Repair Using Goal-Directed Reasoning,"Model-based data structure repair is a promising techniquefor enabling programs to continue to execute successfullyin the face of otherwise fatal data structure corruption errors.Previous research in this  eld relied on the developerto write a speci cation to explicitly translate model repairsinto concrete data structure repairs, raising the possibilityof 1) incorrect translations causing the supposedly repairedconcrete data structures to be inconsistent, and 2) repairedmodels with no corresponding concrete data structure representation.We present a new repair algorithm that uses goal-directedreasoning to automatically translate model repairs into concretedata structure repairs. This new repair algorithm eliminatesthe possibility of incorrect translations and repairedmodels with no corresponding representation as concretedata structures. Unlike our old algorithm, our new algorithmcan also repair linked data structures such as a list ora tree.",MIT-CSAIL-TR-2004-035; MIT-LCS-TR-950,20 p.; 34348723 bytes; 1265029 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Goler, Jonathan A.",2004-10-20T20:32:27Z,2004-10-20T20:32:27Z,2004-05-28,http://hdl.handle.net/1721.1/7115,AITR-2004-003,BioJADE: A Design and Simulation Tool for Synthetic Biological Systems,"The next generations of both biological engineering and computer engineering demand that control be  exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems  may provide us with the ability to build cells that are capable of a plethora of activities, from  computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not  only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a  comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical  design tool built in Java, utilizing a database back end, and supports a range of simulations using an  XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can  compile designs into actual DNA, and then generate synthesis instructions to build the physical parts.  The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for  synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks  repository, which enables the sharing of BioBrick components between researchers, and vastly reduces  the barriers to entry for aspiring Synthetic Biologists.",AITR-2004-003,54 p.; 5905018 bytes; 1459615 bytes,application/postscript; application/pdf,en_US,AI; BioJADE; Synthetic Biology; DNA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Goler, Jonathan A.",2005-12-22T01:31:51Z,2005-12-22T01:31:51Z,2004-05-28,http://hdl.handle.net/1721.1/30475,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,BioJADE: A Design and Simulation Tool for Synthetic Biological Systems,"The next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical design tool built in Java, utilizing a database back end, and supports a range of simulations using an XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can compile designs into actual DNA, and then generate synthesis instructions to build the physical parts. The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks repository, which enables the sharing of BioBrick components between researchers, and vastly reduces the barriers to entry for aspiring Synthetic Biologists.",MIT-CSAIL-TR-2004-036; AITR-2004-003,54 p.; 53265402 bytes; 4755231 bytes,application/postscript; application/pdf,en_US,AI; BioJADE; Synthetic Biology; DNA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taylor, Michael Bedford",2005-12-22T01:31:55Z,2005-12-22T01:31:55Z,2004-06-07,http://hdl.handle.net/1721.1/30476,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Deionizer: A Tool for Capturing and Embedding I/O Cells,"In this paper, we introduce the concept of a deionizer. A deionizeris a special type of partial evaluator whose purpose is to create a newversion of a program that can run without accessing a partial set of I/O resources.Although a deionizer can be used for application embedding, this short paper addresses the use of dionization for improving benchmark accuracy.The paper briefly discusses the key ideas and then explains the implementation and useof the MIT deionizer. This deionizer was used to produce the results for a recent conference paper that compares theRaw processor to a Pentium III.",MIT-CSAIL-TR-2004-037; MIT-LCS-TM-644,3 p.; 3974101 bytes; 235085 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taylor, Michael Bedford; Lee, Walter; Amarasinghe, Saman; Agarwal, Anant",2005-12-22T01:32:06Z,2005-12-22T01:32:06Z,2004-06-08,http://hdl.handle.net/1721.1/30477,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Scalar Operand Networks: Design, Implementation, and Analysis","The bypass paths and multiported register files in microprocessors serve as an implicit interconnect tocommunicate operand values among pipeline stages and multiple ALUs. Previous superscalar designs implementedthis interconnect using centralized structures that do not scale with increasing ILP demands. Insearch of scalability, recent microprocessor designs in industry and academia exhibit a trend toward distributedresources such as partitioned register files, banked caches, multiple independent compute pipelines,and even multiple program counters. Some of these partitioned microprocessor designs have begun to implementbypassing and operand transport using point-to-point interconnects. We call interconnects optimizedfor scalar data transport, whether centralized or distributed, scalar operand networks. Although thesenetworks share many of the challenges of multiprocessor networks such as scalability and deadlock avoidance,they have many unique requirements, including ultra-low latencies (a few cycles versus tens of cycles)and ultra-fast operation-operand matching. This paper discusses the unique properties of scalar operandnetworks (SONs), examines alternative ways of implementing them, and introduces the AsTrO taxonomy todistinguish between them. It discusses the design of two alternative networks in the context of the Raw microprocessor,and presents detailed timing, area and energy statistics for a real implementation. The paperalso presents a 5-tuple performance model for SONs and analyzes their performance sensitivity to networkproperties for ILP workloads.",MIT-CSAIL-TR-2004-038; MIT-LCS-TM-645,27 p.; 60061887 bytes; 2516865 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rabbah, Rodric M.; Bratt, Ian; Asanovic, Krste; Agarwal, Anant",2005-12-22T01:32:17Z,2005-12-22T01:32:17Z,2004-06-14,http://hdl.handle.net/1721.1/30478,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Versatility and VersaBench: A New Metric and a Benchmark Suite for Flexible Architectures,"For the last several decades, computer architecture research has largely benefited from, and continues to be driven by ad-hoc benchmarking. Often the benchmarks are selected to represent workloads that architects believe should run on the computational platforms they design. For example, benchmark suites such as SPEC, Winstone, and MediaBench, which represent workstation, desktop and media workloads respectively, have influenced computer architecture innovation for the last decade. Recently, advances in VLSI technology have created an increasing interest within the computer architecture community to build a new kind of processor that is more flexible than extant general purpose processors. Such new processor architectures must efficiently support a broad class of applications including graphics, networking, and signal processing in addition to the traditional desktop workloads. Thus, given the new focus on flexibility demands, a new benchmark suite and new metrics are necessary to accurately reflect the goals of the architecture community. This paper thus proposes VersaBench as a new benchmark suite, and a new Versatility measure to characterize architectural flexibility, or in other words, the ability of the architecture to effectively execute a wide array of workloads. The benchmark suite is composed of applications drawn from several domains including desktop, server, stream, and bit-level processing. The Versatility measure is a single scalar metric inspired by the SPEC paradigm. It normalizes processor performance on each benchmark by that of the highest-performing machine for that application. This paper reports the measured versatility for several existing processors, as well as for some new and emerging research processors. The benchmark suite is freely distributed, and we are actively cataloging and sharing results for various reference processors.",MIT-CSAIL-TR-2004-039; MIT-LCS-TM-646,17 p.; 39574971 bytes; 2672127 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hearn, Robert A.",2005-12-22T01:34:55Z,2005-12-22T01:34:55Z,2004-06-16,http://hdl.handle.net/1721.1/30479,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Building Grounded Abstractions for Artificial Intelligence Programming,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",MIT-CSAIL-TR-2004-040; AITR-2004-004,58 p.; 45433655 bytes; 1795607 bytes,application/postscript; application/pdf,en_US,AI; Artificial Intelligence; Society of Mind; Multi-Agent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hearn, Robert A.",2004-10-20T20:32:29Z,2004-10-20T20:32:29Z,2004-06-16,http://hdl.handle.net/1721.1/7116,AITR-2004-004,Building Grounded Abstractions for Artificial Intelligence Programming,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from  particular drawbacks. High-level AI uses abstractions that often have no relation to the way real,  biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. I have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit.  I then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. The new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. As a  demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a  network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior.",AITR-2004-004,58 p.; 330188 bytes; 26969 bytes,application/postscript; application/pdf,en_US,AI; Artificial Intelligence; Society of Mind; Multi-Agent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Teevan, Jaime",2004-10-08T20:43:15Z,2004-10-08T20:43:15Z,2004-06-18,http://hdl.handle.net/1721.1/6739,AIM-2004-012,How People Re-find Information When the Web Changes,"This paper investigates how people return to information in a dynamic information environment. For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed. Changes can benefit users by providing new information, but they hinder returning to previously viewed information. The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment. A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not. While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution. The implications of these observations for systems that support re-finding in dynamic environments are discussed.",AIM-2004-012,9 p.; 1451699 bytes; 688288 bytes,application/postscript; application/pdf,en_US,AI; re-finding; information management; dynamic information,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Teevan, Jaime",2005-12-22T01:35:01Z,2005-12-22T01:35:01Z,2004-06-18,http://hdl.handle.net/1721.1/30480,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,How People Re-find Information When the Web Changes,"This paper investigates how people return to information in a dynamic information environment.  For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed.  Changes can benefit users by providing new information, but they hinder returning to previously viewed information.  The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  The implications of these observations for systems that support re-finding in dynamic environments are discussed.",MIT-CSAIL-TR-2004-041; AIM-2004-012,9 p.; 16783533 bytes; 654765 bytes,application/postscript; application/pdf,en_US,AI; re-finding; information management; dynamic information,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Walfish, Michael; Stribling, Jeremy; Krohn, Maxwell; Balakrishnan, Hari; Morris, Robert; Shenker, Scott",2005-12-22T01:35:08Z,2005-12-22T01:35:08Z,2004-06-24,http://hdl.handle.net/1721.1/30481,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Middleboxes No Longer Considered Harmful,"Intermediate network elements, such as network address translators (NATs), firewalls, and transparent caches are now commonplace. The usual reaction in the network architecture community to these so-called middleboxes is a combination of scorn (because they violate important architectural principles) and dismay (because these violations make the Internet less flexible). While we acknowledge these concerns, we also recognize that middleboxes have become an Internet fact of life for important reasons. To retain their functions while eliminating their dangerous side-effects, we propose an extension to the Internet architecture, called the Delegation-Oriented Architecture (DOA), that not only allows, but also facilitates, the deployment of middleboxes. DOA involves two relatively modest changes to the current architecture: (a) a set of references that are carried in packets and serve as persistent host identifiers and (b) a way to resolve these references to delegates chosen by the referenced host.",MIT-CSAIL-TR-2004-042; MIT-LCS-TR-954,16 p.; 31058710 bytes; 1338617 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2005-12-22T01:35:14Z,2005-12-22T01:35:14Z,2004-06-25,http://hdl.handle.net/1721.1/30482,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Contextual models for object detection using boosted random fields,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",MIT-CSAIL-TR-2004-043; AIM-2004-013,10 p.; 11085755 bytes; 604755 bytes,application/postscript; application/pdf,en_US,AI; Object detection; context; boosting; BP; random fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2004-10-08T20:43:16Z,2004-10-08T20:43:16Z,2004-06-25,http://hdl.handle.net/1721.1/6740,AIM-2004-013,Contextual models for object detection using boosted random fields,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",AIM-2004-013,10 p.; 2184856 bytes; 906515 bytes,application/postscript; application/pdf,en_US,AI; Object detection; context; boosting; BP; random fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hsu, Eugene; Gentry, Sommer; Popovic, Jovan",2008-08-25T19:00:19Z,2008-08-25T19:00:19Z,2004-07-01,http://hdl.handle.net/1721.1/41944,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Example-Based Control of Human Motion,"In human motion control applications, the mapping between a control specification and an appropriate target motion often defies an explicit encoding. We present a method that allows such a mapping to be defined by example, given that the control specification is recorded motion. Our method begins by building a database of semantically meaningful instances of the mapping, each of which is represented by synchronized segments of control and target motion. A dynamic programming algorithm can then be used to interpret an input control specification in terms of mapping instances. This interpretation induces a sequence of target segments from the database, which is concatenated to create the appropriate target motion. We evaluate our method on two examples of indirect control. In the first, we synthesize a walking human character that follows a sampled trajectory. In the second, we generate a synthetic partner for a dancer whose motion is acquired through motion capture.",,,,,,,,,,,"In ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pages 69-77, July 2004.",Jovan Popovic; Computer Graphics,,,,,,,,,,,,,,,,,,,,,,
,"Indyk, Piotr; Woodruff, David",2004-10-08T20:43:17Z,2004-10-08T20:43:17Z,2004-07-02,http://hdl.handle.net/1721.1/6741,AIM-2004-014,Optimal Approximations of the Frequency Moments,"We give a one-pass, O~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. Together with known lower bounds, this resolves the main problem left open by Alon, Matias, Szegedy, STOC'96. Our algorithm enables deletions as well as insertions of stream elements.",AIM-2004-014,18 p.; 3705201 bytes; 761567 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Indyk, Piotr; Woodruff, David",2005-12-22T01:35:22Z,2005-12-22T01:35:22Z,2004-07-02,http://hdl.handle.net/1721.1/30483,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Optimal Approximations of the Frequency Moments,"We give a one-pass, O~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. Together with known lower bounds, this resolves the main problem left open by Alon, Matias, Szegedy, STOC'96. Our algorithm enables deletions as well as insertions of stream elements.",MIT-CSAIL-TR-2004-044; AIM-2004-014,18 p.; 18493060 bytes; 833343 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Badoiu, Mihai; Indyk, Piotr; Sidiropoulos, Anastasios",2005-12-22T01:35:27Z,2005-12-22T01:35:27Z,2004-07-05,http://hdl.handle.net/1721.1/30484,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Constant-Factor Approximation Algorithm for Embedding Unweighted Graphs into Trees,"We present a constant-factor approximation algorithm for computing anembedding of the shortest path metric of an unweighted graph into atree, that minimizes the multiplicative distortion.",MIT-CSAIL-TR-2004-045; AIM-2004-015,8 p.; 6685848 bytes; 331083 bytes,application/postscript; application/pdf,en_US,AI; embeddings; approximation algorithms; trees,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Badoiu, Mihai; Indyk, Piotr; Sidiropoulos, Anastasios",2004-10-08T20:43:19Z,2004-10-08T20:43:19Z,2004-07-05,http://hdl.handle.net/1721.1/6742,AIM-2004-015,A Constant-Factor Approximation Algorithm for Embedding Unweighted Graphs into Trees,"We present a constant-factor approximation algorithm for computing an embedding of the shortest path metric of an unweighted graph into a tree, that minimizes the multiplicative distortion.",AIM-2004-015,8 p.; 981451 bytes; 626039 bytes,application/postscript; application/pdf,en_US,AI; embeddings; approximation algorithms; trees,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Heo, Seongmoo; Asanovic, Krste",2005-12-22T01:35:34Z,2005-12-22T01:35:34Z,2004-07-12,http://hdl.handle.net/1721.1/30485,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Dynamically Resizable Static CMOS Logic for Fine-Grain Leakage,"Digital circuits often have a critical path that runs through a smallsubset of the component subblocks, but where the path changes dynamicallyduring operation.  Dynamically resizable static CMOS (DRCMOS) logic isproposed as a fine-grain leakage reduction technique that dynamicallydownsizes transistors in inactive subblocks while maintaining speed insubblocks along the current critical path.  A 64-entry register free listand a 64-entry pick-two arbiter are used to evaluate DRCMOS. DRCMOS isshown to give a 50% reduction in total power for equal delay in a70 nm technology.",MIT-CSAIL-TR-2004-046; MIT-LCS-TR-957,5 p.; 7595008 bytes; 408267 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tauber, Joshua A.; Garland, Stephen J.",2005-12-22T01:35:52Z,2005-12-22T01:35:52Z,2004-07-19,http://hdl.handle.net/1721.1/30487,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Definition and Expansion of Composite Automata in IOA,"The IOA language provides notations for defining both primitive and composite I/O automata.This note describes, both formally and with examples, the constraints on these definitions, thecomposability requirements for the components of a composite automaton, and the transformationof a composite automaton into an equivalent primitive automaton.Section 2 introduces four examples used throughout this note to illustrate new definitions andoperations. Section 3 treats IOA programs for primitive I/O automata: it introduces notationsfor describing the syntactic structures that appear in these programs, and it lists syntactic andsemantic conditions that these programs must satisfy to represent valid primitive I/O automata.Section 4 describes how to reformulate primitive IOA programs into an equivalent but more regular(desugared) form that is used in later definitions in this note. Section 5 treats IOA programsfor composite I/O automata: it introduces notations for describing the syntactic structures thatappear in these programs, describes resortings induced by them, and lists syntactic and semanticconditions that these programs must satisfy to represent valid composite I/O automata. Section 6describes the translation of the name spaces of component automata into a unified name spacefor the composite automaton. Section 7 shows how to expand an IOA program for a compositeautomaton into an equivalent IOA program for a primitive automaton. The expansion is generatedby combining syntactic structures of the desugared programs for the component automata afterapplying appropriate replacements of sorts and variables. Section 8 details the expansion of thecomposite automaton introduced in Section 2 using the desugared forms developed throughoutSections 4Â–6 and the techniques described in Section 7. Finally, Section 9 gives a precise definitionof the resortings and substitutions used to replace sorts and variables.",MIT-CSAIL-TR-2004-048; MIT-LCS-TR-959,91 p.; 77470182 bytes; 2979546 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Nguyen, Huu Hai; Rinard, Martin",2005-12-22T01:35:58Z,2005-12-22T01:35:58Z,2004-07-19,http://hdl.handle.net/1721.1/30488,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,An Algorithm for Deciding BAPA: Boolean Algebra with Presburger Arithmetic,"We describe an algorithm for deciding the first-order multisorted theory BAPA, which combines 1) Boolean algebras of sets of uninterpreted elements (BA) and 2) Presburger arithmetic operations (PA). BAPA can express the relationship between integer variables and cardinalities of sets, and supports arbitrary quantification over both sets and integers.Our motivation for BAPA is deciding verification conditions that arise in the static analysis of data structure consistency properties. Data structures often use an integer variable to keep track of the number of elements they store; an invariant of such a data structure is that the value of the integer variable is equal to the number of elements stored in the data structure. When the data structure content is represented by a set, the resulting constraints can be captured in BAPA. BAPA formulas with quantifier alternations arise when annotations contain quantifiers themselves, or when proving simulation relation conditions for refinement and equivalence of program fragments. Furthermore, BAPA constraints can be used to extend the techniques for proving the termination of integer programs to programs that manipulate data structures, and have applications in constraint databases.We give a formal description of a decision procedure for BAPA, which implies the decidability of the satisfiability and validity problems for BAPA. We analyze our algorithm and obtain an elementary upper bound on the running time, thereby giving the first complexity bound for BAPA. Because it works by a reduction to PA, our algorithm yields the decidability of a combination of sets of uninterpreted elements with any decidable extension of PA. Our algorithm can also be used to yield an optimal decision procedure for BA though a reduction to PA with bounded quantifiers.We have implemented our algorithm and used it to discharge verification conditions in the Jahob system for data structure consistency checking of Java programs; our experience with the algorithm is promising.",MIT-CSAIL-TR-2004-049; MIT-LCS-TR-958,26 p.; 26003902 bytes; 1120584 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Vaziri, Mandana; Tauber, Joshua A.; Tsai, Michael J.; Lynch, Nancy",2005-12-22T01:35:40Z,2005-12-22T01:35:40Z,2004-07-19,http://hdl.handle.net/1721.1/30486,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Systematic Removal of Nondeterminism for Code Generation in I/O Automata,"The Input/Output  (I/O) automaton model  developed by Lynch and Tuttle models components in asynchronous concurrentsystems as labeled transition systems.  IOA is a precise language for describing I/O automata and for stating their properties.  A toolset is beingdeveloped for IOA  to support distributed software design and implementation. One of the tools consists of a userassisted code generator fromIOA into an imperative programming language such as C or Java. One aspect that distinguishes IOA programs from programs written inimperative languages  is the presence of nondeterminism  which comesin the form of explicit nondeterministic statements and implicit scheduling choices made during execution.  Code generation therefore consistspartially of systematically removing all forms of nondeterminism. In this paper, we describe our approach and design for code generation.We focus on the issue of removing implicit nondeterminism  and specify a transformation on IOA programs that makes all nondeterminismexplicit.  The programmer can then replace all explicit nondeterminismwith deterministic statements  prior to code generation.  We also describethis transformation at a semantic level  i.e., at the level of the I/O automaton mathematical model.  We show that the transformation definedat the IOA level conforms to the one at the semantic level.",MIT-CSAIL-TR-2004-047; MIT-LCS-TR-960,16 p.; 14677208 bytes; 626419 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kemp, Charles; Griffiths, Thomas L.; Tenenbaum, Joshua B.",2005-12-22T01:36:09Z,2005-12-22T01:36:09Z,2004-07-22,http://hdl.handle.net/1721.1/30489,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Discovering Latent Classes in Relational Data,"We present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  Our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  This model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.We apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",MIT-CSAIL-TR-2004-050; AIM-2004-019,12 p.; 13382538 bytes; 572002 bytes,application/postscript; application/pdf,en_US,AI; learning; categorization; relations; kinship,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Uzuner, Ozlem",2005-12-22T01:36:15Z,2005-12-22T01:36:15Z,2004-07-25,http://hdl.handle.net/1721.1/30490,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Distribution Volume Tracking on Privacy-Enhanced Wireless Grid,"In this paper, we discuss a wireless grid in which users are highly mobile, and form ad-hoc and sometimes short-lived connections with other devices.  As they roam through networks, the users may choose to employ privacy-enhancing technologies to address their privacy needs and benefit from the computational power of the grid for a variety of tasks, including sharing content.  The high rate of mobility of the users on the wireless grid, when combined with privacy enhancing mechanisms and ad-hoc connections, makes it difficult to conclusively link devices and/or individuals with network activities and to hold them liable for particular downloads.  Protecting intellectual property in this scenario requires a solution that can work in absence of knowledge about behavior of particular individuals.  Building on previous work, we argue for a solution that ensures proper compensation to content owners without inhibiting use and dissemination of works.  Our proposal is based on digital tracking for measuring distribution volume of content and compensation of authors based on this accounting information.  The emphasis is on obtaining good estimates of rate of popularity of works, without keeping track of activities of individuals or devices.  The contribution of this paper is a revenue protection mechanism, Distribution Volume Tracking, that does not invade the privacy of users in the wireless grid and works even in the presence of privacy-enhancing technologies they may employ.",MIT-CSAIL-TR-2004-051; AIM-2004-018,7 p.; 11942167 bytes; 409314 bytes,application/postscript; application/pdf,en_US,AI; Intellectual Property Protection; Privacy; Wireless Computational Grid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Serre, Thomas; Riesenhuber, Maximilian",2005-12-22T01:36:22Z,2005-12-22T01:36:22Z,2004-07-27,http://hdl.handle.net/1721.1/30491,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Realistic Modeling of Simple and Complex Cell Tuning in the HMAXModel, and Implications for Invariant Object Recognition in Cortex","Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",MIT-CSAIL-TR-2004-052; AIM-2004-017; CBCL-239,11 p.; 24089158 bytes; 2715073 bytes,application/postscript; application/pdf,en_US,AI; object recognition; simple cell; complex cell; hmax; V1; IT; view-tuned unit; in,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sezgin, Tevfik Metin; Davis, Randall",2005-12-22T01:36:30Z,2005-12-22T01:36:30Z,2004-07-28,http://hdl.handle.net/1721.1/30492,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Early Sketch Processing with Application in HMM Based Sketch Recognition,"Freehand sketching is a natural and crucial part of everyday humaninteraction, yet is almost totally unsupported by current user interfaces. With the increasing availability of tablet notebooks and pen based PDAs, sketchbased interaction has gained attention as a natural interaction modality.We are working to combine the flexibility and ease of use of paper and pencilwith the processing power of a computer, to produce a user interface fordesign that feels as natural as paper, yet is considerably smarter. One of themost basic tasks in accomplishing this is converting the original digitizedpen strokes in a sketch into the intended geometric objects. In this paper wedescribe an implemented system that combines multiple sources of knowledge toprovide robust early processing for freehand sketching. We also show how thisearly processing system can be used as part of a fast sketch recognition system with polynomial time segmentation and recognition algorithms.",MIT-CSAIL-TR-2004-053; AIM-2004-016,16 p.; 29102772 bytes; 1401214 bytes,application/postscript; application/pdf,en_US,AI; Sketch Recognition; Early Sketch Processing; Shape Approximation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Leong, Ben; Liskov, Barbara; Demaine, Erik D.",2005-12-22T01:36:42Z,2005-12-22T01:36:42Z,2004-08-13,http://hdl.handle.net/1721.1/30493,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,EpiChord: Parallelizing the Chord Lookup Algorithm with Reactive Routing State Management,"EpiChord is a DHT lookup algorithm that demonstrates that we canremove the O(log n)-state-per-node restriction on existing DHTtopologies to achieve significantly better lookup performance andresilience using a novel reactive routing state maintenance strategythat amortizes network maintenance costs into existing lookups and byissuing parallel queries. Our technique allows us to design a newclass of unlimited-state-per-node DHTs that is able to adapt naturallyto a wide range of lookup workloads. EpiChord is able to achieveO(1)-hop lookup performance under lookup-intensive workloads, and atleast O(log n)-hop lookup performance under churn-intensiveworkloads even in the worst case (though it is expected to performbetter on average).Our reactive routing state maintenance strategy allows us to maintainlarge amounts of routing state with only a modest amount of bandwidth,while parallel queries serve to reduce lookup latency and allow us toavoid costly lookup timeouts.  In general, EpiChord exploits theinformation gleaned from observing lookup traffic to improve lookupperformance, and only sends network probes when necessary. Nodespopulate their caches mainly from observing network traffic, andcache entries are flushed from the cache after a fixed lifetime.Our simulations show that with our approach can reduce both lookuplatencies and pathlengths by a factor of 3 by issuing only 3 queriesasynchronously in parallel per lookup.  Furthermore, we show that weare able to achieve this result with minimal additional communicationoverhead and the number of messages generated per lookup is no morethan that for the corresponding sequential Chord lookup algorithm overa range of lookup workloads.  We also present a novel token-passingstabilization scheme that automatically detects and repairs globalrouting inconsistencies.",MIT-CSAIL-TR-2004-054; MIT-LCS-TR-963,18 p.; 27633175 bytes; 1206942 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rodrigues, Rodrigo; Liskov, Barbara",2005-12-22T01:40:26Z,2005-12-22T01:40:26Z,2004-08-13,http://hdl.handle.net/1721.1/30494,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Byzantine Fault Tolerance in Long-Lived Systems,"This paper proposes counter-measures that can be deployedas part of a replicated system to reduce the size ofW, and thus reduce the class of attacks to which the system is vulnerable. Obviously it will not be possible to withstandall attacks via this technique, in particular attacks with verysmall A. But we will propose techniques that can reduceWto quite a small value.In the remainder of this paper, we discuss how to lowerthe value of W. We begin by discussing attacks. Then wediscuss some prior work in this area and why it is insufficient.The final section describes the approach we propose.",MIT-CSAIL-TR-2004-055; MIT-LCS-TR-962,3 p.; 6194127 bytes; 262522 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob",2006-06-01T16:22:22Z,2006-06-01T16:22:22Z,2004-09,http://hdl.handle.net/1721.1/32985,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Programming an Amorphous Computational Medium,"Amorphous computing considers the problem of controllingmillions of spatially distributed unreliable devices which communicateonly with nearby neighbors. To program such a system, we need a highleveldescription language for desired global behaviors, and a system tocompile such descriptions into locally executing code which robustly createsand maintains the desired global behavior. I survey existing amorphouscomputing primitives and give desiderata for a language describingcomputation on an amorphous computer. I then bring these together inAmorphous Medium Language, which computes on an amorphous computeras though it were a space-filling computational medium.",MIT-CSAIL-TR-2006-039,16 p.; 4880361 bytes; 386051 bytes,application/postscript; application/pdf,en_US,distributed computing sensor networks,Mathematics and Computation,,,,,"J.-P. Banatre et al. (Eds.): UPP 2004, LNCS 3566, pp. 121Â–136, 2005.Springer-Verlag Berlin Heidelberg 2005",,,,,,,,,,,,,,,,,,,,,,,
,"Kreiman, Gabriel; Hung, Chou; Poggio, Tomaso; DiCarlo, James",2005-12-19T23:29:04Z,2005-12-19T23:29:04Z,2004-09-21,http://hdl.handle.net/1721.1/30417,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Selectivity of Local Field Potentials in Macaque Inferior Temporal Cortex,"While single neurons in inferior temporal (IT) cortex show differential responses to distinct complex stimuli, little is known about the responses of populations of neurons in IT.  We recorded single electrode data, including multi-unit activity (MUA) and local field potentials (LFP), from 618 sites in the inferior temporal cortex of macaque monkeys while the animals passively viewed 78 different pictures of complex stimuli. The LFPs were obtained by low-pass filtering the extracellular electrophysiological signal with a corner frequency of 300 Hz. As reported previously, we observed that spike counts from MUA showed selectivity for some of the pictures.  Strikingly, the LFP data, which is thought to constitute an average over large numbers of neurons, also showed significantly selective responses.  The LFP responses were less selective than the MUA responses both in terms of the proportion of selective sites as well as in the selectivity of each site. We observed that there was only little overlap between the selectivity of MUA and LFP recordings from the same electrode.  To assess the spatial organization of selective responses, we compared the selectivity of nearby sites recorded along the same penetration and sites recorded from different penetrations.  We observed that MUA selectivity was correlated on spatial scales up to 800 &#61549;m while the LFP selectivity was correlated over a larger spatial extent, with significant correlations between sites separated by several mm.  Our data support the idea that there is some topographical arrangement to the organization of selectivity in inferior temporal cortex and that this organization may be relevant for the representation of object identity in IT.",MIT-CSAIL-TR-2004-056; AIM-2004-020; CBCL-240,1 p.; 154663209 bytes; 29794050 bytes,application/postscript; application/pdf,en_US,AI; object recognition; inferior temporal cortex; local field potentials,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Arsenio, Artur Miguel",2005-12-22T14:51:02Z,2005-12-22T14:51:02Z,2004-09-26,http://hdl.handle.net/1721.1/30591,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Cognitive-Developmental Learning for a Humanoid Robot: A Caregiver's Gift,"The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties.This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes.Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",MIT-CSAIL-TR-2004-057; AITR-2004-006,388 p.; 610948893 bytes; 19073417 bytes,application/postscript; application/pdf,en_US,AI; Humanoid Robots; Developmental Learning; Perception; Human-robot Interactions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Benjamin, Michael R.",2005-12-19T23:28:02Z,2005-12-19T23:28:02Z,2004-09-27,http://hdl.handle.net/1721.1/30416,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Interval Programming Model for Multi-objective Decision Making,"The interval programming model (IvP) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  The central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. The piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.In this paper the model and solution algorithms are described, and theapplicability of IvP to certain applications arediscussed.",MIT-CSAIL-TR-2004-058; AIM-2004-021,32 p.; 42228177 bytes; 2845444 bytes,application/postscript; application/pdf,en_US,AI; multi-objective decision making; behavior-based control; action selection; MCDM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yeo, Gene; Van Nostrand, Eric; Holste, Dirk; Poggio, Tomaso; Burge, Christopher",2005-12-19T23:22:45Z,2005-12-19T23:22:45Z,2004-09-30,http://hdl.handle.net/1721.1/30411,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Predictive identification of alternative events conserved in human and mouse,"Alternative pre-messenger RNA splicing affects a majority of human genes and plays important roles in development and disease.  Alternative splicing (AS) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  Here we describe sequence features that distinguish exons subject to evolutionarily conserved AS, which we call 'alternative-conserved exons' (ACEs) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, ACEScan.  Genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted ACEs.  Alternative splicing was verified in both human and mouse tissues using an RT-PCR-sequencing protocol for 21 of 30 (70%) predicted ACEs tested, supporting the validity of a majority of ACEScan predictions.  By contrast, AS was observed in mouse tissues for only 2 of 15 (13%) tested exons which had EST or cDNA evidence of AS in human but were not predicted ACEs, and was never observed for eleven negative control exons in human or mouse tissues.  Predicted ACEs were much more likely to preserve reading frame, and less likely to disrupt protein domains than other AS events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, RNA processing and development.  Our results also imply that the vast majority of AS events represented in the human EST databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",MIT-CSAIL-TR-2004-059; AIM-2004-022,56 p.; 49600059 bytes; 2638503 bytes,application/postscript; application/pdf,en_US,AI; alternative splicing; comparative genomics; classification; regularization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tucker-Kellogg, Lisa",2005-12-19T23:32:53Z,2005-12-19T23:32:53Z,2004-10-01,http://hdl.handle.net/1721.1/30419,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Systematic Conformational Search with Constraint Satisfaction,"Throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  This thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state NMR spectroscopy.  A separate portion of the thesis isabout protein-DNA binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.The search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromNMR experiments.  Systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  Our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) OmniMerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  Although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  Propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of OmniMerge.(3) The search may be run in two stages, once at low-resolutionusing a side-effect of OmniMerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) An A* function prioritizes each subchain based onestimated future search costs.  Subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.A common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.In addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.These novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",MIT-CSAIL-TR-2004-060; AITR-2004-007,177 p.; 127791565 bytes; 5501537 bytes,application/postscript; application/pdf,en_US,AI; Distance Geometry; Nuclear Magnetic Resonance (NMR); Molecular Modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lam, Patrick; Kuncak, Viktor; Rinard, Martin",2005-12-19T23:39:47Z,2005-12-19T23:39:47Z,2004-10-04,http://hdl.handle.net/1721.1/30421,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Our Experience with Modular Pluggable Analyses,"We present a technique that enables the focused applicationof multiple analyses to di erent modules in thesame program. In our approach, each module encapsulatesone or more data structures and uses membershipin abstract sets to characterize how objects participatein data structures. Each analysis veri es that the implementationof the module 1) preserves important internaldata structure consistency properties and 2) correctlyimplements an interface that uses formulas in a set algebrato characterize the e ects of operations on theencapsulated data structures. Collectively, the analysesuse the set algebra to 1) characterize how objects participatein multiple data structures and to 2) enable theinter-analysis communication required to verify propertiesthat depend on multiple modules analyzed by differentanalyses.We have implemented our system and deployed threepluggable analyses into it: a ag analysis for modulesin which abstract set membership is determined by aag  eld in each object, a plugin for modules that encapsulatelinked data structures such as lists and trees,and an array plugin in which abstract set membershipis determined by membership in an array. Our experimentalresults indicate that our approach makes it possibleto e ectively combine multiple analyses to verifyproperties that involve objects shared by multiple modules,with each analysis analyzing only those modulesfor which it is appropriate.",MIT-CSAIL-TR-2004-061; MIT-LCS-TR-965,21 p.; 36771229 bytes; 1343453 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Georgiou, Chryssis; Mavrommatis, Panayiotis P.; Tauber, Joshua A.",2005-12-19T23:25:19Z,2005-12-19T23:25:19Z,2004-10-06,http://hdl.handle.net/1721.1/30412,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Implementing Asynchronous Distributed Systems Using the IOA Toolkit,"This document is a report about the capabilities and performance of the IOA Toolkit, and in particularthe tools that provide support for implementing and running distributed systems (checker,composer, code generator). The Toolkit compiles distributed systems specified in IOA into Javaclasses, which run on a network of workstations and communicate using the Message Passing Interface(MPI). In order to test the toolkit, several distributed algorithms were implemented, rangingfrom simple algorithms such as LCR leader election in a ring network to more complex algorithmssuch as the GHS algorithm for computing the minimum spanning tree in an arbitrary graph. Allof our experiments completed successfully, and several runtime measurements were made.",MIT-CSAIL-TR-2004-062; MIT-LCS-TR-966,107 p.; 63601552 bytes; 2713486 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Pacheo, Carlos; Ernst, Michael D.",2005-12-19T23:26:54Z,2005-12-19T23:26:54Z,2004-10-14,http://hdl.handle.net/1721.1/30414,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Eclat: Automatic Generation and Classification of Test Inputs,"This paper describes a technique that helps a test engineerselect, from a large set of randomly generated testinputs, a small subset likely to reveal faults in the softwareunder test. The technique takes a program or software component,plus a set of normal executionsÂ—say, from an existingtest suite, or from observations of the software runningproperly. The technique works by extracting an operationalmodel of the softwareÂ’s operation, and comparingeach inputÂ’s operational pattern of execution against themodel. Test inputs whose operational pattern is suggestiveof a fault are further reduced by selecting only one inputper such pattern. The result is a small portion of the originalinputs, deemed most likely to reveal faults. Thus, ourtechnique can also be seen as an error-detection technique.We have implemented these ideas in the Eclat tool, designedfor unit testing of Java classes. Eclat generates alarge number of inputs and uses our technique to select onlya few of them as fault-revealing. The inputs that it selectsare an order of magnitude more likely to reveal faults thannon-selected inputs.",MIT-CSAIL-TR-2004-063; MIT-LCS-TR-968,10 p.; 16601827 bytes; 677668 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yang, Xiaowei",2005-12-22T01:41:13Z,2005-12-22T01:41:13Z,2004-10-14,http://hdl.handle.net/1721.1/30495,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,NIRA: A New Internet Routing Architecture,"The present Internet routing system faces two challengingproblems. First, unlike in the telephone system, Internet users cannotchoose their wide-area Internet service providers (ISPs) separatelyfrom their local access providers.  With the introduction of newtechnologies such as broadband residential service andfiber-to-the-home, the local ISP market is often a monopoly or aduopoly. The lack of user choice is likely to reduce competition amongwide-area ISPs, limiting the incentives for wide-area ISPs to improvequality of service, reduce price, and offer new services. Second, thepresent routing system fails to scale effectively in the presence ofreal-world requirements such as multi-homing for robust and redundantInternet access. A multi-homed site increases the amount of routingstate maintained globally by the Internet routing system. As thedemand for multi-homing continues to rise, the amount of routing statecontinues to grow.This dissertation presents the design of a new Internet routingarchitecture (NIRA) that simultaneously addresses these twoproblems. NIRA gives a user the ability to choose the sequence ofInternet service providers his packets traverse. It also has betterscaling characteristics than today's routing system. The design ofNIRA is decomposed into four modular components: route discovery,route availability discovery, route representation and packetforwarding, and provider compensation. This dissertation describesmechanisms to realize each of these components. It also makes clearthose places in the design where a globally agreed mechanism isneeded, and those places where alternative mechanisms can be designedand deployed locally. In particular, this dissertation describes ascalable route discovery mechanism. With this mechanism, a user onlyneeds to know a small region of the Internet in order to select aroute to reach a destination. In addition, a novel routerepresentation and packet forwarding scheme is designed such that asource and a destination address can uniquely represent a sequence ofproviders a packet traverses.Network measurement, simulation, and analytic modeling are used incombination to evaluate the design of NIRA. The evaluation suggeststhat NIRA is scalable.",MIT-CSAIL-TR-2004-064; MIT-LCS-TR-967,183 p.; 173514347 bytes; 5643389 bytes,application/postscript; application/pdf,en_US,,Advanced Network Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Steinkraus, Kurt; Kaelbling, Leslie Pack",2005-12-22T01:41:41Z,2005-12-22T01:41:41Z,2004-10-21,http://hdl.handle.net/1721.1/30496,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining dynamic abstractions in large MDPs,"One of the reasons that it is difficult to plan and act in real-worlddomains is that they are very large.  Existing research generallydeals with the large domain size using a static representation andexploiting a single type of domain structure.  In this paper, wecreate a framework that encapsulates existing and new abstraction andapproximation methods into modules, and combines arbitrary modulesinto a system that allows for dynamic representation changes.  We showthat the dynamic changes of representation allow our framework tosolve larger and more interesting domains than were previouslypossible, and while there are no optimality guarantees, suitablemodule choices gain tractability at little cost to optimality.",MIT-CSAIL-TR-2004-065; AIM-2004-023,12 p.; 9975204 bytes; 424481 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kandula, Srikanth; Katabi, Dina; Jacob, Matthias; Berger, Arthur",2005-12-22T02:14:46Z,2005-12-22T02:14:46Z,2004-10-22,http://hdl.handle.net/1721.1/30497,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Botz-4-Sale: Surviving Organized DDoS Attacks that Mimic Flash Crowds,"Recent denial of service attacks are mounted by professionalsusing Botnets of tens of thousands of compromisedmachines. To circumvent detection, attackers areincreasingly moving away from pure bandwidth  oods toattacks that mimic the Web browsing behavior of a largenumber of clients, and target expensive higher-layer resourcessuch as CPU, database and disk bandwidth. Theresulting attacks are hard to defend against using standardtechniques as the malicious requests differ from thelegitimate ones in intent but not in content.We present the design and implementation of Kill-Bots, a kernel extension to protect Web servers againstDDoS attacks that masquerade as  ash crowds. Kill-Botsprovides authentication using graphical tests but is differentfrom other systems that use graphical tests. First,instead of authenticating clients based on whether theysolve the graphical test, Kill-Bots uses the test to quicklyidentify the IP addresses of the attack machines. Thisallows it to block the malicious requests while allowingaccess to legitimate users who are unable or unwillingto solve graphical tests. Second, Kill-Bots sends a testand checks the client's answer without allowing unauthenticatedclients access to sockets, TCBs, worker processes,etc. This protects the authentication mechanismfrom being DDoSed. Third, Kill-Bots combines authenticationwith admission control. As a result, it improvesperformance, regardless of whether the server overloadis caused by DDoS or a true Flash Crowd. We have implementedKill-Bots in the Linux kernel and evaluated itin the wide-area Internet using PlanetLab.",MIT-CSAIL-TR-2004-066; MIT-LCS-TR-969,15 p.; 27361453 bytes; 1271267 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Rinard, Martin",2005-12-22T02:14:54Z,2005-12-22T02:14:54Z,2004-10-25,http://hdl.handle.net/1721.1/30498,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Spatial Conjunction as Second-Order Logic,"Spatial conjunction is a powerful construct for reasoning about dynamically allocateddata structures, as well as concurrent, distributed and mobile computation. Whileresearchers have identified many uses of spatial conjunction, its precise expressive powercompared to traditional logical constructs was not previously known.In this paper we establish the expressive power of spatial conjunction. We construct anembedding from first-order logic with spatial conjunction into second-order logic, and moresurprisingly, an embedding from full second order logic into first-order logic with spatialconjunction. These embeddings show that the satisfiability of formulas in first-order logicwith spatial conjunction is equivalent to the satisfiability of formulas in second-order logic.These results explain the great expressive power of spatial conjunction and can be usedto show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidablelogic. As one example, we show that adding unrestricted spatial conjunction totwo-variable logic leads to undecidability.On the side of decidability, the embedding into second-order logic immediately implies thedecidability of first-order logic with a form of spatial conjunction over trees. The embeddinginto spatial conjunction also has useful consequences: because a restricted form of spatialconjunction in two-variable logic preserves decidability, we obtain that a correspondinglyrestricted form of second-order quantification in two-variable logic is decidable. The resultinglanguage generalizes the first-order theory of boolean algebra over sets and is useful inreasoning about the contents of data structures in object-oriented languages.",MIT-CSAIL-TR-2004-067; MIT-LCS-TR-970,16 p.; 15950544 bytes; 692945 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Monteleoni, Claire; Balakrishnan, Hari; Feamster, Nick; Jaakkola, Tommi",2005-12-22T02:15:02Z,2005-12-22T02:15:02Z,2004-10-27,http://hdl.handle.net/1721.1/30499,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Managing the 802.11 Energy/Performance Tradeoff with Machine Learning,"This paper addresses the problem of managing the tradeoff betweenenergy consumption and performance in wireless devices implementingthe IEEE 802.11 standard. To save energy, the 802.11 specificationproposes a power-saving mode (PSM), where a device can sleep to saveenergy, periodically waking up to receive packets from a neighbor(e.g., an access point) that may have buffered packets for thesleeping device. Previous work has shown that a fixed polling time forwaking up degrades the performance of Web transfers, because networkactivity is bursty and time-varying. We apply a new online machinelearning algorithm to this problem and show, using ns simulation andtrace analysis, that it is able to adapt well to network activity. Thelearning process makes no assumptions about the underlying networkactivity being stationary or even Markov. Our learning power-savingalgorithm, LPSM, guides the learning using a ""loss function"" thatcombines the increased latency from potentially sleeping too long andthe wasted use of energy in waking up too soon.  In our nssimulations, LPSM saved 7%-20% more energy than 802.11 in power-savingmode, with an associated increase in average latency by a factor of1.02, and not more than 1.2.  LPSM is straightforward to implementwithin the 802.11 PSM framework.",MIT-CSAIL-TR-2004-068; MIT-LCS-TR-971,14 p.; 23210224 bytes; 1542849 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gilbert, Seth; Malewicz, Grzegorz",2005-12-19T23:27:35Z,2005-12-19T23:27:35Z,2004-10-29,http://hdl.handle.net/1721.1/30415,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Quorum Deployment Problem,"Quorum systems are commonly used to maintain the consistency of replicated data in adistributed system. Much research has been devoted to developing quorum systems with good theoreticalproperties, such as fault tolerance and high availability. However, even given a theoreticallygood quorum system, it is not obvious how to efficiently deploy such a system in a real network. Thispaper introduces a new combinatorial optimization problem, the Quorum Deployment Problem, andstudies its complexity. We demonstrate that it is NP-hard to approximate the Quorum DeploymentProblem within any factor of n?, where n is the number of nodes in the distributed network and ? > 0.The problem is NP-hard in even the simplest possible distributed network: a one-dimensional line withmetric cost. We begin to study algorithms for variants of the problem. Some variants can be solved optimallyin polynomial time and some NP-hard variants can be approximated to within a constant factor.",MIT-CSAIL-TR-2004-069; MIT-LCS-TR-972,20 p.; 25684763 bytes; 1002668 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Werfel, Justin",2005-12-22T02:15:12Z,2005-12-22T02:15:12Z,2004-11-09,http://hdl.handle.net/1721.1/30500,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Neural Network Models for Zebra Finch Song Production and Reinforcement Learning,"The zebra finch is a standard experimental system for studying learning and generation of temporally extended motor patterns.  The first part of this project concerned the evaluation of simple models for the operation and structure of the network in the motor nucleus RA.  A directed excitatory chain with a global inhibitory network, for which experimental evidence exists, was found to produce waves of activity similar to those observed in RA; this similarity included one particularly important feature of the measured activity, synchrony between the onset of bursting in one neuron and the offset of bursting in another.  Other models, which were simpler and more analytically tractable, were also able to exhibit this feature, but not for parameter values quantitatively close to those observed.Another issue of interest concerns how these networks are initially learned by the bird during song acquisition.  The second part of the project concerned the analysis of exemplars of REINFORCE algorithms, a general class of algorithms for reinforcement learning in neural networks, which are on several counts more biologically plausible than standard prescriptions such as backpropagation.  The former compared favorably with backpropagation on tasks involving single input-output pairs, though a noise analysis suggested it should not perform so well.  On tasks involving trajectory learning, REINFORCE algorithms meet with some success, though the analysis that predicts their success on input-output-pair tasks fails to explain it for trajectories.",MIT-CSAIL-TR-2004-070; AITR-2004-008,61 p.; 385088 bytes; 121048 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tan, Godfrey; Guttag, John",2005-12-22T02:15:36Z,2005-12-22T02:15:36Z,2004-11-12,http://hdl.handle.net/1721.1/30503,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Capacity Allocation in Wireless LANs,"Today's access point based wireless LANs (WLANs) are inefficient and unfair. For many traffic loads they provide far less total throughput than they should, and do a poor job allocating what throughput they do deliver. Inappropriate association of nodes to access points and rates to flows plays a large role in these problems. We address a major root cause of this problem in this paper.Current practice ignores the distinction between flows that connect two wireless nodes via an access point and flows that connect wireless nodes to the wired infrastructure. As wireless devices and applications become more pervasive, ignoring this distinction will lead to a significant degradation in perceived performance.In this paper, we i) describe a series of examples that illustrates the impact of two-hop flows on the performance of the system, ii) provide a practical algorithm to solve the AP-assignment problem and iii) evaluate the performance of our algorithm against other approaches. Our preliminary results show that our algorithm can increase average achieved throughput by as much as 50% for some traffic loads.",MIT-CSAIL-TR-2004-073; MIT-LCS-TR-973,17 p.; 20308180 bytes; 807306 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Cadieu, Charles; Kouh, Minjoon; Riesenhuber, Maximilian; Poggio, Tomaso",2005-12-22T02:15:22Z,2005-12-22T02:15:22Z,2004-11-12,http://hdl.handle.net/1721.1/30501,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Shape Representation in V4: Investigating Position-Specific Tuning for Boundary Conformation with the Standard Model of Object Recognition,"The computational processes in the intermediate stages of the ventral pathway responsible for visual object recognition are not well understood. A recent physiological study by A. Pasupathy and C. Connor in intermediate area V4 using contour stimuli, proposes that a population of V4 neurons display bjectcentered,position-specific curvature tuning [18]. The Â“standard modelÂ” of object recognition, a recently developed model [23] to account for recognition properties of IT cells (extending classical suggestions by Hubel, Wiesel and others [9, 10, 19]), is used here to model the response of the V4 cells described in [18]. Our results show that a feedforward, network level mechanism can exhibit selectivity and invariance properties that correspond to the responses of the V4 cells described in [18]. These results suggest howobject-centered, position-specific curvature tuning of V4 cells may arise from combinations of complex V1 cell responses. Furthermore, the model makes predictions about the responses of the same V4 cells studied by Pasupathy and Connor to novel gray level patterns, such as gratings and natural images. Thesepredictions suggest specific experiments to further explore shape representation in V4.",MIT-CSAIL-TR-2004-071; AIM-2004-024; CBCL-241,12 p.; 38801213 bytes; 5292186 bytes,application/postscript; application/pdf,en_US,AI; V4 Visual Cortex Object Recognition Standard Model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wolf, Lior; Martin, Ian",2005-12-22T02:15:29Z,2005-12-22T02:15:29Z,2004-11-12,http://hdl.handle.net/1721.1/30502,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Regularization Through Feature Knock Out,"In this paper, we present and analyze a novel regularization technique based on enhancing our dataset with corrupted copies of the original data. The motivation is that since the learning algorithm lacks information about which parts of thedata are reliable, it has to produce more robust classification functions. We then demonstrate how this regularization leads to redundancy in the resulting  classifiers, which is somewhat in contrast to the common interpretations of the OccamÂ’s razor principle. Using this framework, we propose a simple addition to the gentle boosting algorithm which enables it to work with only a few examples. We test this new algorithm on a variety of datasets and show convincing results.",MIT-CSAIL-TR-2004-072; AIM-2004-025; CBCL-242,0 p.; 16224097 bytes; 656543 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Serre, Thomas; Wolf, Lior; Poggio, Tomaso",2005-12-22T02:15:45Z,2005-12-22T02:15:45Z,2004-11-14,http://hdl.handle.net/1721.1/30504,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A new biologically motivated framework for robust object recognition,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",MIT-CSAIL-TR-2004-074; AIM-2004-026; CBCL-243,10 p.; 17638397 bytes; 793841 bytes,application/postscript; application/pdf,en_US,AI; visual cortex; object recognition; face detection; hierarchy; feature learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Balazinska, Magdalena; Balakrishnan, Hari; Madden, Samuel; Stonebraker, Mike",2005-12-22T02:16:16Z,2005-12-22T02:16:16Z,2004-11-22,http://hdl.handle.net/1721.1/30506,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Availability-Consistency Trade-Offs in a Fault-Tolerant Stream Processing System,"processing. In contrast to previous techniques that handlenode failures, our approach also tolerates network failuresand network partitions. The approach is based on a principledtrade-off between consistency and availability in theface of failure, that (1) ensures that all data on an inputstream is processed within a specified time threshold, but(2) reduces the impact of failures by limiting if possible thenumber of results produced based on partially available inputdata, and (3) corrects these results when failures heal.Our approach is well-suited for applications such as environmentmonitoring, where high availability and Â“real-timeÂ”response is preferable to perfect answers.Our approach uses replication and guarantees that all processingreplicas achieve state consistency, both in the absenceof failures and after a failure heals. We achieve consistencyin the former case by defining a data-serializing operatorthat ensures that the order of tuples to a downstreamoperator is the same at all the replicas. To achieve consistencyafter a failure heals, we develop approaches based oncheckpoint/redo and undo/redo techniques.We have implemented these schemes in a prototype distributedstream processing system, and present experimentalresults that show that the system meets the desiredavailability-consistency trade-offs.",MIT-CSAIL-TR-2004-077; MIT-LCS-TR-974,12 p.; 25764097 bytes; 1231086 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Srebro, Nathan",2005-12-22T02:16:24Z,2005-12-22T02:16:24Z,2004-11-22,http://hdl.handle.net/1721.1/30507,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning with Matrix Factorizations,"Matrices that can be factored into a product of two simpler matricescan serve as a useful and often natural model in the analysis oftabulated or high-dimensional data.  Models based on matrixfactorization (Factor Analysis, PCA) have been extensively used instatistical analysis and machine learning for over a century, withmany new formulations and models suggested in recent years (LatentSemantic Indexing, Aspect Models, Probabilistic PCA, Exponential PCA,Non-Negative Matrix Factorization and others).  In this thesis weaddress several issues related to learning with matrix factorizations:we study the asymptotic behavior and generalization ability ofexisting methods, suggest new optimization methods, and present anovel maximum-margin high-dimensional matrix factorizationformulation.",MIT-CSAIL-TR-2004-076; AITR-2004-009,132 p.; 96239481 bytes; 5561927 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen; Darrell, Trevor",2005-12-22T02:16:07Z,2005-12-22T02:16:07Z,2004-11-22,http://hdl.handle.net/1721.1/30505,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Efficient Image Matching with Distributions of Local Invariant Features,"Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes the minimal-cost correspondence between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.We evaluate our technique with scene recognition and texture classification tasks.",MIT-CSAIL-TR-2004-075; AIM-2004-027,18 p.; 78764149 bytes; 16484168 bytes,application/postscript; application/pdf,en_US,AI; image matching; object recognition; content-based image retrieval; texture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Harvey, Nicholas J.; Kleinberg, Robert D.; Lehman, April Rasala",2005-12-22T02:16:27Z,2005-12-22T02:16:27Z,2004-11-24,http://hdl.handle.net/1721.1/30508,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Comparing Network Coding with Multicommodity Flow for the k-pairs Communication Problem,"Given a graph G = (V,E) and k source-sink pairs of vertices, this papers investigates the maximum rate r at which all pairs can simultaneously communicate. We view this problem from two perspectives and compare their advantages. In the multicommodity flow formulation, a solution provides dedicated bandwidth r between each source-sink pair. In the information flow formulation, a vertex can transmit a function of the information it received thereby allowing multiple source-sink pairs to share bandwidth. For directed acyclic graphs with n vertices, we show that the rate achievable in the information flow formulation can be a multiplicative factor n larger than the rate achievable in the multicommodity flow formulation. It is well known [5] that for undirected graphs with n vertices, in the multicommodity flow formulation, the maximum rate achievable can be an O(1/log|V|) multiplicative factor smaller than the value of the sparsest cut. We extend this result to show that the maximum rate achievable in the information flow setting can be an O(1/log|V|) multiplicative factor smaller than the sparsest cut value.For directed acyclic graphs G, we define a parameter called the value of the most meager cut which is an upper bound for the maximum rate achievable in the information flow setting.We also present an example illustrating that this upper bound is not always tight.",MIT-CSAIL-TR-2004-078; MIT-LCS-TR-964,13 p.; 14089646 bytes; 597139 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Rinard, Martin",2005-12-22T02:19:36Z,2005-12-22T02:19:36Z,2004-11-30,http://hdl.handle.net/1721.1/30509,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Decision Procedures for Set-Value Fields,"An important feature of object-oriented programming languages is the ability todynamically instantiate user-defined container data structures such as lists, trees,and hash tables. Programs implement such data structures using references todynamically allocated objects, which allows data structures to store unboundednumbers of objects, but makes reasoning about programs more difficult. Reasoningabout object-oriented programs with complex data structures is simplified if datastructure operations are specified in terms of abstract sets of objects associatedwith each data structure. For example, an insertion into a data structure in thisapproach becomes simply an insertion into a dynamically changing set-valued fieldof an object, as opposed to a manipulation of a dynamically linked structure linkedto the object.In this paper we explore reasoning techniques for programs that manipulate datastructures specified using set-valued abstract fields associated with container objects.We compare the expressive power and the complexity of specification languagesbased on 1) decidable prefix vocabulary classes of first-order logic, 2) twovariablelogic with counting, and 3) Nelson-Oppen combinations of multisortedtheories. Such specification logics can be used for verification of object-orientedprograms with supplied invariants. Moreover, by selecting an appropriate subsetof properties expressible in such logic, the decision procedures for these logics yieldautomated computation of lattice operations in abstract interpretation domain, aswell as automated computation of abstract program semantics.",MIT-CSAIL-TR-2004-079; MIT-LCS-TR-975,16 p.; 19825924 bytes; 756829 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Salcianu, Alexandru; Arkoudas, Konstantine",2005-12-22T02:19:45Z,2005-12-22T02:19:45Z,2004-12-16,http://hdl.handle.net/1721.1/30510,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Machine-Checkable Correctness Proofs forIntra-procedural Dataflow Analyses,"This technical report describes our experience using the interactive theorem proverAthena for proving the correctness of abstract interpretation-based dataflow analyses.For each analysis, our methodology requires the analysis designer to formallyspecify the property lattice, the transfer functions, and the desired modeling relationbetween the concrete program states and the results computed by the analysis. Thegoal of the correctness proof is to prove that the desired modeling relation holds.The proof allows the analysis clients to rely on the modeling relation for their owncorrectness. To reduce the complexity of the proofs, we separate the proof of eachdataflow analysis into two parts: a generic part, proven once, independent of anyspecific analysis; and several analysis-specific conditions proven in Athena.",MIT-CSAIL-TR-2004-080; MIT-LCS-TR-976,16 p.; 18540541 bytes; 798716 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liang, Percy; Srebro, Nathan",2005-12-22T02:19:52Z,2005-12-22T02:19:52Z,2004-12-30,http://hdl.handle.net/1721.1/30511,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Methods and Experiments With Bounded Tree-width Markov Networks,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",MIT-CSAIL-TR-2004-081; AIM-2004-030,10 p.; 10714507 bytes; 473643 bytes,application/postscript; application/pdf,en_US,AI; tree-width; hypertrees; Markov Networks; maximum likelihood; MDL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kouh, Minjoon; Poggio, Tomaso",2005-12-22T02:19:58Z,2005-12-22T02:19:58Z,2004-12-31,http://hdl.handle.net/1721.1/30512,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A general mechanism for tuning: Gain control circuits and synapses underlie tuning of cortical neurons,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,MIT-CSAIL-TR-2004-082; AIM-2004-031; CBCL-245,9 p.; 11464000 bytes; 738607 bytes,application/postscript; application/pdf,en_US,AI; tuning; gain control; normalization; Gaussian; neuron; cortex; biophysics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Richards, Whitman; Seung, H. Sebastian",2005-12-22T02:20:05Z,2005-12-22T02:20:05Z,2004-12-31,http://hdl.handle.net/1721.1/30513,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Neural Voting Machines,"Â“Winner-take-allÂ” networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",MIT-CSAIL-TR-2004-083; AIM-2004-029,12 p.; 13714512 bytes; 523751 bytes,application/postscript; application/pdf,en_US,AI; WTA; Borda machine; Condorcet procedure; neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Radul, Alexey",2006-04-14T15:26:55Z,2006-04-14T15:26:55Z,2005,http://hdl.handle.net/1721.1/32531,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Symmetriad: A Journey of Discovery Through the Land of the Polychora,"I devised and implemented a method for constructing regular andsemiregular geometric objects in n-dimensional Euclidean space.Given a finite reflection group (a Coxeter group) G, there is a standard way to give G a group action on n-space.Reflecting a point through this group action yieldsan object that exhibits the symmetries specified by G.  If the pointis chosen well, the object is guaranteed to be regular or semiregular,and many interesting regular and semiregular objectsarise this way.  By starting with the symmetry group, I can use thegroup structure both to simplify the actual graphics involved withdisplaying the object, and to illustrate various aspects of itsstructure.  For example, subgroups of the symmetry group (and theircosets) correspond to substructures of the object.  Conversely, bydisplaying such symmetric objects and their various substructures, Ifind that I can elucidate the structure of the symmetry group thatgives rise to them.I have written The Symmetriad, the computer system whose name thisdocument has inherited, and used it to explore 3- and 4-dimensionalsymmetric objects and their symmetry groups.  The 3-dimensionalobjects are already well understood, but they serve to illustrate thetechniques used on the 4-dimensional objects and make them morecomprehensible.  Four dimensions offers a treasure trove of intriguingstructures, many of which have no ready 3D analogue.  These are what Iwill show you here.",MIT-CSAIL-TR-2006-024,111 p.; 1054739 bytes; 8185809 bytes,application/pdf; application/postscript,en_US,,Mathematics and Computation,,,,MEng thesis,"Radul, Alexey.    The Symmetriad : a journey of discovery through the land of the polychora / by Alexey Radul.    c2005.Institute Archives - Noncirculating Collection 3 |  THESIS Thesis E.E. 2005 M.Eng",,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Izmalkov, Sergei; Lepinski, Matt; Micali, Silvio",2007-07-30T16:01:48Z,2007-07-30T16:01:48Z,2005,http://hdl.handle.net/1721.1/38208,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Perfect Implementation of Normal-Form Mechanisms,"Privacy and trust affect our strategic thinking, yet they have not been precisely modeled in mechanism design. In settings of incomplete information,  traditional implementations of a normal-form mechanism ---by disregarding the players' privacy, or assuming trust in a mediator--- may not be realistic and fail to reach the mechanism's objectives. We thus investigate implementations of a new type.We put forward the notion of a  perfect implementation of a normal-form mechanism M: in essence, an extensive-form mechanism exactly preserving all strategic properties of M, without relying on a trusted  party or violating the privacy of the players.We prove that ANY normal-form mechanism  can be perfectly implemented via envelopes and an envelope-randomizing device (i.e., the same tools used for running fair lotteries or tallying secret votes).",MIT-CSAIL-TR-2007-040,51 p.,,,Mechanism Design; Privacy; Privacy Equivalence; Strategic Equivalence; Perfect Implementation,Theory of Computation,,,,,Ealier Version in 46th Foundation of Computer Science Conference,,,,,,,,,,,,,,,,,,,,,,,
,"Liang, Percy; Srebro, Nati",2005-12-22T02:20:16Z,2005-12-22T02:20:16Z,2005-01-03,http://hdl.handle.net/1721.1/30514,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Dynamic Data Structure for Checking Hyperacyclicity,"We present a dynamic data structure that keeps track of an acyclic hypergraph (equivalently, a triangulated graph) and enables verifying that adding a candidate hyperedge (clique) will not break the acyclicity of the augmented hypergraph. This is a generalization of the use of Tarjan's Union-Find data structure for maintaining acyclicity when augmenting forests, and the amortized time per operation has a similar almost-constant dependence on the size of the hypergraph. Such a data structure is useful when augmenting acyclic hypergraphs, e.g.\~in order to greedily construct a high-weight acyclic hypergraph. In designing this data structure, we introduce a hierarchical decomposition of acyclic hypergraphs that aid in understanding {\em hyper-connectivity}, and introduce a novel concept of a {\em hypercycle} which is excluded from acyclic hypergraphs.",MIT-CSAIL-TR-2005-001; MIT-LCS-TR-977,12 p.; 17306554 bytes; 656257 bytes,application/postscript; application/pdf,en_US,,Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liang, Percy; Srebro, Nati",2005-12-22T02:20:23Z,2005-12-22T02:20:23Z,2005-01-03,http://hdl.handle.net/1721.1/30515,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,How Much of a Hypertree can be Captured by Windmills?,"Current approximation algorithms for maximum weight {\em hypertrees} find heavy {\em windmill farms}, and are based on the fact that a constant ratio (for constant width $k$) of the weight of a $k$-hypertree can be captured by a $k$-windmill farm. However, the exact worst case ratio is not known and is only bounded to be between $1/(k+1)!$ and $1/(k+1)$. We investigate this worst case ratio by searching for weighted hypertrees that minimize the ratio of their weight that can be captured with a windmill farm. To do so, we use a novel approach in which a linear program is used to find ``bad'' inputs to a dynamic program.",MIT-CSAIL-TR-2005-002; MIT-LCS-TR-978,12 p.; 13845223 bytes; 531507 bytes,application/postscript; application/pdf,en_US,,Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob; Sussman, Gerald",2005-12-22T02:20:32Z,2005-12-22T02:20:32Z,2005-01-18,http://hdl.handle.net/1721.1/30516,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Biologically-Inspired Robust Spatial Programming,"Inspired by the robustness and flexibility of biological systems, we are developing linguistic and programming tools to allow us to program spatial systems populated by vast numbers of unreliable components interconnected in unknown, irregular, and time-varying ways. We organize our computations around geometry, making the fact that our system is made up of discrete individuals implicit. Geometry allows us to specify requirements in terms of the behavior of the space occupied by the aggregate rather than the behavior of individuals, thereby decreasing complexity. So we describe the behavior of space explicitly, abstracting away the discrete nature of the components. As an example, we present the Amorphous Medium Language, which describes behavior in terms of homeostatic maintenance of constraints on nested regions of space.",MIT-CSAIL-TR-2005-003; AIM-2005-001,17 p.; 42144634 bytes; 6024067 bytes,application/postscript; application/pdf,en_US,AI; amorphous robust biological spatial sensor networks language programming medium,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dolev, Shlomi; Gilbert, Seth; Lahiani, Limor; Lynch, Nancy; Nolte, Tina",2005-12-22T02:20:41Z,2005-12-22T02:20:41Z,2005-01-21,http://hdl.handle.net/1721.1/30517,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Virtual Stationary Automata for Mobile Networks,"We define a programming abstraction formobile networks called the Virtual Stationary Automataprogramming layer, consisting of real mobile clients, virtualtimed I/O automata called virtual stationary automata(VSAs), and a communication service connecting VSAs andclient nodes. The VSAs are located at prespecified regionsthat tile the plane, defining a static virtual infrastructure.We present a self-stabilizing algorithm to emulate a VSAusing the real mobile nodes that are currently residingin the VSAÂ’s region. We also describe several examplesof applications whose implementations benefit from thesimplicity obtained through use of the VSA abstraction.",MIT-CSAIL-TR-2005-004; MIT-LCS-TR-979,16 p.; 24963489 bytes; 1042588 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kondacs, Attila",2005-12-22T02:20:51Z,2005-12-22T02:20:51Z,2005-01-28,http://hdl.handle.net/1721.1/30518,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Determining articulator configuration in voiced stop consonants by matching time-domain patterns in pitch periods,"In this thesis I will be concerned with linking the observed speechsignal to the configuration of articulators.Due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. The typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.I argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. The patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.My patterns are an alternative to frequency. I use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. These features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. No linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.The measurements I use are neurophysiologically plausible: theauditory system could be using similar methods.I have demonstrated this approach by constructing a robust techniquefor categorizing the English voiced stops as the consonants B, D, or Gbased on the vocalic portions of their releases. The classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",MIT-CSAIL-TR-2005-005; AITR-2005-001,96 p.; 85678480 bytes; 3087600 bytes,application/postscript; application/pdf,en_US,AI; speech processing; stop consonants; pitch period; spatio-temporal patterns,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lepinski, Matt; Izmalkov, Sergei",2005-12-22T02:20:57Z,2005-12-22T02:20:57Z,2005-02-02,http://hdl.handle.net/1721.1/30519,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Security Power of the Ballot Box,"We show that any function F can be securely evaluated by a protocolwith ballots and a ballot box. That is, N mutually suspicious players,each player possessing a secret input, can use ballots and a ballotbox to jointly evaluate F on their secret inputs so that (no matterhow many players may collude and deviate from their prescribed instructions, and no matter how long they compute!) each player learnsexactly the output of the function with the same privacy and correctnessas if all players privately handed their secret inputs to a trustedparty, who privately evaluates F and privately returns the outputs toeach player.Our protocol is (1) efficient, (2) enjoys perfect privacy, (3) guarantees perfect correctness, (4) is universally composable, and (5)is collusion-free even for games with secret actions.",MIT-CSAIL-TR-2005-006; MIT-LCS-TM-647,12 p.; 10126165 bytes; 427313 bytes,application/postscript; application/pdf,en_US,,Cryptography and Information Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sussman, Gerald Jay; Wisdom, Jack",2005-12-22T02:21:07Z,2005-12-22T02:21:07Z,2005-02-02,http://hdl.handle.net/1721.1/30520,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Functional Differential Geometry,"Differential geometry is deceptively simple.  It is surprisingly easyto get the right answer with unclear and informal symbol manipulation.To address this problem we use computer programs to communicate aprecise understanding of the computations in differential geometry.Expressing the methods of differential geometry in a computer languageforces them to be unambiguous and computationally effective.  The taskof formulating a method as a computer-executable program and debuggingthat program is a powerful exercise in the learning process.  Also,once formalized procedurally, a mathematical idea becomes a tool thatcan be used directly to compute results.",MIT-CSAIL-TR-2005-007; AIM-2005-003,77 p.; 38556269 bytes; 1665777 bytes,application/postscript; application/pdf,en_US,AI; Scheme  differential geometry calculus manifolds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Balas, Benjamin",2005-12-22T02:21:13Z,2005-12-22T02:21:13Z,2005-02-07,http://hdl.handle.net/1721.1/30521,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using computational models to study texture representations in the human visual system.,"Traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. At the same time, computer algorithms for the synthesis of natural textures have improved dramatically. The current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. We employ Portilla and SimoncelliÂ’s 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). We find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",MIT-CSAIL-TR-2005-008; AIM-2005-002; CBCL-244,11 p.; 33342937 bytes; 4614631 bytes,application/postscript; application/pdf,en_US,AI; texture perception; texture synthesis; psychophysics; natural images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Abbott, Tim; Kane, Daniel; Valiant, Paul",2005-12-22T02:24:28Z,2005-12-22T02:24:28Z,2005-02-08,http://hdl.handle.net/1721.1/30523,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Complexity of finding Nash equilibria in 0-1 bimatrix games,We exhibit a polynomial reduction from the problem of finding a Nashequilibrium of a bimatrix game with rational coefficients to the problemof finding a Nash equilibrium of a bimatrix game with 0-1 coefficients.,MIT-CSAIL-TR-2005-010; MIT-LCS-TM-648,6 p.; 6499249 bytes; 336493 bytes,application/postscript; application/pdf,en_US,,Cryptography and Information Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Feamster, Nick; Johari, Ramesh; Balakrishnan, Hari",2005-12-22T02:24:22Z,2005-12-22T02:24:22Z,2005-02-08,http://hdl.handle.net/1721.1/30522,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Stable Policy Routing with Provider Independence,"Thousands of competing autonomous systems (ASes) mustcooperate with each other to provide global Internet connectivity.These ASes encode various economic, business,and performance decisions in their routing policies. The currentinterdomain routing system enables ASes to express policyusing rankings that determine how each router in an ASorders the different routes to a destination, and filters thatdetermine which routes are hidden from each neighboringAS. Since the Internet is composed of many independent,competing networks, the interdomain routing system shouldallow providers to set their rankings independently, and tohave no constraints on allowed filters. This paper studiesrouting protocol stability under these constraints. We firstdemonstrate that certain rankings that are commonly usedin practice may not ensure routing stability. We then provethat, with ranking independence and unrestricted filtering,guaranteeing that the routing system will converge to a stablepath assignment essentially requires ASes to rank routesbased on AS-path lengths. Finally, we discuss the implicationsof these results for the future of interdomain routing.",MIT-CSAIL-TR-2005-009; MIT-LCS-TR-981,14 p.; 28950106 bytes; 1209509 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Riemann, Reina; Winstein, Keith",2005-12-22T02:24:37Z,2005-12-22T02:24:37Z,2005-02-24,http://hdl.handle.net/1721.1/30524,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Improving 802.11 Range with Forward Error Correction,"The ISO/IEC 8802-11:1999(E) specification uses a 32-bit CRC for error detection and whole-packet retransmissions for recovery. In long-distance orhigh-interference links where the probability of a bit error is high,this strategy results in excessive losses, because any erroneous bitcauses an entire packet to be discarded. By ignoring the CRC andadding redundancy to 802.11 payloads in software, we achievedsubstantially reduced loss rates on indoor and outdoor long-distancelinks and extended line-of-sight range outdoors by 70 percent.",MIT-CSAIL-TR-2005-011; AIM-2005-004,10 p.; 8245944 bytes; 467111 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Attie, Paul; Guerraoui, Rachid; Kouznetsov, Petr; Lynch, Nancy; Rajsbaum, Sergio",2005-12-22T02:24:53Z,2005-12-22T02:24:53Z,2005-02-25,http://hdl.handle.net/1721.1/30526,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Impossibility of boosting distributed service resilience,"We prove two theorems saying that no distributed system in whichprocesses coordinate using reliable registers and f-resilient servicescan solve the consensus problem in the presence of f+1 undetectableprocess stopping failures.  (A service is f-resilient if it isguaranteed to operate as long as no more than f of the processesconnected to it fail.)Our first theorem assumes that the given services are atomic objects,and allows any connection pattern between processes and services.  Incontrast, we show that it is possible to boost the resilience ofsystems solving problems easier than consensus: the k-set consensusproblem is solvable for 2k-1 failures using 1-resilient consensusservices.  The first theorem and its proof generalize to the largerclass of failure-oblivious services.Our second theorem allows the system to contain failure-awareservices, such as failure detectors, in addition to failure-obliviousservices; however, it requires that each failure-aware service beconnected to all processes.  Thus, f+1 process failures overall candisable all the failure-aware services.  In contrast, it is possibleto boost the resilience of a system solving consensus if arbitrarypatterns of connectivity are allowed between processes andfailure-aware services: consensus is solvable for any number offailures using only 1-resilient 2-process perfect failure detectors.",MIT-CSAIL-TR-2005-013; MIT-LCS-TR-982,20 p.; 30546523 bytes; 1297125 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Sivic, Josef; Russell, Bryan C.; Efros, Alexei A.; Zisserman, Andrew; Freeman, William T.",2005-12-22T02:24:48Z,2005-12-22T02:24:48Z,2005-02-25,http://hdl.handle.net/1721.1/30525,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Discovering object categories in image collections,"Given a set of images containing multiple object categories,we seek to discover those categories and their image locations withoutsupervision.  We achieve this using generative modelsfrom the statistical text literature: probabilistic Latent SemanticAnalysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysisthese are used to discover topics in a corpus using the bag-of-wordsdocument representation. Here we discover topics as object categories, sothat an image containing instances of several categories is modelled as amixture of topics.The models are applied to images by using avisual analogue of a word, formed by vector quantizing SIFT like regiondescriptors.  We investigate a set of increasingly demanding scenarios,starting with image sets containing only two object categories through tosets containing multiple categories (including airplanes, cars, faces,motorbikes, spotted cats) and background clutter. The object categoriessample both intra-class and scale variation, and both the categories andtheir approximate spatial layout are found without supervision.We also demonstrate classification of unseen images and images containingmultiple objects. Performance of the proposed unsupervised method is compared tothe semi-supervised approach of Fergus et al.",MIT-CSAIL-TR-2005-012; AIM-2005-005,0 p.; 29582654 bytes; 2849946 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Cox, Russ; Josephson, William",2005-12-22T02:24:58Z,2005-12-22T02:24:58Z,2005-02-28,http://hdl.handle.net/1721.1/30527,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,File Synchronization with Vector Time Pairs,"Vector time pairs are a new method for trackingsynchronization metadata.  A vector time pairconsists of two vector times: one tracking filemodification history and one tracking filesynchronization history.  Because the vectortimes are maintained separately and used fordifferent purposes, different algorithms andoptimizations can be applied to each.  As aresult, vector time pairs impose no restrictionon synchronization patterns, never falsely detectconflicts, require no space to store deletionnotices, require network bandwidth proportionalonly to the number of files changed, and supportpartial synchronizations.  No other currentsynchronization method has all these properties.Results from an implementation of vector timepairs in a new user-level file synchronizercalled Tra confirm the benefits of vectortime pairs.",MIT-CSAIL-TR-2005-014; MIT-LCS-TM-650,14 p.; 22930099 bytes; 909750 bytes,application/postscript; application/pdf,en_US,,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Balas, Benjamin; Sinha, Pawan",2005-12-22T02:25:04Z,2005-12-22T02:25:04Z,2005-03-01,http://hdl.handle.net/1721.1/30528,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Receptive field structures for recognition,"Localized operators, like Gabor wavelets and difference-of-Gaussian filters, are considered to be useful tools for image representation. This is due to their ability to form a Â‘sparse codeÂ’ that can serve as a basis set for high-fidelity reconstruction of natural images. However, for many visual tasks, the more appropriate criterion of representational efficacy is Â‘recognitionÂ’, rather than Â‘reconstructionÂ’. It is unclear whether simple local features provide the stability necessary to subserve robust recognition of complex objects. In this paper, we search the space of two-lobed differential operators for those that constitute a good representational code under recognition/discrimination criteria. We find that a novel operator, which we call the Â‘dissociated dipoleÂ’ displays useful properties in this regard. We describe simple computational experiments to assess the merits of such dipoles relative to the more traditional local operators. The results suggest that non-local operators constitute a vocabulary that is stable across a range of image transformations.",MIT-CSAIL-TR-2005-015; AIM-2005-006; CBCL-246,17 p.; 34808229 bytes; 3368874 bytes,application/postscript; application/pdf,en_US,AI; object recognition; face recognition; sparse coding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kaynor, Dilsun K.; Lynch, Nancy; Segala, Roberto; Vaandrager, Frits",2005-12-19T22:54:38Z,2005-12-19T22:54:38Z,2005-03-02,http://hdl.handle.net/1721.1/30407,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Theory of Timed I/O Automata,"This monograph presents the Timed Input/Output Automaton (TIOA) modeling framework, a basic mathematical framework to support description and analysis of timed systems.",MIT-CSAIL-TR-2003-015; MIT-LCS-TR-917a,94 p.; 83828733 bytes; 3132366 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taycher, Leonid; Fisher III, John W.; Darrell, Trevor",2005-12-22T02:25:14Z,2005-12-22T02:25:14Z,2005-03-02,http://hdl.handle.net/1721.1/30529,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining Object and Feature Dynamics in Probabilistic Tracking,"Objects can exhibit different dynamics at different scales, a property that isoftenexploited by visual tracking algorithms. A local dynamicmodel is typically used to extract image features that are then used as inputsto a system for tracking the entire object using a global dynamic model.Approximate local dynamicsmay be brittle---point trackers drift due to image noise and adaptivebackground models adapt to foreground objects that becomestationary---but constraints from the global model can make them more robust.We propose a probabilistic framework for incorporating globaldynamics knowledge into the local feature extraction processes.A global tracking algorithm can beformulated as a generative model and used to predict feature values thatinfluence the observation process of thefeature extractor. We combine such models in a multichain graphicalmodel framework.We show the utility of our framework for improving feature tracking and thusshapeand motion estimates in a batch factorization algorithm.We also propose an approximate filtering algorithm appropriate for onlineapplications, and demonstrate its application to problems such as backgroundsubtraction, structure from motion and articulated body tracking.",MIT-CSAIL-TR-2005-016; AIM-2005-008,0 p.; 44997544 bytes; 4278776 bytes,application/postscript; application/pdf,en_US,AI; graphical models; feature extraction; tracking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Grauman, Kristen; Darrell, Trevor",2005-12-19T23:36:26Z,2005-12-19T23:36:26Z,2005-03-17,http://hdl.handle.net/1721.1/30420,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pyramid Match Kernels: Discriminative Classification with Sets of Image Features,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",MIT-CSAIL-TR-2005-017; AIM-2005-007,12 p.; 68553886 bytes; 13808123 bytes,application/postscript; application/pdf,en_US,AI; kernel; unordered sets; correspondence; object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rademacher, Luis; Vempala, Santosh; Wang, Grant",2005-12-22T02:25:21Z,2005-12-22T02:25:21Z,2005-03-29,http://hdl.handle.net/1721.1/30530,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Matrix Approximation and Projective Clustering via Iterative Sampling,"We present two new results for the problem of approximating a given real m by n matrix A by a rank-k matrix D, where k < min{m, n}, so as to minimize ||A-D||_F^2.  It is known that bysampling O(k/eps) rows of the matrix, one can find a low-rank approximation with additive error eps||A||_F^2.  Our first result shows that with adaptive sampling in t rounds and O(k/eps) samples in each round, the additive error drops exponentially as eps^t; the computation time is nearly linear in the number of nonzero entries. This demonstrates that multiple passes can be highly beneficial for a natural (and widely studied) algorithmic problem. Our second result is that there exists a subset of O(k^2/eps) rows such that their span contains a rank-k approximation with multiplicative (1+eps) error (i.e., the sum of squares distance has a small \""core-set\"" whose span determines a good approximation). This existence theorem leads to a PTAS for the following projective clustering probl! em: Given a set of points P in R^d, and integers k,j, find a set of j subspaces F_1,...,F_j, each of dimension at most k, that minimize \\sum_{p \\in P} min_i d(p,F_i)^2.",MIT-CSAIL-TR-2005-018; MIT-LCS-TR-983,19 p.; 17195378 bytes; 797052 bytes,application/postscript; application/pdf,en_US,,Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wolf, Lior; Bileschi, Stanley",2005-12-22T02:25:27Z,2005-12-22T02:25:27Z,2005-03-30,http://hdl.handle.net/1721.1/30531,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining Variable Selection with Dimensionality Reduction,"This paper bridges the gap between variable selection methods (e.g., Pearson coefficients, KS test) and dimensionality reductionalgorithms (e.g., PCA, LDA). Variable selection algorithms encounter difficulties dealing with highly correlated data,since many features are similar in quality. Dimensionality reduction algorithms tend to combine all variables and cannotselect a subset of significant variables.Our approach combines both methodologies by applying variable selection followed by dimensionality reduction. Thiscombination makes sense only when using the same utility function in both stages, which we do. The resulting algorithmbenefits from complex features as variable selection algorithms do, and at the same time enjoys the benefits of dimensionalityreduction.1",MIT-CSAIL-TR-2005-019; AIM-2005-009; CBCL-247,10 p.; 14957523 bytes; 722450 bytes,application/postscript; application/pdf,en_US,AI; Computer Vision; Statistical Learning; Variable Selection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Pohl, Kilian M.; Fisher, John; Grimson, W. Eric L.; Wells, William M.",2005-12-22T02:25:35Z,2005-12-22T02:25:35Z,2005-04-01,http://hdl.handle.net/1721.1/30532,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"An Expectation Maximization Approach for Integrated Registration, Segmentation, and Intensity Correction","This paper presents a statistical framework which combines the registration of an atlas with the segmentation of MR images. We use an Expectation Maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. An example of the approach is given for a brain structure-dependent affine mapping approach. The algorithm produces high quality segmentations for brain tissues as well as their substructures. We demonstrate the approach on a set of 30 brain MR images. In addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",MIT-CSAIL-TR-2005-020; AIM-2005-010,13 p.; 18741928 bytes; 826219 bytes,application/postscript; application/pdf,en_US,AI; Expectation Maximization; Segmentation; Registration; Medical Image Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kuncak, Viktor; Jackson, Daniel",2005-12-22T02:25:50Z,2005-12-22T02:25:50Z,2005-04-05,http://hdl.handle.net/1721.1/30534,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Relational Analysis of Algebraic Datatypes,"We present a technique that enables the use of finite modelfinding to check the satisfiability of certain formulaswhose intended models are infinite.  Such formulas arisewhen using the language of sets and relations to reasonabout structured values such as algebraic datatypes.  Thekey idea of our technique is to identify a natural syntacticclass of formulas in relational logic for which reasoningabout infinite structures can be reduced to reasoning aboutfinite structures.  As a result, when a formula belongs tothis class, we can use existing finite model findingtools to check whether the formula holds in the desiredinfinite model.",MIT-CSAIL-TR-2005-022; MIT-LCS-TR-985,13 p.; 21208035 bytes; 893051 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Abraham, Ittai; Chockler, Gregory; Keidar, Idit; Malkhi, Dahlia",2005-12-22T02:25:42Z,2005-12-22T02:25:42Z,2005-04-05,http://hdl.handle.net/1721.1/30533,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Wait-free Regular Storage from Byzantine Components,"We present a simple, efficient, and self-contained construction of a wait-free regular register from Byzantine storage components.  Our construction utilizes a novel building block, called 1-regular register, which can be implemented from Byzantine fault-prone components with the same round complexity as a safe register, and with only a slight increase in storage space.",MIT-CSAIL-TR-2005-021; MIT-LCS-TR-984,13 p.; 15869015 bytes; 669233 bytes,application/postscript; application/pdf,en_US,,Theory of Distributed Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lynch, Nancy; Mitra, Sayan; Nolte, Tina",2005-12-22T02:25:56Z,2005-12-22T02:25:56Z,2005-04-06,http://hdl.handle.net/1721.1/30535,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Motion Coordination Using Virtual Nodes,"We describe how a virtual node abstraction layer can be used to coordinate the motion of real mobile nodes in a region of 2-space. In particular, we consider how nodes in a mobile ad hoc network can arrange themselves along a predetermined curve in the plane, and can maintain themselves in such a configuration in the presence of changes in the underlying mobile ad hoc network, specifically, when nodes may join or leave the system or may fail. Our strategy is to allow the mobile nodes to implement a virtual layer consisting of mobile client nodes, stationary Virtual Nodes (VNs) for predetermined zones in the plane, and local broadcast communication.  The VNs coordinate among themselves to distribute the client nodesbetween zones based on the length of the curve through those zones, while each VN directs its zone's local client nodes to move themselves to equally spaced locations on the local portion of the target curve.",MIT-CSAIL-TR-2005-023; MIT-LCS-TR-986,12 p.; 26698908 bytes; 1192908 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Werfel, Justin; Bar-Yam, Yaneer; Nagpal, Radhika",2005-12-22T02:28:10Z,2005-12-22T02:28:10Z,2005-04-08,http://hdl.handle.net/1721.1/30536,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Construction by robot swarms using extended stigmergy,"We describe a system in which simple, identical, autonomous robots assemble two-dimensional structures out of identical building blocks.  We show that, in a system divided in this way into mobile units and structural units, giving the blocks limited communication abilities enables robots to have sufficient global structural knowledge to rapidly build elaborate pre-designed structures.  In this way we extend the principle of stigmergy (storing information in the environment) used by social insects, by increasing the capabilities of the blocks that represent that environmental information.  As a result, arbitrary solid structures can be built using a few fixed, local behaviors, without requiring construction to be planned out in detail.",MIT-CSAIL-TR-2005-024; AIM-2005-011,19 p.; 16033902 bytes; 615762 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Su, Sara L.; Durand, Fredo; Agrawala, Maneesh",2005-12-22T02:28:16Z,2005-12-22T02:28:16Z,2005-04-12,http://hdl.handle.net/1721.1/30537,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,De-Emphasis of Distracting Image Regions Using Texture Power Maps,"A major obstacle in photography is the presence of distracting elements that pull attention away from the main subject and clutter the composition. In this article, we present a new image-processing technique that reduces the salience of distracting regions. It is motivated by computational models of attention that predict that texture variation influences bottom-up attention mechanisms. Our method reduces the spatial variation of texture using power maps, high-order features describing local frequency content in an image. We show how modification of power maps results in  powerful image de-emphasis. We validate our results using a user search experiment and eye tracking data.",MIT-CSAIL-TR-2005-025; MIT-LCS-TR-987,12 p.; 24844567 bytes; 1742248 bytes,application/postscript; application/pdf,en_US,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Beal, Jacob",2005-12-22T02:28:22Z,2005-12-22T02:28:22Z,2005-04-13,http://hdl.handle.net/1721.1/30538,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning From Snapshot Examples,"Examples are a powerful tool for teaching both humans and computers.In order to learn from examples, however, a student must first extractthe examples from its stream of perception. Snapshot learning is ageneral approach to this problem, in which relevant samples ofperception are used as examples.  Learning from these examples can inturn improve the judgement of the snapshot mechanism, improving thequality of future examples.  One way to implement snapshot learning isthe Top-Cliff heuristic, which identifies relevant samples using ageneralized notion of peaks. I apply snapshot learning with theTop-Cliff heuristic to solve a distributed learning problem and showthat the resulting system learns rapidly and robustly, and canhallucinate useful examples in a perceptual stream from a teacherlesssystem.",MIT-CSAIL-TR-2005-026; AIM-2005-012,22 p.; 16733589 bytes; 735336 bytes,application/postscript; application/pdf,en_US,AI; unsupervised supervised learning examples,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Caponnetto, Andrea; Vito, Ernesto De",2005-12-22T02:28:27Z,2005-12-22T02:28:27Z,2005-04-14,http://hdl.handle.net/1721.1/30539,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Fast Rates for Regularized Least-squares Algorithm,"We develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel Hilbert spaces for supervised learning.  We show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  In fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",MIT-CSAIL-TR-2005-027; AIM-2005-013; CBCL-248,25 p.; 16130108 bytes; 833989 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; regularized least-squares; reproducing kernel Hilbert space; effe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Eisenstein, Jacob; Davis, Randall",2005-12-22T02:28:32Z,2005-12-22T02:28:32Z,2005-04-19,http://hdl.handle.net/1721.1/30540,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Gestural Cues for Sentence Segmentation,"In human-human dialogues, face-to-face meetings are often preferred over phone conversations.One explanation is that non-verbal modalities such as gesture provide additionalinformation, making communication more efficient and accurate. If so, computerprocessing of natural language could improve by attending to non-verbal modalitiesas well. We consider the problem of sentence segmentation, using hand-annotatedgesture features to improve recognition. We find that gesture features correlate wellwith sentence boundaries, but that these features improve the overall performance of alanguage-only system only marginally. This finding is in line with previous research onthis topic. We provide a regression analysis, revealing that for sentence boundarydetection, the gestural features are largely redundant with the language model andpause features. This suggests that gestural features can still be useful when speech recognition is inaccurate.",MIT-CSAIL-TR-2005-028; AIM-2005-014,13 p.; 13772256 bytes; 521371 bytes,application/postscript; application/pdf,en_US,AI; gesture; natural language processing; multimodal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taylor, Christopher; Rahimi, Ali; Bachrach, Jonathan; Shrobe, Howard",2005-12-22T02:28:41Z,2005-12-22T02:28:41Z,2005-04-26,http://hdl.handle.net/1721.1/30541,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Simultaneous Localization, Calibration, and Tracking in an ad Hoc Sensor Network","We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",MIT-CSAIL-TR-2005-029; AIM-2005-016,18 p.; 40655574 bytes; 2128443 bytes,application/postscript; application/pdf,en_US,AI; sensor network; localization; bayesian filter; extended kalman filter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"McCamant, Stephen; Morrisett, Greg",2005-12-22T02:28:49Z,2005-12-22T02:28:49Z,2005-05-02,http://hdl.handle.net/1721.1/30542,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Efficient, Verifiable Binary Sandboxing for a CISC Architecture","Executing untrusted code while preserving security requiresenforcement of memory and control-flow safety policies:untrusted code must be prevented from modifying memory orexecuting code except as explicitly allowed.  Software-basedfault isolation (SFI) or \""sandboxing\"" enforces thosepolicies by rewriting the untrusted code at the level ofindividual instructions.  However, the original sandboxingtechnique of Wahbe et al. is applicable only to RISCarchitectures, and other previous work is either insecure,or has been not described in enough detail to giveconfidence in its security properties.  We present a noveltechnique that allows sandboxing to be easily applied to aCISC architecture like the IA-32.  The technique can beverified to have been applied at load time, so that neitherthe rewriting tool nor the compiler needs to be trusted.  Wedescribe a prototype implementation which provides a robustsecurity guarantee, is scalable to programs of any size, andhas low runtime overheads.  Further, we give amachine-checked proof that any program approved by theverification algorithm is guaranteed to respect the desiredsafety property.",MIT-CSAIL-TR-2005-030; MIT-LCS-TR-988,17 p.; 29512899 bytes; 1053603 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Vito, Ernesto De; Caponnetto, Andrea",2005-12-22T02:28:54Z,2005-12-22T02:28:54Z,2005-05-16,http://hdl.handle.net/1721.1/30543,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Risk Bounds for Regularized Least-squares Algorithm with Operator-valued kernels,We show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel Hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  We first briefly introduce central concepts on operator-valued kernels.  Then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,MIT-CSAIL-TR-2005-031; AIM-2005-015; CBCL-249,17 p.; 12090406 bytes; 642646 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; reproducing kernel Hilbert space; effective dimension,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Singh, Neha",2005-12-22T02:29:27Z,2005-12-22T02:29:27Z,2005-05-17,http://hdl.handle.net/1721.1/30544,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Region-based Architecture for Service-Providing Distributed Systems,"A service-providing system consists of hosts that provide services such as data, content, computational and memory resources and data-based services to other entities in the system. Consumers that wish to use services describe their needs with a set of high-level objectives. In this thesis, we address the problem of locating services in a large-scale distributed system using their descriptions, rather than their addresses. We propose a network architecture that is based on the concept of dividing the service-providing hosts into Regions. A Region is a grouping of elements of the network that share a set of common characteristics and policies. Members of a region manage their interactions with other regions and their elements according to some defined rules and policies. Hosts can be divided into regions based on various properties such as their content, their commercial model or their security characteristics to name a few. The service provided by a region is an ! aggregate of the services provided by all its member hosts. The region-based architecture routes a service request through the network efficiently based on its description and on the advertisements from regions providing services. Division of hosts into a set of independent regions partitions the search space and produces a scalable structure. The architecture also does not impose any rules on the internal organization of regions making the system flexible and dynamic.",MIT-CSAIL-TR-2005-032; MIT-LCS-TR-989,136 p.; 114357185 bytes; 3081677 bytes,application/postscript; application/pdf,en_US,,Advanced Network Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Caponnetto, Andrea; Rakhlin, Alexander",2005-12-22T02:29:32Z,2005-12-22T02:29:32Z,2005-05-17,http://hdl.handle.net/1721.1/30545,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Some Properties of Empirical Risk Minimization over Donsker Classes,"We study properties of algorithms which minimize (or almost minimize) empirical error over a Donsker class of functions. We show that the L2-diameter of the set of almost-minimizers is converging to zero in probability. Therefore, as the number of samples grows, it is becoming unlikely that adding a point (or a number of points) to the training set will result in a large jump (in L2 distance) to a new hypothesis. We also show that under some conditions the expected errors of the almost-minimizers are becoming close with a rate faster than n^{-1/2}.",MIT-CSAIL-TR-2005-033; AIM-2005-018; CBCL-250,9 p.; 7033622 bytes; 434782 bytes,application/postscript; application/pdf,en_US,AI; empirical risk minimization; stability; empirical processes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Nahnsen, Thade; Uzuner, Ozlem; Katz, Boris",2005-12-22T02:29:37Z,2005-12-22T02:29:37Z,2005-05-19,http://hdl.handle.net/1721.1/30546,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Lexical Chains and Sliding Locality Windows in Content-based Text Similarity Detection,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",MIT-CSAIL-TR-2005-034; AIM-2005-017,9 p.; 17827888 bytes; 7011726 bytes,application/postscript; application/pdf,en_US,AI; Natural Language Processing; N-grams; Text Similarity; Lexical Chains,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Li, Hui X.",2010-04-15T17:30:04Z,2010-04-15T17:30:04Z,2005-05-20,http://hdl.handle.net/1721.1/53718,MIT-CSAIL-TR-2010-017,Generalized Conflict Learning For Hybrid Discrete Linear Optimization,"Conflict-directed search algorithms have formed the core of practical, model-based reasoning systems for the last three decades. In many of these applications there is a series of discrete constraint optimization problems and a conflict-directed search algorithm, which uses conflicts in the forward search step to focus search away from known infeasibilities and towards the optimal solution. In the arena of model-based autonomy, discrete systems, like deep space probes, have given way to more agile systems, such as coordinated vehicle control, which must robustly control their continuous dynamics. Controlling these systems requires optimizing over continuous, as well as discrete variables, using linear and non-linear as well as logical constraints. This thesis explores the development of algorithms for solving ybrid discrete/linear optimization problems that use conflicts in the forward search direction, generalizing from the conflict-directed search algorithms of based reasoning. We introduce a novel algorithm called Generalized Conflict-directed Branch and Bound (GCD-BB). GCD-BB extends traditional Branch and Bound (B&B), by first constructing conflicts from nodes of the search tree that are found to be infeasible or sub-optimal, and then by using these conflicts to guide the forward search away from known infeasible and sub-optimal states. We evaluate GCD-BB empirically on a range of test problems of coordinated air vehicle control. GCD-BB demonstrates a substantial improvement in performance compared to a traditional B&B algorithm, applied to either disjunctive linear programs or an equivalent binary integer program encoding.",,76 p.,,,Constraint satisfaction; Optimization; Hybrid systems,Model-based Embedded and Robotic Systems,,,,SM thesis,"Li, H., Generalized Conflict Learning For Hybrid Discrete Linear Optimization, Master's Thesis, MIT, 2005",,http://hdl.handle.net/1721.1/32466,SM,,,,,,,,,,,,,,,,,,,,
,"Wu, Jia Jane",2005-12-22T02:29:44Z,2005-12-22T02:29:44Z,2005-05-25,http://hdl.handle.net/1721.1/30547,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Comparing Visual Features for Morphing Based Recognition,"This thesis presents a method of object classification using the idea of deformable shape matching.  Three types of visual features, geometric blur, C1 and SIFT, are used to generate feature descriptors.  These feature descriptors are then used to find point correspondences between pairs of images.  Various morphable models are created by small subsets of these correspondences using thin-plate spline.  Given these morphs, a simple algorithm, least median of squares (LMEDS), is used to find the best morph.  A scoring metric, using both LMEDS and distance transform, is used to classify test images based on a nearest neighbor algorithm.  We perform the experiments on the Caltech 101 dataset [5].  To ease computation, for each test image, a shortlist is created containing 10 of the most likely candidates.  We were unable to duplicate the performance of [1] in the shortlist stage because we did not use hand-segmentation to extract objects for our training images.  However, our gain from the shortlist to correspondence stage is comparable to theirs.  In our experiments, we improved from 21% to 28% (gain of 33%), while [1] improved from 41% to 48% (gain of 17%).  We find that using a non-shape based approach, C2 [14], the overall classification rate of 33.61% is higher than all of the shaped based methods tested in our experiments.",MIT-CSAIL-TR-2005-035; AITR-2005-002; CBCL-251,42 p.; 39773758 bytes; 1459526 bytes,application/postscript; application/pdf,en_US,AI; object recognition; shape-based,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Caponnetto, Andrea; Rosasco, Lorenzo; Vito, Ernesto De; Verri, Alessandro",2005-12-22T02:29:53Z,2005-12-22T02:29:53Z,2005-05-27,http://hdl.handle.net/1721.1/30548,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Empirical Effective Dimension and Optimal Rates for Regularized Least Squares Algorithm,"This paper presents an approach to model selection for regularized least-squares on reproducing kernel Hilbert spaces in the semi-supervised setting.  The role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  The main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  The empirical effective dimension can be computed from independent unlabelled samples.  This makes the approach particularly appealing in the semi-supervised setting.",MIT-CSAIL-TR-2005-036; AIM-2005-019; CBCL-252,14 p.; 11158573 bytes; 526018 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; effective dimension; semi-supervised learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taylor, Christopher J.",2005-12-22T02:30:00Z,2005-12-22T02:30:00Z,2005-05-31,http://hdl.handle.net/1721.1/30549,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Simultaneous Localization and Tracking in Wireless Ad-hoc Sensor Networks,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",MIT-CSAIL-TR-2005-037; AITR-2005-003,69 p.; 81859537 bytes; 3510560 bytes,application/postscript; application/pdf,en_US,AI; Localization; Target Tracking; Sensor Network; Calibration,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Segonne, Florent; Pons, Jean-Philippe; Fischl, Bruce; Grimson, Eric",2005-12-22T02:32:21Z,2005-12-22T02:32:21Z,2005-06-01,http://hdl.handle.net/1721.1/30550,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Novel Active Contour Framework. Multi-component Level Set  Evolution under Topology Control,"We present a novel framework to exert a topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. A method, based on the concept of simple point borrowed from digital topology, was recently proposed to achieve a strict topology preservation during a level set evolution. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. We demonstrate the strength of our method on a wide range of numerical experiments.",MIT-CSAIL-TR-2005-038; AIM-2005-020,16 p.; 60440380 bytes; 1311817 bytes,application/postscript; application/pdf,en_US,AI; digital topology; level set; active contour,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Larsen, Sam; Rabbah, Rodric; Amarasinghe, Saman",2005-12-19T23:46:12Z,2005-12-19T23:46:12Z,2005-06-03,http://hdl.handle.net/1721.1/30423,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Exploiting Vector Parallelism in Software Pipelined Loops,"An emerging trend in processor design is the incorporation of short vector instructions into the ISA.  In fact, vector extensions have appeared in most general-purpose microprocessors.  To utilize these instructions, traditional vectorization technology can be used to identify and exploit data parallelism. In contrast, efficient use of a processor\'s scalar resources is typically achieved through ILP techniques such as software pipelining.  In order to attain the best performance, it is necessary to utilize both sets of resources.  This paper presents a novel approach for exploiting vector parallelism in a software pipelined loop.  At its core is a method for judiciously partitioning operations between vector and scalar resources.  The proposed algorithm (i) lowers the burden on the scalar resources by offloading computation to the vector functional units, and (ii) partially (or fully) inhibits the optimizations when full vectorization will decrease performance. !  This results in better resource usage and allows for software pipelining with shorter initiation intervals.  Although our techniques complement statically scheduled machines most naturally, we believe they are applicable to any architecture that tightly integrates support for ILP and data parallelism.An important aspect of the proposed methodology is its ability to manage explicit communication of operands between vector and scalar instructions.  Our methodology also allows for a natural handling of misaligned vector memory operations.  For architectures that provide hardware support for misaligned references, software pipelining effectively hides the latency of these potentially expensive instructions.  When explicit alignment is required in software, our algorithm accounts for these extra costs and vectorizes only when it is profitable.  Finally, our heuristic can take advantage of alignment information where it is available.We evaluate our methodology using several DSP and SPEC FP benchmarks.  Compared to software pipelining, our approach is able to achieve an average speedup of 1.30x and 1.18x for the two benchmark sets, respectively.",MIT-CSAIL-TR-2005-039; MIT-LCS-TM-651,14 p.; 19708112 bytes; 690985 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kumar, Ravi; Liben-Nowell, David; Novak, Jasmine; Raghavan, Prabhakar; Tomkins, Andrew",2005-12-22T02:32:28Z,2005-12-22T02:32:28Z,2005-06-03,http://hdl.handle.net/1721.1/30551,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Theoretical Analysis of Geographic Routing in Social Networks,"We introduce a formal model for geographic social networks, and introduce the notion of rank-based friendship, in which the probability that a person v is a friend of a person u is inversely proportional to the number of people w who live closer to u than v does.  We then prove our main theorem, showing that rank-based friendship is a sufficient explanation of the navigability of any geographic social network that adheres to it.",MIT-CSAIL-TR-2005-040; MIT-LCS-TR-990,8 p.; 8282908 bytes; 444233 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"rahimi, ali; recht, ben; darrell, trevor",2005-12-22T02:32:35Z,2005-12-22T02:32:35Z,2005-06-06,http://hdl.handle.net/1721.1/30552,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Nonlinear Latent Variable Models for Video Sequences,"Many high-dimensional time-varying signals can be modeled as a  sequence of noisy nonlinear observations of a low-dimensional  dynamical process.  Given high-dimensional observations and a  distribution describing the dynamical process, we present a  computationally inexpensive approximate algorithm for estimating the  inverse of this mapping. Once this mapping is learned, we can invert  it to construct a generative model for the signals. Our algorithm  can be thought of as learning a manifold of images by taking into  account the dynamics underlying the low-dimensional representation  of these images. It also serves as a nonlinear system identification  procedure that estimates the inverse of the observation function in  nonlinear dynamic system.  Our algorithm reduces to a generalized  eigenvalue problem, so it does not suffer from the computational or  local minimum issues traditionally associated with nonlinear system  identification, allowing us to apply it to the problem of learning  generative models for video sequences.",MIT-CSAIL-TR-2005-041; AIM-2005-021,11 p.; 13801637 bytes; 2196348 bytes,application/postscript; application/pdf,en_US,"AI; Manifold learning,nonlinear system identification; unsupervised learning",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Saff, David; Artzi, Shay; Perkins, Jeff H.; Ernst, Michael D.",2005-12-22T02:32:40Z,2005-12-22T02:32:40Z,2005-06-08,http://hdl.handle.net/1721.1/30553,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Test Factoring for Java,"Test factoring creates fast, focused unit tests from slow system-widetests; each new unit test exercises only a subset of the functionalityexercised by the system test.  Augmenting a test suite with factoredunit tests should catch errors earlier in a test run.One way to factor a test is to introduce 'mock' objects.  If a testexercises a component T, which interacts with another component E (the'environment'), the implementation of E can be replaced by a mock.The mock checks that T's calls to E are as expected, and it simulatesE's behavior in response.  We introduce an automatic technique fortest factoring.  Given a system test for T and E, and a record of T'sand E's behavior when the system test is run, test factoring generatesunit tests for T in which E is mocked.  The factored tests can isolatebugs in T from bugs in E and, if E is slow or expensive, improve testperformance or cost.We have built an implementation of automatic dynamic test factoring for theJava language.  Our experimental data indicates that it can reduce therunning time of a system test suite by up to an order of magnitude.",MIT-CSAIL-TR-2005-042; MIT-LCS-TR-991,10 p.; 21413970 bytes; 780396 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Muthitacharoen, Athicha; Gilbert, Seth; Morris, Robert",2005-12-22T02:32:54Z,2005-12-22T02:32:54Z,2005-06-15,http://hdl.handle.net/1721.1/30555,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Etna: a Fault-tolerant Algorithm for Atomic Mutable DHT Data,"This paper presents Etna, an algorithm for atomic reads and writes of replicated data stored in a distributed hash table. Etna correctly handles dynamically changing sets of replica hosts, and is optimized for reads, writes, and reconfiguration, in that order.Etna maintains a series of replica configurations as nodes in the system change, using new sets of replicas from the pool supplied by the distributed hash table system. It uses the Paxos protocol to ensure consensus on the members of each new configuration. For simplicity and performance, Etna serializes all reads and writes through a primary during the lifetime of each configuration. As a result, Etna completes read and write operations in only a single round from the primary.Experiments in an environment with high network delaysshow that Etna's read latency is determined by round-tripdelay in the underlying network, while write and reconfiguration latency is determined by the transmission time required to send data to each replica. Etna's write latency is about the same as that of a non-atomic replicating DHT, and Etna's read latency is about twice that of a non-atomic DHT due to Etna assembling a quorum for every read.",MIT-CSAIL-TR-2005-044; MIT-LCS-TR-993,10 p.; 16474627 bytes; 693416 bytes,application/postscript; application/pdf,en_US,,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dolev, Shlomi; Gilbert, Seth; Schiller, Elad; Shvartsman, Alex; Welch, Jennifer",2005-12-22T02:32:47Z,2005-12-22T02:32:47Z,2005-06-15,http://hdl.handle.net/1721.1/30554,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Autonomous Virtual Mobile Nodes,"This paper presents a new abstraction for virtual infrastructure in mobile ad hoc networks. An AutonomousVirtual Mobile Node (AVMN) is a robust and reliable entity that is designed to cope with theinherent difficulties caused by processors arriving, leaving, and moving according to their own agendas,as well as with failures and energy limitations. There are many types of applications that may make useof the AVMN infrastructure: tracking, supporting mobile users, or searching for energy sources.The AVMN extends the focal point abstraction in [9] and the virtual mobile node abstraction in [10].The new abstraction is that of a virtual general-purpose computing entity, an automaton that can makeautonomous on-line decisions concerning its own movement. We describe a self-stabilizing implementationof this new abstraction that is resilient to the chaotic behavior of the physical processors and providesautomatic recovery from any corrupted state of the system.",MIT-CSAIL-TR-2005-043; MIT-LCS-TR-992,11 p.; 16020110 bytes; 624285 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob",2006-06-01T16:22:48Z,2006-06-01T16:22:48Z,2005-07,http://hdl.handle.net/1721.1/32986,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Amorphous Medium Language,"Programming reliable behavior on a large mesh network composed of unreliable parts is difficult. Amorphous Medium Language addresses this problem by abstracting robustness and networking issues away from the programmer via language of geometric primitives and homeostasis maintenance.AML is designed to operate on a high diameter network composed of thousands to billions of nodes, and does not assume coordinate, naming, or routing services. Computational processes are distributed through geometric regions of the space approximated by the network and specify behavior in terms of homeostasis conditions and actions to betaken when homeostasis is violated.AML programs are compiled for local execution using previously developed amorphous computing primitives which provide robustness against ongoing failures and joins and localize the impact of changes in topology. I show some examples of how AML allows complex robust behavior to be expressed in simple programs and some preliminary results from simulation.",MIT-CSAIL-TR-2006-040,7 p.; 408752 bytes; 2548880 bytes,application/pdf; application/postscript,en_US,distributed computing sensor networks,Mathematics and Computation,,,,,"LSMAS Workshop, AAMAS'05, July 25-­29, 2005, Utrecht, Netherlands.",,,,,,,,,,,,,,,,,,,,,,,
,"Hung, Chou; Kreiman, Gabriel; Poggio, Tomaso; DiCarlo, James J.",2005-12-22T02:33:06Z,2005-12-22T02:33:06Z,2005-07-06,http://hdl.handle.net/1721.1/30556,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Ultra-fast Object Recognition from Few Spikes,"Understanding the complex brain computations leading to object recognition requires quantitatively characterizing the information represented in inferior temporal cortex (IT), the highest stage of the primate visual stream. A read-out technique based on a trainable classifier is used to characterize the neural coding of selectivity and invariance at the population level. The activity of very small populations of independently recorded IT neurons (~100 randomly selected cells) over very short time intervals (as small as 12.5 ms) contains surprisingly accurate and robust information about both object Â‘identityÂ’ and Â‘categoryÂ’, which is furthermore highly invariant to object position and scale. Significantly, selectivity and invariance are present even for novel objects, indicating that these properties arise from the intrinsic circuitry and do not require object-specific learning. Within the limits of the technique, there is no detectable difference in the latency or temporal resolution of the IT information supporting so-called Â‘categorizationÂ’ (a.k. basic level) and Â‘identificationÂ’ (a.k. subordinate level) tasks.  Furthermore, where information, in particular information about stimulus location and scale, can also be read-out from the same small population of IT neurons. These results show how it is possible to decode invariant object information rapidly, accurately and robustly from a small population in IT and provide insights into the nature of the neural code for different kinds of object-related information.",MIT-CSAIL-TR-2005-045; AIM-2005-022; CBCL-253,30 p.; 77109103 bytes; 12556007 bytes,application/postscript; application/pdf,en_US,AI; object recognition; neural coding; inferior temporal cortex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Yokono, Jerry Jun; Poggio, Tomaso",2005-12-22T02:33:20Z,2005-12-22T02:33:20Z,2005-07-07,http://hdl.handle.net/1721.1/30557,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Boosting a Biologically Inspired Local Descriptor for Geometry-free Face and Full Multi-view 3D Object Recognition,"Object recognition systems relying on local descriptors are increasingly used because of their perceived robustness with respect to occlusions and to global geometrical deformations.  Descriptors of this type -- based on a set of oriented Gaussian derivative filters -- are used in our recognition system.  In this paper, we explore a multi-view 3D object recognition system that does not use explicit geometrical information. The basic idea is to find discriminant features to describe an object across different views.  A boosting procedure is used to select features out of a large feature pool of local features collected from the positive training examples.  We describe experiments on face images with excellent recognition rate.",MIT-CSAIL-TR-2005-046; AIM-2005-023; CBCL-254,22 p.; 49560015 bytes; 7562398 bytes,application/postscript; application/pdf,en_US,AI; 3D multiview; object recognition; SVM and boosting classifiers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Liskov, Barbara; Rodrigues, Rodrigo",2005-12-22T02:33:26Z,2005-12-22T02:33:26Z,2005-07-21,http://hdl.handle.net/1721.1/30558,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Byzantine Clients Rendered Harmless,"Byzantine quorum systems have been proposed that work properly even when up to f replicas fail arbitrarily.However, these systems are not so successful when confronted with Byzantine faulty clients. This paper presents novelprotocols that provide atomic semantics despite Byzantine clients. Our protocols are the first to handle all problemscaused by Byzantine clients. They prevent Byzantine clients from interfering with good clients: bad clients cannotprevent good clients from completing reads and writes, and they cannot cause good clients to see inconsistencies. Inaddition we also prevent bad clients that have been removed from operation from leaving behind more than a boundednumber of writes that could be done on their behalf by a colluder.Our protocols are designed to work in an asynchronous system like the Internet and they are highly efficient. Werequire 3f +1 replicas, and either two or three phases to do writes; reads normally complete in one phase and requireno more than two phases, no matter what the bad clients are doing.We also present strong correctness conditions for systems with Byzantine clients that limit what can be done onbehalf of bad clients once they leave the system. Furthermore we prove that our protocols are both safe (they meetthose conditions) and live.",MIT-CSAIL-TR-2005-047; MIT-LCS-TR-994,11 p.; 20805987 bytes; 680924 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chockler, Gregory; Lynch, Nancy; Mitra, Sayan; Tauber, Joshua",2005-12-22T02:33:34Z,2005-12-22T02:33:34Z,2005-07-22,http://hdl.handle.net/1721.1/30559,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Proving Atomicity: An Assertional Approach,"Atomicity (or linearizability) is a commonly used  consistency criterion for distributed services and objects. Although  atomic object implementations are abundant, proving that algorithms  achieve atomicity has turned out to be a challenging problem. In  this paper, we initiate the study of systematic ways of verifying  distributed implementations of atomic objects, beginning with  read/write objects (registers).  Our general approach is to replace  the existing operational reasoning about events and partial orders  with assertional reasoning about invariants and simulation  relations.  To this end, we define an abstract state machine that  captures the atomicity property and prove correctness of the object  implementations by establishing a simulation mapping between the  implementation and the specification automata. We demonstrate the  generality of our specification by showing that it is implemented by  three different read/write register constructions (the  message-passing register emulation of Attiya, Bar-Noy and Dolev, its  optimized version based on real time, and the shared memory register  construction of Vitanyi and Awerbuch), and by a general atomic  object implementation based on the Lamport\'s replicated state  machine algorithm.",MIT-CSAIL-TR-2005-048; MIT-LCS-TR-995,15 p.; 14829213 bytes; 699536 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hsu, Eugene; Pulli, Kari; Popovic, Jovan",2008-08-28T18:45:44Z,2008-08-28T18:45:44Z,2005-08-01,http://hdl.handle.net/1721.1/42004,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Style Translation for Human Motion (Supplemental Material),"Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel correspondence algorithm to align motions, and a linear time-invariant model to represent stylistic differences. Once the model is estimated with system identification, our system is capable of translating streaming input with simple linear operations at each frame.",,,,,,,,,,,"Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel corres",Jovan Popovic; Computer Graphics,,,,,,,,,,,,,,,,,,,,,,
,"Hsu, Eugene; Pulli, Kari; Popovic, Jovan",2008-08-25T19:00:49Z,2008-08-25T19:00:49Z,2005-08-01,http://hdl.handle.net/1721.1/41945,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Style Translation for Human Motion,"Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel correspondence algorithm to align motions, and a linear time-invariant model to represent stylistic differences. Once the model is estimated with system identification, our system is capable of translating streaming input with simple linear operations at each frame.",,,,,,,,,,,"ACM Transactions on Graphics, 24(3):1082-1089, August 2005.",Jovan Popovic; Computer Graphics,,,,,,,,,,,,,,,,,,,,,,
,"Marnette, Bruno; Kuncak, Viktor; Rinard, Martin",2005-12-22T02:33:49Z,2005-12-22T02:33:49Z,2005-08-03,http://hdl.handle.net/1721.1/30561,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Algorithms and Complexity for Sets with Cardinality Constraints,"Typestate systems ensure many desirable properties of imperativeprograms, including initialization of object fields and correct use ofstateful library interfaces.  Abstract sets with cardinalityconstraints naturally generalize typestate properties: relationshipsbetween the typestates of objects can be expressed as subset anddisjointness relations on sets, and elements of sets can berepresented as sets of cardinality one.  In addition, sets withcardinality constraints provide a natural language for specifyingoperations and invariants of data structures.Motivated by these program analysis applications, thispaper presents new algorithms and new complexity results forconstraints on sets and their cardinalities.  We studyseveral classes of constraints and demonstrate a trade-offbetween their expressive power and their complexity.Our first result concerns a quantifier-free fragment of BooleanAlgebra with Presburger Arithmetic.  We give a nondeterministicpolynomial-time algorithm for reducing the satisfiability of sets withsymbolic cardinalities to constraints on constant cardinalities, andgive a polynomial-space algorithm for the resulting problem.  The bestpreviously existing algorithm runs in exponential space andnondeterministic exponential time.In a quest for more efficient fragments, we identify severalsubclasses of sets with cardinality constraints whose satisfiabilityis NP-hard.  Finally, we identify a class of constraints that haspolynomial-time satisfiability and entailment problems and can serveas a foundation for efficient program analysis.  We give a system ofrewriting rules for enforcing certain consistency properties of theseconstraints and show how to extract complete information fromconstraints in normal form.  This result implies the soundness andcompleteness of our algorithms.",MIT-CSAIL-TR-2005-050; MIT-LCS-TR-997,19 p.; 33171425 bytes; 1663929 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Vutukuru, Mythili; Valiant, Paul; Kopparty, Swastik; Balakrishnan, Hari",2005-12-22T02:33:41Z,2005-12-22T02:33:41Z,2005-08-03,http://hdl.handle.net/1721.1/30560,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,How to Construct a Correct and Scalable iBGP Configuration,"The Border Gateway Protocol (BGP), the current inter domain routing protocol in the Internet, has two modes of operation: eBGP (External BGP), used to exchange routing information between autonomous systems, and iBGP (Internal BGP), used to propagate that information within an autonomous system (AS).  This paper focuses on the construction of an iBGP session configuration that guarantees two correctness properties - loop-free forwarding paths and complete visibility to all eBGP-learned best routes - while attempting to minimize the number of iBGP sessions (for scalability) and ensuring that the constructed configuration guarantees the two correctness properties even in the face of link failures and IGPpath changes.  Our algorithm constructs an iBGP configuration based on route reflectors, a commonly used way to control the number of iBGP sessions.  The algorithm, BGPSep, uses the notion of a graph separator, a (small) set of nodes that partition a graph into connected components of roughly equal sizes, recursively applies this idea to the connected components, and produces a route reflector hierarchy and the associated iBGP sessions.  We prove thatBGPSep guarantees the desired correctness properties, andevaluate an implementation of the BGPSep algorithm on several real-world and simulated network topologies.  Across these topologies, we find that the number of iBGP sessions with is afactor of 2.5 to 5 times smaller than with a \""full mesh\"" iBGP, while guaranteeing the desired correctness properties.",MIT-CSAIL-TR-2005-049; MIT-LCS-TR-996,13 p.; 18855558 bytes; 842183 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Bhattacharyya, Arnab",2005-12-22T02:33:55Z,2005-12-22T02:33:55Z,2005-08-08,http://hdl.handle.net/1721.1/30562,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Implementing Probabilistically Checkable Proofs of Proximity,"Abstract: In this paper, we describe a proof-of-concept implementation of the probabilistically checkable proof of proximity (PCPP) system described by Ben-Sasson and Sudan in \\cite{bs05}.  In particular, we implement a PCPP prover and verifier for Reed-Solomon codes; the prover converts an evaluation of a polynomial on a linear set into a valid PCPP, while the verifier queries the evaluation and the PCPP to check that the evaluation is close to a Reed-Solomon codeword.  We prove tight bounds on the various parameters associated with the prover and verifier and describe some interesting programmatic issues that arise during their implementation.",MIT-CSAIL-TR-2005-051; MIT-LCS-TR-998,16 p.; 13280259 bytes; 636640 bytes,application/postscript; application/pdf,en_US,,Complexity Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dolev, Shlomi; Lahiani, Limor; Lynch, Nancy; Nolte, Tina",2005-12-22T02:36:07Z,2005-12-22T02:36:07Z,2005-08-11,http://hdl.handle.net/1721.1/30563,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Self-Stabilizing Mobile Node Location Management and Message,"We present simple algorithms for achieving self-stabilizing locationmanagement and routing in mobile ad-hoc networks. While mobile clients maybe susceptible to corruption and stopping failures, mobile networks areoften deployed with a reliable GPS oracle, supplying frequent updates ofaccurate real time and location information to mobile nodes. Informationfrom a GPS oracle provides an external, shared source of consistency formobile nodes, allowing them to label and timestamp messages, and henceaiding in identification of, and eventual recovery from, corruption andfailures. Our algorithms use a GPS oracle.Our algorithms also take advantage of the Virtual Stationary Automataprogramming abstraction, consisting of mobile clients, virtual timedmachines called virtual stationary automata (VSAs), and a local broadcastservice connecting VSAs and mobile clients. VSAs are distributed at knownlocations over the plane, and emulated in a self-stabilizing manner by themobile nodes in the system. They serve as fault-tolerant building blocksthat can interact with mobile clients and each other, and can simplifyimplementations of services in mobile networks.We implement three self-stabilizing, fault-tolerant services, each builton the prior services: (1) VSA-to-VSA geographic routing, (2) mobileclient location management, and (3) mobile client end-to-end routing. Weuse a greedy version of the classical depth-first search algorithm toroute messages between VSAs in different regions. The mobile clientlocation management service is based on home locations: Each clientidentifier hashes to a set of home locations, regions whose VSAs areperiodically updated with the client\'s location. VSAs maintain thisinformation and answer queries for client locations. Finally, theVSA-to-VSA routing and location management services are used to implementmobile client end-to-end routing.",MIT-CSAIL-TR-2005-052; MIT-LCS-TR-999,20 p.; 27793653 bytes; 1205701 bytes,application/postscript; application/pdf,en_US,,Theory of Distributed Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Katti, Sachin; Katabi, Dina; Puchala, Katarzyna",2005-12-22T02:36:13Z,2005-12-22T02:36:13Z,2005-08-15,http://hdl.handle.net/1721.1/30564,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Slicing the Onion: Anonymous Routing Without PKI,"Recent years have witnessed many proposals for anonymous routing in overlay peer-to-peer networks. The proposed protocols either expose the receiver and the message content, or require the overlay nodes to have public-private key pairs with the public keys known to everyone. In practice, however, key distribution and management are well-known difficultproblems and have crippled any widespread deployment of anonymous routing. This paper uses a combination of information slicing and source routing to provide anonymous communication in a way similar to Onion Routing but without a public key infrastructure (PKI).",MIT-CSAIL-TR-2005-053; MIT-LCS-TR-1000,7 p.; 13490568 bytes; 735059 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Richards, Whitman",2005-12-22T02:36:19Z,2005-12-22T02:36:19Z,2005-08-16,http://hdl.handle.net/1721.1/30565,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Collective Choice with Uncertain Domain Moldels,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a Â“top-cycleÂ”.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voterÂ’s view of how alternatives are related. As more and more voterÂ’s preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",MIT-CSAIL-TR-2005-054; AIM-2005-024,18 p.; 17793797 bytes; 614937 bytes,application/postscript; application/pdf,en_US,AI; collective choice; uncertainty; voting; top-cycles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Olivier; Segala, Roberto",2005-12-22T02:36:36Z,2005-12-22T02:36:36Z,2005-08-19,http://hdl.handle.net/1721.1/30566,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Probabilistic I/O Automata to Analyze an Oblivious Transfer Protocol,"We demonstrate how to carry out cryptographic security analysis ofdistributed protocols within the Probabilistic I/O Automata frameworkof Lynch, Segala, and Vaandrager.This framework provides tools for arguing rigorously about theconcurrency and scheduling aspects of protocols, and about protocolspresented at different levels of abstraction.Consequently, it can help in making cryptographic analysis moreprecise and less susceptible to errors.We concentrate on a relatively simple two-party Oblivious Transferprotocol, in the presence of a semi-honest adversary (essentially, aneavesdropper).For the underlying cryptographic notion of security, we use a versionof Canetti's Universally Composable security.In spite of the relative simplicity of the example, the exercise isquite nontrivial.It requires taking many fundamental issues into account,including nondeterministic behavior, scheduling, resource-boundedcomputation, and computational hardness assumptions for cryptographicprimitives.",MIT-CSAIL-TR-2005-055; MIT-LCS-TR-1001,123 p.; 130601890 bytes; 5801647 bytes,application/postscript; application/pdf,en_US,,Theory of Distributed Systems,,,,,,,,,http://hdl.handle.net/1721.1/33154,http://hdl.handle.net/1721.1/33154,,,,,,,,,,,,,,,,,,
,"Russell, Bryan C.; Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2005-12-22T02:36:40Z,2005-12-22T02:36:40Z,2005-09-08,http://hdl.handle.net/1721.1/30567,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,LabelMe: a database and web-based tool for image annotation,"Research in object detection and recognition in cluttered scenes requires large image collections with ground truth labels.  The labels should provide information about the object classes present in each image, as well as their shape and locations, and possibly other attributes such as pose.  Such data is useful for testing, as well as for supervised learning.  This project provides a web-based annotation tool that makes it easy to annotate images, and to instantly sharesuch annotations with the community.  This tool, plus an initial set of 10,000 images (3000 of which have been labeled), can be found at http://www.csail.mit.edu/$\sim$brussell/research/LabelMe/intro.html",MIT-CSAIL-TR-2005-056; AIM-2005-025,11 p.; 12518984 bytes; 559659 bytes,application/postscript; application/pdf,en_US,AI; object recognition detection database annotation tool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Stauffer, Chris",2005-12-22T02:36:45Z,2005-12-22T02:36:45Z,2005-09-20,http://hdl.handle.net/1721.1/30568,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automated Audio-visual Activity Analysis,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",MIT-CSAIL-TR-2005-057; AIM-2005-026,9 p.; 32903979 bytes; 1153580 bytes,application/postscript; application/pdf,en_US,AI; Unsupervised; activity analysis; scene modeling; tracking; event detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Theocharous, Georgios; Mahadevan, Sridhar; Kaelbling, Leslie Pack",2005-12-22T02:36:53Z,2005-12-22T02:36:53Z,2005-09-27,http://hdl.handle.net/1721.1/30569,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Spatial and Temporal Abstractions in POMDPs Applied to Robot Navigation,"Partially observable Markov decision processes (POMDPs) are a well studied paradigm for programming autonomous robots, where the robot sequentially chooses actions to achieve long term goals efficiently.  Unfortunately, for real world robots and other similar domains, the uncertain outcomes of the actions and the fact that the true world state may not be completely observable make learning of models of the world extremely difficult, and using them algorithmically infeasible.  In this paper we show that learning POMDP models and planning with them can become significantly easier when we incorporate into our algorithms the notions of spatial and tempral abstraction.  We demonstrate the superiority of our algorithms by comparing them with previous flat approaches for large scale robot navigation.",MIT-CSAIL-TR-2005-058; AIM-2005-027,72 p.; 73465696 bytes; 2744720 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Arkoudas, Konstantine",2005-12-22T02:37:00Z,2005-12-22T02:37:00Z,2005-10-06,http://hdl.handle.net/1721.1/30570,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining diagrammatic and symbolic reasoning,"We introduce a domain-independent framework for heterogeneous natural deduction that combines diagrammatic and sentential reasoning. The framework is presented in the form of a family of denotational proof languages (DPLs). Diagrams are represented as possibly partial descriptions of finite system states. This allows us to dealwith incomplete information, which we formalize by admitting sets as attribute values. We introduce a notion of attribute interpretations that enables us to interpret  first-order signatures into such system states, and develop a formal semantic framework based on Kleene\'s strong three-valued logic. We extend the assumption-base semantics of DPLs to accodomodate diagrammatic reasoning by introducing general inference mechanisms  for the valid extraction of information from diagrams and for the incorporation of sentential information into diagrams. A rigorous big-step operational semantics is given, on the basis of which we prove that our framework is sound. In addition, we specify detailed algorithms for implementing proof checkers for the resulting languages, and discuss associated efficiency issues.",MIT-CSAIL-TR-2005-059; MIT-LCS-TR-1002,58 p.; 48633844 bytes; 2003976 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ajmani, Sameer",2005-12-19T23:30:00Z,2005-12-19T23:30:00Z,2005-10-06,http://hdl.handle.net/1721.1/30418,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems (PhD thesis),"Upgrading the software of long-lived, highly-available distributedsystems is difficult.  It is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  Instead, upgrades may happengradually, and there may be long periods of time when different nodesare running different software versions and need to communicate usingincompatible protocols.  We present a methodology and infrastructurethat address these challenges and make it possible to upgradedistributed systems automatically while limiting service disruption.Our methodology defines how to enable nodes to interoperate acrossversions, how to preserve the state of a system across upgrades, and howto schedule an upgrade so as to limit service disruption.  The approachis modular: defining an upgrade requires understanding only the newsoftware and the version it replaces.The upgrade infrastructure is a generic platform for distributing andinstalling software while enabling nodes to interoperate acrossversions.  The infrastructure requires no access to the system sourcecode and is transparent: node software is unaware that differentversions even exist.  We have implemented a prototype of theinfrastructure called Upstart that intercepts socket communication usinga dynamically-linked C++ library.  Experiments show that Upstart has lowoverhead and works well for both local-area and Internet systems.",MIT-CSAIL-TR-2005-061; MIT-LCS-TR-1004,164 p.; 135378118 bytes; 5540633 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Gassend, B.; O'Donnell, C. W.; Thies, W.; Lee, A.; van Dijk, M.; Devadas, S.",2005-12-22T02:37:06Z,2005-12-22T02:37:06Z,2005-10-06,http://hdl.handle.net/1721.1/30571,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Secondary Structure Prediction of All-Helical Proteins Using Hidden Markov Support Vector Machines,"Our goal is to develop a state-of-the-art predictor with an intuitive and biophysically-motivated energy model through the use of Hidden Markov Support Vector Machines (HM-SVMs), a recent innovation in the field of machine learning.  We focus on the prediction of alpha helices in proteins and show that using HM-SVMs, a simple 7-state HMM with 302 parameters can achieve a Q_alpha value of 77.6% and a SOV_alpha value of 73.4%.  We briefly describe how our method can be generalized to predicting beta strands and sheets.",MIT-CSAIL-TR-2005-060; MIT-LCS-TR-1003,15 p.; 18110378 bytes; 702915 bytes,application/postscript; application/pdf,en_US,,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ajmani, Sameer; Liskov, Barbara; Shrira, Liuba; Curtis, Dorothy",2005-12-22T02:37:15Z,2005-12-22T02:37:15Z,2005-10-06,http://hdl.handle.net/1721.1/30572,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems,"Upgrading the software of long-lived, highly-available distributedsystems is difficult.  It is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  Instead, upgrades must happengradually, and there may be long periods of time when different nodesrun different software versions and need to communicate usingincompatible protocols.  We present a methodology and infrastructurethat make it possible to upgrade distributed systems automatically whilelimiting service disruption.  We introduce new ways to reason aboutcorrectness in a multi-version system. We also describe a prototypeimplementation that supports automatic upgrades with modest overhead.",MIT-CSAIL-TR-2005-062; MIT-LCS-TR-1005,14 p.; 26794595 bytes; 1207166 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Das, Sanmay",2005-12-22T02:37:21Z,2005-12-22T02:37:21Z,2005-10-07,http://hdl.handle.net/1721.1/30573,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning to Trade with Insider Information,"This paper introduces algorithms for learning how to trade usinginsider (superior) information in Kyle's model of financial markets.Prior results in finance theory relied on the insider having perfectknowledge of the structure and parameters of the market. I show herethat it is possible to learn the equilibrium trading strategy whenits form is known even without knowledge of the parameters governingtrading in the model. However, the rate of convergence toequilibrium is slow, and an approximate algorithm that does notconverge to the equilibrium strategy achieves better utility whenthe horizon is limited. I analyze this approximate algorithm fromthe perspective of reinforcement learning and discuss the importanceof domain knowledge in designing a successful learning algorithm.",MIT-CSAIL-TR-2005-063; AIM-2005-028; CBCL-255,15 p.; 16523384 bytes; 613212 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Zhang, MIchael; Asanovic, Krste",2005-12-22T02:37:29Z,2005-12-22T02:37:29Z,2005-10-10,http://hdl.handle.net/1721.1/30574,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Victim Migration: Dynamically Adapting Between Private and Shared CMP Caches,"Future CMPs will have more cores and greater onchip cache capacity. The on-chip cache can either be divided into separate private L2 caches for each core, or treated as a large shared L2 cache. Private caches provide low hit latency but low capacity, while shared caches have higher hit latencies but greater capacity. Victim replication was previously introduced as a way of reducing the average hit latency of a shared cache by allowing a processor to make a replica of a primary cache victim in its local slice of the global L2 cache. Although victim replication performs well on multithreaded and single-threaded codes, it performs worse than the private scheme for multiprogrammed workloads where there is little sharing between the different programs running at the same time. In this paper, we propose victim migration, which improves on victim replication by adding an additional set of migration tags on each node which are used to implement an exclusive cache policy for replicas. When a replica has been created on a remote node, it is not also cached on the home node, but only recorded in the migration tags. This frees up space on the home node to store shared global lines or replicas for the local processor. We show that victim migration performs better than private, shared, and victim replication schemes across a range of single threaded, multithreaded, and multiprogrammed workloads, while using less area than a private cache design. Victim migration provides a reduction in average memory access latency of up to 10% over victim replication.",MIT-CSAIL-TR-2005-064; MIT-LCS-TR-1006,17 p.; 18487877 bytes; 796263 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Geiger, Gadi; Amara, Domenic G",2005-12-22T02:37:36Z,2005-12-22T02:37:36Z,2005-10-18,http://hdl.handle.net/1721.1/30575,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Towards the Prevention of Dyslexia,"Previous studies have shown that dyslexic individuals who supplement windowed reading practice with intensive small-scale hand-eye coordination tasks exhibit marked improvement in their reading skills. Here we examine whether similar hand-eye coordination activities, in the form of artwork performed by children in kindergarten, first and second grades, could reduce the number of students at-risk for reading problems. Our results suggest that daily hand-eye coordination activities significantly reduce the number of students at-risk. We believe that the effectiveness of these activities derives from their ability to prepare the students perceptually for reading.",MIT-CSAIL-TR-2005-065; AIM-2005-029; CBCL-256,0 p.; 14865502 bytes; 6633149 bytes,application/postscript; application/pdf,en_US,AI; dyslexia; prevention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Torlak, Emina; van Dijk, Marten; Gassend, Blaise; Jackson, Daniel; Devadas, Srinivas",2005-12-22T02:37:42Z,2005-12-22T02:37:42Z,2005-10-19,http://hdl.handle.net/1721.1/30576,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Knowledge Flow Analysis for Security Protocols,"Knowledge flow analysis offers a simple and flexible way to find flaws in security protocols. A protocol is described by a collection of rules constraining the propagation of knowledge amongst principals. Because this characterization corresponds closely to informal descriptions of protocols, it allows a succinct and natural formalization; because it abstracts away message ordering, and handles communications between principals and applications of cryptographic primitives uniformly, it is readily represented in a standard logic. A generic framework in the Alloy modelling language is presented, and instantiated for two standard protocols, and a new key management scheme.",MIT-CSAIL-TR-2005-066; MIT-LCS-TR-1007,23 p.; 22738148 bytes; 915258 bytes,application/postscript; application/pdf,en_US,,Software Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lippert, Ross; Rifkin, Ryan",2005-12-22T02:40:10Z,2005-12-22T02:40:10Z,2005-10-20,http://hdl.handle.net/1721.1/30577,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Asymptotics of Gaussian Regularized Least-Squares,"We consider regularized least-squares (RLS) with a Gaussian kernel. Weprove that if we let the Gaussian bandwidth $\sigma \rightarrow\infty$ while letting the regularization parameter $\lambda\rightarrow 0$, the RLS solution tends to a polynomial whose order iscontrolled by the relative rates of decay of $\frac{1}{\sigma^2}$ and$\lambda$: if $\lambda = \sigma^{-(2k+1)}$, then, as $\sigma \rightarrow\infty$, the RLS solution tends to the $k$th order polynomial withminimal empirical error.  We illustrate the result with an example.",MIT-CSAIL-TR-2005-067; AIM-2005-030; CBCL-257,1 p.; 7286963 bytes; 527607 bytes,application/postscript; application/pdf,en_US,AI; machine learning; regularization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Drake, Matthew; Hoffmann, Hank; Rabbah, Rodric; Amarasinghe, Saman",2005-12-22T02:40:17Z,2005-12-22T02:40:17Z,2005-10-22,http://hdl.handle.net/1721.1/30578,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,MPEG-2 in a Stream Programming Language,"Image and video codecs are prevalent in multimedia applications, ranging from embedded systems, to desktop computers, to high-end servers such as HDTV editing consoles. It is not uncommon however that developers create (from scratch) and customize their codec implementations for each of the architecture targets they intend their coders and decoders to run on. This practice is time consuming anderror prone, leading to code that is not malleable or portable.  In this paper we describe an implementation of the MPEG-2 codec using the StreamIt programming language. StreamIt is an architecture-independent stream language that aims to improve programmer productivity, while concomitantly exposing the inherent parallelism and communication topology of the application. We describe why MPEG is a good match forthe streaming programming model, and illustrate the malleability of the implementation using a simple modification to the decoder to support alternate color compression formats. StreamIt allows for modular application development, which also reduces the complexity of the debugging process since stream components can be verifiedindependently. This in turn leads to greater programmer productivity. We implement a fully functional MPEG-2 decoder in StreamIt. The decoder was developed in eight weeks by a single student programmer who did not have any prior experience with MPEG or other video codecs. Many of the MPEG-2 components were subsequently reused to assemble a JPEG codec.",MIT-CSAIL-TR-2005-068; MIT-LCS-TM-652,15 p.; 22871205 bytes; 786081 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Nguyen, Huu Hai; Rinard, Martin",2005-12-22T02:40:21Z,2005-12-22T02:40:21Z,2005-10-26,http://hdl.handle.net/1721.1/30579,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Cyclic Memory Allocation to Eliminate Memory Leaks,"We present and evaluate a new memory management technique foreliminating memory leaks in programs with dynamic memoryallocation. This technique observes the execution of the program on asequence of training inputsto find m-bounded allocation sites,which have the property that at any time during the execution of theprogram, the program accesses at most only the last m objects allocated atthat site. The technique then transforms the program to usecyclic memory allocation at that site: it preallocates a buffercontaining m objects of the type allocated at that site, with eachallocation returning the next object in the buffer. At the end of thebuffer the allocations wrap back around to the first object.  Cyclicallocation eliminates any memory leak at the allocation site - thetotal amount of memory required to hold all of the objects everallocated at the site is simply $m$ times the object size.We evaluate our technique by applying it to several widely-used opensource programs.  Our results show that it is able to successfullyeliminate important memory leaks in these programs.  A potentialconcern is that the estimated bounds m may be too small, causing theprogram to overlay live objects in memory.  Our results indicate thatour bounds estimation technique is quite accurate in practice,providing incorrect results for only one of the 160 m-bounded sitesthat it identifies. To evaluate the potential impact ofoverlaying live objects, we artificially reduce the bounds at$m$-bounded sites and observe the resulting behavior.The resulting overlayingof live objects often does not affect thefunctionality of the program at all; even when it does impairpart of the functionality, the program does not fail andis still able to acceptably deliver the remaining functionality.",MIT-CSAIL-TR-2005-069; MIT-LCS-TR-1008,10 p.; 20338887 bytes; 816035 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Rahul, Hariharan; Kasbekar, Mangesh; Sitaraman, Ramesh; Berger, Arthur",2005-12-22T02:40:26Z,2005-12-22T02:40:26Z,2005-11-01,http://hdl.handle.net/1721.1/30580,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Towards Realizing the Performance and Availability Benefits of a Global Overlay Network,"Prior analyses of the benefits of routing overlays are based onplatforms consisting of nodes located primarily in North America, onthe academic Internet, and at the edge of the network. This paper isthe first global study of the benefits of overlays on the commercialInternet in terms of round trip latencies and availability, usingmeasurements from diverse ISPs over 1100 locations (77 countries, 630cities and 6 continents).Our study shows that while overlays provide some improvements in North America, their benefits are especially significant for paths withAsian endpoints.  Regarding practical considerations in constructingoverlay routes, we show that an algorithm that randomly chooses asmall number of alternate redundant paths achieves an availability ofover 99.5%. We also propose and evaluate a simple predictive schemethat achieves almost optimal latency using only 2-3 paths, and thatthis is achievable with surprisingly persistent routing choices.",MIT-CSAIL-TR-2005-070; MIT-LCS-TR-1009,23 p.; 23260667 bytes; 831725 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lepinski, Matthew; Micali, Silvio",2005-12-22T02:40:29Z,2005-12-22T02:40:29Z,2005-11-02,http://hdl.handle.net/1721.1/30581,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Subcontracted Rational SFE,"In their paper, ""Rational Secure Computation and Ideal Mechanism Design,"" Izmalkov, Lepinski and Micali show that any one-shot mediated game can be simulated by the players themselves, without the help of a trusted mediator, using physical envelopes and a ballot-box. We show that communication between the players is not essential to the ILM protocol. That is, we provide a protocol for rational secure function evaluation (Rational SFE) where the players just send a set of envelopes to a referee who simply performs a sequence of publicly verifiable actions. That is, the players can ""subcontract"" all of the computation to an untrusted referee. In addition to providing a communication structure that more closely matches the ideal game, our protocol also enables us to better simulate mediated games in which abort is not a dominated action.",MIT-CSAIL-TR-2005-071; MIT-LCS-TM-653,6 p.; 8137326 bytes; 296971 bytes,application/postscript; application/pdf,en_US,,Cryptography and Information Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Wies, Thomas; Kuncak, Viktor; Lam, Patrick; Podelski, Andreas; Rinard, Martin",2005-12-22T02:40:34Z,2005-12-22T02:40:34Z,2005-11-03,http://hdl.handle.net/1721.1/30582,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Field Constraint Analysis,"We introduce field constraint analysis, a new  technique for verifying data structure invariants.  A  field constraint for a field is a formula specifying a set of objects  to which the field can point.  Field constraints enable  the application of decidable logics to data structures  which were originally beyond the scope of these logics, by verifying the  backbone of the data structure and then verifying  constraints on fields that cross-cut the backbone in  arbitrary ways.  Previously, such cross-cutting fields  could only be verified when they were uniquely determined by  the backbone, which significantly limited the range of  analyzable data structures.  Our field constraint analysis permits \\emph{non-deterministic} field  constraints on cross-cutting fields, which allows to verify  invariants of data structures such as skip lists.  Non-deterministic  field constraints also enable the verification of invariants between  data structures, yielding an expressive generalization of static  type declarations.  The generality of our field constraints requires new  techniques, which are orthogonal to the traditional use of  structure simulation.  We present one such technique and  prove its soundness.  We have implemented this technique  as part of a symbolic shape analysis deployed in  the context of the Hob system for verifying data structure  consistency.  Using this implementation we were able to  verify data structures that were previously beyond the  reach of similar techniques.",MIT-CSAIL-TR-2005-072; MIT-LCS-TR-1010,23 p.; 22680509 bytes; 928319 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Andoni, Alexandr; Indyk, Piotr",2005-12-22T02:40:39Z,2005-12-22T02:40:39Z,2005-11-04,http://hdl.handle.net/1721.1/30583,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New LSH-based Algorithm for Approximate Nearest Neighbor,"We present an algorithm for c-approximate nearest neighbor problem in a d-dimensional Euclidean space,  achieving query time ofO(dn^{1/c^2+o(1)}) and space O(dn + n^{1+1/c^2+o(1)}).",MIT-CSAIL-TR-2005-073; AIM-2005-031,12 p.; 11656417 bytes; 559939 bytes,application/postscript; application/pdf,en_US,AI; locality sensitive hashing; nearest neighbor; high dimensions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Erik Demaine,"Hajiaghayi, MohammadTaghi; Kortsarz, Guy; Salavatipour, Mohammad R.",2006-01-05T20:37:59Z,2006-01-05T20:37:59Z,2005-11-15,http://hdl.handle.net/1721.1/30601,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Approximating Buy-at-Bulk k-Steiner trees,"In the buy-at-bulk $k$-Steiner tree (or rent-or-buy$k$-Steiner tree) problem we are given a graph $G(V,E)$ with a setof terminals $T\subseteq V$ including a particular vertex $s$ calledthe root, and an integer $k\leq |T|$. There are two cost functionson the edges of $G$, a buy cost $b:E\longrightarrow \RR^+$ and a rentcost $r:E\longrightarrow \RR^+$. The goal is to find a subtree $H$ of$G$ rooted at $s$ with at least $k$ terminals so that the cost$\sum_{e\in H} b(e)+\sum_{t\in T-s} dist(t,s)$ is minimize, where$dist(t,s)$ is the distance from $t$ to $s$ in $H$ with respect tothe $r$ cost. Our main result is  an $O(\log^5 n)$-approximation forthe buy-at-bulk $k$-Steiner tree problem.To achieve this we also design an approximation algorithm forbicriteria $k$-Steiner tree. In the bicriteria $k$-Steiner tree problem weare given a graph $G$ with edge costs $b(e)$ and distance costs$r(e)$ over the edges, and an integer $k$. Our goal is to find aminimum cost (under $b$-cost) $k$-Steiner tree such that thediameter under $r$-cost is at most some given bound $D$. An$(\alpha,\beta)$-approximation finds a subgraph of diameter at most$\alpha\cdot {D}$ (with respect to $r$) and cost with respect to$b$ of at most $\beta\cdot opt$ where $opt$ is the minimum cost ofany solution with diameter at most $D$. Marathe et al \cite{ravi}gave an $(O(\log n),O(\log n))$-approximation algorithm for thebicriteria Steiner tree problem. Their algorithm does not extend tothe bicriteria $k$-Steiner tree problem.Our algorithm for the buy-at-bulk $k$-Steiner tree problem relies on an$(O(\log^2 n),O(\log^4 n))$-approximation algorithm we develop for the(shallow-light) bicriteria  $k$-Steiner tree problem, which is ofindependent interest. Indeed, this is also one of the main tools we use to obtainthe first polylogarithmic approximation algorithm for non-uniformmulticommodity buy-at-bulk~\cite{HKS}.",MIT-CSAIL-TR-2006-001,14 p.; 18505768 bytes; 766175 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Monteleoni, Claire; Jaakkola, Tommi",2005-12-22T02:40:44Z,2005-12-22T02:40:44Z,2005-11-17,http://hdl.handle.net/1721.1/30584,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Online Learning of Non-stationary Sequences,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,MIT-CSAIL-TR-2005-074; AIM-2005-032,8 p.; 10189026 bytes; 760649 bytes,application/postscript; application/pdf,en_US,AI; online learning; regret bounds; non-stationarity; HMM; wireless networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dasgupta, Sanjoy; Kalai, Adam Tauman; Monteleoni, Claire",2005-12-22T02:40:49Z,2005-12-22T02:40:49Z,2005-11-17,http://hdl.handle.net/1721.1/30585,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Analysis of Perceptron-Based Active Learning,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",MIT-CSAIL-TR-2005-075; AIM-2005-033,15 p.; 11491832 bytes; 599624 bytes,application/postscript; application/pdf,en_US,AI; active learning; perceptron; label-complexity; mistake bound; selective sampling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Uzuner, Ozlem",2005-12-22T02:41:36Z,2005-12-22T02:41:36Z,2005-11-18,http://hdl.handle.net/1721.1/30587,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Identifying Expression Fingerprints using Linguistic Information,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",MIT-CSAIL-TR-2005-077; AITR-2005-004,216 p.; 179019584 bytes; 5410679 bytes,application/postscript; application/pdf,en_US,AI; natural language processing; syntactic information; content; expression,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Zeng, Gang; Paris, Sylvain; Quan, Long; Sillion, Francois",2005-12-22T02:41:10Z,2005-12-22T02:41:10Z,2005-11-18,http://hdl.handle.net/1721.1/30586,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Accurate and Scalable Surface Representation and Reconstruction from Images,"We introduce a new surface representation, the patchwork, to extend the problem of surface reconstruction from multiple images. A patchwork is the combination of several patches that are built one by one. This design potentially allows the reconstruction of an object of arbitrarily large dimensions while preserving a fine level of detail. We formally demonstrate that this strategy leads to a spatial complexity independent of the dimensions of the reconstructed object, and to a time complexity linear with respect to the object area. The former property ensures that we never run out of storage (memory) and the latter means that reconstructing an object can be done in a reasonable amount of time. In addition, we show that the patchwork representation handles equivalently open and closed surfaces whereas most of the existing approaches are limited to a specific scenario (open or closed surface but not both).Most of the existing optimization techniques can be cast into this framework. To illustrate the possibilities offered by this approach, we propose two applications that expose how it dramatically extends a recent accurate graph-cut technique. We first revisit the popular carving techniques. This results in a well-posed reconstruction problem that still enjoys the tractability of voxel space. We also show how we can advantageously combine several image-driven criteria to achieve a finely detailed geometry by surface propagation. The above properties of the patchwork representation and reconstruction are extensively demonstrated on real image sequences.",MIT-CSAIL-TR-2005-076; MIT-LCS-TR-1011,35 p.; 126753884 bytes; 4493080 bytes,application/postscript; application/pdf,en_US,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Erik Demaine,"Hajiaghayi, MohammadTaghi; Kortsarz, Guy; Salavatipour, Mohammad R.",2006-01-05T20:42:38Z,2006-01-05T20:42:38Z,2005-11-26,http://hdl.handle.net/1721.1/30602,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Polylogarithmic Approximation Algorithm for Non-Uniform Multicommodity Buy-at-Bulk,"We consider the non-uniform multicommodity buy-at-bulknetworkdesign problem. In this problem we are given a graph $G(V,E)$withtwo cost functions on the edges, a buy cost $b:E\longrightarrow \RR^+$and a rent cost$r:E\longrightarrow\RR^+$, and a set of source-sink pairs$s_i,t_i\in V$ ($1\leq i\leq \alpha$)with each pair $i$ having a positivedemand $\delta_i$. Our goal is to designa minimum cost network $G(V,E')$such that for every $1\leq i\leq\alpha$,  $s_i$ and $t_i$ are in thesameconnected component in $G(V,E')$. Thetotal cost of $G(V,E')$ is the sum ofbuy costs of the edges in $E'$plus sum of total demand going through everyedge in $E'$ times therent cost of that edge. Since the costs of differentedges can bedifferent, we say that the problem is non-uniform. Thefirstnon-trivial approximation algorithm for this problem is due toCharikarand Karagiozova (STOC' 05) whose algorithm has anapproximation guarantee of$\exp(O(\sqrt{\log n\log\log n}))$,when all $\delta_i=1$ and$\exp(O(\sqrt{\log N\log\log N}))$ for the generaldemand case where $N$ isthe sum of all demands. We improve upon this result, bypresenting the firstpolylogarithmic (specifically, $O(\log^4 n)$ for unit demandsand $O(\log^4N)$ for the general demands)approximation for this problem. The algorithmrelies on a recent result\cite{HKS1} for the buy-at-bulk $k$-Steiner treeproblem.",MIT-CSAIL-TR-2006-002,16 p.; 19386222 bytes; 818306 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ajmani, Sameer",2005-12-22T02:42:02Z,2005-12-22T02:42:02Z,2005-11-30,http://hdl.handle.net/1721.1/30589,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems,"Upgrading the software of long-lived, highly-available distributed systems is difficult. It is not possible to upgrade all the nodes in a system at once, since some nodes may be unavailable and halting the system for an upgrade is unacceptable. Instead, upgrades may happen gradually, and there may be long periods of time when different nodes are running different software versions and need to communicate using incompatible protocols. We present a methodology and infrastructure that address these challenges and make it possible to upgrade distributed systems automatically while limiting service disruption.Our methodology defines how to enable nodes to interoperate across versions, how to preserve the state of a system across upgrades, and how to schedule an upgrade so as to limit service disruption. The approach is modular: defining an upgrade requires understanding only the new software and the version it replaces.The upgrade infrastructure is a generic platform for distributing and installing software while enabling nodes to interoperate across versions. The infrastructure requires no access to the system source code and is transparent: node software is unaware that different versions even exist. We have implemented a prototype of the infrastructure called Upstart that intercepts socket communication using a dynamically-linked C++ library. Experiments show that Upstart has low overhead and works well for both local-area and Internet systems.",MIT-CSAIL-TR-2005-078; MIT-LCS-TR-1012,164 p.; 135376801 bytes; 5539474 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Taycher, Leonid; Shakhnarovich, Gregory; Demirdjian, David; Darrell, Trevor",2005-12-22T02:41:53Z,2005-12-22T02:41:53Z,2005-12-01,http://hdl.handle.net/1721.1/30588,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Conditional Random People: Tracking Humans with CRFs and Grid Filters,"We describe a state-space tracking approach based on a Conditional Random Field(CRF) model, where the observation potentials are \emph{learned} from data. Wefind functions that embed both state and observation into a space wheresimilarity corresponds to $L_1$ distance, and define an observation potentialbased on distance in this space. This potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. We show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse HMM, resulting in real-time system performance. The resulting system isused for human pose tracking in video sequences.",MIT-CSAIL-TR-2005-079; AIM-2005-034,9 p.; 21558399 bytes; 932744 bytes,application/postscript; application/pdf,en_US,AI; articulated tracking; grid filter; conditional random field,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Ivanov, Yuri; Serre, Thomas; Bouvrie, Jacob",2005-12-22T02:44:17Z,2005-12-22T02:44:17Z,2005-12-14,http://hdl.handle.net/1721.1/30590,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Error weighted classifier combination for multi-modal human identification,In this paper we describe a technique of classifier combination used in a human identification system. The system integrates all available features from multi-modal sources within a Bayesian framework. The framework allows representinga class of popular classifier combination rules and methods within a single formalism. It relies on a Â“per-classÂ” measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. The method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. We show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,MIT-CSAIL-TR-2005-081; AIM-2005-035; CBCL-258,7 p.; 22108540 bytes; 952178 bytes,application/postscript; application/pdf,en_US,AI; classifier combination; face recognition; identification; multi-modal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Serre, T.; Kouh, M.; Cadieu, C.; Knoblich, U.; Kreiman, G.; Poggio, T.",2007-03-12T16:41:47Z,2007-03-12T16:41:47Z,2005-12-19,http://hdl.handle.net/1721.1/36407,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Theory of Object Recognition: Computations and Circuits in the Feedforward Path of the Ventral Stream in Primate Visual Cortex,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",MIT-CSAIL-TR-2005-082; AIM-2005-36; CBCL-259,130 p.,,,AI; object recognition; standard model; theory; visual cortex; ventral stream; hmax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Boris Katz,"Marton, Gregory",2006-01-10T18:47:00Z,2006-01-10T18:47:00Z,2006-01-09,http://hdl.handle.net/1721.1/30604,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgements,"TREC Definition and Relationship questions are evaluated on thebasis of information nuggets that may be contained in systemresponses.  Human evaluators provide informal descriptions of eachnugget, and judgements (assignments of nuggets to responses) for eachresponse submitted by participants.The best present automatic evaluation for these kinds of questions isPourpre.  Pourpre uses a stemmed unigram similarity of responses withnugget descriptions, yielding an aggregate result that is difficult tointerpret, but is useful for relative comparison.  Nuggeteer, bycontrast, uses both the human descriptions and the human judgements,and makes binary decisions about each response, so that the end resultis as interpretable as the official score.I explore n-gram length, use of judgements, stemming, and termweighting, and provide a new algorithm quantitatively comparable to,and qualitatively better than the state of the art.",,15 p.; 236402 bytes,application/pdf,en_US,natural language; question answering,Infolab,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Seth Teller,"Koch, Olivier; Teller, Seth",2006-01-10T18:51:59Z,2006-01-10T18:51:59Z,2006-01-09,http://hdl.handle.net/1721.1/30605,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Wide-Area Egomotion Estimation from Known 3D Structure,"We describe an algorithm that takes as inputs a coarse3D model of an environment, and a video sequence acquiredwithin the environment, and produces as output an estimateof the cameraÂ’s 6-DOF egomotion expressed in the coordinatesof the 3D model. Our method has several novelaspects: it performs line-based structure-from-motion; italigns the local line constellation to the known model; andit uses off-line visibility analysis to dramatically acceleratethe alignment process.We present simulation results demonstrating themethodÂ’s operation in a multi-room environment. We showthat the method can estimate metric egomotion accuratelyand could be used for for many minutes of operation andthousands of video frames.",MIT-CSAIL-TR-2006-003,8 p.; 20452907 bytes; 718869 bytes,application/postscript; application/pdf,en_US,"computer vision,visual geometry,omnivision,3D","Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
David Clark,"Masiello, Elizabeth",2006-01-12T16:07:26Z,2006-01-12T16:07:26Z,2006-01-11,http://hdl.handle.net/1721.1/30606,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Service Identification in TCP/IP: Well-Known versus Random Port Numbers,"The sixteen-bit well-known port number is often overlooked as a network identifier in Internet communications. Its purpose at the most fundamental level is only to demultiplex flows of traffic. Several unintended uses of the port number evolved from associating services with a list of well-known port numbers. This thesis documents those unintended consequences in an effort to describe the port number's influence on Internet players from ISPs to application developers to individual users. Proposals and examples of moving away from well-known port numbers to randomly assigned ones are then presented, with analysis of impacts on the political and economic systems on which Internet communication is dependent.",MIT-CSAIL-TR-2006-004,52 p.; 66099985 bytes; 3014151 bytes,application/postscript; application/pdf,en_US,,Advanced Network Architecture,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,
Eric Grimson,"Zollei, Lilla",2006-01-25T21:04:03Z,2006-01-25T21:04:03Z,2006-01-25,http://hdl.handle.net/1721.1/30970,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Unified Information Theoretic Framework for Pair- and Group-wise Registration of Medical Images,"The field of medical image analysis has been rapidly growing for the past two decades. Besides a significant growth in computational power, scanner performance, and storage facilities, this acceleration is partially due to an unprecedented increase in the amount of data sets accessible for researchers. Medical experts traditionally rely on manual comparisons of images, but the abundance of information now available makes this task increasingly difficult. Such a challenge prompts for more automation in processing the images.In order to carry out any sort of comparison among multiple medical images, onefrequently needs to identify the proper correspondence between them. This step allows us to follow the changes that happen to anatomy throughout a time interval, to identify differences between individuals, or to acquire complementary information from different data modalities. Registration achieves such a correspondence. In this dissertation we focus on the unified analysis and characterization of statistical registration approaches.We formulate and interpret a select group of pair-wise registration methods in the context of a unified statistical and information theoretic framework. This clarifies the implicit assumptions of each method and yields a better understanding of their relative strengths and weaknesses. This guides us to a new registration algorithm that incorporates the advantages of the previously described methods. Next we extend the unified formulation with analysis of the group-wise registration algorithms that align a population as opposed to pairs of data sets. Finally, we present our group-wise registration framework, stochastic congealing. The algorithm runs in a simultaneous fashion, with every member of the population approaching the central tendency of the collection at the same time. It eliminates the need for selecting a particular referenceframe a priori, resulting in a non-biased estimate of a digital template. Our algorithm adopts an information theoretic objective function which is optimized via a gradientbased stochastic approximation process embedded in a multi-resolution setting. We demonstrate the accuracy and performance characteristics of stochastic congealing via experiments on both synthetic and real images.",MIT-CSAIL-TR-2006-005,152 p.; 254354390 bytes; 7278255 bytes,application/postscript; application/pdf,en_US,"population alignment, spatial normalization, congealing",Vision,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Kushman, Nate; Katabi, Dina; Wroclawski, John",2006-01-27T18:01:59Z,2006-01-27T18:01:59Z,2006-01-27,http://hdl.handle.net/1721.1/30971,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Consistency Management Layer for Inter-Domain Routing,"This paper proposes an isolation layer -- a shim -- betweeninter-domain routing and packet forwarding. The job of this layer isto coordinate between Autonomous Systems (AS's) on when and how tomodify the forwarding state to ensure inter-domain routing loops donot cause forwarding loops. The benefits of a consistency layer aretwofold.  First, it prevents the creation of transient inter-domainforwarding loops and the resulting packet loss, high latency, andconnection failures.Second, by taking the burden of forwarding consistency off theinter-domain routing protocol, it enables inter-domain routingprotocols with more complex convergence characteristics than BGP, suchas protocols that optimize route selection based on performance.  Weoffer two possible designs for the consistency layer. We prove thatboth designs are free of forwarding loops and show they are easy todeploy in the current Internet.",MIT-CSAIL-TR-2006-006,13 p.; 20452936 bytes; 768834 bytes,application/postscript; application/pdf,en_US,BGP; inter-domain routing; internet; routing; routing loops; forwarding loops,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Daniel Weitzner,"Weitzner, Daniel J.; Abelson, Harold; Berners-Lee, Tim; Hanson, Chris; Hendler, James; Kagal, Lalana; McGuinness, Deborah L.; Sussman, Gerald Jay; Waterman, K. Krasnow",2006-01-27T19:27:14Z,2006-01-27T19:27:14Z,2006-01-27,http://hdl.handle.net/1721.1/30972,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Transparent Accountable Data Mining: New Strategies for Privacy Protection,"Attempts to address issues of personal privacy in a world of computerized databases and information networks -- from security technology to data protection regulation to Fourth Amendment law jurisprudence -- typically proceed from the perspective of controlling or preventing access to information.  We argue that this perspective has become inadequate and obsolete, overtaken by the ease of sharing and copying data and of aggregating and searching across multiple data bases, to reveal private information from public sources.  To replace this obsolete framework, we propose that issues of privacy protection currently viewed in terms of data access be re-conceptualized in terms of data use.  From a technology perspective, this requires supplementing legal and technical mechanisms for access control with new mechanisms for transparency and accountability of data use.  In this paper, we present a technology infrastructure -- the Policy Aware Web -- that supports transparent and accountable data use on the World Wide Web, and elements of a new legal and regulatory regime that supports privacy through provable accountability to usage rules rather than merely data access restrictions.",MIT-CSAIL-TR-2006-007,10 p.; 54814550 bytes; 5102588 bytes,application/postscript; application/pdf,en_US,,Decentralized Information Group,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eric Grimson,"Wang, Xiaogang; Tieu, Kinh; Grimson, Eric",2006-02-10T22:46:57Z,2006-02-10T22:46:57Z,2006-02-10,http://hdl.handle.net/1721.1/31208,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning Semantic Scene Models by Trajectory Analysis,"In this paper, we describe an unsupervised learning framework to segment a scene into semantic regions and to build semantic scene models from long-term observations of moving objects in the scene. First, we introduce two novel similarity measures for comparing trajectories in far-field visual surveillance. The measures simultaneously compare the spatial distribution of trajectories and other attributes, such as velocity and object size, along the trajectories. They also pro-vide a comparison confidence measure which indicates how well the measured im-age-based similarity approximates true physical similarity.  We also introduce novel clustering algorithms which use both similarity and comparison confidence. Based on the proposed similarity measures and clustering methods, a framework to learn semantic scene models by trajectory analysis is developed. Trajectories are first clustered into vehicles and pedestrians, and then further grouped based on spatial and velocity distributions. Different trajectory clusters represent different activities. The geometric and statistical models of structures in the scene, such as roads, walk paths, sources and sinks, are automatically learned from the trajectory clusters. Abnormal activities are detected using the semantic scene models. The system is robust to low-level tracking errors.",MIT-CSAIL-TR-2006-008,16 p.; 18317255 bytes; 2106560 bytes,application/postscript; application/pdf,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Katti, Sachin; Rahul, Hariharan; Hu, Wenjun; Katabi, Dina; Crowcroft, Jon",2006-02-21T12:00:00Z,2006-02-21T12:00:00Z,2006-02-16,http://hdl.handle.net/1721.1/31212,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Network Coding Made Practical,"We propose a new architecture for wireless mesh networks. In addition to forwarding packets, routers mix (i.e., code) packets from different sources to increase the information content of each transmission. We show that intelligently mixing packets increases network throughput. Our design is rooted in the theory of network coding. In contrast to prior work on network coding, which is mainly theoretical and focuses on multicast traffic, ours is practical and solves the common case of unicast traffic.  We present the first implementation of network coding in a wireless network. Our system introduces a coding layer between the IP and MAC layers. It works with UDP and TCP traffic, and hence seamlessly integrates with existing applications. We evaluate our design on a 34-node wireless testbed and show that it delivers a 3-4x increase in the throughput ofwireless mesh networks.",MIT-CSAIL-TR-2006-009,14 p; 24604312 bytes; 1314870 bytes,application/postscript; application/pdf,en_US,"Network Coding, wireless networks, forwarding architectures",Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Artzi, Shay; Kiezun, Adam; Newport, Calvin; Schultz, David",2006-02-23T20:36:51Z,2006-02-23T20:36:51Z,2006-02-23,http://hdl.handle.net/1721.1/31216,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Encrypted Keyword Search in a Distributed Storage System,"Encrypted keyword search allows a server to perform a search over a set of encrypted documents on behalf of a client without learning the contents of the documents or the words being searched for. Designing a practical system is challenging because the privacy constraint thwarts standard indexing and ranking techniques. We present Mafdet, an encrypted keyword search system we have implemented. Our system makes the search practical even for large data sets. We evaluated Mafdet's performance on a set of queries and a large collection of documents. In these queries, Mafdet's accuracy is within 6% of Google Desktop, and the search time is on the order of seconds for document sets as large as 2.6 GB.",MIT-CSAIL-TR-2006-010,11 p.; 16238506 bytes; 625872 bytes,application/postscript; application/pdf,en_US,Bloomfilters,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Blackmore, Lars; Block, Steve",2006-02-28T19:47:07Z,2006-02-28T19:47:07Z,2006-02-28,http://hdl.handle.net/1721.1/31217,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Control and Estimation for Cooperative Manipulator Tasks,"The objective of this project is to achieve reliable transfer of an object from one robotic manipulator to another. This capability is useful for a number of applications, for instance robotic assembly, or robots with multiple manipulators, such as humanoid robots.Achieving reliable object transfer poses a number of challenges for both control and estimation. As with most manipulation problems, the inverse kinematics problem must be solved so that the desired endpoint location can be specified in Cartesian coordinates, rather than in the joint space of the manipulator. Anadditional challenge particular to the cooperative robotics problem is that more than one manipulator may have a grasp on the same object. Manipulators that are carrying out simple position control may encounter problems when grasping the same object. Minor errors in forward kinematics can lead to large controllerforces, or even unstable dynamics, as each controller tries to counteract the other to drive the perceived error to zero.On the estimation side, carrying out reliable transfer depends critically on determining the grasp state; in other words, does a particular robot have a grasp on the object, or do both have the object? The grasp state must be determined before the sequence of events in a transfer task can proceed. For example, the manipulator receiving the object cannot move away until it is certain that the manipulator passing the object has released. In many instances, having pressure sensors mounted in the hand is infeasible. For example, packaging reasons can mean that the necessary space is not available, as is the case with the JPL LEMUR hexapod. We therefore need to infer the grasp state from the available observations, which are usually supplied by position encoders at the joints.For this project we assume that each manipulator carries out estimation independently, without joint angle observations from the other robot, but with knowledge of its own joint angles and of the commands to be issued to both robots. This is typical of a multi-agent cooperative task, and the lack of observations makes the estimation task even more challenging.This report describes the approach we use to solve this problem, which is comprised of an impedance controller and a hybrid estimator.",MIT-CSAIL-TR-2006-011,19 p.; 21923202 bytes; 1004827 bytes,application/postscript; application/pdf,en_US,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Blackmore, Lars; Williams, Brian",2006-02-28T22:36:51Z,2006-02-28T22:36:51Z,2006-02-28,http://hdl.handle.net/1721.1/31219,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Finite Horizon Control Design for Optimal Discrimination between Several Models,"Multiple-Model fault detection is a powerful method for detecting changes, such as faults, in dynamic systems. In many cases, the ability of such a detection scheme to distinguish between possible models for the system dynamics depends critically on the control inputs applied to the system. Prior work has therefore aimed to design control inputs in order to improve fault detection. We previously developed a new method that uses constrained finite horizon control design to create control inputs that minimize an upper bound on the probability of model selection error. This method is limited, however, to the problem of selection between two models. In this paper we describe a new method that extends this approach to handle an arbitrary number of models. By optimizing subject to hard constraints, the new method can ensure that a defined task is fulfilled, while optimally discriminating between models. This means that the discrimination power of the designed control input can be much greater than that created by other approaches, which typically design Â‘auxiliaryÂ’ signals with limited power so that the effect on the system state is small. Furthermore, the optimization criterion, which is an upper bound on the probability of model selection error, has a more meaningful interpretation than alternative approaches that are based on information gain, for example.We demonstrate the method using an aircraft fault detectionscenario and show that the new method significantly reducesthe bound on the probability of error when compared to amanually generated identification sequence and a fuel-optimalsequence.",MIT-CSAIL-TR-2006-013,6 p.; 11551717 bytes; 591011 bytes,application/postscript; application/pdf,en_US,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jovan Popovic,"Abe, Yeuhi; Popovic, Jovan",2006-02-28T20:43:06Z,2006-02-28T20:43:06Z,2006-02-28,http://hdl.handle.net/1721.1/31218,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Interactive Animation of Dynamic Manipulation,"Lifelike animation of manipulation must account for the dynamicinteraction between animated characters, objects, and their environment. Failing to do so would ignore the often significant effects objectshave on the motion of the character. For example, lifting a heavy objectwould appear identical to lifting a light one. Physical simulationhandles such interaction correctly, with a principled approach thatadapts easily to different circumstances, changing environments, andunexpected disturbances. Our work shows how to control lifelike animatedcharacters so that they accomplish manipulation tasks within aninteractive physical simulation. Our new multi-task control algorithmsimplifies descriptions of manipulation by supporting prioritized goalsin both the joint space of the character and the task-space of theobject. The end result is a versatile algorithm that incorporatesrealistic force limits and recorded motion postures to portray lifelikemanipulation automatically.",MIT-CSAIL-TR-2006-012,9  p.; 20884299 bytes; 844533 bytes,application/postscript; application/pdf,en_US,"Motion Control,Physically Based Animation,Robotics",Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob; Bachrach, Jonathan",2006-06-01T16:24:45Z,2006-06-01T16:24:45Z,2006-03,http://hdl.handle.net/1721.1/32988,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Infrastructure for Engineered Emergence on Sensor/Actuator Networks,"The ability to control emergent phenomena depends on decomposingthem into aspects susceptible to independent engineering. Forspatial self-managing systems, the amorphous-medium abstraction lets youseparate the systemÂ’s specification from its implementation.",MIT-CSAIL-TR-2006-042,10 p.; 10529783 bytes; 1132453 bytes,application/postscript; application/pdf,en_US,amorphous computing distributed sensor networks space-time programming,Mathematics and Computation,,,,,"IEEE Intelligent Systems, (Vol. 21, No. 2) pp. 10-19, March/April 2006.",,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob; Sussman, Gerald",2006-06-01T16:23:27Z,2006-06-01T16:23:27Z,2006-03,http://hdl.handle.net/1721.1/32987,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"CogSci to AI: It's the Brainware, Stupid!","Current modularization techniques fail when applied to hard AI problems.But cognitive science shows that the mind has modules specialized for particular functions.Unlike current engineered modules, the modules of themind learn to communicate with each other as a child matures.Kirby's ideas on language evolution, combined with constraints derivedfrom neuroanatomy, yield a new mechanism for integrating modules intoa system: a communications bootstrapping system in which two agentsbuild a shared vocabulary capturing information common to their mutualexperience, including cross-module knowledge about the world.",MIT-CSAIL-TR-2006-041,2 p.; 4176387 bytes; 211150 bytes,application/postscript; application/pdf,en_US,artificial intelligence,Mathematics and Computation,,,,,"AAAI 2006 Spring Symposium ""Between a Rock and a Hard Place: Cognitive Science Principles Meet AI-Hard Problems"", Stanford, March 2006.",,,,,,,,,,,,,,,,,,,,,,,
Rodney Brooks,"Torres-Jara, Eduardo; Vasilescu, Iuliu; Coral, Raul",2006-03-01T17:56:56Z,2006-03-01T17:56:56Z,2006-03-01,http://hdl.handle.net/1721.1/31220,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A soft touch: Compliant Tactile Sensors for Sensitive Manipulation,"We present the design, analysis and construction of a biologicallyinspired tactile sensor. The sensor can measure normal and lateralforces, conform to the surfaces with which it comes in contact andincrease the friction of the surface for a good grasp.The sensor is built using a simple process and the applied forcesare read using standard electronics. These features make thesensors ideal for mass production.We are motivated to build tactile sensors that are useful forrobotic manipulation given that the current ones do not have thefeatures that we consider necessary. The sensors presented in thispaper have been designed to deal with these issues. They have beendesigned and implemented in the fingers of the humanoid robotObrero.",MIT-CSAIL-TR-2006-014,6 p.; 17555094 bytes; 2414073 bytes,application/postscript; application/pdf,en_US,Humanoid Robotics Manipulation. Deformable Tactile Sensor.,Humanoid Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Look, Gary; Peters, Stephen; Shrobe, Howard",2006-03-02T14:01:55Z,2006-03-02T14:01:55Z,2006-03-01,http://hdl.handle.net/1721.1/31222,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Plan-Driven Pervasive Computing,"The goal of human-centered, pervasive computing should be to hide the details of the computing environment, allowing users to concentrate on their goals, rather than on the direct management of devices. This paper describes a system that operates at the level of goals and plans, rather than individual resources. It adaptively selects from its plan library that plan which is likely to best achieve the userÂ’s goal in view of his preferences and current resource availability. Once the plan and resources are selected, it monitors the execution of the plan, dispatching subtasks when they are ready to be executed.",MIT-CSAIL-TR-2006-016,14p.; 18134831 bytes; 577156 bytes,application/postscript; application/pdf,en_US,,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Peters, Stephen; Look, Gary; Quigley, Kevin; Shrobe, Howard; Gajos, Krzysztof",2006-03-02T14:01:58Z,2006-03-02T14:01:58Z,2006-03-01,http://hdl.handle.net/1721.1/31223,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Hyperglue: Designing High-Level Agent Communication for Distributed Applications,"We are building a new communication model and discoverysystem which will allow agent-based intelligent spacesto interact with one another. This new infrastructure layer,called Hyperglue, coordinates agent actions at a higher levelthan most agent communication does, providing an interfacefor communication at the level of ""real-world"" entities suchas people, places, organizations, and information sources.The resulting structure is one which allows these agent communitiesto interact, while preserving the privacy, privileges,and preferences of the entities they represent. In this paperwe describe the rationale for Hyperglue, and present theinitial design as an extension of the existing Metaglue agentframework developed at the MIT AI Lab.",MIT-CSAIL-TR-2006-017,6 p.; 12119609 bytes; 486781 bytes,application/postscript; application/pdf,en_US,,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tommi Jaakkola,"Perez-Breva, Luis; Ortiz, Luis E.; Yeang, Chen-Hsiang, 1969-; Jaakkola, Tommi",2006-03-10T01:22:50Z,2006-03-10T01:22:50Z,2006-03-06,http://hdl.handle.net/1721.1/31311,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,DNA Binding and Games,"We propose a game-theoretic approach tolearn and predict coordinate binding of multiple DNA bindingregulators. The framework implements resource constrainedallocation of proteins to local neighborhoods as well as to sitesthemselves, and explicates coordinate and competitive bindingrelations among proteins with affinity to the site or region. The focus of this paper is on mathematical foundationsof the new modeling approach. We demonstrate the approachin the context of the lambda-phage switch, a well-known biologicalsubsystem, and provide simulation results that successfully illustrate the predictions that can be derived from the modelwith known structure and affinities. Subsequentwork will elaborate on methods for learning the affinities and gamestructures from available binding data.",MIT-CSAIL-TR-2006-018,11 p.; 15597128 bytes; 598367 bytes,application/postscript; application/pdf,en_US,,Tommi's Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2006-03-10T01:22:47Z,2006-03-10T01:22:47Z,2006-03-08,http://hdl.handle.net/1721.1/31310,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Task-Structured Probabilistic I/O Automata to Analyze an Oblivious Transfer Protocol,"AbstractThe Probabilistic I/O Automata framework of Lynch, Segala and Vaandrager provides tools for precisely specifying protocols and reasoning about their correctness using multiple levels of abstraction, based on implementation relationships between these levels. We enhance this framework to allow analyzing protocols that use cryptographic primitives. This requires resolving and reconciling issues such as nondeterministic behavior and scheduling, randomness, resource-bounded computation, and computational hardness assumptions. The enhanced framework allows for more rigorous and systematic analysis of cryptographic protocols. To demonstrate the use of this framework, wepresent an example analysis that we have done for an Oblivious Transfer protocol.",MIT-CSAIL-TR-2006-019,98 p.; 104102681 bytes; 4467324 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,http://hdl.handle.net/1721.1/33217,http://hdl.handle.net/1721.1/33217,,,,,,,,,,,,,,,,,,
Trevor Darrell,"Grauman, Kristen; Darrell, Trevor",2006-03-20T19:24:36Z,2006-03-20T19:24:36Z,2006-03-18,http://hdl.handle.net/1721.1/31338,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pyramid Match Kernels: Discriminative Classification with Sets of Image Features (version 2),"Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering.  Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences -- generally a computationally expensive task that becomes impractical for largeset sizes.  We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space.  This ``pyramid match"" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kerneldoes not penalize the presence of extra features, it is robust to clutter.  We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels.  We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches.  (This tech report updates MIT-CSAIL-TR-2005-017 and the paper ""The Pyramid Match Kernel: Discriminative Classification with Sets of Images Features"" which appeared in the proceedings of ICCV 2005.)",MIT-CSAIL-TR-2006-020,10 p.; 7963746 bytes; 1702359 bytes,application/postscript; application/pdf,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Ortiz, Luis E.; Schapire, Robert E.; Kakade, Sham M.",2006-03-20T19:24:59Z,2006-03-20T19:24:59Z,2006-03-20,http://hdl.handle.net/1721.1/31339,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Maximum Entropy Correlated Equilibria,"We study maximum entropy correlated equilibria in (multi-player)games and provide two gradient-based algorithms that are guaranteedto converge to such equilibria. Although we do not provideconvergence rates for these algorithms, they do have strong connectionsto other algorithms (such as iterative scaling) which are effectiveheuristics for tasks such as statistical estimation.",MIT-CSAIL-TR-2006-021,15 p.; 301190 bytes; 876790 bytes,application/pdf; application/postscript,en_US,"Game Theory, Graphical Games, Graphical Models, Normal-form Games, Optimization",Learning and Intelligent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Gardiol, Natalia H.; Kaelbling, Leslie Pack",2006-03-20T19:23:27Z,2006-03-20T19:23:27Z,2006-03-20,http://hdl.handle.net/1721.1/31337,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Computing action equivalences for planning under time-constraints,"In order for autonomous artificial decision-makers to solverealistic tasks, they need to deal with the dual problems of searching throughlarge state and action spaces under time pressure.We study the problem of planning in domains with lots of objects.  Structuredrepresentations of action can help provide guidance when the number of actionchoices and size of the state space is large.We show how structured representations ofaction effects can help us partition the action space in to a smallerset of approximate equivalence classes. Then, the pared-downaction space can be used to identify a useful subset of the state space in whichto search for a solution.  As computational resources permit, we thenallow ourselves to elaborate the original solution. This kind of analysisallows us to collapse the action space and permits faster planning in muchlarger domains than before.",MIT-CSAIL-TR-2006-022,27 p.; 418358 bytes; 3755323 bytes,application/pdf; application/postscript,en_US,,Learning and Intelligent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2006-03-31T17:51:50Z,2006-03-31T17:51:50Z,2006-03-31,http://hdl.handle.net/1721.1/32525,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Task-Structured Probabilistic I/O Automata,"In the Probabilistic I/O Automata (PIOA) framework, nondeterministicchoices are resolved using perfect-information schedulers,which are similar to history-dependent policies for Markov decision processes(MDPs). These schedulers are too powerful in the setting of securityanalysis, leading to unrealistic adversarial behaviors. Therefore, weintroduce in this paper a novel mechanism of task partitions for PIOAs.This allows us to define partial-information adversaries in a systematicmanner, namely, via sequences of tasks.The resulting task-PIOA framework comes with simple notions of externalbehavior and implementation, and supports simple compositionalityresults. A new type of simulation relation is defined and proven soundwith respect to our notion of implementation. To illustrate the potentialof this framework, we summarize our verification of an ObliviousTransfer protocol, where we combine formal and computational analyses.Finally, we present an extension with extra expressive power, usinglocal schedulers of individual components.",MIT-CSAIL-TR-2006-023,45 p.; 2404679 bytes; 487620 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,http://hdl.handle.net/1721.1/33964,http://hdl.handle.net/1721.1/33964,,,,,,,,,,,,,,,,,,
Hari Balakrishnan,"Vutukuru, Mythili; Feamster, Nick; Walfish, Michael; Balakrishnan, Hari; Shenker, Scott",2006-04-15T02:06:49Z,2006-04-15T02:06:49Z,2006-04-14,http://hdl.handle.net/1721.1/32532,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Revisiting Internet Adressing: Back to the Future!,"IP prefixes undermine three goals of Internet routing: accurate reflection of network-layer reachability, secure routing messages, and effective traffic control. This paper presents Atomic IP (AIP), a simple change to Internet addressing (which in fact reverts to how addressing once worked), that allows Internet routing to achieve these goals.",MIT-CSAIL-TR-2006-025,8 p.; 226111 bytes; 705478 bytes,application/pdf; application/postscript,en_US,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Gilbert, Seth; Guerraoui, Rachid; Newport, Calvin",2006-04-19T17:01:59Z,2006-04-19T17:01:59Z,2006-04-19,http://hdl.handle.net/1721.1/32534,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Of Malicious Motes and Suspicious Sensors,"How much damage can a malicious tiny device cause in a single-hopwireless network?  Imagine two players, Alice and Bob, who want toexchange information.  Collin, a malicious adversary, wants to preventthem from communicating.  By broadcasting at the same time as Alice orBob, Collin can destroy their messages or overwhelm them with his ownmalicious data.  Being a tiny device, however, Collin can onlybroadcast up to B times. Given that Alice and Bob do not knowB, and cannot distinguish honest from malicious messages, howlong can Collin prevent them from communicating?  We show the answerto be 2B + Theta(lg|V|) communication rounds, where V is theset of values that Alice and Bob may transmit.  We prove this resultto be optimal by deriving an algorithm that matches our lowerbound---even in the stronger case where Alice and Bob do not start thegame at the same time.We then argue that this specific 3-player game captures the generalextent to which a malicious adversary can disrupt coordination in asingle-hop wireless network. We support this claim by deriving---via reduction from the 3-player game---round complexity lower boundsfor several classical n-player problems: 2B + Theta(lg|V|) for reliable broadcast,2B + Omega(lg(n/k)) for leader election among k contenders,and 2B + Omega(k*lg(|V|/k)) for static k-selection.  We then consider an extension of our adversary model that also includes up to t crash failures. We study binary consensus as the archetypal problem for this environment and show a bound of 2B + Theta(t) rounds. We conclude by providing tight, or nearly tight, upper bounds for all four problems.  The new upper and lower bounds in this paper represent the first such results for a wireless network in which the adversary has the ability to disrupt communication.",MIT-CSAIL-TR-2006-026,20 p.; 22447191 bytes; 841434 bytes,application/postscript; application/pdf,en_US,wireless ad hoc networks; byzantine; reliable broadcast; leader election; k-selection; consensus; collision detection,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Serre, Thomas",2006-05-10T16:52:26Z,2006-05-10T16:52:26Z,2006-04-25,http://hdl.handle.net/1721.1/32544,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Learning a Dictionary of Shape-Components in Visual Cortex: Comparison with Neurons, Humans and Machines","In this thesis, I describe a quantitative model that accounts for the circuits and computations of the feedforward path of the ventral stream of visual cortex.  This model is consistent with a general theory of visual processing that extends the hierarchical model of (Hubel & Wiesel, 1959) from primary to extrastriate visual areas. It attempts to explain the first few hundred milliseconds of visual processing and Â“immediate recognitionÂ”. One of the key elements in the approach is the learning of a generic dictionary of shape-components from V2 to IT, which provides an invariant representation to task-specific categorization circuits in higher brain areas. This vocabulary of shape-tuned units is learned in an unsupervised manner from natural images, and constitutes a large and redundant set of image features with different complexities and invariances.  This theory significantly extends an earlier approach by (Riesenhuber & Poggio, 1999) and builds upon several existing neurobiological models and conceptual proposals.First, I present evidence to show that the model can duplicate the tuning properties of neurons in various brain areas (e.g., V1, V4 and IT). In particular, the model agrees with data from V4 about the response of neurons to combinations of simple two-bar stimuli (Reynolds et al, 1999) (within the receptive field of the S2 units) and some of the C2 units in the model show a tuning for boundary conformations which is consistent with recordings from V4 (Pasupathy & Connor, 2001). Second, I show that not only can the model duplicate the tuning properties of neurons in various brain areas when probed with artificial stimuli, but it can also handle the recognition of objects in the real-world, to the extent of competing with the best computer vision systems. Third, I describe a comparison between the performance of the model and the performance of human observers in a rapid animal vs. non-animal recognition task for which recognition is fast and cortical back-projections are likely to be inactive.  Results indicate that the model predicts human performance extremely well when the delay between the stimulus and the mask is about 50 ms.  This suggests that cortical back-projections may not play a significant role when the time interval is in this range, and the model may therefore provide a satisfactory description of the feedforward path.Taken together, the evidences suggest that we may have the skeleton of a successful theory of visual cortex.  In addition, this may be the first time that a neurobiological model, faithful to the physiology and the anatomy of visual cortex, not only competes with some of the best computer vision systems thus providing a realistic alternative to engineered artificial vision systems, but also achieves performance close to that of humans in a categorization task involving complex natural images.",MIT-CSAIL-TR-2006-028; CBCL-260,211 p.; 75497472 bytes; 6889446 bytes,application/postscript; application/pdf,en_US,"object recognition, model, machine vision, neuroscience, visual system, feedforward, biologically inspired, hierarchy",Vision,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Eric Grimson,"Bose, Biswajit; Wang, Xiaogang; Grimson, Eric",2006-04-25T13:33:26Z,2006-04-25T13:33:26Z,2006-04-25,http://hdl.handle.net/1721.1/32536,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Detecting and tracking multiple interacting objects without class-specific models,"We propose a framework for detecting and tracking multiple interacting objects from a single, static, uncalibrated camera. The number of objects is variable and unknown, and object-class-specific models are not available. We use background subtraction results as measurements for object detection and tracking. Given these constraints, the main challenge is to associate pixel measurements with (possibly interacting) object targets. We first track clusters of pixels, and note when they merge or split. We then build an inference graph, representing relations between the tracked clusters. Using this graph and a generic object model based on spatial connectedness and coherent motion, we label the tracked clusters as whole objects, fragments of objects or groups of interacting objects. The outputs of our algorithm are entire tracks of objects, which may include corresponding tracks from groups of objects during interactions. Experimental results on multiple video sequences are shown.",MIT-CSAIL-TR-2006-027,19 p.; 80453528 bytes; 1840042 bytes,application/postscript; application/pdf,en_US,visual surveillance; multi-target tracking; background subtraction; group tracking,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Leaute, Thomas",2006-04-28T18:22:21Z,2006-04-28T18:22:21Z,2006-04-28,http://hdl.handle.net/1721.1/32537,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Coordinating Agile Systems through the Model-based Execution of Temporal Plans,"Agile autonomous systems are emerging, such as unmanned aerial vehicles (UAVs), that must robustly perform tightly coordinated time-critical missions; for example, military surveillance or search-and-rescue scenarios. In the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness, in the context of space probes and planetary rovers, modeled as discrete systems. We address the challenge of extending plan execution to systems with continuous dynamics, such as air vehicles and robot manipulators, and that are controlled indirectly through the setting of continuous state variables.Systems with continuous dynamics are more challenging than discrete systems, because they require continuous, low-level control, and cannot be controlled by issuing simple sequences of discrete commands. Hence, manually controlling these systems (or plants) at a low level can become very costly, in terms of the number of human operators necessary to operate the plant. For example, in the case of a fleet of UAVs performing a search-and-rescue scenario, the traditional approach to controlling the UAVs involves providing series of close waypoints for each aircraft, which incurs a high workload for the human operators, when the fleet consists of a large number of vehicles.Our solution is a novel, model-based executive, called Sulu, that takes as input a qualitative state plan, specifying the desired evolution of the state of the system. This approach elevates the interaction between the human operator and the plant, to a more abstract level where the operator is able to Â“coachÂ” the plant by qualitatively specifying the tasks, or activities, the plant must perform. These activities are described in a qualitative manner, because they specify regions in the plantÂ’s state space in which the plant must be at a certain point in time. Time constraints are also described qualitatively, in the form of flexible temporal constraints between activities in the state plan. The design of low-level control inputs in order to meet this abstract goal specification is then delegated to the autonomous controller, hence decreasing the workload per human operator. This approach also provides robustness to the executive, by giving it room to adapt to disturbances and unforeseen events, while satisfying the qualitative constraints on the plant state, specified in the qualitative state plan.Sulu reasons on a model of the plant in order to dynamically generate near-optimal control sequences to fulfill the qualitative state plan. To achieve optimality and safety, Sulu plans into the future, framing the problem as a disjunctive linear programming problem. To achieve robustness to disturbances and maintain tractability, planning is folded within a receding horizon, continuous planning and execution framework. The key to performance is a problem reduction method based on constraint pruning. We benchmark performance using multi-UAV firefighting scenarios on a real-time, hardware-in-the-loop testbed.",MIT-CSAIL-TR-2006-029,155 p.; 41524644 bytes; 17696521 bytes,application/postscript; application/pdf,en_US,Model-based Programming; Qualitative Reasoning; Temporal Reasoning; Continuous Scheduling; Path Planning; Model Predictive Control,Model-based Embedded and Robotic Systems,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Blackmore, Lars; Funiak, Stanislav; Williams, Brian",2006-04-28T18:22:31Z,2006-04-28T18:22:31Z,2006-04-28,http://hdl.handle.net/1721.1/32539,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Combined Stochastic and Greedy Hybrid Estimation Capability for Concurrent Hybrid Models with Autonomous Mode Transitions,"Robotic and embedded systems have become increasingly pervasive in applicationsranging from space probes and life support systems to robot assistants. In order to act robustly in the physical world, robotic systems must be able to detect changes in operational mode, such as faults, whose symptoms manifest themselves only in the continuous state. In such systems, the state is observed indirectly, and must therefore be estimated in a robust, memory-efficient manner from noisy observations.Probabilistic hybrid discrete/continuous models, such as Concurrent Probabilistic Hybrid Automata (CPHA) are convenient modeling tools for such systems. In CPHA, the hidden state is represented with discrete and continuous state variables that evolve probabilistically. In this paper, we present a novel method for estimating the hybrid state of CPHA that achieves robustness by balancing greedy and stochastic search. The key insight is that stochastic and greedy search methods, taken together, are often particularly effective in practice.To accomplish this, we first develop an efficient stochastic sampling approach for CPHA based on Rao-Blackwellised Particle Filtering. We then propose a strategy for mixing stochastic and greedy search. The resulting method is able to handle three particularly challenging aspects of real-world systems, namely that they 1) exhibit autonomous mode transitions, 2) consist of a large collection of concurrently operating components, and 3) are non-linear. Autonomous mode transitions, that is, discrete transitions that depend on thecontinuous state, are particularly challenging to address, since they couple the discrete and continuous state evolution tightly. In this paper we extend the class of autonomous mode transitions that can be handled to arbitrary piecewise polynomial transition distributions.We perform an empirical comparison of the greedy and stochastic approaches to hybrid estimation, and then demonstrate the robustness of the mixed method incorporated with our HME (Hybrid Mode Estimation) capability. We show that this robustness comes at only a small performance penalty.",MIT-CSAIL-TR-2006-032,50 p.; 5299899 bytes; 781961 bytes,application/postscript; application/pdf,en_US,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Hofmann, Andreas",2006-06-30T14:42:28Z,2006-06-30T14:42:28Z,2006-04-28,http://hdl.handle.net/1721.1/33229,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Robust Execution of Bipedal Walking Tasks From Biomechanical Principles,"Effective use of robots in unstructured environments requires that they have sufficient autonomy and agility to execute task-level commands successfully.  A challenging example of such a robot is a bipedal walking machine.  Such a robot should be able to walk to a particular location within a particular time, while observing foot placement constraints, and avoiding a fall, if this is physically possible.  Although stable walking machines have been built, the problem of task-level control, where the tasks have stringent state-space and temporal requirements, and where significant disturbances may occur, has not been studied extensively.  This thesis addresses this problem through three objectives.  The first is to devise a plan specification where task requirements are expressed in a qualitative form that provides for execution flexibility.  The second is to develop a task-level executive that accepts such a plan, and outputs a sequence of control actions that result in successful plan execution.  The third is to provide this executive with disturbance handling ability.Development of such an executive is challenging because the biped is highly nonlinear and has limited actuation due to its limited base of support.  We address these challenges with three key innovations.  To address the nonlinearity, we develop a dynamic virtual model controller to linearize the biped, and thus, provide an abstracted biped that is easier to control.  The controller is model-based, but uses a sliding control technique to compensate for model inaccuracy.  To address the under-actuation, our system generates flow tubes, which define valid operating regions in the abstracted biped.  The flow tubes represent sets of state trajectories that take into account dynamic limitations due to under-actuation, and also satisfy plan requirements.  The executive keeps trajectories in the flow tubes by adjusting a small number of control parameters for key state variables in the abstracted biped, such as center of mass.  Additionally, our system uses a novel strategy that employs angular momentum to enhance translational controllability of the systemÂ’s center of mass.  We evaluate our approach using a high-fidelity biped simulation.  Tests include walking with foot-placement constraints, kicking a soccer ball, and disturbance recovery.",MIT-CSAIL-TR-2006-030,407 p.; 19361102 bytes; 7126131 bytes,application/postscript; application/pdf,en_US,biomechanics; nonlinear control; temporally flexible plan execution,,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Blackmore, Lars",2006-04-28T18:22:27Z,2006-04-28T18:22:27Z,2006-04-28,http://hdl.handle.net/1721.1/32538,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"A Probabilistic Particle Control Approach to Optimal, Robust Predictive Control","Autonomous vehicles need to be able to plan trajectories to a specified goal that avoid obstacles, and are robust to the inherent uncertainty in the problem. This uncertainty arises due to uncertain state estimation, disturbances and modeling errors. Previous solutions to the robust path planning problem solved this problem using a finite horizon optimal stochastic control approach. This approach finds the optimal path subject to chance constraints, which ensure that the probability of collision with obstacles is below a given threshold. This approach is limited to problems where all uncertain distributions are Gaussian, and typically result in highly conservative plans. In many cases, however, the Gaussian assumption is invalid; for example in the case of localization, the belief state about a vehicleÂ’s position can consist of highly non-Gaussian, even multimodal, distributions.In this paper we present a novel method for finite horizon stochastic control ofdynamic systems subject to chance constraints. The method approximates the distribution of the system state using a finite number of particles. By expressing these particles in terms of the control variables, we are able to approximate the original stochastic control problem as a deterministic one; furthermore the approximation becomes exact as the number of particles tends to infinity. For a general class of chance constrained problems with linear system dynamics, we show that the approximate problem can be solved using efficient Mixed-Integer Linear Programming techniques. We apply the new method to aircraft control in turbulence, and show simulation results that demonstrate the efficacy of the approach.",MIT-CSAIL-TR-2006-031,15 p.; 7896873 bytes; 673988 bytes,application/postscript; application/pdf,en_US,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hari Balakrishnan,"Walfish, Michael; Zamfirescu, J.D.; Balakrishnan, Hari; Karger, David; Shenker, Scott",2006-05-05T19:42:05Z,2006-05-05T19:42:05Z,2006-04-29,http://hdl.handle.net/1721.1/32542,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Supplement to ""Distributed Quota Enforcement for Spam Control""","This report is a supplement to our paper ""Distributed Quota Enforcement forSpam Control"" (NSDI 2006). We assume here that the reader has readthe main paper. In this report, we first analyze the enforcer nodes'key-value maps and then analyze two of the experiments from the main paper.",MIT-CSAIL-TR-2006-033,6 p.; 611915 bytes; 214834 bytes,application/postscript; application/pdf,en_US,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saman Amarasinghe,"Thies, William; Urbanski, John Paul; Thorsen, Todd; Amarasinghe, Saman",2006-05-05T19:42:11Z,2006-05-05T19:42:11Z,2006-05-05,http://hdl.handle.net/1721.1/32543,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Abstraction Layers for Scalable Microfluidic Biocomputers (Extended Version),"Microfluidic devices are emerging as an attractive technology for automatically orchestrating the reactions needed in a biological computer.  Thousands of microfluidic primitives have already been integrated on a single chip, and recent trends indicate that the hardware complexity is increasing at rates comparable to Moore's Law.  As in the case of silicon, it will be critical to develop abstraction layers--such as programming languages and Instruction Set Architectures (ISAs)--that decouple software development from changes in the underlying device technology.Towards this end, this paper presents BioStream, a portable language for describing biology protocols, and the Fluidic ISA, a stable interface for microfluidic chip designers.  A novel algorithm translates microfluidic mixing operations from the BioStream layer to the Fluidic ISA.  To demonstrate the benefits of these abstraction layers, we build two microfluidic chips that can both execute BioStream code despite significant differences at the device level.  We consider this to be an important step towards building scalable biocomputers.",MIT-CSAIL-TR-2006-034,14 p.; 6101702 bytes; 453129 bytes,application/postscript; application/pdf,en_US,biological computing; DNA computing; microfluidics,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"McCamant, Stephen",2006-05-11T19:32:15Z,2006-05-11T19:32:15Z,2006-05-11,http://hdl.handle.net/1721.1/32546,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Machine-Checked Safety Proof for a CISC-Compatible SFI Technique,"Executing untrusted code while preserving security requires that thecode be prevented from modifying memory or executing instructionsexcept as explicitly allowed.  Software-based fault isolation (SFI) or""sandboxing"" enforces such a policy by rewriting code at theinstruction level.  In previous work, we developed a new SFI techniquethat is applicable to CISC architectures such as the Intel IA-32,based on enforcing additional alignment constraints to avoiddifficulties with variable-length instructions.  This report describesa machine-checked proof we developed to increase our confidence in thesafety provided by the technique.  The proof, constructed for asimplified model of the technique using the ACL2 theorem provingenvironment, certifies that if the code rewriting has been checked tohave been performed correctly, the resulting program cannot perform adangerous operation when run.  We describe the high-level structure ofthe proof, then give the intermediate lemmas with interspersedcommentary, and finally evaluate the process of the proof'sconstruction.",MIT-CSAIL-TR-2006-035,33 p.; 499398 bytes; 1456845 bytes,application/pdf; application/postscript,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Werfel, Justin",2006-08-14T12:49:01Z,2006-08-14T12:49:01Z,2006-05-12,http://hdl.handle.net/1721.1/33791,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Anthills Built to Order: Automating Construction with Artificial Swarms,"Social insects build large, complex structures, which emerge through the collective actions of many simple agents acting with no centralized control or preplanning.  These natural systems motivate investigating the use of artificial swarms to automate construction or fabrication.  The goal is to be able to take an unspecified number of simple robots and a supply of building material, give the system a high-level specification for any arbitrary structure desired, and have a guarantee that it will produce that structure without further intervention.In this thesis I describe such a distributed system for automating construction, in which autonomous mobile robots collectively build user-specified structures from square building blocks.  The approach preserves many desirable features of the natural systems, such as considerable parallelism and robustness to factorslike robot loss and variable order or timing of actions.  Further, unlike insect colonies, it can build particular desired structures according to a high-level design provided by the user.Robots in this system act without explicit communication or cooperation, instead using the partially completed structure to coordinate their actions.  This mechanism is analogous to that of stigmergy used by social insects, in which insects take actions that affect the environment, and the environmental state influences further actions.  I introduce a framework of ""extended stigmergy"" in which building blocks are allowed to store, process or communicate information.  Increasing the capabilities of the building material (rather than of the robots) in this way increases the availability of nonlocal structure information.  Benefits include significant improvements in construction speed and in ability to take advantage of the parallelism of the swarm.This dissertation describes system design and control rules for decentralized teams of robots that provably build arbitrary solid structures in two dimensions.  I present a hardware prototype, and discuss extensions to more general structures, including those built with multiple block types and in three dimensions.",MIT-CSAIL-TR-2006-052,116 p.; 3106746 bytes; 130023424 bytes,application/pdf; application/postscript,en_US,,Mathematics and Computation,,,,PhD thesis,"Ph.D. thesis, Massachusetts Institute of Technology",,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Wolf, Lior",2006-05-16T22:16:59Z,2006-05-16T22:16:59Z,2006-05-16,http://hdl.handle.net/1721.1/32978,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning using the Born Rule,"In Quantum Mechanics the transition from a deterministic descriptionto a probabilistic one is done using a simple rule termed the Bornrule. This rule states that the probability of an outcome ($a$)given a state ($\Psi$) is the square of their inner products($(a^\top\Psi)^2$).In this paper, we unravel a new probabilistic justification forpopular algebraic algorithms, based on the Born rule. Thesealgorithms include two-class and multiple-class spectral clustering,and algorithms based on Euclidean distances.",MIT-CSAIL-TR-2006-036; CBCL-261,28 p.; 650559 bytes; 1572192 bytes,application/pdf; application/postscript,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Daniel Jackson,"Edwards, Jonathan",2006-05-23T11:16:56Z,2006-05-23T11:16:56Z,2006-05-22,http://hdl.handle.net/1721.1/32980,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,First Class Copy & Paste,"The Subtext project seeks to make programming fundamentally easier by altering the nature of programming languages and tools. This paper defines an operational semantics for an essential subset of the Subtext language. It also presents a fresh approach to the problems of mutable state, I/O, and concurrency.Inclusions reify copy & paste edits into persistent relationships that propagate changes from their source into their destination. Inclusions formulate a programming language in which there is no distinction between a programÂ’s representation and its execution. Like spreadsheets, programs are live executions within a persistent runtime, and programming is direct manipulation of these executions via a graphical user interface. There is no need to encode programs into source text.Mutation of state is effected by the computation of hypothetical recursive variants of the state, which can then be lifted into new versions of the state. Transactional concurrency is based upon queued single-threaded execution. Speculative execution of queued hypotheticals provides concurrency as a semantically transparent implementation optimization.",MIT-CSAIL-TR-2006-037,20 p.; 610092 bytes; 11137489 bytes,application/pdf; application/postscript,en_US,prototypes; copy and paste; modularity; reactivity; transactions,Software Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob",2006-06-01T16:22:12Z,2006-06-01T16:22:12Z,2006-05-27,http://hdl.handle.net/1721.1/32984,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,What the Assassin's Guild Taught Me About Distributed Computing,"Distributed computing and live-action roleplaying share many of thesame fundamental problems, as live-action roleplaying games commonly include simulations carried out by their players.Games run by the MIT Assassin's Guild are particularly illustrative ofdistributed computing issues due to their large scope and highcomplexity.I discuss three distributed computing issues addressed by Assassin'sGuild game design---information hiding, error correction, andliveness/consistency tradeoffs---and the relevance of the solutionsused by game writers to current problems in distributed computing.",MIT-CSAIL-TR-2006-038,9 p.; 168640 bytes; 609620 bytes,application/pdf; application/postscript,en_US,,Mathematics and Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Bachrach, Jonathan; Beal, Jacob",2006-10-03T14:06:44Z,2006-10-03T14:06:44Z,2006-06,http://hdl.handle.net/1721.1/34223,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Programming a Sensor Network as an Amorphous Medium,"In many sensor network applications, the network is deployedto approximate a physical space. The network itself is not ofinterest: rather, we are interested in measuring the propertiesof the space it fills, and of establishing control over thebehavior of that space.The spatial nature of sensor network applications meansthat many can be expressed naturally and succinctly in termsof the global behavior of an amorphous medium---a continuouscomputational material filling the space of interest. Althoughwe cannot construct such a material, we can approximateit using a sensor network.Using this amorphous medium abstraction separates sensornetwork problems into two largely independent domains.Above the abstraction barrier we are concerned with longrangecoordination and concise description of applications,while below the barrier we are concerned with fast, efficient,and robust communication between neighboring devices.We apply the amorphous medium abstraction with Proto,a high-level language for programming sensor/actuator networks.Existing applications, such as target tracking andthreat avoidance, can be expressed in only a few lines of Protocode. The applications are then compiled for execution on akernel that approximates an amorphous medium. Programswritten using our Proto implementation have been verified insimulation on over ten thousand nodes, as well as on a networkof Berkeley Motes.",MIT-CSAIL-TR-2006-069,6 p.; 374454 bytes; 4831946 bytes,application/pdf; application/postscript,en_US,amorphous computing,AIRE,,,,,DCOSS 2006 (Extended Abstract),,,,,,,,,,,,,,,,,,,,,,,
Eric Grimson,"Dalley, Gerald; Izo, Tomas",2006-06-12T18:36:58Z,2006-06-12T18:36:58Z,2006-06-12,http://hdl.handle.net/1721.1/32999,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Schematic Querying of Large Tracking Databases,"In dealing with long-term tracking databases withwide-area coverage, an important problem is in formulating anintuitive and fast query system for analysis. In such a querysystem, a user who is not a computer vision research should beable to readily specify a novel query to the system and obtainthe desired results. Furthermore, these queries should be able tonot only search out individual actors (e.g. ""find all white cars"")but also find interactions amongst multiple actors (e.g. ""find alldrag racing activities in the city""). Informally, we have foundthat people often use sketches when describing activities andinteractions. In this paper, we demonstrate a preliminary systemthat automatically interprets schematic drawings of activities.The system transforms the schematics into executable code thatsearches a tracking database. Through our query optimization,these queries tend to take orders of magnitude less time to executethan equivalent queries running on a partially-optimized SQLdatabase.",MIT-CSAIL-TR-2006-043,7 p.; 391836 bytes; 7529339 bytes,application/pdf; application/postscript,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Barbara Liskov,"Leong, Ben",2006-06-14T14:57:35Z,2006-06-14T14:57:35Z,2006-06-14,http://hdl.handle.net/1721.1/33000,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New Techniques for Geographic Routing,"As wireless sensor networks continue to grow in size, we are facedwith the prospect of emerging wireless networks with hundreds orthousands of nodes. Geographic routing algorithms are a promisingalternative to tradition ad hoc routing algorithms in this new domainfor point-to-point routing, but deployments of such algorithms arecurrently uncommon because of some practical difficulties.This dissertation explores techniques that address two major issues inthe deployment of geographic routing algorithms: (i) the costsassociated with distributed planarization and (ii) the unavailabilityof location information.  We present and evaluate two new algorithmsfor geographic routing: Greedy Distributed Spanning Tree Routing(GDSTR) and Greedy Embedding Spring Coordinates (GSpring).Unlike previous geographic routing algorithms which require theplanarization of the network connectivity graph, GDSTR switches torouting on a spanning tree instead of a planar graph when packets endup at dead ends during greedy forwarding. To choose a direction on thetree that is most likely to make progress towards the destination,each GDSTR node maintains a summary of the area covered by the subtreebelow each of its tree neighbors using convex hulls. This distributeddata structure is called a hull tree. GDSTR not only requires an orderof magnitude less bandwidth to maintain these hull trees than CLDP,the only distributed planarization algorithm that is known to workwith practical radio networks, it often achieves better routingperformance than previous planarization-based geographic routingalgorithms.GSpring is a new virtual coordinate assignment algorithm that derivesgood coordinates for geographic routing when location information isnot available. Starting from a set of initial coordinates for a set ofelected perimeter nodes, GSpring uses a modified spring relaxationalgorithm to incrementally adjust virtual coordinates to increase theconvexity of voids in the virtual routing topology. This reduces theprobability that packets will end up in dead ends during greedyforwarding, and improves the routing performance of existinggeographic routing algorithms.The coordinates derived by GSpring yield comparable routingperformance to that for actual physical coordinates and significantlybetter performance than that for NoGeo, the best existing algorithmfor deriving virtual coordinates for geographic routing. Furthermore,GSpring is the first known algorithm that is able to derivecoordinates that achieve better geographic routing performance thanactual physical coordinates for networks with obstacles.",MIT-CSAIL-TR-2006-044,150 p.; 3957081 bytes; 32739307 bytes,application/pdf; application/postscript,en_US,,Programming Methodology,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Trevor Darrell,"Grauman, Kristen; Darrell, Trevor",2006-06-15T21:37:00Z,2006-06-15T21:37:00Z,2006-06-15,http://hdl.handle.net/1721.1/33002,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Approximate Correspondences in High Dimensions,"Pyramid intersection is an efficient method for computing an approximate partial matching between two sets of feature vectors. We introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors.  The matching similarity is computed in linear time and forms a Mercer kernel.  We also show how the matching itself (a correspondence field) may be extracted for a small increase in computational cost. Whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate thatour approach can maintain constant accuracy even as the feature dimension increases. When used as a kernel in a discriminative classifier, our approach achieves improved object recognition results over a state-of-the-art set kernel.",MIT-CSAIL-TR-2006-045,10 p.; 14140112 bytes; 5515480 bytes,application/postscript; application/pdf,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2006-06-19T18:52:04Z,2006-06-19T18:52:04Z,2006-06-19,http://hdl.handle.net/1721.1/33154,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Probabilistic I/O Automata to Analyze an Oblivious Transfer Protocol,"We demonstrate how to carry out cryptographic security analysis ofdistributed protocols within the Probabilistic I/O Automataframework of Lynch, Segala, and Vaandrager. This framework providestools for arguing rigorously about the concurrency and schedulingaspects of protocols, and about protocols presented at differentlevels of abstraction. Consequently, it can help in makingcryptographic analysis more precise and less susceptible to errors.We concentrate on a relatively simple two-party Oblivious Transferprotocol, in the presence of a semi-honest adversary (essentially,an eavesdropper). For the underlying cryptographic notion ofsecurity, we use a version of Canetti's Universally Composablesecurity.In spite of the relative simplicity of the example, the exercise isquite nontrivial. It requires taking many fundamental issues intoaccount, including nondeterministic behavior, scheduling,resource-bounded computation, and computational hardness assumptionsfor cryptographic primitives.",MIT-CSAIL-TR-2006-046,129 p.; 1111678 bytes; 7337435 bytes,application/pdf; application/postscript,en_US,,Theory of Computation,,,,,"January 10, 2006",,,,,http://hdl.handle.net/1721.1/30566,http://hdl.handle.net/1721.1/30566,,,,,,,,,,,,,,,,,
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2006-06-20T21:18:08Z,2006-06-20T21:18:08Z,2006-06-20,http://hdl.handle.net/1721.1/33217,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Task-Structured Probabilistic I/O Automata to Analyze an Oblivious Transfer Protocol,"The Probabilistic I/O Automata framework of Lynch, Segala and Vaandrager provides tools for precisely specifying protocols and reasoning about theircorrectness using multiple  levels of abstraction, based on implementation relationships between these levels. We enhance this framework to allow analyzingprotocols that use cryptographic primitives. This requires resolving andreconciling issues such as nondeterministic behavior and scheduling, randomness,resource-bounded computation, and computational hardness assumptions.  The enhanced framework allows for more rigorous and systematic analysis of cryptographic protocols. To demonstrate the use of this framework, we present an example analysis that we have  done for an Oblivious Transfer protocol.",MIT-CSAIL-TR-2006-047,98 p.; 898579 bytes; 5581045 bytes,application/pdf; application/postscript,en_US,,Theory of Computation,,,,,"June 19, 2006",,,,http://hdl.handle.net/1721.1/35918,http://hdl.handle.net/1721.1/35918; http://hdl.handle.net/1721.1/31310,http://hdl.handle.net/1721.1/31310,,,,,,,,,,,,,,,,,
Peter Szolovits,"Sibanda, Tawanda Carleton",2006-06-29T13:22:10Z,2006-06-29T13:22:10Z,2006-06-28,http://hdl.handle.net/1721.1/33223,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Was the Patient Cured? Understanding Semantic Categories and Their Relationships in Patient Records,"In this thesis, we detail an approach to extracting key information in medical discharge summaries. Starting with a narrative patient report, we first identify and remove information that compromises privacy (de-identification);next we recognize words and phrases in the text belonging to semantic categories of interest to doctors (semantic category recognition).For disease and symptoms, we determine whether the problem is present, absent, uncertain, or associated with somebody else (assertion classification). Finally, we classify the semantic relationships existing between our categories (semantic relationship classification).Our approach utilizes a series of statistical models that rely heavily on local lexical and syntactic context, and achieve competitive results compared to more complexNLP solutions. We conclude the thesis by presenting the design for the Category and Relationship Extractor (CaRE). CaRE combines our solutions to de-identification, semantic category recognition, assertion classification, and semantic relationship classification into a singleapplication that facilitates the easy extraction of semantic information from medical text.",MIT-CSAIL-TR-2006-048,108 p.; 2366072 bytes; 9070788 bytes,application/pdf; application/postscript,en_US,medical language processing; MLP; discharge summary,Clinical Decision-Making,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Chachulski, Szymon; Jennings, Michael; Katti, Sachin; Katabi, Dina",2006-07-03T14:16:54Z,2006-07-03T14:16:54Z,2006-06-30,http://hdl.handle.net/1721.1/33230,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,MORE: A Network Coding Approach to Opportunistic Routing,"Opportunistic routing has the potential to substantially increase wireless network throughput. Prior work on opportunistic routing, however, requires tight node coordination. Different nodes in a network must have knowledge of which packets other nodes have received. Furthermore, the nodes have to agree on which nodes should transmit which packets. Such coordination becomes fragile in dense or large networks.This paper introduces MORe, a new opportunistic routing protocol that avoids node-coordination. Our design is rooted in the theory of network coding.Routers code packets going to the same destination and forward the coded versions. The destination decodes and recovers the original packets. This approach needs no coordination and provably maximizes network throughput. We have implemented our design and evaluated it in a 25-node testbed. Our results show that MORE provides an average throughput increase of 60% and a maximum of 10-fold, demonstrating that the theoretical gains promised by network coding are realizable in practice.",MIT-CSAIL-TR-2006-049,12 p.; 22051455 bytes; 883727 bytes,application/postscript; application/pdf,en_US,Network Coding; Opportunistic Routing; Wireless Networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Teow, Loo Nin; Katabi, Dina",2006-07-12T17:06:52Z,2006-07-12T17:06:52Z,2006-07-04,http://hdl.handle.net/1721.1/33234,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Iterative Collaborative Ranking of  Customers and Providers,"This paper introduces a new application: predicting the Internet provider-customer market. We cast the problem in the collaborative filtering framework, where we use current and past customer-provider relationships to compute for each Internet customer a ranking of potential future service providers. Furthermore, for each Internet service provider (ISP), we rank potential future customers. We develop a novel iterative ranking algorithm that draws inspiration from several sources, including collaborative filtering, webpage ranking, and kernel methods. Further analysis of our algorithm shows that it can be formulated in terms of an affine eigenvalue problem. Experiments on the actual Internet customer-provider data show promising results.",MIT-CSAIL-TR-2006-050,9 p.; 1308662 bytes; 269609 bytes,application/postscript; application/pdf,en_US,Network Analysis; Autonomous Systems,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Das, Sanmay",2006-07-13T11:26:51Z,2006-07-13T11:26:51Z,2006-07-12,http://hdl.handle.net/1721.1/33235,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Dealers, Insiders and Bandits: Learning and its Effects on Market Outcomes","This thesis seeks to contribute to the understanding of markets populated by boundedly rational agents who learn from experience. Bounded rationality and learning have both been the focus of much research in computer science, economics and finance theory. However, we are at a critical stage in defining the direction of future research in these areas. It is now clear that realistic learning problems faced by agents in market environments are often too hard to solve in a classically rational fashion. At the same time, the greatly increased computational power available today allows us to develop and analyze richer market models and to evaluate different learning procedures and algorithms within these models. The danger is that the ease with which complex markets can be simulated could lead to a plethora of models that attempt to explain every known fact about different markets. The first two chapters of this thesis define a principled approach to studying learning in rich models of market environments, and the rest of the thesis provides a proof of concept by demonstrating the applicability of this approach in modeling settings drawn from two different broad domains, financial market microstructure and search theory. In the domain of market microstructure, this thesis extends two important models from the theoretical finance literature. The third chapter introduces an algorithm for setting prices in dealer markets based on the model of Glosten and Milgrom (1985), and produces predictions about the behavior of prices in securities markets. In some cases, these results confirm economic intuitions in a significantly more complex setting (like the existence of a local profit maximum for a monopolistic market-maker) and in others they can be used to provide quantitative guesses for variables such as rates of convergence to efficient market conditions following price jumps that provide insider information. The fourth chapter studies the problem faced by a trader with insider information in KyleÂ’s (1985) model. I show how the insider trading problem can be usefully analyzed from the perspective of reinforcement learning when some important market parameters are unknown, and that the equilibrium behavior of an insider who knows these parameters can be learned by one who does not, but also that the time scale of convergence to the equilibrium behavior may be impractical, and agents with limited time horizons may be better off using approximate algorithms that do not converge to equilibrium behavior. The fifth and sixth chapters relate to search problems. Chapter 5 introduces models for a class of problems in which there is a search Â“seasonÂ” prior to hiring or matching, like academic job markets. It solves for expected values in many cases, and studies the difference between a Â“high informationÂ” process where applicants are immediately told when they have been rejected and a Â“low informationÂ” process where employers do not send any signal when they reject an applicant. The most important intuition to emerge from the results is that the relative benefit of the high information process is much greater when applicants do not know their own Â“attractiveness,Â” which implies that search markets might be able to eliminate inefficiencies effectively by providing good information, and we do not always have to think about redesigning markets as a whole. Chapter 6 studies two-sided search explicitly and introduces a new class of multi-agent learning problems, two-sided bandit problems, that capture the learning and decision problems of agents in matching markets in which agents must learn their preferences. It also empirically studies outcomes under different periodwise matching mechanisms and shows that some basic intuitions about the asymptotic stability of matchings are preserved in the model. For example, when agents are matched in each period using the Gale-Shapley algorithm, asymptotic outcomes are always stable, while a matching mechanism that induces a stopping problem for some agents leads to the lowest probabilities of stability. By contributing to the state of the art in modeling different domains using computational techniques, this thesis demonstrates the success of the approach to modeling complex economic and social systems that is prescribed in the first two chapters.",MIT-CSAIL-TR-2006-051; CBCL-262,149 p.; 860631 bytes; 5532424 bytes,application/pdf; application/postscript,en_US,,Center for Biological and Computational Learning (CBCL),,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Fan, Rui; Lynch, Nancy",2008-07-28T13:30:11Z,2008-07-28T13:30:11Z,2006-07-23,http://hdl.handle.net/1721.1/41890,,An $\Omega(n \log n)$ Lower Bound on the Cost of Mutual Exclusion,"We prove an $\Omega(n \log n)$ lower bound on the number ofnon-busywaiting memory accesses by any deterministic algorithm solving$n$ process mutual exclusion that communicates via shared registers.The cost of the algorithm is measured in the \emph{state change} costmodel, a variation of the cache coherent model. Our bound is tight inthis model. We introduce a novel information theoretic prooftechnique. We first establish a lower bound on the information neededby processes to solve mutual exclusion. Then we relate the amount ofinformation processes can acquire through shared memory accesses tothe cost they incur. We believe our proof technique is flexible andintuitive, and may be applied to a variety of other problems andsystem models.",MIT-CSAIL-TR-2008-047,14 p.,,,Mutual exclusion; Time complexity; Lower bound techniques,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Dina Katabi,"Jaggi, Sidharth; Langberg, Michael; Katti, Sachin; Ho, Tracy; Katabi, Dina; Medard, Muriel",2006-08-14T12:43:17Z,2006-08-14T12:43:17Z,2006-08-05,http://hdl.handle.net/1721.1/33790,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Resilient Network Coding In the Presence of Byzantine Adversaries,"Network coding substantially increases network throughput. But since it involves mixing of information inside the network, a single corrupted packet generated by a malicious node can end up contaminating all the information reaching a destination, preventing decoding. This paper introduces the first distributed polynomial-time rate-optimal network codes that work in the presence of Byzantine nodes. We present algorithms that target adversaries with different attacking capabilities. When the adversary can eavesdrop on all links and jam Z links , our first algorithm achieves a rate of C-2Z, where C is the network capacity. In contrast, when the adversary has limited snooping capabilities, we provide algorithms that achieve the higher rate of C-Z.",MIT-CSAIL-TR-2006-053,9 p.; 347172 bytes; 1152836 bytes,application/pdf; application/postscript,en_US,"Coding Theory; Security, trust, & privacy; Network Coding; Byzantine Adversarie",Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Wolf, Florian; Poggio, Tomaso; Sinha, Pawan",2006-08-14T12:29:13Z,2006-08-14T12:29:13Z,2006-08-09,http://hdl.handle.net/1721.1/33789,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Human Document Classification Using Bags of Words,"Humans are remarkably adept at classifying text documents into cate-gories.  For instance, while reading a news story, we are rapidly able to assess whether it belongs to the domain of finance, politics or sports.  Automating this task would have applications for content-based search or filtering of digital documents.  To this end, it is interesting to investigate the nature of information humans use to classify documents.  Here we report experimental results suggesting that this information might, in fact, be quite simple.  Using a paradigm of progressive revealing, we determined classification performance as a function of number of words.  We found that subjects are able to achieve similar classification accuracy with or without syntactic information across a range of passage sizes.  These results have implications for models of human text-understanding and also allow us to estimate what level of performance we can expect, in principle, from a system without requiring a prior step of complex natural language processing.",MIT-CSAIL-TR-2006-054; CBCL-263,7 p.; 1617611 bytes; 134084 bytes,application/postscript; application/pdf,en_US,text classification,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trevor Darrell,"Grauman, Kristen Lorraine",2008-06-16T14:15:28Z,2008-06-16T14:15:28Z,2006-08-11,http://hdl.handle.net/1721.1/41864,,Matching Sets of Features for Efficient Retrieval and Recognition,"In numerous domains it is useful to represent a single example by the collection of local features or parts that comprise it. In computer vision in particular, local image features are a powerful way to describe images of objects and scenes. Their stability under variable image conditions is critical for success in a wide range of recognition and retrieval applications. However, many conventional similarity measures and machine learning algorithms assume vector inputs. Comparing and learning from images represented by sets of local features is therefore challenging, since each set may vary in cardinality and its elements lack a meaningful ordering. In this thesis I present computationally efficient techniques to handle comparisons, learning, and indexing with examples represented by sets of features. The primary goal of this research is to design and demonstrate algorithms that can effectively accommodate this useful representation in a way that scales with both the representation size as well as the number of images available for indexing or learning.I introduce the pyramid match algorithm, which efficiently forms an implicit partial matching between two sets of feature vectors. The matching has a linear time complexity, naturally forms a Mercer kernel, and is robust to clutter or outlier features, a critical advantage for handling images with variable backgrounds, occlusions, and viewpoint changes. I provide bounds on the expected error relative to the optimal partial matching. For very large databases, even extremely efficient pairwise comparisons may not offer adequately responsive query times. I show how to perform sub-linear time retrievals under the matching measure with randomized hashing techniques, even when input sets have varying numbers of features.My results are focused on several important vision tasks, including applications to content-based image retrieval, discriminative classification for object recognition, kernel regression, and unsupervised learning of categories. I show how the dramatic increase in performance enables accurate and flexible image comparisons to be made on large-scale data sets, and removes the need to artificially limit the number of local descriptions used per image when learning visual categories.",MIT-CSAIL-TR-2008-035,153 p.,,,"object recognition, partial match, matching approximation, pyramid match kernel, image matching, image search, correspondences, local image features",Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Brian Williams,"Effinger, Robert",2018-01-30T23:46:07Z,2018-01-30T23:46:07Z,2006-08-25,http://hdl.handle.net/1721.1/113365,MIT-CSAIL-TR-2018-005,Optimal Temporal Planning at Reactive Time Scales via Dynamic Backtracking Branch and Bound,"Autonomous robots are being considered for increasingly capable roles in our society, such as urban search and rescue, automation for assisted living, and lunar habitat construction. To fulfill these roles, teams of autonomous robots will need to cooperate together to accomplish complex mission objectives in uncertain and dynamic environments. In these environments, autonomous robots face a host of new challenges, such as responding robustly to timing uncertainties and perturbations, task and coordination failures, and equipment malfunctions. In order to address these challenges, this thesis advocates a novel planning approach, called temporally-flexible contingent planning. A temporally-flexible contingent plan is a compact encoding of methods for achieving the mission objectives which incorporates robustness through flexible task durations, redundant methods, constraints on when methods are applicable, and preferences between methods. This approach enables robots to adapt to unexpected changes on-the-fly by selecting alternative methods at runtime in order to satisfy as best possible the mission objectives. The drawback to this approach, however, is the computational overhead involved in selecting alternative methods at runtime in response to changes. If a robot takes too long to select a new plan, it could fail to achieve its near-term mission objectives and potentially incur damage. To alleviate this problem, and extend the range of applicability of temporally-flexible contingent planning to more demanding real-time systems, this thesis proposes a temporally-flexible contingent plan executive that selects new methods quickly and optimally in response to changes in a robot's health and environment. We enable fast and optimal method selection through two complimentary approaches. First, we frame optimal method selection as a constraint satisfaction problem (CSP) variant, called an Optimal Conditional CSP (OCCSP). Second, we extend fast CSP search algorithms, such as Dynamic Backtracking and Branch-and-Bound Search, to solve OCCSPs. Experiments on an autonomous rover test-bed and on randomly generated plans show that these contributions significantly improve the speed at which robots perform optimal method selection in response to changes in their health status and environment.",,115 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-30T23:46:07Z,,,,,,,,,,,,,,,
Peter Szolovits,"Hug, Caleb W.",2006-08-31T14:29:11Z,2006-08-31T14:29:11Z,2006-08-30,http://hdl.handle.net/1721.1/33957,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Predicting the Risk and Trajectory of Intensive Care Patients Using Survival Models,"Using artificial intelligence to assist physicians in patient care has received sustained interest over the past several decades.  Recently, with automated systems at most bedsides, the amount of patient information collected continues to increase, providing specific impetus for intelligent systems that can interpret this information. In fact, the large set of sensors and test results, often measured repeatedly over long periods of time, make it challenging for caregivers to quickly utilize all of the data for optimal patient treatment.This research focuses on predicting the survival of ICU patients throughout their stay.  Unlike traditional static mortality models, this survival prediction is explored as an indicator of patient state and trajectory.  Using survival analysis techniques and machine learning, models are constructed that predict individual patient survival probabilities at fixed intervals in the future.  These models seek to help physicians interpret the large amount of data available in order to provide optimal patient care.We find that the survival predictions from our models are comparable to survival predictions using the SAPS score, but are available throughout the patient's ICU course instead of only at 24 hours after admission.  Additionally, we demonstrate effective prediction of patient mortality over fixed windows in the future.",MIT-CSAIL-TR-2006-055,126 p.; 3075332 bytes; 103809024 bytes,application/pdf; application/postscript,en_US,Intelligent Monitoring,Clinical Decision-Making,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Artzi, Shay; Ernst, Michael D.; Kiezun, Adam; Pacheco, Carlos; Perkins, Jeff H.",2006-09-05T16:31:22Z,2006-09-05T16:31:22Z,2006-08-31,http://hdl.handle.net/1721.1/33959,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Finding the needles in the haystack: Generating legal test inputs for object-oriented programs,"A test input for an object-oriented program typically consists of asequence of method calls that use the API defined by the programunder test. Generating legal test inputs can be challenging because,for some programs, the set of legal method sequences is much smallerthan the set of all possible sequences; without a formalspecification of legal sequences, an input generator is bound toproduce mostly illegal sequences.We propose a scalable technique that combines dynamic analysis withrandom testing to help an input generator create legal test inputswithout a formal specification, even for programs in whichmost sequences are illegal. The technique uses an example executionof the program to infer a model of legal call sequences, and usesthe model to guide a random input generator towards legal butbehaviorally-diverse sequences.We have implemented our technique for Java, in a tool calledPalulu, and evaluated its effectiveness in creating legal inputsfor real programs. Our experimental results indicate that thetechnique is effective and scalable. Our preliminary evaluationindicates that the technique can quickly generate legal sequencesfor complex inputs: in a case study, Palulu created legal testinputs in seconds for a set of complex classes, for which it took anexpert thirty minutes to generate a single legal input.",MIT-CSAIL-TR-2006-056,8 p.; 412269 bytes; 923064 bytes,application/pdf; application/postscript,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tommi Jaakkola,"Monteleoni, Claire E.",2006-09-05T16:26:12Z,2006-09-05T16:26:12Z,2006-09-01,http://hdl.handle.net/1721.1/33958,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning with Online Constraints: Shifting Concepts and Active Learning,"Many practical problems such as forecasting, real-time decisionmaking, streaming data applications, and resource-constrainedlearning, can be modeled as learning with online constraints.  Thisthesis is concerned with analyzing and designing algorithms forlearning under the following online constraints: 1) The algorithm hasonly sequential, or one-at-time, access to data.  2) The time andspace complexity of the algorithm must not scale with the number ofobservations.  We analyze learning with online constraints in avariety of settings, including active learning.  The active learningmodel is applicable to any domain in which unlabeled data is easy tocome by and there exists a (potentially difficult or expensive)mechanism by which to attain labels.First, we analyze a supervised learning framework in which nostatistical assumptions are made about the sequence of observations,and algorithms are evaluated based on their regret, i.e. theirrelative prediction loss with respect to the hindsight-optimalalgorithm in a comparator class.  We derive a lower bound on regretfor a class of online learning algorithms designed to track shiftingconcepts in this framework.  We apply an algorithm we provided inprevious work, that avoids this lower bound, to an energy-managementproblem in wireless networks, and demonstrate this application in anetwork simulation. Second, we analyze a supervised learning frameworkin which the observations are assumed to be iid, and algorithms arecompared by the number of prediction mistakes made in reaching atarget generalization error.  We provide a lower bound on mistakes forPerceptron, a standard online learning algorithm, for this framework.We introduce a modification to Perceptron and show that it avoids thislower bound, and in fact attains the optimal mistake-complexity forthis setting.Third, we motivate and analyze an online active learning framework.The observations are assumed to be iid, and algorithms are judged bythe number of label queries to reach a target generalizationerror. Our lower bound applies to the active learning setting as well,as a lower bound on labels for Perceptron paired with any activelearning rule.  We provide a new online active learning algorithm thatavoids the lower bound, and we upper bound its label-complexity.  Theupper bound is optimal and also bounds the algorithm's total errors(labeled and unlabeled).  We analyze the algorithm further, yielding alabel-complexity bound under relaxed assumptions.  Using opticalcharacter recognition data, we empirically compare the new algorithmto an online active learning algorithm with data-dependent performanceguarantees, as well as to the combined variants of these twoalgorithms.",MIT-CSAIL-TR-2006-057,102 p.; 11237521 bytes; 2402633 bytes,application/postscript; application/pdf,en_US,,Tommi's Machine Learning,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
William Freeman,"Fergus, Rob; Torralba, Antonio; Freeman, William T.",2006-09-07T18:46:57Z,2006-09-07T18:46:57Z,2006-09-02,http://hdl.handle.net/1721.1/33962,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Random Lens Imaging,"We call a random lens one for which the function relating the input light ray to the output sensor location is pseudo-random. Imaging systems with random lensescan expand the space of possible camera designs, allowing new trade-offs in optical design and potentially adding new imaging capabilities. Machine learningmethods are critical for both camera calibration and image reconstruction from the sensor data. We develop the theory and compare two different methods for calibration and reconstruction: an MAP approach, and basis pursuit from compressive sensing. We show proof-of-concept experimental results from a random lens made from a multi-faceted mirror, showing successful calibration and image reconstruction. We illustrate the potential for super-resolution and 3D imaging.",MIT-CSAIL-TR-2006-058,9 p.; 52671890 bytes; 1133927 bytes,application/postscript; application/pdf,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Kiezun, Adam; Ernst, Michael D.; Tip, Frank; Fuhrer, Robert M.",2006-09-07T20:30:25Z,2006-09-07T20:30:25Z,2006-09-05,http://hdl.handle.net/1721.1/33965,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Refactoring for parameterizing Java classes,"Type safety and expressiveness of many existing Java libraries and theirclient applications would improve, if the libraries were upgraded to definegeneric classes.  Efficient and accurate tools exist to assist clientapplications to use generics libraries, but so far the libraries themselvesmust be parameterized manually, which is a tedious, time-consuming, anderror-prone task.  We present a type-constraint-based algorithm forconverting non-generic libraries to add type parameters.  The algorithmhandles the full Java language and preserves backward compatibility, thusmaking it safe for existing clients.  Among other features, it is capableof inferring wildcard types and introducing type parameters formutually-dependent classes.  We have implemented the algorithm as a fullyautomatic refactoring in Eclipse.We evaluated our work in two ways.  First, our tool parameterized code thatwas lacking type parameters.  We contacted the developers of several ofthese applications, and in all cases where we received a response, theyconfirmed that the resulting parameterizations were correct and useful.Second, to better quantify its effectiveness, our tool parameterizedclasses from already-generic libraries, and we compared the results tothose that were created by the libraries' authors.  Our tool performed therefactoring accurately -- in 87% of cases the results were as good as thosecreated manually by a human expert, in 9% of cases the tool results werebetter, and in 4% of cases the tool results were worse.",MIT-CSAIL-TR-2006-061,11 p.; 268513 bytes; 1340485 bytes,application/pdf; application/postscript,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Tschantz, Matthew S.",2006-09-07T18:51:29Z,2006-09-07T18:51:29Z,2006-09-05,http://hdl.handle.net/1721.1/33963,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Javari: Adding Reference Immutability to Java,"This paper describes a programming language, Javari, that is capable of expressing and enforcing immutability constraints.  The specific constraint expressed is that the abstract state of the object to which an immutable reference refers cannot be modified using that reference.  The abstract state is (part of) the transitively reachable state: that is, the state of the object and all state reachable from it by following references.  The type system permits explicitly excluding fields from the abstract state of an object.  For a statically type-safe language, the type system guarantees reference immutability.The type system is distinguishes the notions of assignability and mutability; integrates with Java's generic types and with multi-dimensional arrays; provides a mutability polymorphism approach to avoiding code duplication; and has type-safe support for reflection and serialization.  This paper describes a core calculus including formal type rules for the language.Additionally, this paper describes a type inference algorithm that can be used convert existing Java programs to Javari.  Experimental results from a prototype implementation of the algorithm are presented.",MIT-CSAIL-TR-2006-059,133 p.; 1217863 bytes; 4225617 bytes,application/pdf; application/postscript,en_US,assignable; languages; mutable; readonly; type system; verification,Program Analysis,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Canetti,, Ran; Cheung,, Ling; Kaynar,, Dilsun; Liskov,, Moses; Lynch,, Nancy; Pereira,, Olivier; Segala, Roberto",2006-09-07T20:14:22Z,2006-09-07T20:14:22Z,2006-09-05,http://hdl.handle.net/1721.1/33964,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Task-Structured Probabilistic I/O Automata,"Modeling frameworks such as Probabilistic I/O Automata (PIOA) andMarkov Decision Processes permit both probabilistic andnondeterministic choices.  In order to use such frameworks to express claims about probabilities of events, one needs mechanisms for resolving nondeterministic choices.  For PIOAs, nondeterministic choices have traditionally been resolved by schedulers that have perfect information about the past execution.  However, such schedulers are too powerful for certain settings, such as cryptographic protocol analysis, where information must sometimes be hidden. Here, we propose a new, less powerful nondeterminism-resolutionmechanism for PIOAs, consisting of tasks and local schedulers.Tasks are equivalence classes of system actions that are scheduled byoblivious, global task sequences.  Local schedulers resolve nondeterminism within system components, based on local information only.  The resulting task-PIOA framework yields simple notions of external behavior and implementation, and supports simple compositionality results.We also define a new kind of simulation relation, and show it to besound for proving implementation.  We illustrate the potential of the task-PIOA    framework by outlining its use in verifying an Oblivious Transfer protocol.",MIT-CSAIL-TR-2006-060,34 p.; 2343188 bytes; 369503 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,http://hdl.handle.net/1721.1/32525,http://hdl.handle.net/1721.1/32525,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Caponnetto, Andrea; Yao, Yuan",2006-09-29T18:36:45Z,2006-09-29T18:36:45Z,2006-09-10,http://hdl.handle.net/1721.1/34217,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Adaptation for Regularization Operators in Learning Theory,"We consider learning algorithms induced by regularization methods in the regression setting.  We show that previously obtained error bounds for these algorithms using a-priori choices of the regularization parameter, can be attained using a suitable a-posteriori choice based on validation.  In particular, these results prove adaptation of the rate of convergence of the estimators to the minimax rate induced by the ""effective dimension"" of the problem.  We also show universal consistency for theses class methods.",MIT-CSAIL-TR-2006-063; CBCL-265,19 p.; 963649 bytes; 819523 bytes,application/postscript; application/pdf,en_US,"optimal rates, Learning, regularization methods, adaptation, cross-validation",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Caponnetto, Andrea",2006-09-29T18:36:42Z,2006-09-29T18:36:42Z,2006-09-10,http://hdl.handle.net/1721.1/34216,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Optimal Rates for Regularization Operators in Learning Theory,"We develop some new error bounds for learning algorithms induced by regularization methods in the regression setting.  The ""hardness"" of the problem is characterized in terms of the parameters r and s, the first related to the ""complexity"" of the target function, the second connected to the effective dimension of the marginal probability measure over the input space.  We show, extending previous results, that by a suitable choice of the regularization parameter as a function of the number of the available examples, it is possible attain the optimal minimax rates of convergence for the expected squared loss of the estimators, over the family of priors fulfilling the constraint r + s > 1/2.  The setting considers both labelled and unlabelled examples, the latter being crucial for the optimality results on the priors in the range r < 1/2.",MIT-CSAIL-TR-2006-062; CBCL-264,16 p.; 776374 bytes; 738421 bytes,application/postscript; application/pdf,en_US,"optimal rates, regularized least-squares algorithm, regularization methods, adaptation",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"Sarmenta, Luis F. G.; van Dijk, Marten; O'Donnell, Charles W.; Rhodes, Jonathan; Devadas, Srinivas",2006-09-11T22:20:24Z,2006-09-11T22:20:24Z,2006-09-11,http://hdl.handle.net/1721.1/33966,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Virtual Monotonic Counters and Count-Limited Objects using a TPM without a Trusted OS (Extended Version),"A trusted monotonic counter is a valuable primitive thatenables a wide variety of highly scalable offlineand decentralized applications that would otherwise be prone to replay attacks, including offline payment, e-wallets, virtual trusted storage, and digital rights management (DRM).In this paper, we show how one can implement a very large number of virtual monotonic counters on an untrusted machine with a Trusted Platform Module (TPM) or similar device, without relying on a trusted OS.  We first present a log-based scheme that can be implemented with the current version of the TPM (1.2) and used incertain applications.We then show how the addition of a few simple features tothe TPM makes it possible to implement a hash-tree-based schemethat not only offers improved performance and scalability compared to the log-based scheme, but also makes it possible to implement count-limited objects (or ``clobs'' for short) -- i.e., encrypted keys, data, and other objectsthat can only be used when an associated virtual monotonic counter is within a certain range.Such count-limited objects include n-time use keys, n-out-of-m data blobs,n-copy migratable objects, and other variants, which have many potential uses in digital rights management (DRM), digital cash, digital voting, itinerant computing,and other application areas.",MIT-CSAIL-TR-2006-064,18 p.; 430350 bytes; 694048 bytes,application/pdf; application/postscript,en_US,trusted storage; key delegation; stored-value; e-wallet; smartcard; memory integrity checking; certified execution,Computation Structures,,,,,A shorter version of this paper will appear in the 1st ACM CCS Workshop on Scalable Trusted Computing (STC'06).,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Artzi, Shay; Ernst, Michael D.; Glasser, David; Kiezun, Adam",2006-09-18T17:55:18Z,2006-09-18T17:55:18Z,2006-09-17,http://hdl.handle.net/1721.1/33968,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combined static and dynamic mutability analysis,"Knowing which method parameters may be mutated during a method'sexecution is useful for many software engineering tasks.  We presentan approach to discovering parameter immutability, in which severallightweight, scalable analyses are combined in stages, with each stagerefining the overall result.  The resulting analysis is scalable andcombines the strengths of its component analyses.  As one of thecomponent analyses, we present a novel, dynamic mutability analysisand show how its results can be improved by random input generation.Experimental results on programs of up to 185 kLOC demonstrate that,compared to previous approaches, our approach increases both scalabilityand overall accuracy.",MIT-CSAIL-TR-2006-065,10 p.; 176363 bytes; 1038435 bytes,application/pdf; application/postscript,en_US,immutability; mutability; side effect analysis; purity; pointer analysis; dynamic analysis; mutation,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Krste Asanovic,"Tseng, Jessica H.; Asanovic, Krste",2006-09-25T16:01:50Z,2006-09-25T16:01:50Z,2006-09-18,http://hdl.handle.net/1721.1/34012,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,RingScalar: A Complexity-Effective Out-of-Order Superscalar Microarchitecture,"RingScalar is a complexity-effective microarchitecture for out-of-order superscalar processors, that reduces the area, latency, and power of all major structures in the instruction flow.  The design divides an N-way superscalar into N columns connected in a unidirectional ring, where each column contains a portion of the instruction window, a bank of the register file, and an ALU.  The design exploits the fact that most decoded instructions are waiting on just one operand to use only a single tag per issue window entry, and to restrict instruction wakeup and value bypass to only communicate with the neighboring column.  Detailed simulations of four-issue single-threaded machines running SPECint2000 show that RingScalar has IPC only 13% lower than an idealized superscalar, while providing large reductions in area, power, and circuit latency.",MIT-CSAIL-TR-2006-066,14 p.; 1561908 bytes; 957204 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saman Amarasinghe,"Zhao, Qin; Rabbah, Rodric; Amarasinghe, Saman; Rudolph, Larry; Wong, Weng-Fai",2006-09-25T21:21:43Z,2006-09-25T21:21:43Z,2006-09-25,http://hdl.handle.net/1721.1/34013,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Ubiquitous Memory Introspection (Preliminary Manuscript),"Modern memory systems play a critical role in the performance ofapplications, but a detailed understanding of the application behaviorin the memory system is not trivial to attain. It requires timeconsuming simulations of the memory hierarchy using long traces, andoften using detailed modeling. It is increasingly possible to accesshardware performance counters to measure events in the memory system,but the measurements remain coarse grained, better suited forperformance summaries than providing instruction level feedback. Theavailability of a low cost, online, and accurate methodology forderiving fine-grained memory behavior profiles can prove extremelyuseful for runtime analysis and optimization of programs.This paper presents a new methodology for Ubiquitous MemoryIntrospection (UMI). It is an online and lightweight mini-simulationmethodology that focuses on simulating short memory access tracesrecorded from frequently executed code regions. The simulations arefast and can provide profiling results at varying granularities, downto that of a single instruction or address. UMI naturally complementsruntime optimizations techniques and enables new opportunities formemory specific optimizations.In this paper, we present a prototype implementation of a runtimesystem implementing UMI. The prototype is readily deployed oncommodity processors, requires no user intervention, and can operatewith stripped binaries and legacy software. The prototype operateswith an average runtime overhead of 20% but this slowdown is only 6%slower than a state of the art binary instrumentation tool.  We used32 benchmarks, including the full suite of SPEC2000 benchmarks, forour evaluation. We show that the mini-simulation results accuratelyreflect the cache performance of two existing memory systems, anIntel Pentium~4 and an AMD Athlon MP (K7) processor. We alsodemonstrate that low level profiling information from the onlinesimulation can serve to identify high-miss rate load instructions with a77% rate of accuracy compared to full offline simulations thatrequired days to complete. The online profiling results are used atruntime to implement a simple software prefetching strategy thatachieves a speedup greater than 60% in the best case.",MIT-CSAIL-TR-2006-067,23 p.; 278689 bytes; 1358949 bytes,application/pdf; application/postscript,en_US,Performance Monitoring; Online Simulation; Runtime Optimization; Cache Modelling; Memory Hierarchy,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Daniel Jackson,"Torlak, Emina; Jackson, Daniel",2006-09-29T21:46:41Z,2006-09-29T21:46:41Z,2006-09-29,http://hdl.handle.net/1721.1/34218,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Design of a Relational Engine,"The key design challenges in the construction of a SAT-based relational engine are described, and novel techniques are proposed to address them.  An efficient engine must have a mechanism for specifying partial solutions, an effective symmetry detection and breaking scheme, and an economical translation from relational to boolean logic.  These desiderata are addressed with three new techniques: a symmetry detection algorithm that works in the presence of partial solutions, a sparse-matrix representation of relations, and a compact representation of boolean formulas inspired by boolean expression diagrams and reduced boolean circuits.  The presented techniques have been implemented and evaluated, with promising results.",MIT-CSAIL-TR-2006-068,11 p.; 444636 bytes; 1204261 bytes,application/pdf; application/postscript,en_US,,Software Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Konwar, K.; Musial, P.M.; Nicolau, N.C.; Shvartsman., A.A.",2006-10-17T16:04:56Z,2006-10-17T16:04:56Z,2006-10-12,http://hdl.handle.net/1721.1/34249,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Implementing Atomic Data through Indirect Learning in Dynamic Network,"Developing middleware services for dynamic distributed systems, e.g., ad-hoc networks, is a challenging task given that suchservices must deal with communicating devices that may join and leave the system, and fail or experience arbitrary delays. Algorithmsdeveloped for static settings are often not usable in dynamic settings because they rely on (logical) all-to-all connectivityor assume underlying routing protocols, which may be unfeasible in highly dynamic settings. This paper explores the indirectlearning approach to information dissemination within a dynamic distributed data service. The indirect learning scheme is usedto improve the liveness of the atomic read/write object service in the settings with uncertain connectivity. The service is formallyproved to be correct, i.e., the atomicity of the objects is guaranteed in all executions. Conditional analysis of the performanceof the new service is presented. This analysis has the potential of being generalized to other similar dynamic algorithms. Underthe assumption that the network is connected, and assuming reasonable timing conditions, the bounds on the duration of theread/write operations of the new service are calculated. Finally, the paper proposes a deployment strategy where indirect learningleads to an improvement in communication costs relative to a previous solution.",MIT-CSAIL-TR-2006-070,21 p.; 482192 bytes; 1655060 bytes,application/pdf; application/postscript,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Patrick Winston,"Finlayson, Mark Alan; Winston, Patrick Henry",2006-11-07T15:36:52Z,2006-11-07T15:36:52Z,2006-11-07,http://hdl.handle.net/1721.1/34635,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Analogical Retrieval via Intermediate Features: The Goldilocks Hypothesis,"Analogical reasoning has been implicated in many important cognitive processes, such as learning, categorization, planning, and understanding natural language. Therefore, to obtain a full understanding of these processes, we must come to a better understanding of how people reason by analogy. Analogical reasoning is thought to occur in at least three stages: retrieval of a source description from memory upon presentation of a target description, mapping of the source description to the target description, and transfer of relationships from source description to target description. Here we examine the first stage, the retrieval of relevant sources from long-term memory for their use in analogical reasoning. Specifically we ask: what can people retrieve from long-term memory, and how do they do it?Psychological experiments show that subjects display two sorts of retrieval patterns when reasoning by analogy: a novice pattern and an expert pattern. Novice-like subjects are more likely to recall superficiallysimilar descriptions that are not helpful for reasoning by analogy. Conversely, expert-like subjects are more likely to recall structurally-related descriptions that are useful for further analogical reasoning. Previous computational models of the retrieval stage have only attempted to model novice-like retrieval. We introduce a computational model that can demonstrate both novice-like and expert-like retrieval with the same mechanism. The parameter of the model that is varied to produce these two types of retrieval is the average size of the features used to identify matches in memory. We find that, in agreement with an intuition from the work of Ullman and co-workers regarding the use of features in visual classification (Ullman, Vidal-Naquet,& Sali, 2002), that features of an intermediate size are most useful for analogical retrieval.We conducted two computational experiments on our own dataset of fourteen formally described stories, which showed that our model gives the strongest analogical retrieval, and is most expert-like, when it uses features that are on average of intermediate size. We conducted a third computational experiment on the Karla the Hawk dataset which showed a modest effect consistent with our predictions. Because our model and Ullman’s work both rely on intermediate-sized features to perform recognition-like tasks, we take both as supporting what we call the Goldilocks hypothesis: that on the average those features that are maximally useful for recognition are neither too small nor too large, neither too simple nor too complex, but rather are in the middle, of intermediate size and complexity.",MIT-CSAIL-TR-2006-071,34 p.; 2808906 bytes; 859593 bytes,application/postscript; application/pdf,en_US,analogical access; precedent retrieval; intermediate features; symbolic computational modelling; mere-appearance; literally-similar,Genesis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Bouillaguet, Charles; Kuncak, Viktor; Wies, Thomas; Zee, Karen; Rinard, Martin",2006-11-09T15:26:55Z,2006-11-09T15:26:55Z,2006-11-09,http://hdl.handle.net/1721.1/34874,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Using First-Order Theorem Provers in the Jahob Data Structure Verification System,"This paper presents our integration of efficient  resolution-based theorem provers into the Jahob data  structure verification system.  Our experimental results  show that this approach enables Jahob to automatically  verify the correctness of a range of complex dynamically  instantiable data structures, including data structures  such as hash tables and search trees, without the need for  interactive theorem proving or techniques tailored to  individual data structures.  Our primary technical results include: (1) a translation  from higher-order logic to first-order logic that enables  the application of resolution-based theorem provers and  (2) a proof that eliminating type (sort) information in  formulas is both sound and complete, even in the presence  of a generic equality operator.  Our  experimental results show that the elimination of   type information dramatically decreases the time required  to prove the resulting formulas.  These techniques enabled us to verify complex correctness  properties of Java programs such as a mutable set  implemented as an imperative linked list, a finite map  implemented as a functional ordered tree, a hash table  with a mutable array, and a simple library system example  that uses these container data structures.  Our system  verifies (in a matter of minutes) that data structure  operations correctly update the finite map, that they  preserve data structure invariants (such as ordering of  elements, membership in appropriate hash table buckets, or  relationships between sets and relations), and that there  are no run-time errors such as null dereferences or array  out of bounds accesses.",MIT-CSAIL-TR-2006-072,32 p.; 397902 bytes; 1759318 bytes,application/pdf; application/postscript,en_US,program verification; shape analysis; multisorted logic,Computer Architecture,,,,,"Short version to appear in VMCAI'07, Nice, January 2007",,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Paris, Sylvain; Durand, Fredo",2006-11-13T18:32:46Z,2006-11-13T18:32:46Z,2006-11-09,http://hdl.handle.net/1721.1/34876,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Fast Approximation of the Bilateral Filter using a Signal Processing Approach,"The bilateral filter is a nonlinear filter that smoothes a signal while preserving strong edges. It has demonstrated great effectiveness for a variety of problems in computer vision and computer graphics, and fast versions have been proposed. Unfortunately, little is known about the accuracy of such accelerations. In this paper, we propose a new signal-processing analysis of the bilateral filter which complements the recent studies that analyzed it as a PDE or as a robust statistical estimator. The key to our analysis is to express the filter in a higher-dimensional space where the signal intensity is added to the original domain dimensions. Importantly, this signal-processing perspective allows us to develop a novel bilateral filtering acceleration using downsampling in space and intensity.  This affords a principled expression of accuracy in terms of bandwidth and sampling. The bilateral filter can be expressed as linear convolutions in this augmented space followed by two simple nonlinearities. This allows us to derive criteria for downsampling the key operations and achieving important acceleration of the bilateral filter. We show that, for the same running time, our method is more accurate than previous acceleration techniques. Typically, we are able to process a 2~megapixel image using our acceleration technique in less than a second, and have the result be visually similar to the exact computation that takes several tens of minutes. The acceleration is most effective with large spatial kernels. Furthermore, this approach extends naturally to color images and cross bilateral filtering.",MIT-CSAIL-TR-2006-073,38 p.; 11564290 bytes; 21570337 bytes,application/pdf; application/postscript,en_US,color image processing; cross bilateral filter; edge-preserving filter,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hari Balakrishnan,"Jung, Jaeyeon; Milito, Rodolfo A.; Paxson, Vern",2006-11-13T18:32:38Z,2006-11-13T18:32:38Z,2006-11-10,http://hdl.handle.net/1721.1/34875,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On the Adaptive Real-Time Detection of Fast-Propagating Network Worms,"We present two light-weight worm detection algorithms thatoffer significant advantages over fixed-threshold methods.The first algorithm, RBS (rate-based sequential hypothesis testing)aims at the large class of worms that attempts to quickly propagate, thusexhibiting abnormal levels of the rate at which hosts initiateconnections to new destinations. The foundation of RBS derives fromthe theory of sequential hypothesis testing, the use of which fordetecting randomly scanning hosts was first introduced by our previouswork with the TRW (Threshold Random Walk) scan detection algorithm. The sequential hypothesistesting methodology enables engineering the detectors to meet falsepositives and false negatives targets, rather than triggering whenfixed thresholds are crossed. In this sense, the detectors that weintroduce are truly adaptive.We then introduce RBS+TRW, an algorithm that combines fan-out rate (RBS)and probability of failure (TRW) of connections to new destinations.RBS+TRW provides a unified framework that at one end acts as a pure RBSand at the other end as pure TRW, and extends RBS's power in detectingworms that scan randomly selected IP addresses.",MIT-CSAIL-TR-2006-074,17 p.; 400578 bytes; 1658364 bytes,application/pdf; application/postscript,en_US,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trevor Darrell,"Morency, Louis-Philippe",2006-11-17T11:12:55Z,2006-11-17T11:12:55Z,2006-11-15,http://hdl.handle.net/1721.1/34893,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Context-based Visual Feedback Recognition,"During face-to-face conversation, people use visual feedback (e.g.,head and eye gesture) to communicate relevant information and tosynchronize rhythm between participants. When recognizing visualfeedback, people often rely on more than their visual perception.For instance, knowledge about the current topic and from previousutterances help guide the recognition of nonverbal cues. The goal ofthis thesis is to augment computer interfaces with the ability toperceive visual feedback gestures and to enable the exploitation ofcontextual information from the current interaction state to improvevisual feedback recognition.We introduce the concept of visual feedback anticipationwhere contextual knowledge from an interactive system (e.g. lastspoken utterance from the robot or system events from the GUIinterface) is analyzed online to anticipate visual feedback from ahuman participant and improve visual feedback recognition. Ourmulti-modal framework for context-based visual feedback recognitionwas successfully tested on conversational and non-embodiedinterfaces for head and eye gesture recognition.We also introduce Frame-based Hidden-state Conditional RandomField model, a new discriminative model for visual gesturerecognition which can model the sub-structure of a gesture sequence,learn the dynamics between gesture labels, and can be directlyapplied to label unsegmented sequences. The FHCRF model outperformsprevious approaches (i.e. HMM, SVM and CRF) for visual gesturerecognition and can efficiently learn relevant contextualinformation necessary for visual feedback anticipation.A real-time visual feedback recognition library for interactiveinterfaces (called Watson) was developed to recognize head gaze,head gestures, and eye gaze using the images from a monocular orstereo camera and the context information from the interactivesystem. Watson was downloaded by more then 70 researchers around theworld and was successfully used by MERL, USC, NTT, MIT Media Lab andmany other research groups.",MIT-CSAIL-TR-2006-075,195 p.; 5912220 bytes; 20231190 bytes,application/pdf; application/postscript,en_US,,Vision,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"McCamant, Stephen; Ernst, Michael D.",2006-11-17T11:12:32Z,2006-11-17T11:12:32Z,2006-11-17,http://hdl.handle.net/1721.1/34892,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Quantitative Information-Flow Tracking for C and Related Languages,"We present a new approach for tracking programs' use of data througharbitrary calculations, to determine how much information about secretinputs is revealed by public outputs.  Using a fine-grained dynamicbit-tracking analysis, the technique measures the information revealedduring a particular execution.  The technique accounts for indirectflows, e.g. via branches and pointer operations.  Two kinds ofuntrusted annotation improve the precision of the analysis.  Animplementation of the technique based on dynamic binary translation isdemonstrated on real C, C++, and Objective C programs of up to half amillion lines of code.  In case studies, the tool checked multiplesecurity policies, including one that was violated by a previouslyunknown bug.",MIT-CSAIL-TR-2006-076,18 p.; 450616 bytes; 1216950 bytes,application/pdf; application/postscript,en_US,Confidentiality; Privacy; Information disclosure; Tainting; Implicit flows; Valgrind; Memcheck; OpenSSH,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sam Madden,"Abadi, Daniel J.; Myers, Daniel S.; DeWitt, David J.; Madden, Samuel R.",2006-11-28T19:34:39Z,2006-11-28T19:34:39Z,2006-11-27,http://hdl.handle.net/1721.1/34929,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Materialization Strategies in a Column-Oriented DBMS,"There has been renewed interest in column-oriented database architectures in recent years. For read-mostly query workloads such as those found in data warehouse and decision support applications, ``column-stores'' have been shown to perform particularly well relative to ``row-stores.'' In order for column-stores to be readily adopted as a replacement for row-stores, however, they must present the same interface to client applications as do row stores, which implies that they must output row-store-style tuples.Thus, the input columns stored on disk must be converted to rows at some point in the query plan, but the optimal point at which to do the conversion is not obvious. This problem can be considered as the opposite of the projection problem in row-store systems: while row-stores need to determine where in query plans to place projection operators to make tuples narrower, column-stores need to determine when to combine single-column projections into wider tuples. This paper describes a variety of strategies for tuple construction and intermediate result representations and provides a systematic evaluation of these strategies.",MIT-CSAIL-TR-2006-078,13 p.; 952582 bytes; 2497163 bytes,application/pdf; application/postscript,en_US,,Database,,,,,Extension of Publication (of the same title) in the Proceedings of ICDE 2007,,,,,,,,,,,,,,,,,,,,,,,
Sam Madden,"Gil, Thomer M.; Madden, Samuel",2006-11-27T14:39:40Z,2006-11-27T14:39:40Z,2006-11-27,http://hdl.handle.net/1721.1/34916,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Scoop: An Adaptive Indexing Scheme for Stored Data in Sensor Networks,"In this paper, we present the design of Scoop, a system for indexing and querying stored data in sensor networks. Scoop works by collecting statistics about the rate of queries and distribution of sensor readings over a sensor network, and uses those statistics to build an index that tells nodes where in the network to store their readings. Using this index, a user’s queries over that stored data can be answered efficiently, without &#64258;ooding those queries throughout the network. This approach offers a substantial advantage over other solutions that either store all data externally on a basestation (requiring every reading to be collected from all nodes), or that store all data locally on the node that produced it (requiring queries to be &#64258;ooded throughout the network). Our results, in fact, show that Scoop offers a factor of four improvement over existing techniques in a real implementation on a 64-node mote-based sensor network. These results also show that Scoop is able to efficciently adapt to changes in the distribution and rates of data and queries.",MIT-CSAIL-TR-2006-077,10 p.; 334459 bytes; 1062863 bytes,application/pdf; application/postscript,en_US,database,Database,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Tzanov, Velin K.",2006-12-05T20:42:13Z,2006-12-05T20:42:13Z,2006-12-05,http://hdl.handle.net/1721.1/34943,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Distributed Area Search with a Team of Robots,"The main goal of this thesis is to demonstrate the applicability of the distributed systems paradigm to robotic systems. This goal is accomplished by presenting two solutions to the Distributed Area Search problem: organizing a team of robots to collaborate in the task of searching through an area. The first solution is designed for unreliable robots equipped with a reliable GPS-style localization system. This solution demonstrates the efficiency and fault-tolerance of this type of distributed robotic systems, as well as their applicability to the real world. We present a theoretically near-optimal algorithm for solving Distributed Area Search under this setting, and we also present an implementation of our algorithm on an actual system, consisting of twelve robots. The second solution is designed for a completely autonomous system, without the aid of any centralized subsystem. It demonstrates how a distributed robotic system can solve a problem that is practically unsolvable for a single-robot system.",MIT-CSAIL-TR-2006-079,111 p.; 90018576 bytes; 2179689 bytes,application/postscript; application/pdf,en_US,robotics; theory; distributed systems,AIRE,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"O'Donnell, Charles W.; Suh,, G. Edward; Dijk, Marten vn; Devadas, Srinivas",2006-12-08T16:51:48Z,2006-12-08T16:51:48Z,2006-12-08,http://hdl.handle.net/1721.1/34954,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Memoization Attacks and Copy Protection in Partitioned Applications,"Application source code protection is a major concern for software architects today. Secure platforms have been proposed that protect the secrecy of application algorithms and enforce copy protection assurances. Unfortunately, these capabilities incur a sizeable performance overhead. Partitioning an application into secure and insecure regions can help diminish these overheads but invalidates guarantees of code secrecy and copy protection.This work examines one of the problems of partitioning an application into public and private regions, the ability of an adversary to recreate those private regions. To our knowledge, it is the first to analyze this problem when considering application operation as a whole. Looking at the fundamentals of the issue, we analyze one of the simplest attacks possible, a ``Memoization Attack.'' We implement an efficient Memoization Attack and discuss necessary techniques that limit storage and computation consumption. Experimentation reveals that certain classes of real-world applications are vulnerable to Memoization Attacks. To protect against such an attack, we propose a set of indicator tests that enable an application designer to identify susceptible application code regions.",MIT-CSAIL-TR-2006-080,11 p.; 436268 bytes; 1481611 bytes,application/pdf; application/postscript,en_US,Security; Architecture; Trusted Execution; Security Analysis; Digital Rights Management,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shafi Goldwasser,"Pass, Rafael; Shelat, Abhi; Vaikuntanathan, Vinod",2006-12-14T14:51:47Z,2006-12-14T14:51:47Z,2006-12-14,http://hdl.handle.net/1721.1/34968,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Bounded CCA2-Secure Non-Malleable Encryption,"Under an adaptive chosen ciphertext attack (CCA2), the security of an encryption scheme must hold against  adversaries that have access to a decryption oracle.   We consider a weakening of CCA2 security, wherein security need only hold against  adversaries making an a-priori bounded number of queries to the decryption oracle. Concerning this notion, which we call bounded-CCA2 security, we show the following two results.  (1) Bounded-CCA2 secure non-malleable encryption schemes exist if and only if semantically-secure (IND-CPA-secure) encryption schemes exist.(As far as we know, bounded-CCA2 non-malleability is the strongest notion of security known to be satisfiable assuming only the existence of semantically-secure encryption schemes.)  (2) In contrast to CCA2 security, bounded-CCA2 security alone does not imply non-malleability.  In particular, if there exists an encryption scheme that is bounded-CCA2 secure, then there exists another encryption scheme which remains   bounded-CCA2 secure, but is malleable under a simple chosen-plaintext attack.",MIT-CSAIL-TR-2006-081,17 p.; 1222225 bytes; 239045 bytes,application/postscript; application/pdf,en_US,Public-key Encryption; Non-Malleability; Chosen Ciphertext Security,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hal Abelson,"Abelson, Hal; Beal, Jacob; Sussman, Gerald Jay",2007-06-01T18:01:47Z,2007-06-01T18:01:47Z,2007,http://hdl.handle.net/1721.1/37591,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Amorphous Computing,"The goal of amorphous computing is to identify organizationalprinciples and create programming technologies for obtainingintentional, pre-specified behavior from the cooperation of myriadunreliable parts that are arranged in unknown, irregular, andtime-varying ways.  The heightened relevance of amorphous computingtoday stems from the emergence of new technologies that could serve assubstrates for information processing systems of immense power atunprecedentedly low cost, if only we could master the challenge ofprogramming them.  This document is a review of amorphous computing.",MIT-CSAIL-TR-2007-030,33 p.,,,,Mathematics and Computation,,,,,preprint of article for Springer-Verlag Encyclopedia of Complexity and System Science,,,,,,,,,,,,,,,,,,,,,,,
Gerald Jay Sussman,"Bachrach, Jonathan; Beal, Jacob",2009-06-25T21:30:10Z,2009-06-25T21:30:10Z,2007,http://hdl.handle.net/1721.1/45652,MIT-CSAIL-TR-2009-032,Programming Manifolds,"Many programming domains involve the manipulation of values distributed through a manifold - examples include sensor networks, smart materials, and biofilms. This paper describes a programming semantics for manifolds based on the amorphous medium abstraction, which places a computational device at every point in the manifold. This abstraction enables the creation of programs that automatically scale to networks of different size and device density. This semantics is currently implemented in our language Proto and compiles for execution on Mica2 Motes and several other platforms.",,15 p.,,,amorphous computing; spatial computing,Mathematics and Computation,,Creative Commons Attribution-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,"Updated version; original published in Computing Media and Languages for Space-Oriented Computation, Dagstuhl Seminar 06361, September 2006",,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Valiant, Paul",2008-06-30T13:00:26Z,2008-06-30T13:00:26Z,2007- 11-02,http://hdl.handle.net/1721.1/41872,,Revenue in Truly Combinatorial Auctions and Adversarial Mechanism Design,"Little is known about generating revenue in UNRESTRICTED combinatorial auctions. (In particular, the VCG mechanism has no revenue guarantees.) In this paper we determine how much revenue can be guaranteed in such auctions. Our analysis holds both in the standard model, when all players are independent and rational, as well as in a most adversarial model, where some players may bid collusively or even totally irrationally.",MIT-CSAIL-TR-2008-039,30 p.,,,Revenue Benchmarks; Revenue lowerbounds; revenue upperbounds; Probabilistic DST mechanisms,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Martin Rinard,"Kuncak, Viktor",2007-01-02T20:21:50Z,2007-01-02T20:21:50Z,2007-01-01,http://hdl.handle.net/1721.1/35258,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Quantifier-Free Boolean Algebra with Presburger Arithmetic is NP-Complete,"Boolean Algebra with Presburger Arithmetic (BAPA) combines1) Boolean algebras of sets of uninterpreted elements (BA)and 2) Presburger arithmetic operations (PA).  BAPA canexpress the relationship between integer variables andcardinalities of unbounded finite sets and can be used toexpress verification conditions in verification of datastructure consistency properties.In this report I consider the Quantifier-Free fragment ofBoolean Algebra with Presburger Arithmetic (QFBAPA).Previous algorithms for QFBAPA had non-deterministicexponential time complexity.  In this report I show thatQFBAPA is in NP, and is therefore NP-complete.  My resultyields an algorithm for checking satisfiability of QFBAPAformulas by converting them to polynomially sized formulasof quantifier-free Presburger arithmetic.  I expect thisalgorithm to substantially extend the range of QFBAPAproblems whose satisfiability can be checked in practice.",MIT-CSAIL-TR-2007-001,14 p.; 315999 bytes; 842090 bytes,application/pdf; application/postscript,en_US,Caratheodory theorem; integer linear programming; integer cone; Hilbert basis,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trevor Darrell,"Morency, Louis-Philippe; Quattoni, Ariadna; Darrell, Trevor",2007-01-08T01:16:48Z,2007-01-08T01:16:48Z,2007-01-07,http://hdl.handle.net/1721.1/35276,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Latent-Dynamic Discriminative Models for Continuous Gesture Recognition,"Many problems in vision involve the prediction of a class label for each frame in an unsegmented sequence. In this paper we develop a discriminative framework for simultaneous sequence segmentation and labeling which can capture both intrinsic and extrinsic class dynamics. Our approach incorporates hidden state variables which model the sub-structure of a class sequence and learn the dynamics between class labels. Each class label has a disjoint set of associated hidden states, which enables efficient training and inference in our model. We evaluated our method on the task of recognizing human gestures from unsegmented video streams and performed experiments on three different datasets of head and eye gestures. Our results demonstrate that our model for visual gesture recognition outperform models based on Support Vector Machines, Hidden Markov Models, and Conditional Random Fields.",MIT-CSAIL-TR-2007-002,8 p.; 366380 bytes; 946243 bytes,application/pdf; application/postscript,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Krste Asanovic,"Batten, Christopher; Krashinsky, Ronny; Asanovic, Krste",2007-01-12T19:51:55Z,2007-01-12T19:51:55Z,2007-01-12,http://hdl.handle.net/1721.1/35724,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Scale Control Processor Test-Chip,"We are investigating vector-thread architectures which provide competitive performance and efficiency across a broad class of application domains. Vector-thread architectures unify data-level, thread-level, and instruction-level parallelism, providing new ways of parallelizing codes that are difficult to vectorize or that incur excessive synchronization costs when multithreaded. To illustrate these ideas we have developed the Scale processor, which is an example of a vector-thread architecture designed for low-power and high-performance embedded systems. The prototype includes a single-issue 32-bit RISC control processor, a vector-thread unit which supports up to 128 virtual processor threads and can execute up to 16 instructions per cycle, and a 32 KB shared primary cache.Since the Scale Vector-Thread Processor is a large and complex design (especially for an academic project), we first designed and fabricated the Scale Test Chip (STC1). STC1 includes a simplified version of the Scale control processor, 8 KB of RAM, a host interface, and a custom clock generator.  STC1 helped mitigate the risk involved in fabricating the full Scale chip in several ways. First, we were able to establish and test our CAD toolflow. Our toolflow included several custom tools which had not previously been used in any tapeouts. Second, we were able to better characterize our target package and process. For example, STC1 enabled us to better correlate the static timing numbers from our CAD tools with actual silicon and also to characterize the expected rise/fall times of our external signal pins. Finally, STC1 allowed us to test our custom clock generator. We used our experiences with STC1 to help us implement the Scale vector-thread processor. Scale was taped out on October 15, 2006 and it is currently being fabricated through MOSIS. This report discusses the fabrication of STC1 and presents power and performance results.",MIT-CSAIL-TR-2007-003,10 p.; 2613150 bytes; 4101404 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rodney Brooks,"Edsinger, Aaron",2007-01-17T02:59:49Z,2007-01-17T02:59:49Z,2007-01-16,http://hdl.handle.net/1721.1/35727,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Robot Manipulation in Human Environments,"Human environments present special challenges for robot manipulation. They are often dynamic, difficult to predict, and beyond the control of a robot engineer. Fortunately, many characteristics of these settings can be used to a robot's advantage. Human environments are typically populated by people, and a robot can rely on the guidance and assistance of a human collaborator. Everyday objects exhibit common, task-relevant features that reduce the cognitive load required for the object's use. Many tasks can be achieved through the detection and control of these sparse perceptual features. And finally, a robot is more than a passive observer of the world. It can use its body to reduce its perceptual uncertainty about the world.In this thesis we present advances in robot manipulation that address the unique challenges of human environments. We describe the design of a humanoid robot named Domo, develop methods that allow Domo to assist a person in everyday tasks, and discuss general strategies for building robots that work alongside people in their homes and workplaces.",MIT-CSAIL-TR-2007-004,228 p.; 772900059 bytes; 30793533 bytes,application/postscript; application/pdf,en_US,embodied intelligence; behavior-based,Humanoid Robotics,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Tommi Jaakkola,"Monteleoni, Claire; Kaariainen, Matti",2007-01-24T12:56:52Z,2007-01-24T12:56:52Z,2007-01-23,http://hdl.handle.net/1721.1/35784,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Online Active Learning in Practice,"We compare the practical performance of several recently proposed algorithms for active learning in the online setting.  We consider two algorithms (and their combined variants) that are strongly online, in that they do not store any previously labeled examples, and for which formal guarantees have recently been proven under various assumptions.  We perform an empirical evaluation on optical character recognition (OCR) data, an application that we argue to be appropriately served by online active learning.  We compare the performance between the algorithm variants and show significant reductions in label-complexity over random sampling.",MIT-CSAIL-TR-2007-005,9 p.; 2188813 bytes; 767442 bytes,application/postscript; application/pdf,en_US,online learning; active learning; selective sampling; optical character recognition; OCR,Tommi's Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rodney Brooks,"Kumpf, Adam",2007-01-31T12:27:15Z,2007-01-31T12:27:15Z,2007-01-30,http://hdl.handle.net/1721.1/35821,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Explorations in Low-Cost Compliant Robotics,"This thesis presents the findings of exploratory research in low-cost compliant robotics.  The most heavily leveraged trade-off is that of mechanical precision for computational power, with the hope that the price of future computation will continue to fall exponentially while the expected price of precision mechanical parts will remain relatively constant.  The most novel contribution of this research is the Torsionally Compliant Elastomer Joint (TCEJ) which allows for compliance and sensing in a very small package while using extremely inexpensive components.  Computational modeling of hysteresis, signal compression, and backlash are also explored to compensate for the non-idealities often found in cheap mechanical parts.  Three proof-of-concept systems are described along with a set of experiments used to test their capabilities.  Finally, future work is proposed that will likely shape the next generation of low-cost compliant robotics.",MIT-CSAIL-TR-2007-006,99 p.; 89615040 bytes; 4874283 bytes; 2654023 bytes,application/postscript; video/quicktime; application/pdf,en_US,,Humanoid Robotics,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Peter Szolovits,"McGeachie, Michael",2007-06-01T11:41:48Z,2007-06-01T11:41:48Z,2007-02-01,http://hdl.handle.net/1721.1/37590,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Local Geometry of Multiattribute Tradeoff Preferences,"Existing preference reasoning systems have been successful insimple domains. Broader success requires more natural and moreexpressive preference representations.  This thesis develops arepresentation of logical preferences that combines numericaltradeoff ratios between partial outcome descriptions withqualitative preference information. We argue our system is uniqueamong preference reasoning systems; previous work has focused onqualitative or quantitative preferences, tradeoffs, exceptions andgeneralizations, or utility independence, but none have combinedall of these expressions under a unified methodology.We present new techniques for representing and giving meaning toquantitative tradeoff statements between different outcomes.  Thetradeoffs we consider can be multi-attribute tradeoffs relatingmore than one attribute at a time, they can refer to discrete orcontinuous domains, be conditional or unconditional, andquantified or qualitative.  We present related methods ofrepresenting judgments of attribute importance.  We then buildupon a methodology for representing arbitrary qualitative ceteris paribuspreference, or preferences ``other things being equal,"" aspresented in MD04.  Tradeoff preferences inour representation are interpreted as constraints on the partialderivatives of the utility function. For example, a decision makercould state that ``Color is five times as important as price,availability, and time,"" a sentiment one might express in thecontext of repainting a home, and this is interpreted asindicating that utility increases in the positive color directionfive times faster than utility increases in the positive pricedirection.  We show that these representations generalize both theeconomic notion of marginal rates of substitution and previousrepresentations of preferences in AI.",MIT-CSAIL-TR-2007-029,129 p.,,,Decision Making; Preference Reasoning; Utility Functions,Clinical Decision-Making,,,,PhD thesis,"McGeachie, Michael.  PhD Thesis, MIT, 2007.",,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Rifkin, Ryan; Bouvrie, Jake; Schutte, Ken; Chikkerur, Sharat; Kouh, Minjoon; Ezzat, Tony; Poggio, Tomaso",2007-02-01T18:26:47Z,2007-02-01T18:26:47Z,2007-02-01,http://hdl.handle.net/1721.1/35835,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Phonetic Classification Using Hierarchical, Feed-forward, Spectro-temporal Patch-based Architectures","A preliminary set of experiments are described in which a biologically-inspired computer vision system (Serre, Wolf et al. 2005; Serre 2006; Serre, Oliva et al. 2006; Serre, Wolf et al. 2006) designed for visual object recognition was applied to the task of phonetic classification. During learning, the systemprocessed 2-D wideband magnitude spectrograms directly as images, producing a set of 2-D spectrotemporal patch dictionaries at different spectro-temporal positions, orientations, scales, and of varying complexity. During testing, features were computed by comparing the stored patches with patches fromnovel spectrograms. Classification was performed using a regularized least squares classifier (Rifkin, Yeo et al. 2003; Rifkin, Schutte et al. 2007) trained on the features computed by the system. On a 20-class TIMIT vowel classification task, the model features achieved a best result of 58.74% error, compared to 48.57% error using state-of-the-art MFCC-based features trained using the same classifier. This suggests that hierarchical, feed-forward, spectro-temporal patch-based architectures may be useful for phoneticanalysis.",MIT-CSAIL-TR-2007-007; CBCL-266,16 p.; 2265616 bytes; 383591 bytes,application/postscript; application/pdf,en_US,phonetic classification; hierarchical models; regularized least-squares; spectrotemporal patches,Center for Biological and Computational Learning (CBCL),,,,,,,,,http://hdl.handle.net/1721.1/36865,http://hdl.handle.net/1721.1/36865,,,,,,,,,,,,,,,,,,
Hari Balakrishnan,"Jamieson, Kyle; Balakrishnan, Hari",2007-02-13T06:19:09Z,2007-02-13T06:19:09Z,2007-02-02,http://hdl.handle.net/1721.1/35889,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,PPR: Partial Packet Recovery for Wireless Networks,"Bit errors occur over wireless channels when the signal isn't strongenough to overcome the effects of interference and noise.  Currentwireless protocols may use forward error correction (FEC) to correct forsome (small) number of bit errors, but generally retransmit the wholepacket if the FEC is insufficient.  We observe that current wirelessmesh network protocols retransmit a number of packets and that most ofthese retransmissions end up sending bits that have already beenreceived multiple times, wasting network capacity.  To overcome thisinefficiency, we develop, implement, and evaluate a partial packetrecovery (PPR) system.PPR incorporates three new ideas: (1) SoftPHY, an expandedphysical layer (PHY) interface to provide hints to the higher layersabout how ``close'' the actual received symbol was to the one decoded,(2) a postamble scheme to recover data even when a packet'spreamble is corrupted and not decodable at the receiver, and (3) PP-ARQ, an asynchronous link-layer retransmission protocol that allowsa receiver to compactly encode and request for retransmission only thoseportions of a packet that are likely in error.Our experimental results from a 27-node 802.15.4 testbed that includesTelos motes with 2.4 GHz Chipcon radios and GNU Radio nodes implementingthe Zigbee standard (802.15.4) show that PPR increases the framedelivery rate by a factor of 2x under moderate load, and7x under heavy load when many links have marginal quality.",MIT-CSAIL-TR-2007-008,27 p.,,,802.11; wireless networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Barbara Liskov,"Cowling, James; Myers, Daniel; Liskov, Barbara; Rodrigues, Rodrigo; Shrira, Liuba",2007-02-13T06:17:18Z,2007-02-13T06:17:18Z,2007-02-12,http://hdl.handle.net/1721.1/35888,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,HQ Replication: Properties and Optimizations,"There are currently two approaches to providing Byzantine-fault-tolerant state machine replication: a replica-based approach, e.g., BFT, that uses communication between replicas to agree on a proposed ordering of requests, and a quorum-based approach, such as Q/U, in which clients contact replicas directly to optimistically execute operations. Both approaches have shortcomings: the quadratic cost of inter-replica communication is unnecessary when there is no contention, and Q/U requires a large number of replicas and performs poorly under contention.We present HQ, a hybrid Byzantine-fault-tolerant state machine replication protocol that overcomes these problems. HQ employs a lightweight quorum-based protocol when there is no contention, but  uses BFT to resolve contention when it arises.  Furthermore, HQ uses only 3f+1 replicas to tolerate f faults, providing optimal resilience to node failures.We implemented a prototype of HQ, and we compare its performance to BFT and Q/U analytically and experimentally. Additionally, in this work we use a new implementation of BFT designed to scale as the number of faults increases.  Our results show that both HQ and our new implementation of BFT scale as f increases; additionally our hybrid approach of using BFT to handle contention works well.",MIT-CSAIL-TR-2007-009,18 p.,,,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Marthi, Bhaskara",2007-02-13T19:01:57Z,2007-02-13T19:01:57Z,2007-02-13,http://hdl.handle.net/1721.1/35890,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic shaping and decomposition of reward functions,"This paper investigates the problem of automatically learning how torestructure the reward function of a Markov decision process so as tospeed up reinforcement learning.  We begin by describing a method thatlearns a shaped reward function given a set of state and temporalabstractions.  Next, we consider decomposition of the per-timestepreward in multieffector problems, in which the overall agent can bedecomposed into multiple units that are concurrently carrying outvarious tasks.  We show by example that to find a good rewarddecomposition, it is often necessary to first shape the rewardsappropriately.  We then give a function approximation algorithm forsolving both problems together.  Standard reinforcement learningalgorithms can be augmented with our methods, and we showexperimentally that in each case, significantly faster learningresults.",MIT-CSAIL-TR-2007-010,8 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2007-02-20T13:41:48Z,2007-02-20T13:41:48Z,2007-02-16,http://hdl.handle.net/1721.1/35918,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Task-Structured Probabilistic I/O Automata to Analyze an Oblivious Transfer Protocol,"The Probabilistic I/O Automata framework of Lynch, Segala and Vaandrager provides tools for precisely specifying protocols and reasoning about their correctness using multiple levels of abstraction, based on implementation relationships between these levels. We enhance this framework to allow analyzing protocols that use cryptographic primitives. This requires resolving and reconciling issues such as nondeterministic behavior and scheduling, randomness, resource-bounded computation, and computational hardness assumptions. The enhanced framework allows for more rigorous and systematic analysis of cryptographic protocols. To demonstrate the use of this framework, we present an example analysis that we have done for an Oblivious Transfer protocol.",MIT-CSAIL-TR-2007-011,98 p.,,,,Theory of Computation,,,,,,,,,,http://hdl.handle.net/1721.1/33217,http://hdl.handle.net/1721.1/33217,,,,,,,,,,,,,,,,,
Dina Katabi,"Katti, Sachin; Cohen, Jeffrey; Katabi, Dina",2007-02-23T23:21:45Z,2007-02-23T23:21:45Z,2007-02-23,http://hdl.handle.net/1721.1/36344,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Information Slicing: Anonymity Using Unreliable Overlays,"This paper proposes a new approach to anonymous communication called information slicing. Typically, anonymizers use onion routing, where a message is encrypted in layers with the public keys of the nodes along the path. Instead, our approach scrambles the message, divides it into pieces, and sends the pieces along disjoint paths. We show that information slicing addresses message confidentiality as well as source and destination anonymity. Surprisingly, it does not need any public key cryptography. Further, our approach naturally addresses the problem of node failures. These characteristics make it a good fit for use over dynamic peer-to-peer overlays. We evaluate the anonymity ofinformation slicing via analysis and simulations.  Our prototype implementation on PlanetLab shows that it achieves higher throughput than onion routing and effectively copes with node churn.",MIT-CSAIL-TR-2007-013,15 p,,,Privacy; Security; Overlay Networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Katti, Sachin; Gollakota, Shyamnath; Katabi, Dina",2007-02-23T23:01:46Z,2007-02-23T23:01:46Z,2007-02-23,http://hdl.handle.net/1721.1/36343,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Embracing Wireless Interference: Analog Network Coding,"Traditionally, interference is considered harmful.Wireless networks strive to avoid scheduling multiple transmissions at the same time in order to prevent interference. This paper adopts the opposite approach; it encourages strategically picked senders to interfere. Instead of forwarding packets,routers forward the interfering signals. The destination leverages network-level information to cancel the interference and recover the signal destined to it. The result is analog network coding because it codes signals not bits. So, what if wireless routers forward signals instead of packets? Theoretically, we prove that such an approach doubles the capacity of the canonical relay network. Surprisingly, it is also practical. We implement our design using softwareradios and show that it achieves significantly higher throughput than both traditional wireless routing and prior work on wireless network coding.",MIT-CSAIL-TR-2007-012,14 p,,,Network Coding; Wireless Networks; Software Radios,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Chachulski, Szymon; Jennings, Michael; Katti, Sachin; Katabi, Dina",2007-02-23T23:23:30Z,2007-02-23T23:23:30Z,2007-02-23,http://hdl.handle.net/1721.1/36345,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Trading Structure for Randomness in Wireless Opportunistic Routing,"Opportunistic routing is a recent technique that achieves high throughput in the face of lossy wireless links. The current opportunistic routing protocol, ExOR, ties the MAC with routing, imposing a strict schedule on routers' access to the medium. Although the scheduler delivers opportunistic gains, it misses some of the inherent features of the 802.11 MAC. For example, it prevents spatial reuse and thus may underutilize the wireless medium.  It also eliminates the layering abstraction, making the protocol less amenable to extensions of alternate traffic type such as multicast.This paper presents MORE, a MAC-independent opportunistic routing protocol. MORE randomly mixes packets before forwarding them. This randomness ensures that routers that hear the same transmission do not forward the same packets. Thus, MORE needs no special scheduler to coordinate routers and can run directly on top of 802.11. Experimental results from a 20-node wireless testbed show that MORE's average unicast throughput is 20% higher than ExOR, and the gains rise to 50% over ExOR when there is a chance of spatial reuse. For multicast, MORE's gains increase with the number of destinations, and are 35-200% greater than ExOR.",MIT-CSAIL-TR-2007-014,14 p.,,,Network Coding; Opportunistic Routing; Wireless Networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Izmalkov, Sergei; Lepinski, Matt; Micali, Silvio",2008-05-08T16:30:24Z,2008-05-08T16:30:24Z,2007-03,http://hdl.handle.net/1721.1/41527,,Perfect Implementation of Normal-Form Mechanisms,"Privacy and trust affect our strategic thinking, yet they have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism ---by disregarding the players' privacy, or assuming trust in a mediator--- may not be realistic and fail to reach the mechanism's objectives. We thus investigate implementations of a new type.We put forward the notion of a perfect implementation of a normal-form mechanism M: in essence, an extensive-form mechanism exactly preserving all strategic properties of M, WITHOUT relying on a trusted mediator or violating the privacy of the players. We prove that ANY normal-form mechanism can be perfectly implemented by a PUBLIC mediator using envelopes and an envelope-randomizing device (i.e., the same tools used for running fair lotteries or tallying secret votes). Differently from a trusted mediator, a public one only performs prescribed public actions, so that everyone can verify that he is acting properly, and never learns any information that should remain private.",MIT-CSAIL-TR-2008-028,42 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Rodney Brooks,"Torres-Jara, Eduardo",2007-03-03T15:01:47Z,2007-03-03T15:01:47Z,2007-03-02,http://hdl.handle.net/1721.1/36371,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Sensitive Manipulation,"This thesis presents an effective alternative to the traditionalapproach to robotic manipulation. In our approach, manipulation ismainly guided by tactile feedback as opposed to vision. Themotivation comes from the fact that manipulating an object impliescoming in contact with it, consequently, directly sensing physicalcontact seems more important than vision to control theinteraction of the object and the robot. In this work, thetraditional approach of a highly precise arm and vision systemcontrolled by a model-based architecture is replaced by one thatuses a low mechanical impedance arm with dense tactile sensing andexploration capabilities run by a behavior-based architecture.The robot OBRERO has been built to implement this approach. Newtactile sensing technology has been developed and mounted on therobot's hand. These sensors are biologically inspired and presentmore adequate features for manipulation than those of state of theart tactile sensors. The robot's limb was built with compliantactuators, which present low mechanical impedance, to make theinteraction between the robot and the environment safer than thatof a traditional high-stiffness arm. A new actuator was created tofit in the hand size constraints. The reduced precision ofOBRERO's limb is compensated by the capability of explorationgiven by the tactile sensors, actuators and the softwarearchitecture.The success of this approach is shown by picking up objects in anunmodelled environment. This task, simple for humans, has been achallenge for robots. The robot can deal with new, unmodelledobjects. OBRERO can come gently in contact, explore, lift, andplace the object in a different location. It can also detectslippage and external forces acting on an object while it is held.Each one of these steps are done by using tactile feedback. Thistask can be done with very light objects with no fixtures and onslippery surfaces.",MIT-CSAIL-TR-2007-015,172 p.,,,Manipulation; Compliant tactile sensors; Compliant hand; Small series elastic actuators,Humanoid Robotics,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Block, Stephen",2007-03-06T12:41:49Z,2007-03-06T12:41:49Z,2007-03-05,http://hdl.handle.net/1721.1/36372,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Distributed Method Selection and Dispatching of Contingent, Temporally Flexible Plans","Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and to uncertainty. Previous work developed contingent, temporally flexible plans. These plans provide robustness to uncertain activity durations, through flexible timing constraints, and robustness to plan failure, through alternate approaches to achieving a task. Robust execution of contingent, temporally flexible plans consists of two phases. First, in the plan extraction phase, the executive chooses between the functionally redundant methods in the plan to select an execution sequence that satisfies the temporal bounds in the plan. Second, in the plan execution phase, the executive dispatches the plan, using the temporal flexibility to schedule activities dynamically.Previous contingent plan execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. Likewise, a computation bottleneck may also occur because a single agent conducts all processing.This thesis introduces a robust, distributed executive for temporally flexible plans, called Distributed-Kirk, or D-Kirk. To execute a plan, D-Kirk first distributes the plan between the participating agents, by creating a hierarchical ad-hoc network and by mapping the plan onto this hierarchy. Second, the plan is reformulated using a distributed, parallel algorithm into a form amenable to fast dispatching. Finally, the plan is dispatched in a distributed fashion.We then extend the D-Kirk distributed executive to handle contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. A temporally consistent plan is extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN.At all stages of D-Kirk, the communication load is spread over all agents, thus eliminating the communication bottleneck. In particular, D-Kirk reduces the peak communication complexity of the plan execution phase by a factor of O(A/e'), where e' is the number of edges per node in the dispatchable plan, determined by the branching factor of the input plan, and A is the number of agents involved in executing the plan.In addition, the distributed algorithms employed by D-Kirk reduce the computational load on each agent and provide opportunities for parallel processing, thus increasing efficiency. In particular, D-Kirk reduces the average computational complexity of plan dispatching from O(eN^3) in the centralized case, to typical values of O(eN^2) per node and O(eN^3/A) per agent in the distributed case, where N is the number of nodes in the plan and e is the number of edges per node in the input plan.Both of the above results were confirmed empirically using a C++ implementation of D-Kirk on a set of parameterized input plans. The D-Kirk implementation was also tested in a realistic application where it was used to control a pair of robotic manipulators involved in a cooperative assembly task.",MIT-CSAIL-TR-2007-016,178 p.,,,,Model-based Embedded and Robotic Systems,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Bachrach, Jonathan; Beal, Jacob",2007-03-14T18:21:48Z,2007-03-14T18:21:48Z,2007-03-14,http://hdl.handle.net/1721.1/36840,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Building Spatial Computers,"Programmability is a major challenge in spatial computing, anaggregate control problem found in domains such as sensor networks,swarm robotics, and modular robotics.  We address this challenge witha model of a spatial computer (Proto Abstract Machine) and adistributed operating system, ProtoKernel, which implements PAMapproximately.  ProtoKernel has been demonstrated on platforms inthree spatial computing domains: sensor networks, swarm robotics, andmodular robotics.",MIT-CSAIL-TR-2007-017,5 p.,,,amorphous medium; amorphous computing,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Zibin, Yoav; Potanin, Alex; Artzi, Shay; Kiezun, Adam; Ernst, Michael D.",2007-03-16T21:01:46Z,2007-03-16T21:01:46Z,2007-03-16,http://hdl.handle.net/1721.1/36850,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Object and Reference Immutability using Java Generics,"A compiler-checked immutability guarantee provides useful documentation, facilitates reasoning, and enables optimizations. This paper presents Immutability Generic Java (IGJ), a novel language extension that expresses immutability without changing Java’s syntax by building upon Java’s generics and annotation mechanisms. In IGJ, each class has one additional generic parameter that is Immutable, Mutable, or ReadOnly. IGJ guarantees both reference immutability (only mutable references can mutate an object) and object immutability (an immutable reference points to an immutable object). IGJ is the first proposal for enforcing object immutability, and its reference immutability is more expressive than previous work. IGJ also permits covariant changes of generic arguments in a type-safe manner, e.g., a readonly list of integers is a subtype of a readonly list of numbers. IGJ extends Java’s type system with a few simple rules. We formalize this type system and prove it sound. Our IGJ compiler works by type-erasure and generates byte-code that can be executed on any JVM without runtime penalty.",MIT-CSAIL-TR-2007-018,12 p.,,,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Rifkin, Ryan; Bouvrie, Jake; Schutte, Ken; Chikkerur, Sharat; Kouh, Minjoon; Ezzat, Tony; Poggio, Tomaso",2007-03-22T11:21:47Z,2007-03-22T11:21:47Z,2007-03-21,http://hdl.handle.net/1721.1/36865,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Phonetic Classification Using Hierarchical, Feed-forward, Spectro-temporal Patch-based Architectures","A preliminary set of experiments are described in which a biologically-inspired computer vision system (Serre, Wolf et al. 2005; Serre 2006; Serre, Oliva et al. 2006; Serre, Wolf et al. 2006) designed for visual object recognition was applied to the task of phonetic classification. During learning, the systemprocessed 2-D wideband magnitude spectrograms directly as images, producing a set of 2-D spectrotemporal patch dictionaries at different spectro-temporal positions, orientations, scales, and of varying complexity. During testing, features were computed by comparing the stored patches with patches fromnovel spectrograms. Classification was performed using a regularized least squares classifier (Rifkin, Yeo et al. 2003; Rifkin, Schutte et al. 2007) trained on the features computed by the system. On a 20-classTIMIT vowel classification task, the model features achieved a best result of 58.74% error, compared to 48.57% error using state-of-the-art MFCC-based features trained using the same classifier. This suggests that hierarchical, feed-forward, spectro-temporal patch-based architectures may be useful for phonetic analysis.",MIT-CSAIL-TR-2007-019; CBCL-267,17 p.,,,phonetic classification; hierarchical models; regularized least-squares; spectrotemporal patches,Center for Biological and Computational Learning (CBCL),,,,,,,,,,http://hdl.handle.net/1721.1/35835,http://hdl.handle.net/1721.1/35835,,,,,,,,,,,,,,,,,
Michael Ernst,"Artzi, Shay; Kiezun, Adam; Glasser, David; Ernst, Michael D.",2007-03-26T11:21:46Z,2007-03-26T11:21:46Z,2007-03-23,http://hdl.handle.net/1721.1/36880,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combined Static and Dynamic Mutability Analysis,"Knowing which method parameters may be mutated during a method's execution is useful for many software engineering tasks. We present an approach to discovering parameter immutability, in which several lightweight, scalable analyses are combined in stages, with each stage rening the overall result. The resulting analysis is scalable and combines the strengths of its component  analyses. As one of the component analyses, we present a novel, dynamic mutability analysis and show how its results can be improved by random input generation. Experimental results on programs of up to 185 kLOC show that, compared to previous approaches, our approach increases both scalability and overall accuracy.",MIT-CSAIL-TR-2007-020,17 p.,,,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trevor Darrell,"Urtasun, Raquel; Darrell, Trevor",2007-03-29T11:21:46Z,2007-03-29T11:21:46Z,2007-03-28,http://hdl.handle.net/1721.1/36901,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Discriminative Gaussian Process Latent Variable Model for Classification,"Supervised learning is difficult with high dimensional input spacesand very small training sets, but accurate classification may bepossible if the data lie on a low-dimensional manifold.  GaussianProcess Latent Variable Models can discover low dimensional manifoldsgiven only a small number of examples, but learn a latent spacewithout regard for class labels.  Existing methods for discriminativemanifold learning (e.g., LDA, GDA) do constrain the class distributionin the latent space, but are generally deterministic and may notgeneralize well with limited training data.  We introduce a method forGaussian Process Classification using latent variable models trainedwith discriminative priors over the latent space, which can learn adiscriminative latent space from a small training set.",MIT-CSAIL-TR-2007-021,8 p.,,,Gaussian Processes; Classification; Latent Variable Models; Machine Learning,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rodney Brooks,"Aryananda, Lijin",2007-04-04T17:21:51Z,2007-04-04T17:21:51Z,2007-04-03,http://hdl.handle.net/1721.1/37144,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Few Days of A Robot's Life in the Human's World: Toward Incremental Individual Recognition,"This thesis presents an integrated framework and implementation for Mertz, an expressive robotic creature for exploring the task of face recognition through natural interaction in an incremental and unsupervised fashion.  The goal of this thesis is to advance toward a framework which would allow robots to incrementally ``get to know'' a set of familiar individuals in a natural and extendable way.  This thesis is motivated by the increasingly popular goal of integrating robots in the home.  In order to be effective in human-centric tasks, the robots must be able to not only recognize each family member, but also to learn about the roles of various people in the household.In this thesis, we focus on two particular limitations of the current technology.  Firstly, most of face recognition research concentrate on the supervised classification problem.  Currently, one of the biggest problems in face recognition is how to generalize the system to be able to recognize new test data that vary from the training data.  Thus, until this problem is solved completely, the existing supervised approaches may require multiple manual introduction and labelling sessions to include training data with enough variations. Secondly, there is typically a large gap between research prototypes and commercial products, largely due to lack of robustness and scalability to different environmental settings.In this thesis, we propose an unsupervised approach which wouldallow for a more adaptive system which can incrementally update thetraining set with more recent data or new individuals over time.Moreover, it gives the robots a more natural {\em socialrecognition} mechanism to learn not only to recognize each person'sappearance, but also to remember some relevant contextualinformation that the robot observed during previous interactionsessions. Therefore, this thesis focuses on integrating anunsupervised and incremental face recognition system within aphysical robot which interfaces directly with humans through naturalsocial interaction.  The robot autonomously detects, tracks, andsegments face images during these interactions and automaticallygenerates a training set for its face recognition system.  Moreover,in order to motivate robust solutions and address scalabilityissues, we chose to put the robot, Mertz, in unstructured publicenvironments to interact with naive passersby, instead of with onlythe researchers within the laboratory environment.While an unsupervised and incremental face recognition system is acrucial element toward our target goal, it is only a part of thestory.  A face recognition system typically receives eitherpre-recorded face images or a streaming video from a static camera.As illustrated an ACLU review of a commercial face recognitioninstallation, a security application which interfaces with thelatter is already very challenging.  In this case, our target goalis a robot that can recognize people in a home setting. Theinterface between robots and humans is even more dynamic.  Both therobots and the humans move around.We present the robot implementation and its unsupervised incremental face recognition framework.  We describe analgorithm for clustering local features extracted from a large set of automatically generated face data.  We demonstrate the robot's capabilities and limitations in a series of experiments at a public lobby. In a final experiment, the robot interacted with a few hundred individuals in an eight day period and generated a training set of over a hundred thousand face images. We evaluate the clustering algorithm performance across a range of parameters on this automatically generated training data and also the Honda-UCSD video face database. Lastly, we present some recognition results using the self-labelled clusters.",MIT-CSAIL-TR-2007-022,244 p.,,,Humanoid robotic; Human robot interaction; Face recognition,Humanoid Robotics,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Shrobe, Howard; Laddaga, Robert; Balzer, Robert; Goldman, Neil; Wile, Dave; Tallis, Marcelo; Hollebeek, Tim; Egyed, Alexander",2007-04-10T21:41:47Z,2007-04-10T21:41:47Z,2007-04-10,http://hdl.handle.net/1721.1/37151,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Self-Adaptive Systems for Information Survivability: PMOP and AWDRAT,"Information systems form the backbones of the critical infrastructures of modern societies. Unfortunately, these systems are highly vulnerable to attacks that can result in enormous damage. Furthermore, traditional approaches to information security have not provided all the protections necessary to defeat and recover from a concerted attack; in particular, they are largely irrelevant to the problem of defending against attacks launched by insiders.This paper describes two related systems PMOP and AWDRAT that were developed during the DARPA Self Regenerative Systems program. PMOP defends against insider attacks while AWDRAT is intended to detect compromises to software systems. Both rely on self-monitoring, diagnosis and self-adaptation. We describe both systems and show the results of experiments with each.",MIT-CSAIL-TR-2007-023,10 p.,,,Information Survivability; Model Based Diagnosis; Adaptive Software,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob",2007-04-12T20:21:45Z,2007-04-12T20:21:45Z,2007-04-12,http://hdl.handle.net/1721.1/37152,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Principles for Engineered Emergence (slides),"Principles for Engineered EmergenceIt is difficult to establish engineering control over the behavior ofaggregates of unreliable devices with complicated interactionpatterns.  I take a linguistic view of this problem, searching formechanisms that simplify the composition and abstraction ofcomplicated behaviors.  From my work on various problems of aggregatecontrol in cognitive architectures and spatial computing, I havenoticed common themes in mechanisms that solve them.  From these, Iextract four principles which seem to help in engineering robustaggregate behavior---self-scaling, sparseness, gradual degradation,and failure simplification---and give examples of how they can beexploited.",,28 p.,,,artificial intelligence; amorphous computing,Mathematics and Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
William Freeman,"Torralba, Antonio; Fergus, Rob; Freeman, William T.",2007-04-24T14:01:48Z,2007-04-24T14:01:48Z,2007-04-23,http://hdl.handle.net/1721.1/37291,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Tiny images,"The human visual system is remarkably tolerant to degradations in image resolution: in a scene recognition task, human performance is similar whether $32 \times 32$ color images or multi-mega pixel images are used. With small images, even object recognition and segmentation is performed robustly by the visual system, despite the object being unrecognizable in isolation. Motivated by these observations, we explore the space of 32x32 images using a database of 10^8 32x32 color images gathered from the Internet using image search engines. Each image is loosely labeled with one of the 70,399 non-abstract nouns in English, as  listed in the Wordnet lexical database. Hence the image database represents a dense sampling of all object categories and scenes. With this dataset, we use nearest neighbor methods to perform objectrecognition across the 10^8 images.",MIT-CSAIL-TR-2007-024,9 p.,,,Recognition; Nearest neighbors methods; Image databases,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Rifkin, Ryan M.; Lippert, Ross A.",2007-05-01T16:01:50Z,2007-05-01T16:01:50Z,2007-05-01,http://hdl.handle.net/1721.1/37318,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Notes on Regularized Least Squares,"This is a collection of information about regularized least squares (RLS). The facts here are not “new results”, but we have not seen them usefully collected together before. A key goal of this work is to demonstrate that with RLS, we get certain things “for free”: if we can solve a single supervised RLS problem, we can search for a good regularization parameter lambda at essentially no additional cost.The discussion in this paper applies to “dense” regularized least squares, where we work with matrix factorizations of the data or kernel matrix. It is also possible to work with iterative methods such as conjugate gradient, and this is frequently the method of choice for large data sets in high dimensions with very few nonzero dimensions per point, such as text classifciation tasks. The results discussed here do not apply to iterative methods, which have different design tradeoffs.We present the results in greater detail than strictly necessary, erring on the side of showing our work. We hope that this will be useful to people trying to learn more about linear algebra manipulations in the machine learning context.",MIT-CSAIL-TR-2007-025; CBCL-268,8 p.,,,"machine learning, linear algebra",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob",2007-05-15T17:41:55Z,2007-05-15T17:41:55Z,2007-05-15,http://hdl.handle.net/1721.1/37336,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Developmental Cost for Models of Intelligence,"We can evaluate models of natural intelligence, as well as theirindividual components, by using a model of hardware and developmentcosts, ignoring almost all the details of biology.  The basic argumentis that neither the gross anatomy of the brain nor the behavior ofindividual cells nor the behavior of the whole poses sufficientconstraint on the algorithms that might run within the brain, but thatthe process of engineering an intelligence under this cost model posessimilar challenges to those faced by a human growing from a singlecell to an adult.  This will allow us to explore architectural ideasfreely, yet retain confidence that when a system works, the principlesallowing it to work are likely to be similar to those that allow humanintelligence to work.",MIT-CSAIL-TR-2007-026,4 p.,,,cognitive architectures; artificial intelligence,Mathematics and Computation,,,,,AAAI 2007 Workshop on Evaluating Architectures for Intelligence,,,,,,,,,,,,,,,,,,,,,,,
Hal Abelson,"Abelson, Hal",2007-05-21T12:41:49Z,2007-05-21T12:41:49Z,2007-05-19,http://hdl.handle.net/1721.1/37585,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Creation of OpenCourseWare at MIT,"This paper traces the genesis of the MIT OpenCourseWare project from its initial strategic precursors in 1999 and 2000, through its launch in 2001 and its subsequent evolution.  The story told here illuminates the interplay among institutional leadership, and strategic planning, and with university culture in launching major educational technology enterprises.  It also shows how initiatives can evolve in unexpected ways, and can even surpass their initial goals.  The paper concludes with an overview of challenges facing OpenCourseWare in moving from the end of its production ramp-up and towards sustainability.",,16 p.,,,"educational technology, open educational resources",Mathematics and Computation,,,,,J. Science Education and Technology,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Woo, Grace Rusi; Kheradpour, Pouya; Katabi, Dina",2007-05-29T18:21:50Z,2007-05-29T18:21:50Z,2007-05-29,http://hdl.handle.net/1721.1/37587,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Beyond the Bits: Cooperative Packet Recovery Using Physical Layer Information,"Wireless networks can suffer from high packet loss rates.  This paper shows that the loss rate can be significantly reduced by exposing information readily available at the physical layer. We make the physical layer convey an estimate of its confidence that a particular bit is ``0'' or ``1'' to the higher layers.  When used with cooperative design, this information dramatically improves the throughput of the wireless network. Access points that hear the same transmission combine their information to correct bits in a packet with minimal overhead. Similarly, a receiver may combine multiple erroneous transmissions to recover a correct packet.  We analytically prove that our approach minimizes the errors in packet recovery.  We also experimentally demonstrate its benefits using a testbed of GNU software radios. The results show that our approach can reduce loss rate by up to 10x in comparison with the current approach, and significantly outperforms prior cooperation proposals.",MIT-CSAIL-TR-2007-027,12 p.,,,,Networks & Mobile Systems,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Shrobe, Howard; Knight, Thomas; Hon, Andre de",2007-05-30T18:01:22Z,2007-05-30T18:01:22Z,2007-05-30,http://hdl.handle.net/1721.1/37589,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"TIARA:  Trust Management, Intrusion-tolerance, Accountability, and Reconstitution Architecture","The last 20 years have led to unprecedented improvements in chipdensity and system performance fueled mainly by Moore's Law.  Duringthe same time, system and application software have bloated, leadingto unmanageable complexity, vulnerability to attack, rigidity and lackof robustness and accountability. These problems arise from the factthat all key elements of the computational environment, from hardwarethrough system software and middleware to application code regard theworld as consisting of unconstrained ``raw seething bits''.  No elementof the entire stack is responsible for enforcing over-archingconventions of memory structuring or access control.  Outsiders mayeasily penetrate the system by exploiting vulnerabilities (e.g. bufferoverflows) arising from this lack of basic constraints. Attacks arenot easily contained, whether they originate from the clever outsiderwho penetrates the defenses or from the insider who exploits existingprivileges.  Finally, because there are no facilities for tracing theprovenance of data, even when an attack is detected, it is difficultif not impossible to tell which data are traceable to the attack andwhat data may still be trusted. We have abundant computational resources allowing us to fix thesecritical problems using a combination of hardware, system software,and programming language technology: In this report, we describe theTIARAproject, which is using these resources to design a newcomputer system thatis less vulnerable, more tolerant of intrusions, capable of recoveryfrom attacks, and accountable for their actions.  TIARA provides thesecapabilities without significant impact on overall system performance.  Itachieves these goals through the judicious use of a modest amountof extra, but reasonably generable purpose, hardware that is dedicatedto tracking the provenance of data at a very fine grained level, toenforcing access control policies, and to constructing a coherentobject-oriented model of memory.  This hardware runs in parallel withthe main data-paths of the system and operates on a set of extra bitstagging each word with data-type, bounds, access control andprovenance information. Operations that violate the intendedinvariants are trapped, while normal results are tagged withinformation derived from the tags of the input operands.This hardware level provides fine-grained support for a series ofsoftware layers that enable a variety of comprehensive access controlpolicies, self-adaptive computing, and fine-grained recoveryprocessing.  The first of these software layers establishes aconsistent object-oriented level of computing while higher layersestablish wrappers that may not be bypassed, access controls, dataprovenance tracking.  At the highest level we create the ``planlevel'' of computing in which code is executed in parallel with anabstract model (or executable specification) of the system that checkswhether the code behaves as intended.",MIT-CSAIL-TR-2007-028,18 p.,,,Security; Intrusion Tolerance; Tagged Architecture,AIRE,,,,,,,,,,,,,,,,,,,,,,,,,,,,
David Clark,"Lee, George J.",2007-06-05T14:21:56Z,2007-06-05T14:21:56Z,2007-06-04,http://hdl.handle.net/1721.1/37595,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,CAPRI: A Common Architecture for Distributed Probabilistic Internet Fault Diagnosis,"This thesis presents a new approach to root cause localization and fault diagnosis in the Internet based on a Common Architecture for Probabilistic Reasoning in the Internet (CAPRI) in which distributed, heterogeneous diagnostic agents efficiently conduct diagnostic tests and communicate observations, beliefs, and knowledge to probabilistically infer the cause of network failures.  Unlike previous systems that can only diagnose a limited set of network component failures using a limited set of diagnostic tests, CAPRI provides a common, extensible architecture for distributed diagnosis that allows experts to improve the system by adding new diagnostic tests and new dependency knowledge.To support distributed diagnosis using new tests and knowledge, CAPRI must overcome several challenges including the extensible representation and communication of diagnostic information, the description of diagnostic agent capabilities, and efficient distributed inference.  Furthermore, the architecture must scale to support diagnosis of a large number of failures using many diagnostic agents.  To address these challenges, this thesis presents a probabilistic approach to diagnosis based on an extensible, distributed component ontology to support the definition of new classes of components and diagnostic tests; a service description language for describing new diagnostic capabilities in terms of their inputs and outputs; and a message processing procedure for dynamically incorporating new information from other agents, selecting diagnostic actions, and inferring a diagnosis using Bayesian inference and belief propagation.To demonstrate the ability of CAPRI to support distributed diagnosis of real-world failures, I implemented and deployed a prototype network of agents on Planetlab for diagnosing HTTP connection failures.  Approximately 10,000 user agents and 40 distributed regional and specialist agents on Planetlab collect information from over 10,000 users and diagnose over 140,000 failures using a wide range of active and passive tests, including DNS lookup tests, connectivity probes, Rockettrace measurements, and user connection histories.  I show how to improve accuracy and cost by learning new dependency knowledge and introducing new diagnostic agents.  I also show that agents can manage the cost of diagnosing many similar failures by aggregating related requests and caching observations and beliefs.",MIT-CSAIL-TR-2007-031,222 p.,,,,Advanced Network Architecture,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Una-May O'Reilly,"Salameh, Lynne Rafik",2007-06-06T12:21:53Z,2007-06-06T12:21:53Z,2007-06-05,http://hdl.handle.net/1721.1/37597,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,An Analysis of Posynomial MOSFET Models Using Genetic Algorithms and Visualization,"Analog designers are interested in optimization tools which automate the process of circuit sizing. Geometric programming, which uses posynomial models of MOSFET parameters, represents one such tool. Genetic algorithms have been used to evolve posynomial models for geometric programs, with a reasonable mean error when modeling MOSFET parameters. By visualizing MOSFET data using two dimensional plots, this thesis investigates the behavior of various MOSFET small and large signal parameters and consequently proposes a lower bound on the maximum error, which a posynomial cannot improve upon. It then investigates various error metrics which can be used to balance the mean and maximum errors generated by posynomial MOSFET models. Finally, the thesis uses empirical data to verify the existence of the lower bound, and compares the maximum error from various parameters modeled by the genetic algorithm and by monomial fitting. It concludes that posynomial MOSFET models suffer from inherent inaccuracies. Additionally, although genetic algorithms improve on the maximum model error, the improvement, in general, does not vastly surpass results obtained through monomial fitting, which is a less computationally intensive method. Genetic algorithms are hence best used when modeling partially convex MOSFET parameters, such as r0 .",MIT-CSAIL-TR-2007-032,90 p,,,limiations; convex optimization,Humanoid Robotics,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Peter Szolovits,"Perry, John C.",2007-06-13T11:01:54Z,2007-06-13T11:01:54Z,2007-06-12,http://hdl.handle.net/1721.1/37599,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Psychophysiology of Risk Processing and Decision Making at a Regional Stock Exchange,"A longstanding controversy in philosophy is whether decision-making isgoverned by reason or emotion.  I study the role of physiologicalresponses in the decision-making process within the realm of financialmarkets, where both the environment and decisions---trades---aremeasurable.In an experiment performed on a regional stock exchange, mycollaborators and I record six different types of physiologicalsignals---skin conductance/galvanic skin response (SCR/GSR), bloodvolume pulse (BVP), electrocardiogram (ECG),electroencephalogram (EEG), electromyogram (EMG), andtemperature (Temp)---of monetarily motivated professionals making highpressure decisions.  From these signals I estimate underlyingphysiological features, such as heart rate,changes in body temperature, and amplitude of SCR, which are proxy foraffect.  Simultaneously, we record real-time market information whichthe specialists process and which serves as the basis for theirdecisions, as well as recording their decisions and outcomes.In a sample of eight market-makers, I find statistically significantdifferences in mean skin conductance response and cardiovascularvariables during transient market events relative to no-market-eventcontrol intervals.  In addition, I find a strong relationship betweentrading decisions and physiological responses.  Using regression, Idemonstrate that heart rate variability can statisticallysignificantly improve predictions of trading decisions, although notby much.",MIT-CSAIL-TR-2007-033,215 p.,,,,Clinical Decision-Making,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Daniel Weitzner,"Weitzner, Daniel J.; Abelson, Harold; Berners-Lee, Tim; Feigenbaum, Joan; Hendler, James; Sussman, Gerald Jay",2007-06-14T11:41:47Z,2007-06-14T11:41:47Z,2007-06-13,http://hdl.handle.net/1721.1/37600,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Information Accountability,"Ease of information flow is both the boon and the bane of large-scale, decentralized systems like the World Wide Web.  For all the benefits and opportunities brought by the information revolution, with that same revolution have come the challenges of inappropriate use. Such excesses and abuses in the use of information are most commonly viewed through the lens of information security. This paper argues that debates over online privacy, copyright, and information policy questions have been overly dominated by the access restriction perspective. Our alternative is to design systems that are oriented toward information accountability and appropriate use, rather than information security and access restriction.  Our goal is to extend the Web architecture to support transparency and accountability.",MIT-CSAIL-TR-2007-034,8 p.,,,,Decentralized Information Group,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Karen Sollins,"Hansen, Richard E.",2007-06-22T12:41:46Z,2007-06-22T12:41:46Z,2007-06-21,http://hdl.handle.net/1721.1/37601,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Stateful Anycast for DDoS Mitigation,"Distributed denial-of-service (DDoS) attacks can easily cripple victim hosts or networks, yet effective defenses remain elusive.  Normal anycast can be used to force the diffusion of attack traffic over a group of several hosts to increase the difficulty of saturating resources at or near any one of the hosts.  However, because a packet sent to the anycast group may be delivered to any member, anycast does not support protocols that require a group member to maintain state (such as TCP).  This makes anycast impractical for most applications of interest.This document describes the design of Stateful Anycast, a conceptual anycast-like network service based on IP anycast.  Stateful Anycast is designed to support stateful sessions without losing anycast’s ability to defend against DDoS attacks.  Stateful Anycast employs a set of anycasted proxies to direct packets to the proper stateholder.  These proxies provide DDoS protection by dropping a session’s packets upon group member request.  Stateful Anycast is incrementally deployable and can scale to support many groups.",MIT-CSAIL-TR-2007-035,103 p.,,,,Advanced Network Architecture,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Dave Gifford,"Gerber, Georg K.; Dowell, Robin D.; Jaakkola, Tommi S.; Gifford, David K.",2007-06-26T22:24:22Z,2007-06-26T22:24:22Z,2007-06-25,http://hdl.handle.net/1721.1/37603,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Table 2 (Supplemental): Complete data for all 100 expression programs discovered by GeneProgram from the Novartis Gene Atlas v2,"Table 2 (Supplemental): Complete data for all 100 recurrent expression programs (EPs) discovered by GeneProgram.  Each EP has two identifying rows, a list of meta-genes, and a list of significantly enriched GO categories.  The first identifying row has three columns: (1) the EP identifier (an arbitrarily assigned number), (2) the number of meta-genes in the EP, and (3) the percentage of samples the EP occurs in.  The identifying row lists all tissues that use the EP (h_ = human tissue, m_ = mouse tissue).  Numbers in parentheses next to each tissue indicate the degree to which the tissue uses the EP.After the identifying rows the set of meta-genes in the EP are listed. Each meta-gene has eight columns: (1) the human RefSeq identifier, (2) the mouse RefSeq identifier, (3) the empirical mean expression level, (4) the empirical mean occurrence percentage, (5) the human gene name, (6) the human Swis-Prot description, (7) the mouse gene name, and (8) the mouse Swis-Prot description.Following the meta-genes are lists of significant GO categories (the first list uses human annotations, and the second uses mouse annotations).  The columns for each line in this list are: (1) GO term, (2) enrichment p-value, (3) number of genes in the EP in the category/total genes in the EP with some GO category, (4) category description, and (5) total number of genes in the category that are also in the dataset analyzed.",,,,,,Computational & Systems Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dave Gifford,"Gerber, Georg K.; Dowell, Robin D.; Jaakkola, Tommi S.; Gifford, David K.",2007-06-26T22:22:12Z,2007-06-26T22:22:12Z,2007-06-25,http://hdl.handle.net/1721.1/37602,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Table 1 (Supplemental): Summary of expression programs discovered by GeneProgram from Novartis Tissue Atlas v2 data,"Table 1 (Supplemental): Summary of  recurrent expression programs (EPs) discovered by GeneProgram.  The columns are: (1) the EP identifier (an arbitrarily assigned number), (2) the number of genes in the EP, (3) the number of tissues in the EP, (4) the species using the EP (i.e., one or more tissues from the species uses the EP, H = human, M = mouse), (5) the generality score (GS), (6) the top three tissues using the EP (numbers in parentheses = usage percentages), (7)-(9) the GO category name, GO term, and associated p-value for the most abundant significantly enriched category (i.e., the significant category with the most genes overlapping with the EP's genes).",,,,,,Computational & Systems Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Howard Shrobe,"Bachrach, Jonathan; Beal, Jacob; Fujiwara, Takeshi",2007-07-23T14:41:48Z,2007-07-23T14:41:48Z,2007-07,http://hdl.handle.net/1721.1/38206,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Continuous Space-Time Semantics Allow Adaptive Program Execution,"A spatial computer is a collection of devices filling spacewhose ability to interact is strongly dependent on theirproximity. Previously, we have showed that programmingsuch a computer as a continuous space can allow self-scalingacross computers with different device distributionsand can increase robustness against device failure. Wehave extended these ideas to time, allowing self-scalingacross computers with different communication and executionrates. We have used a network of 24 Mica2 Motes todemonstrate that a program exploiting these ideas showsminimal difference in behavior as the time between programsteps ranges from 100 ms to 300 ms and on a configurationwith mixed rates.",MIT-CSAIL-TR-2007-038,4 p.,,,amorphous computing; Proto,AIRE,,,,,IEEE SASO 2007,,,,,,,,,,,,,,,,,,,,,,,
Sam Madden,"Abadi, Daniel J.; Marcus, Adam; Madden, Samuel R.; Hollenbach, Kate",2007-07-09T17:41:50Z,2007-07-09T17:41:50Z,2007-07-06,http://hdl.handle.net/1721.1/37816,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using The Barton Libraries Dataset As An RDF benchmark,This report describes the Barton Libraries RDF dataset and Longwell querybenchmark that we use for our recent VLDB paper on Scalable Semantic WebData Management Using Vertical Partitioning.,MIT-CSAIL-TR-2007-036,4 p.,,,,Database,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dave Gifford,"Gerber, Georg K.; Dowell, Robin D.; Jaakkola, Tommi S.; Gifford, David K.",2007-07-09T17:43:48Z,2007-07-09T17:43:48Z,2007-07-06,http://hdl.handle.net/1721.1/37817,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Hierarchical Dirichlet Process-Based Models For Discovery of Cross-species Mammalian Gene Expression,"An important research problem in computational biology is theidentification of expression programs, sets of co-activatedgenes orchestrating physiological processes, and thecharacterization of the functional breadth of these programs.  Theuse of mammalian expression data compendia for discovery of suchprograms presents several challenges, including: 1) cellularinhomogeneity within samples, 2) genetic and environmental variationacross samples, and 3) uncertainty in the numbers of programs andsample populations. We developed GeneProgram, a new unsupervisedcomputational framework that uses expression data to simultaneouslyorganize genes into overlapping programs and tissues into groups toproduce maps of inter-species expression programs, which are sortedby generality scores that exploit the automatically learnedgroupings.  Our method addresses each of the above challenges byusing a probabilistic model that: 1) allocates mRNA to differentexpression programs that may be shared across tissues, 2) ishierarchical, treating each tissue as a sample from a population ofrelated tissues, and 3) uses Dirichlet Processes, a non-parametricBayesian method that provides prior distributions over numbers ofsets while penalizing model complexity.  Using real gene expressiondata, we show that GeneProgram outperforms several popularexpression analysis methods in recovering biologically interpretablegene sets.  From a large compendium of mouse and human expressiondata, GeneProgram discovers 19 tissue groups and 100 expressionprograms active in mammalian tissues.  Our method automaticallyconstructs a comprehensive, body-wide map of expression programs andcharacterizes their functional generality. This map can be used forguiding future biological experiments, such as discovery of genesfor new drug targets that exhibit minimal ""cross-talk"" withunintended organs, or genes that maintain general physiologicalresponses that go awry in disease states.  Further, our method isgeneral, and can be applied readily to novel compendia of biologicaldata.",MIT-CSAIL-TR-2007-037,42 p.,,,,Computational & Systems Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Karen Sollins,"Li, Ji",2007-07-26T13:41:47Z,2007-07-26T13:41:47Z,2007-07-26,http://hdl.handle.net/1721.1/38207,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Agent Organization and Request Propagation in the Knowledge Plane,"In designing and building a network like the Internet, we continue to face the problems of scale and distribution. In particular, network management has become an increasingly difficult task, and network applications often need to maintain efficient connectivity graphs for various purposes. The knowledge plane was proposed as a new construct to improve network management and applications. In this proposal, I propose an application-independent mechanism to support the construction of application-specific connectivity graphs. Specifically, I propose to build a network knowledge plane and multiple sub-planes for different areas of network services. The network knowledge plane provides valuable knowledge about the Internet to the sub-planes, and each sub-plane constructs its own connectivity graph using network knowledge and knowledge in its own specific area. I focus on two key design issues: (1) a region-based architecture for agent organization; (2) knowledge dissemination and request propagation. Network management and applications benefit from the underlying network knowledge plane and sub-planes. To demonstrate the effectiveness of this mechanism, I conduct case studies in network management and security.",MIT-CSAIL-TR-2007-039,26 p.,,,"knowledge plane, connectivity graph, agent, region, knowledge dissemination, network topology",Advanced Network Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Hsu, Eugene; Silva, Marco da; Popovic, Jovan",2008-08-25T19:01:02Z,2008-08-25T19:01:02Z,2007-08-01,http://hdl.handle.net/1721.1/41946,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Guided Time Warping for Motion Editing,"Time warping allows users to modify timing without affecting poses. It has many applications in animation systems for motion editing, such as refining motions to meet new timing constraints or modifying the acting of animated characters. However, time warping typically requires many manual adjustments to achieve the desired results. We present a technique which simplifies this process by allowing time warps to be guided by a provided reference motion. Given few timing constraints, it computes a warp that both satisfies these constraints and maximizes local timing similarities to the reference. The algorithm is fast enough to incorporate into standard animation workflows. We apply the technique to two common tasks: preserving the natural timing of motions under new time constraints and modifying the timing of motions for stylistic effects.",,,,,,,,,,,"In ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pages 45-52, July 2007.",Jovan Popovic; Computer Graphics,,,,,,,,,,,,,,,,,,,,,,
Frans Kaashoek,"Brodsky, Micah Z. (Micah Zev); Efstathopoulos, Petros; Kaashoek, Frans; Kohler, Eddie; Krohn, Maxwell; Mazieres, David; Morris, Robert; VanDeBogart, Steve; Yip, Alexander",2007-08-06T15:21:49Z,2007-08-06T15:21:49Z,2007-08-06,http://hdl.handle.net/1721.1/38453,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Toward Secure Services from Untrusted Developers,"We present a secure service prototype built from untrusted,contributed code.The service manages private data for a variety of different users, anduser programs frequently require access to other users' private data.However, aside from covert timing channels, no part of the service cancorrupt private data or leak it between users or outside the systemwithout permission from the data's owners.Instead, owners may choose to reveal their data in a controlled manner.This application model is demonstrated by Muenster, a job searchwebsite that protects both the integrity and secrecy of each user's data.In spite of running untrusted code, Muenster and other services canprevent overt leaks because the untrusted modules are constrained bythe operating system to follow pre-specified security policies, whichare nevertheless flexible enough for programmers to do useful work.We build Muenster atop Asbestos, a recently described operating systembased on a form of decentralized information flowcontrol.",MIT-CSAIL-TR-2007-041,20 p.,,,decentralized information flow control; operating systems; security; web services; untrusted code; debugging; persistent storage,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hal Abelson,"Ehrmann, Stephen C.; Gilbert, Steven W.; McMartin, Flora",2007-08-23T14:41:46Z,2007-08-23T14:41:46Z,2007-08-20,http://hdl.handle.net/1721.1/38482,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Factors Affecting the Adoption of Faculty-Developed Academic Software: A Study of Five iCampus Projects,"Initiated in 1999, iCampus is a research collaboration between Microsoft Research and MIT whose goal is to create and demonstrate technologies with the potential for revolutionary change throughout the university curriculum.” The program was made possible by a $25 million research grant from Microsoft to MIT, and involves extensive collaboration between MIT and Microsoft staff.<p />This assessment study by the TLT Group addresses the question: The TLT Group has been asked, “In light of the experience of iCampus, especially those projects selected by MIT and Microsoft for close study, what can be learned about priorities for educational technology initiatives in the future and about how the spread of such innovations can be more effectively supported?”<p />The major conclusions are that the five projects studied improved important elements of an MIT education by making learning more authentic, active, collaborative, and feedback-rich.  Nevertheless, wider adoption beyond MIT was extremely difficult to achieve, largely due to structure issues in universities that make it difficult for educational technology to spread beyond the initial innovators, even to other departments within the same institution.  The report includes recommendations for universities, external sponsors, and for MIT in particular, about steps to take to achieve more effective dissemination.",,149 p.,,,educational technology; educational assessment,iCampus,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob",2007-08-27T14:21:47Z,2007-08-27T14:21:47Z,2007-08-23,http://hdl.handle.net/1721.1/38483,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning by Learning To Communicate,"Human intelligence is a product of cooperation among many different specialists.  Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the ""high-level"" agile cooperation that permeates our daily lives.I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of ""communication bootstrapping,"" in which shared experiences form a basis for agreement on a system of signals.  In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence.I then demonstrate progress along the course of investigation laid out in my roadmap:* I establish a measure of ""developmental cost"" that allows me to eliminate many possible designs* I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using ""failure simplification.""* I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system.* I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping.",MIT-CSAIL-TR-2007-042,218 p.,,,artificial intelligence; cognitive science,Mathematics and Computation,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Dong, Shuonan",2018-01-30T23:47:22Z,2018-01-30T23:47:22Z,2007-08-23,http://hdl.handle.net/1721.1/113368,MIT-CSAIL-TR-2018-008,Unsupervised Learning and Recognition of Physical Activity Plans,"This thesis desires to enable a new kind of interaction between humans and computational agents, such as robots or computers, by allowing the agent to anticipate and adapt to human intent. In the future, more robots may be deployed in situations that require collaboration with humans, such as scientific exploration, search and rescue, hospital assistance, and even domestic care. These situations require robots to work together with humans, as part of a team, rather than as a stand-alone tool. The intent recognition capability is necessary for computational agents to play a more collaborative role in human-robot interactions, moving beyond the standard master-slave relationship of humans and computers today. We provide an innovative capability for recognizing human intent, through statistical plan learning and online recognition. We approach the plan learning problem by employing unsupervised learning to automatically determine the activities in a plan based on training data. The plan activities are described by a mixture of multivariate probability densities. The number of distributions in the mixture used to describe the data is assumed to be given. The training data trajectories are fed again through the activities' density distributions to determine each possible sequence of activities that make up a plan. These activity sequences are then summarized with temporal information in a temporal plan network, which consists of a network of all possible plans. Our approach to plan recognition begins with formulating the temporal plan network as a hidden Markov model. Next, we determine the most likely path using the Viterbi algorithm. Finally, we refer back to the temporal plan network to obtainpredicted future activities. Our research presents several innovations: First, we introduce a modified representation of temporal plan networks that incorporates probabilistic information into the state space and temporal representations. Second, we learn plans from actual data, such that the notion of an activity is not arbitrarily or manually defined, but is determined by the characteristics of the data. Third, we develop a recognition algorithm that can perform recognition continuously by making probabilistic updates. Finally, our recognizer not only identifies previously executed activities, but also predicts future activities based on the plan network. We demonstrate the capabilities of our algorithms on motion capture data. Our results show that the plan learning algorithm is able to generate reasonable temporal plan networks, depending on the dimensions of the training data and the recognition resolution used. The plan recognition algorithm is also successful in recognizing the correct activity sequences in the temporal plan network corresponding to the observed test data.",,129 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-30T23:47:22Z,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob; Bachrach, Jonathan; Tobenkin, Mark",2007-08-27T14:23:35Z,2007-08-27T14:23:35Z,2007-08-24,http://hdl.handle.net/1721.1/38484,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Constraint and Restoring Force,"Long-lived sensor network applications must be able to self-repair and adapt to changing demands. We introduce a new approach for doing so: Constraint and Restoring Force. CRF is a physics-inspired framework for computing scalar fields across a sensor network with occasional changes. We illustrate CRF’s usefulness by applying it to gradients, a common building block for sensor network systems. The resulting algorithm, CRF-Gradient, determines locally when to self-repair and when to stop and save energy. CRF-Gradient is self-stabilizing, converges in O(diameter) time, and has been verified experimentally in simulation and on a network of Mica2 motes. Finally we show how CRF can be applied to other algorithms as well, such as the calculation of probability fields.",MIT-CSAIL-TR-2007-044,12 p.,,,amorphous computing; spatial computing; Proto,Mathematics and Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Robert Morris,"Brodsky, Micah Z. (Micah Zev); Krohn, Maxwell; Morris, Robert; Walfish, Michael; Yip, Alexander",2007-08-27T14:25:21Z,2007-08-27T14:25:21Z,2007-08-24,http://hdl.handle.net/1721.1/38485,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,World Wide Web Without Walls,"Today's Web is built upon a particular symbiotic relationship betweensites and users: the sites invest capital to create and market a setof features, and users gain access to the sites often in exchange fortheir data (e.g., photos, personal information, creative musings,etc.).  This paper imagines a very different Web ecosystem, in whichusers retain control of their data and developers can justify theirexistence without hoarding user data.",MIT-CSAIL-TR-2007-043,6 p.,,,secure web services; web platforms; coderank; code integrity; difc; distributed information flow control,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Katti, Sachin; Katabi, Dina",2007-09-06T15:41:48Z,2007-09-06T15:41:48Z,2007-09-04,http://hdl.handle.net/1721.1/38871,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,MIXIT: The Network Meets the Wireless Channel,"The traditional contract between the network and the lower layers states that the network does routing and the lower layers deliver correct packets. In a wireless network, however, different nodes may hear most bits in a transmission, yet none of them receives the whole packet uncorrupted. The current approach imposes fatesharing on the bits, dropping a whole packet because of a few incorrect bits. In contrast, this paper proposes MIXIT, a new architecture that performs opportunistic routing on groups of correctly received symbols.  We show using simulations driven with Software Radios measurements that MIXIT provides $4$x throughput improvement over state-of-the-art opportunistic routing.",MIT-CSAIL-TR-2007-046,6 p.,,,Network Coding; Opportunistic Routing; Co-operative Diversity,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Papi, Matthew M.; Ali, Mahmood; Correa Jr., Telmo Luis; Perkins, Jeff H.; Ernst, Michael D.",2007-09-20T19:21:48Z,2007-09-20T19:21:48Z,2007-09-17,http://hdl.handle.net/1721.1/38878,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pluggable type-checking for custom type qualifiers in Java,"We have created a framework for adding custom type qualifiers to the Javalanguage in a backward-compatible way.  The type system designer definesthe qualifiers and creates a compiler plug-in that enforces theirsemantics.  Programmers can write the type qualifiers in their programs andbe informed of errors or assured that the program is free of those errors.The system builds on existing Java tools and APIs.In order to evaluate our framework, we have written four type-checkersusing the framework:  for a non-null type system that can detect andprevent null pointer errors; for an interned type system that can detectand prevent equality-checking errors; for a reference immutability typesystem, Javari, that can detect and prevent mutation errors; and for areference and object immutability type system, IGJ, that can detect andprevent even more mutation errors.  We have conducted case studies usingeach checker to find real errors in existing software.  These case studiesdemonstrate that the checkers and the framework are practical and useful.",MIT-CSAIL-TR-2007-047,10 p.,,,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hal Abelson,"Ehrmann, Stephen C.; Gilbert, Steven W.; McMartin, Flora; Abelson, Harold; Long, Philip D.",2007-08-27T19:01:47Z,2007-08-27T19:01:47Z,2007-10-20,http://hdl.handle.net/1721.1/38487,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Factors Affecting the Adoption of Faculty-Developed Academic Software: A Study of Five iCampus Projects,"Instruction in higher education must adapt more rapidly to: changes in workforce needs, global issues, advances in disciplines, and resource constraints.  The pace of such improvement depends on the speed with which new ideas and materials are adopted across institutions. In 1999 Microsoft pledged $25 million and staff support for iCampus, a seven-year MIT project to develop pioneering uses of educational technology. The TLT Group studied five iCampus projects in order to identify factors affecting institutionalization and widespread dissemination. Among the factors impeding adoption: lack of rewards and support for faculty to adopt innovations; faculty isolation; and a lack of attention to adoption issues among projects selected for funding. The study made recommendations for universities, foundations, government agencies and corporations: 1) continue making education more authentic, active, collaborative, and feedback-rich; 2) create demand to adopt ideas and materials from other sources by encouraging all faculty members to improve and document learning in their programs, year after year; 3) nurture coalitions for instructional improvement, across and within institutions; 4) create more effective higher education – corporate alliances; and 5) improve institutional services to support faculty in educational design, software development, assessment methods, formative evaluation, and/or in sharing ideas with others who teach comparable courses.",MIT-CSAIL-TR-2007-045,6 p.,,,educational technology; educational innovation,iCampus,,,,,37th ASEE/IEEE Frontiers in Education Conference,,,,,,,,,,,,,,,,,,,,,,,
Boris Katz,"Radul, Alexey",2007-12-19T19:15:14Z,2007-12-19T19:15:14Z,2007-10-22,http://hdl.handle.net/1721.1/39831,,Report on the Probabilistic Language Scheme,"Reasoning with probabilistic models is a widespread andsuccessful technique in areas ranging from computer vision, to naturallanguage processing, to bioinformatics. Currently, these reasoningsystems are either coded from scratch in general-purpose languages oruse formalisms such as Bayesian networks that have limited expressivepower. In both cases, the resulting systems are difficult to modify,maintain, compose, and interoperate with. This work presents ProbabilisticScheme, an embedding of probabilistic computation into Scheme. Thisgives programmers an expressive language for implementing modularprobabilistic models that integrate naturally with the rest of Scheme.",MIT-CSAIL-TR-2007-059,9 p.,,,probability,Infolab,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Martin Rinard,"Lam, Patrick; Zee, Karen; Kuncak, Viktor; Rinard, Martin",2007-11-02T18:45:31Z,2007-11-02T18:45:31Z,2007-10-31,http://hdl.handle.net/1721.1/39419,,Set Interfaces for Generalized Typestate and Data Structure Consistency Verification,"Typestate systems allow the type of an object to change during its lifetime in the computation. Unlike standard type systems, they can enforce safety properties that depend on changing object states. We present a new, generalized formulation of typestate that models the typestate of an object through membership in abstract sets. This abstract set formulation enables developers to reason about cardinalities of sets, and in particular to state and verify the condition that certain sets are empty. We support hierarchical typestate classifications by specifying subset and disjointness properties over the typestate sets.We present our formulation of typestate in the context of the Hob program specification and verification framework. The Hob framework allows the combination of typestate analysis with powerful independently developed analyses such as shape analyses or theorem proving techniques. We implemented our analysis and annotated several programs (75-2500 lines of code) with set specifications. Our implementation includes several optimizations that improve the scalability of the analysis and a novel loop invariant inferencealgorithm that eliminates the need to specify loop invariants. We present experimental data demonstrating the effectiveness of our techniques.",MIT-CSAIL-TR-2007-049,30 p.,,,,Computer Architecture,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Russ Tedrake,"Rohanimanesh, Khashayar; Roy, Nicholas; Tedrake, Russ",2007-11-13T14:45:30Z,2007-11-13T14:45:30Z,2007-11-01,http://hdl.handle.net/1721.1/39427,,Towards Feature Selection In Actor-Critic Algorithms,"Choosing features for the critic in actor-critic algorithms with function approximation is known to be a challenge. Too few critic features can lead to degeneracy of the actor gradient, and too many features may lead to slower convergence of the learner. In this paper, we show that a well-studied class of actor policies satisfy the known requirements for convergence when the actor features are selected carefully. We demonstrate that two popular representations for value methods - the barycentric interpolators and the graph Laplacian proto-value functions - can be used to represent the actor in order to satisfy these conditions. A consequence of this work is a generalization of the proto-value function methods to the continuous action actor-critic domain. Finally, we analyze the performance of this approach using a simulation of a torque-limited inverted pendulum.",MIT-CSAIL-TR-2007-051,9 p.,,,reinforcement learning,Robot Locomotion Group,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Silvio Micali,"Valiant, Paul; Micali, Silvio",2007-11-02T20:30:07Z,2007-11-02T20:30:07Z,2007-11-02,http://hdl.handle.net/1721.1/39420,,Collusion-Resilient Revenue In Combinatorial Auctions,"In auctions of a single good, the second-price mechanism achieves, in dominantstrategies, a revenue benchmark that is naturally high and resilient to anypossible collusion.We show how to achieve, to the maximum extent possible, the same propertiesin combinatorial auctions.",MIT-CSAIL-TR-2007-052,20 p.,,,Worst Rational Setting; Natural Solution Pairs; Player-Monotone Benchmarks; Revenue Guarantees; Guaranteed Revenue,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,,,,,,,,,,,,,
Trevor Darrell,"Urtasun, Raquel; Quattoni, Ariadna; Darrell, Trevor",2007-11-13T14:45:17Z,2007-11-13T14:45:17Z,2007-11-06,http://hdl.handle.net/1721.1/39426,,Transfering Nonlinear Representations using Gaussian Processes with a Shared Latent Space,"When a series of problems are related, representations derived fromlearning earlier tasks may be useful in solving later problems. Inthis paper we propose a novel approach to transfer learning withlow-dimensional, non-linear latent spaces. We show how suchrepresentations can be jointly learned across multiple tasks in adiscriminative probabilistic regression framework. When transferred tonew tasks with relatively few training examples, learning can befaster and/or more accurate. Experiments on a digit recognition taskshow significantly improved performance when compared to baselineperformance with the original feature representation or with arepresentation derived from a semi-supervised learning approach.",MIT-CSAIL-TR-2007-053,8 p.,,,transfer learning; latent variable models,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Michael Ernst,"Kim, Sunghun; Artzi, Shay; Ernst, Michael D.",2007-11-20T15:45:09Z,2007-11-20T15:45:09Z,2007-11-20,http://hdl.handle.net/1721.1/39639,,ReCrash: Making Crashes Reproducible,"It is difficult to fix a problem without being able to reproduce it.However, reproducing a problem is often difficult and time-consuming.This paper proposes a novel algorithm, ReCrash, that generatesmultiple unit tests that reproduce a given program crash.ReCrash dynamically tracks method calls during every execution of the target program. If the program crashes, ReCrash saves information about the relevant method calls and uses the saved information to create unit tests reproducing the crash.We present reCrashJ an implementation of ReCrash for Java. reCrashJ reproducedreal crashes from javac, SVNKit, Eclipse JDT, and BST. reCrashJ is efficient, incurring 13%-64% performance overhead. If this overhead is unacceptable, then reCrashJ has another mode that has negligible overhead until a crash occurs and 0%-1.7% overhead until a second crash, at which point the test cases are generated.",MIT-CSAIL-TR-2007-054,9 p.,,,,Program Analysis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Saman Amarasinghe,"Thies, William; Hall, Steven; Amarasinghe, Saman",2007-12-03T13:45:13Z,2007-12-03T13:45:13Z,2007-11-30,http://hdl.handle.net/1721.1/39651,,Mapping Stream Programs into the Compressed Domain,"Due to the high data rates involved in audio, video, and signalprocessing applications, it is imperative to compress the data todecrease the amount of storage used. Unfortunately, this implies thatany program operating on the data needs to be wrapped by adecompression and re-compression stage. Re-compression can incursignificant computational overhead, while decompression swamps theapplication with the original volume of data.In this paper, we present a program transformation that greatlyaccelerates the processing of compressible data. Given a program thatoperates on uncompressed data, we output an equivalent program thatoperates directly on the compressed format. Our transformationapplies to stream programs, a restricted but useful class ofapplications with regular communication and computation patterns. Ourformulation is based on LZ77, a lossless compression algorithm that isutilized by ZIP and fully encapsulates common formats such as AppleAnimation, Microsoft RLE, and Targa.We implemented a simple subset of our techniques in the StreamItcompiler, which emits executable plugins for two popular video editingtools: MEncoder and Blender. For common operations such as coloradjustment and video compositing, mapping into the compressed domainoffers a speedup roughly proportional to the overall compressionratio. For our benchmark suite of 12 videos in Apple Animationformat, speedups range from 1.1x to 471x, with a median of 15x.",MIT-CSAIL-TR-2007-055,13 p.,,,synchronous dataflow,Computer Architecture,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Silvio Micali,"Lepinski, Matt; Micali, Silvio; Izmalkov, Sergei",2007-12-05T18:15:10Z,2007-12-05T18:15:10Z,2007-12-05,http://hdl.handle.net/1721.1/39659,,Verifiably Secure Devices,"We put forward the notion of a verifiably secure device, in essence a stronger notion of secure computation, and achieve it in the ballot-box model. Verifiably secure devices1. Provide a perfect solution to the problem of achieving correlated equilibrium, an important and extensively investigated problem at the intersection of game theory, cryptography and efficient algorithms; and2. Enable the secure evaluation of multiple interdependent functions.",MIT-CSAIL-TR-2007-056,29 p.,,,interdependent functions; correlated equilibrium; Implementation; interdependent mechanisms,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Michael Ernst,"McCamant, Stephen; Ernst, Michael D.",2007-12-10T14:00:11Z,2007-12-10T14:00:11Z,2007-12-10,http://hdl.handle.net/1721.1/39812,,Quantitative Information Flow as Network Flow Capacity,"We present a new technique for determining how much information abouta program's secret inputs is revealed by its public outputs. Incontrast to previous techniques based on reachability from secretinputs (tainting), it achieves a more precise quantitative result bycomputing a maximum flow of information between the inputs andoutputs. The technique uses static control-flow regions to soundlyaccount for implicit flows via branches and pointer operations, butoperates dynamically by observing one or more program executions andgiving numeric flow bounds specific to them (e.g., ""17 bits""). Themaximum flow in a network also gives a minimum cut (a set of edgesthat separate the secret input from the output), which can be used toefficiently check that the same policy is satisfied on futureexecutions. We performed case studies on 5 real C, C++, and ObjectiveC programs, 3 of which had more than 250K lines of code. The toolchecked multiple security policies, including one that was violated bya previously unknown bug.",MIT-CSAIL-TR-2007-057,12 p.,,,Confidentiality; Privacy; Information disclosure; Tainting; Implicit flows; Valgrind; Memcheck,Program Analysis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
John Leonard,"Leonard, John; Barrett, David; How, Jonathan; Teller, Seth; Antone, Matt; Campbell, Stefan; Epstein, Alex; Fiore, Gaston; Fletcher, Luke; Frazzoli, Emilio; Huang, Albert; Jones, Troy; Koch, Olivier; Kuwata, Yoshiaki; Mahelona, Keoni; Moore, David; Moyer, Katy; Olson, Edwin; Peters, Steven; Sanders, Chris; Teo, Justin; Walter, Matthew",2007-12-17T13:50:57Z,2007-12-17T13:50:57Z,2007-12-14,http://hdl.handle.net/1721.1/39822,,Team MIT Urban Challenge Technical Report,"This technical report describes Team MIT’s approach to theDARPA Urban Challenge. We have developed a novel strategy forusing many inexpensive sensors, mounted on the vehicle periphery,and calibrated with a new cross-­modal calibrationtechnique. Lidar, camera, and radar data streams are processedusing an innovative, locally smooth state representation thatprovides robust perception for real­ time autonomous control. Aresilient planning and control architecture has been developedfor driving in traffic, comprised of an innovative combination ofwell­proven algorithms for mission planning, situationalplanning, situational interpretation, and trajectory control. These innovations are being incorporated in two new roboticvehicles equipped for autonomous driving in urban environments,with extensive testing on a DARPA site visit course. Experimentalresults demonstrate all basic navigation and some basic trafficbehaviors, including unoccupied autonomous driving, lanefollowing using pure-­pursuit control and our local frameperception strategy, obstacle avoidance using kino-­dynamic RRTpath planning, U-­turns, and precedence evaluation amongst othercars at intersections using our situational interpreter. We areworking to extend these approaches to advanced navigation andtraffic scenarios.",MIT-CSAIL-TR-2007-058,26 p.,,,autonomous vehicle; robotics; DARPA Grand Challenge; path planning; machine perception; tracking,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Tomaso Poggio,"Masquelier, Timothee; Serre, Thomas; Thorpe, Simon; Poggio, Tomaso",2007-12-27T13:45:13Z,2007-12-27T13:45:13Z,2007-12-26,http://hdl.handle.net/1721.1/39833,,Learning complex cell invariance from natural videos: A plausibility proof,"One of the most striking feature of the cortex is its ability to wire itself. Understanding how the visual cortex wires up through development and how visual experience refines connections into adulthood is a key question for Neuroscience. While computational models of the visual cortex are becoming increasingly detailed, the question of how such architecture could self-organize through visual experience is often overlooked. Here we focus on the class of hierarchical feedforward models of the ventral stream of the visual cortex, which extend the classical simple-to-complex cells model by Hubel and Wiesel (1962) to extra-striate areas, and have been shown to account for a host of experimental data. Such models assume two functional classes of simple and complex cells with specific predictions about their respective wiring and resulting functionalities.In these networks, the issue of learning, especially for complex cells, is perhaps the least well understood. In fact, in most of these models, the connectivity between simple and complex cells is not learned butrather hard-wired. Several algorithms have been proposed for learning invariances at the complex cell level based on a trace rule to exploit the temporal continuity of sequences of natural images, but very few can learn from natural cluttered image sequences.Here we propose a new variant of the trace rule that only reinforces the synapses between the most active cells, and therefore can handle cluttered environments. The algorithm has so far been developed and tested at the level of V1-like simple and complex cells: we verified that Gabor-like simple cell selectivity could emerge from competitive Hebbian learning. In addition, we show how the modified trace rule allows the subsequent complex cells to learn to selectively pool over simple cells with the same preferred orientation but slightly different positions thus increasing their tolerance to the precise position of the stimulus within their receptive fields.",MIT-CSAIL-TR-2007-060; CBCL-269,19 p.,,,primary visual cortex; slowness; temporal continuity; hubel and wiesel; feedforward hierarchical models,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Gardiol, Natalia Hernandez",2008-01-03T13:32:44Z,2008-01-03T13:32:44Z,2007-12-31,http://hdl.handle.net/1721.1/39838,,Relational Envelope-based Planning,"This thesis proposes a synthesis of logic and probability for solving stochastic sequential decision-making problems. We address two main questions: How can we take advantage of logical structure to speed up planning in a principled way? And, how can probability inform the production of a more robust, yet still compact, policy? We can take as inspiration a mobile robot acting in the world: it is faced with a varied amount ofsensory data and uncertainty in its action outcomes. Or, consider a logistics planning system: it must deliver a large number of objects to the right place at the right time. Many interesting sequential decision-making domains involve large statespaces, large stochastic action sets, and time pressure to act. In this work, we show how structured representations of the environment's dynamics can constrain and speed up the planning process. We start with a problem domain described in a probabilistic logical description language.Our technique is based on, first, identifying the most parsimonious representation that permits solution of the described problem. Next, we take advantage of the structured problem description to dynamically partition the action space into a set of equivalence classes with respect to this minimal representation. The partitioned action space results in fewer distinctactions. This technique can yield significant gains in planning efficiency.Next, we develop an anytime technique to elaborate on this initial plan. Our approach uses the envelope MDP framework, which creates a Markov decision process out of a subset of the possible state space. This strategy lets an agent begin acting quicklywithin a restricted part of the full state space, as informed by the original plan,and to judiciously expand its envelope as resources permit.Finally, we show how the representation space itself can be elaborated within the anytime framework. This approach balances the need to respond to time-pressure and to produce the most robust policies possible. We present experimental results in some synthetic planning domains and in a simulated military logistics domain.",MIT-CSAIL-TR-2007-061,143 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Piotr Indyk,"Berinde, Radu; Indyk, Piotr",2008-01-15T14:15:14Z,2008-01-15T14:15:14Z,2008-01-10,http://hdl.handle.net/1721.1/40089,,Sparse recovery using sparse matrices,"We consider the approximate sparse recovery problem, where the goal is to (approximately) recover a high-dimensional vector x from its lower-dimensional sketch Ax. A popular way of performing this recovery is by finding x* such that Ax=Ax*, and ||x*||_1 is minimal. It is known that this approach ``works'' if A is a random *dense* matrix, chosen from a proper distribution.In this paper, we investigate this procedure for the case where A is binary and *very sparse*. We show that, both in theory and in practice, sparse matrices are essentially as ``good'' as the dense ones. At the same time, sparse binary matrices provide additional benefits, such as reduced encoding and decoding time.",MIT-CSAIL-TR-2008-001,13 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Michael Ernst,"Saff, David; Boshernitsan, Marat; Ernst, Michael D.",2008-01-15T14:15:58Z,2008-01-15T14:15:58Z,2008-01-14,http://hdl.handle.net/1721.1/40090,,Theories in Practice: Easy-to-Write Specifications that Catch Bugs,"Automated testing during development helps ensure that software works according to the test suite. Traditional test suites verify a few well-picked scenarios or example inputs. However, such example-based testing does not uncover errors in legal inputs that the test writer overlooked. We propose theory-based testing as an adjunct to example-based testing. A theory generalizes a (possibly infinite) set of example-based tests. A theory is an assertion that should be true for any data, and it can be exercised by human-chosen data or by automatic data generation. A theory is expressed in an ordinary programming language, it is easy for developers to use (often even easier than example-based testing), and it serves as a lightweight form of specification. Six case studies demonstrate the utility of theories that generalize existing tests to prevent bugs, clarify intentions, and reveal design problems.",MIT-CSAIL-TR-2008-002,10 p.,,,"JUnit, testing, partial specification",Program Analysis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,,,,,,,,,,,,,
Jovan Popovic,"Silva, Marco da; Abe, Yeuhi; Popovic, Jovan",2008-01-16T13:45:10Z,2008-01-16T13:45:10Z,2008-01-15,http://hdl.handle.net/1721.1/40091,,Simulation of Human Motion Data using Short-Horizon Model-Predictive Control,"Many data-driven animation techniques are capable of producing high quality motions of human characters. Few techniques, however, are capable of generating motions that are consistent with physically simulated environments. Physically simulated characters, in contrast, are automatically consistent with the environment, but their motionsare often unnatural because they are difficult to control. We present a model-predictive controller that yields natural motions by guiding simulated humans toward real motion data. During simulation, the predictive component of the controller solves a quadratic program to compute the forces for a short window of time into the future. These forces are then applied by a low-gain proportional-derivative component, which makes minor adjustments until the next planning cycle. The controller is fast enough for interactive systems such as games and training simulations. It requires no precomputation and little manual tuning. The controller is resilient to mismatches between the character dynamics and the input motion, which allows it to track motion capture data even where the real dynamics are not known precisely. The same principled formulation can generate natural walks, runs, and jumps in a number of different physically simulated surroundings.",,,,,Computer Graphics; Three Dimensional Graphics and Realism; Animation,Computer Graphics,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Hari Balakrishnan,"Eriksson, Jakob; Balakrishnan, Hari; Madden, Sam",2008-01-29T14:15:28Z,2008-01-29T14:15:28Z,2008-01-17,http://hdl.handle.net/1721.1/40094,,Cabernet: A Content Delivery Network for Moving Vehicles,"This paper describes the design, implementation, and evaluation of Cabernet, a system to deliver data to and from moving vehicles using open 802.11 (WiFi) access points encountered opportunistically during travel. Network connectivity in Cabernet is both fleeting (access points are typicallywithin range for a few seconds) and intermittent (because the access points don't provide continuous coverage), and suffers from high packet loss rates over the wireless channel. On the positive side, in the absence of losses, achievable data rates over WiFi can reach many megabits per second. Unfortunately, current protocols don't establish end-to-end connectivity fast enough, don't cope well with intermittent connectivity, and don't handle high packet loss rates well enough to achieve this potential throughput. Cabernet incorporates two new techniques to improve data delivery throughput: QuickWifi, a streamlined client-side process to establish end-to-end connectivity quickly, reducing the mean time to establish connectivity from 12.9 seconds to less than 366 ms and CTP, a transport protocol that distinguishes congestion on the wired portion of the path from losses over the wireless link to reliably and efficiently deliver data to nodes in cars. We have deployed the system on a fleet of 10 taxis, each running several hours per day in the Boston area. Our experiments show that CTP improves throughput by a factor of 2x over TCP and that QuickWifi increases the number of connectionsby a factor of 4x over unoptimized approaches. Thus, Cabernet is perhaps the first practical system capable of delivering data to moving vehicles over existing short-range WiFi radios, with a mean transfer capacity of approximately 38 megabytes/hour per car, or a mean rate of 87 kbit/s.",MIT-CSAIL-TR-2008-003,14 p.,,,"wifi, IEEE 802.11, vehicular networking",Networks & Mobile Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
David Karger,"Karger, David; Nikolova, Evdokia",2008-01-29T14:15:12Z,2008-01-29T14:15:12Z,2008-01-28,http://hdl.handle.net/1721.1/40093,,Exact Algorithms for the Canadian Traveller Problem on Paths and Trees,"The Canadian Traveller problem is a stochastic shortest paths problem in which one learns the cost of an edge only when arriving at one of its endpoints. The goal is to find an adaptive policy (adjusting as one learns more edge lengths) that minimizes the expected cost of travel. The problem is known to be #P hard. Since there has been no significant progress on approximation algorithms for several decades, we have chosen to seek out special cases for which exact solutions exist, in the hope of demonstrating techniques that could lead to further progress. Applying techniques from the theory of Markov Decision Processes, we give an exact solution for graphs of parallel (undirected) paths from source to destination with random two-valued edge costs. We also offer a partial generalization to traversing perfect binary trees.",MIT-CSAIL-TR-2008-004,14 p.,,,"Canadian Traveller, stochastic shortest path, route planning under uncertainty, path planning",Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Sam Madden,"Newton, Ryan; Girod, Lewis; Craig, Michael; Madden, Sam; Morrisett, Greg",2008-01-31T19:00:11Z,2008-01-31T19:00:11Z,2008-01-31,http://hdl.handle.net/1721.1/40095,,WaveScript: A Case-Study in Applying a Distributed Stream-Processing Language,"Applications that combine live data streams with embedded, parallel,and distributed processing are becoming more commonplace. WaveScriptis a domain-specific language that brings high-level, type-safe,garbage-collected programming to these domains. This is made possibleby three primary implementation techniques. First, we employ a novelevaluation strategy that uses a combination of interpretation andreification to partially evaluate programs into stream dataflowgraphs. Second, we use profile-driven compilation to enable manyoptimizations that are normally only available in the synchronous(rather than asynchronous) dataflow domain. Finally, we incorporatean extensible system for rewrite rules to capture algebraic propertiesin specific domains (such as signal processing).We have used our language to build and deploy a sensor-network for theacoustic localization of wild animals, in particular, theYellow-Bellied marmot. We evaluate WaveScript's performance on thisapplication, showing that it yields good performance on both embeddedand desktop-class machines, including distributed execution andsubstantial parallel speedups. Our language allowed us to implementthe application rapidly, while outperforming a previous Cimplementation by over 35%, using fewer than half the lines of code.We evaluate the contribution of our optimizations to this success.",MIT-CSAIL-TR-2008-005; CBCL-270,11 p.,,,,Computation Structures,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Michael Ernst,"Artzi, Shay; Kiezun, Adam; Dolby, Julian; Tip, Frank; Dig, Danny; Paradkar, Amit; Ernst, Michael D.",2008-02-06T14:15:11Z,2008-02-06T14:15:11Z,2008-02-06,http://hdl.handle.net/1721.1/40249,,Finding Bugs In Dynamic Web Applications,"Web script crashes and malformed dynamically-generated web pages are common errors, and they seriously impact usability of web applications. Currenttools for web-page validation cannot handle the dynamically-generatedpages that are ubiquitous on today's Internet.In this work, we apply a dynamic test generation technique, based oncombined concrete and symbolic execution, to the domain of dynamic webapplications. The technique generates tests automatically andminimizes the bug-inducing inputs to reduce duplication and to makethe bug reports small and easy to understand and fix.We implemented the technique in Apollo, an automated tool thatfound dozens of bugs in real PHP applications. Apollo generatestest inputs for the web application, monitors the application forcrashes, and validates that the output conforms to the HTMLspecification. This paper presents Apollo's algorithms andimplementation, and an experimental evaluation that revealed a totalof 214 bugs in 4 open-source PHP web applications.",MIT-CSAIL-TR-2008-006,12 p.,,,html; syntax; validation; dynamic; bug,Program Analysis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
David Karger,"Bernstein, Michael; Van Kleek, Max; Khushraj, Deepali; Nayak, Rajeev; Liu, Curtis; schraefel, mc; Karger, David R.",2008-02-13T18:00:20Z,2008-02-13T18:00:20Z,2008-02-10,http://hdl.handle.net/1721.1/40281,,Wicked Problems and Gnarly Results: Reflecting on Design and Evaluation Methods for Idiosyncratic Personal Information Management Tasks,"This paper is a case study of an artifact design and evaluation process; it is a reflection on how right thinking about design methods may at times result in sub-optimal results. Our goal has been to assess our decision making processthroughout the design and evaluation stages for a software prototype in order to consider where design methodology may need to be tuned to be more sensitive to the domain of practice, in this case software evaluation in personal information management. In particular, we reflect on design methods around (1) scale of prototype, (2) prototyping and design process, (3) study design, and (4) study population.",MIT-CSAIL-TR-2008-007,10 p.,,,Case study; User-centered design; Personal information management,Undecided,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Karen Sollins,"Beverly, Robert; Sollins, Karen",2008-02-19T13:45:28Z,2008-02-19T13:45:28Z,2008-02-15,http://hdl.handle.net/1721.1/40287,,Exploiting Transport-Level Characteristics of Spam,"In the arms race to secure electronic mail users and servers fromunsolicited messages (spam), the most successful solutions employtechniques that are difficult for spammers to circumvent. Thisresearch investigates the transport-layer characteristics ofemail in order to provide a new, novel and robust defense againstspam. We find that spam SMTP flows exhibit TCP behavior consistentwith traffic competing for link access, large round trip times andresource constrained hosts. Thus, SMTP flow characteristics providesufficient statistical power to differentiate between spam andlegitimate mail (ham). We build ""SpamFlow"" to learn and exploitthese differences. Using machine learning feature selection weidentify the most discriminatory flow properties and effect greaterthan 90% spam classification accuracy without content or reputationanalysis. SpamFlow correctly identifies 78% of the false negativesgenerated by a popular content filtering application -- demonstratingthe power in combining SpamFlow with existing techniques. Finally, weargue that SpamFlow is not easily subvertible due to economicand practical constraints inherent in sourcing spam.",MIT-CSAIL-TR-2008-008,12 p.,,,,Advanced Network Architecture,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Trevor Darrell,"Christoudias, C. Mario; Urtasun, Raquel; Darrell, Trevor",2008-02-19T13:45:16Z,2008-02-19T13:45:16Z,2008-02-17,http://hdl.handle.net/1721.1/40286,,Unsupervised Distributed Feature Selection for Multi-view Object Recognition,"Object recognition accuracy can be improved when information frommultiple views is integrated, but information in each view can oftenbe highly redundant. We consider the problem of distributed objectrecognition or indexing from multiple cameras, where thecomputational power available at each camera sensor is limited andcommunication between sensors is prohibitively expensive. In thisscenario, it is desirable to avoid sending redundant visual featuresfrom multiple views, but traditional supervised feature selectionapproaches are inapplicable as the class label is unknown at thecamera. In this paper we propose an unsupervised multi-view featureselection algorithm based on a distributed compression approach.With our method, a Gaussian Process model of the joint viewstatistics is used at the receiver to obtain a joint encoding of theviews without directly sharing information across encoders. Wedemonstrate our approach on recognition and indexing tasks withmulti-view image databases and show that our method comparesfavorably to an independent encoding of the features from eachcamera.",MIT-CSAIL-TR-2008-009,10 p.,,,Distributed Compression; Gaussian Processes; Multi-view Object Recognition,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Peter Szolovits,"Rudin, Robert",2008-02-19T13:45:08Z,2008-02-19T13:45:08Z,2008-02-17,http://hdl.handle.net/1721.1/40285,,Making Medical Records More Resilient,"Hurricane Katrina showed that the current methods for handling medicalrecords are minimally resilient to large scale disasters. This research presents a preliminary model for measuring the resilience of medical records systemsagainst public policy goals and uses the model to illuminate the current state of medical record resilience. From this analysis, three recommendations for how to make medical records more resilient are presented.The recommendations are: 1) Federal and state governments should use the preliminary resiliencemodel introduced here as the basis for compliance requirements for electronicmedical record technical architectures. 2) Regional Health Information Organizations (RHIOs) should consideroffering services in disaster management to healthcare organizations. This willhelp RHIOs create sustainable business models. 3) Storage companies should consider developing distributed storagesolutions based on Distributed Hash Table (DHT) technology for medical recordstorage. Distributed storage would alleviate public concerns over privacy withcentralized storage of medical records. Empirical evidence is presenteddemonstrating the performance of DHT technology using a prototype medicalrecord system.",MIT-CSAIL-TR-2008-010,92 p.,,,,Clinical Decision-Making,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Charles Leiserson,"Agrawal, Kunal; Lee, I-Ting Angelina; Sukha, Jim",2008-06-30T13:00:16Z,2008-06-30T13:00:16Z,2008-02-20 and 2008-06-14,http://hdl.handle.net/1721.1/41871,,Safe Open-Nested Transactions Through Ownership,"Researchers in transactional memory (TM) have proposed open nesting asa methodology for increasing the concurrency of a program. The ideais to ignore certain ""low-level"" memory operations of anopen-nested transaction when detecting conflicts for its parenttransaction, and instead perform abstract concurrency control for the""high-level"" operation that nested transaction represents. Tosupport this methodology, TM systems use an open-nested commitmechanism that commits all changes performed by an open-nestedtransaction directly to memory, thereby avoiding low-levelconflicts. Unfortunately, because the TM runtime is unaware of thedifferent levels of memory, an unconstrained use of open-nestedcommits can lead to anomalous program behavior.In this paper, we describe a framework of ownership-awaretransactional memory which incorporates the notion of modules into theTM system and requires that transactions and data be associated withspecific transactional modules or Xmodules. We propose a newownership-aware commit mechanism, a hybrid between anopen-nested and closed-nested commit which commits a piece of datadifferently depending on whether the current Xmodule owns the data ornot. Moreover, we give a set of precise constraints on interactionsand sharing of data among the Xmodules based on familiar notions ofabstraction. We prove that ownership-aware TM has has cleanmemory-level semantics and can guarantee serializability bymodules, which is an adaptation of multilevel serializability fromdatabases to TM. In addition, we describe how a programmer canspecify Xmodules and ownership in a Java-like language. Our typesystem can enforce most of the constraints required by ownership-awareTM statically, and can enforce the remaining constraints dynamically.Finally, we prove that if transactions in the process of aborting obeyrestrictions on their memory footprint, the OAT model is free fromsemantic deadlock.",MIT-CSAIL-TR-2008-038,36 p.,,,"abstract serializability, open-nested transactions, ownership types, ownership-aware transactions, serializability by levels, serializability by modules, transactional memory, Xmodules",Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Aycinena, Meg; Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2008-02-25T19:46:04Z,2008-02-25T19:46:04Z,2008-02-25,http://hdl.handle.net/1721.1/40288,,Learning Grammatical Models for Object Recognition,"Many object recognition systems are limited by their inability to share common parts or structure among related object classes. This capability is desirable because it allows information about parts and relationships in one object class to be generalized to other classes for which it is relevant. With this goal in mind, we have designed a representation and recognition framework that captures structural variability and shared part structure within and among object classes. The framework uses probabilistic geometric grammars (PGGs) to represent object classes recursively in terms of their parts, thereby exploiting the hierarchical and substitutive structure inherent to many types of objects. To incorporate geometric and appearance information, we extend traditional probabilistic context-free grammars to represent distributions over the relative geometric characteristics of object parts as well as the appearance of primitive parts. We describe an efficient dynamic programming algorithm for object categorization and localization in images given a PGG model. We also develop an EM algorithm to estimate the parameters of a grammar structure from training data, and a search-based structure learning approach that finds a compact grammar to explain the image data while sharing substructure among classes. Finally, we describe a set of experiments that demonstrate empirically that the system provides a performance benefit.",MIT-CSAIL-TR-2008-011,28 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Gerald Sussman,"Beal, Jacob; Bachrach, Jonathan; Vickery, Dan; Tobenkin, Mark",2007-11-02T18:45:13Z,2007-11-02T18:45:13Z,2008-03,http://hdl.handle.net/1721.1/39418,,Fast Self-Healing Gradients,"We present CRF-Gradient, a self-healing gradient algorithm that provably reconfigures in O(diameter) time. Self-healing gradients are a frequently used building block for distributed self-healing systems, but previous algorithms either have a healing rate limited by the shortest link in the network or must rebuild invalid regions from scratch. We have verified CRF-Gradient in simulation and on a network of Mica2 motes. Our approach can also be generalized and applied to create other self-healing calculations, such as cumulative probability fields.",MIT-CSAIL-TR-2007-050,7 p.,,,amorphous computing; spatial computing; spatial computer,Mathematics and Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Trevor Darrell,"Quattoni, Ariadna; Collins, Michael; Darrell, Trevor",2008-03-03T14:45:13Z,2008-03-03T14:45:13Z,2008-03-03,http://hdl.handle.net/1721.1/40797,,Transfer learning for image classification with sparse prototype representations,"To learn a new visual category from few examples, prior knowledge from unlabeled data as well as previous related categories may be useful.  We develop a new method for transfer learning which exploits available unlabeled data and an arbitrary kernel function; we form a representation based on kernel distances to a large set of unlabeled data points. To transfer knowledge from previous related problems we observe that a category might be learnable using only a small subset of reference prototypes. Related problems may share a significant number of relevant prototypes; we find such a reduced representation by performing a joint loss minimization over the training sets of related problems with a shared regularization penalty that minimizes the total number of prototypes involved in the approximation.This optimization problem can be formulated as a linear program thatcan be solved efficiently. We conduct experiments on a news-topic prediction task where the goal is to predict whether an image belongs to a particularnews topic. Our results show that when only few examples are available for training a target topic, leveraging knowledge learnt from other topics can significantly improve performance.",MIT-CSAIL-TR-2008-012,8 p.,,,transfer learning; image classification,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Brian Williams,"Ono, Masahiro; Williams, Brian C.",2008-03-06T14:30:20Z,2008-03-06T14:30:20Z,2008-03-06,http://hdl.handle.net/1721.1/40804,,Two-stage Optimization Approach to Robust Model Predictive Control with a Joint Chance Constraint,"When controlling dynamic systems such as mobile robots in uncertain environments, there is a trade off between risk and reward. For example, a race car can turn a corner faster by taking a more challenging path. This paper proposes a new approach to planning a control sequence with guaranteed risk bound. Given a stochastic dynamic model, the problem is to find a control sequence that optimizes a performance metric, while satisfying chance constraints i.e. constraints on the upper bound of the probability of failure. We propose a two-stage optimization approach, with the upper stage optimizing the risk allocation and the lower stage calculating the optimal control sequence that maximizes the reward. In general, upper-stage is a non-convex optimization problem, which is hard to solve. We develop a new iterative algorithm for this stage that efficiently computes the risk allocation with a small penalty to optimality. The algorithm is implemented and tested on the autonomous underwater vehicle (AUV) depth planning problem, which demonstrates the substantial improvement in computation cost and suboptimality compared to the prior arts.",MIT-CSAIL-TR-2008-014,8 p.,,,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Brian Williams,"Ono, Masahiro; Williams, Brian C.",2008-03-06T14:30:11Z,2008-03-06T14:30:11Z,2008-03-06,http://hdl.handle.net/1721.1/40803,,Efficient Motion Planning Algorithm for Stochastic Dynamic Systems with Constraints on Probability of Failure,"When controlling dynamic systems such as mobile robots in uncertain environments, there is a trade off between risk and reward. For example, a race car can turn a corner faster by taking a more challenging path. This paper proposes a new approach to planning a control sequence with guaranteed risk bound. Given a stochastic dynamic model, the problem is to find a control sequence that optimizes a performance metric, while satisfying chance constraints i.e. constraints on the upper bound of the probability of failure. We propose a two-stage optimization approach, with the upper stage optimizing the risk allocation and the lower stage calculating the optimal control sequence that maximizes the reward. In general, upper-stage is a non-convex optimization problem, which is hard to solve. We develop a new iterative algorithm for this stage that efficiently computes the risk allocation with a small penalty to optimality. The algorithm is implemented and tested on the autonomous underwater vehicle (AUV) depth planning problem, which demonstrates the substantial improvement in computation cost and suboptimality compared to the prior arts.",MIT-CSAIL-TR-2008-013,7 p.,,,Joint chance constraint; Model Predictive Control; MPC; Robust Model Predictive Control; RMPC,Model-based Embedded and Robotic Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Gerald Sussman,"Greenstadt, Rachel; Beal, Jacob",2008-03-24T18:15:12Z,2008-03-24T18:15:12Z,2008-03-17,http://hdl.handle.net/1721.1/40810,,Cognitive Security for Personal Devices,"Humans should be able to think of computers as extensions of their body, as craftsmen do with their tools. Current security models, however, are too unlike those used in human minds---for example, computers authenticate users by challenging them to repeat a secret rather than by continually observing the many subtle cues offered by their appearance and behavior. We propose three lines of research that can be combined to produce cognitive security on computers and other personal devices: imprinting and continuously deployed multi-modal biometrics, self-protection through virtualization and trusted computing, and adjustably autonomous security.",MIT-CSAIL-TR-2008-016,6 p.,,,artificial intelligence; trusted computing,Mathematics and Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Tomaso Poggio,"Caponnetto, Andrea; Poggio, Tomaso; Smale, Steve",2008-06-05T18:00:40Z,2008-06-05T18:00:40Z,2008-04-04,http://hdl.handle.net/1721.1/41858,,On a model of visual cortex: learning invariance and selectivity,"In this paper we present a class of algorithms for similarity learning on spaces of images. The general framework that we introduce is motivated by some well-known hierarchical pre-processing architectures for object recognition which have been developed during the last decade, and which have been in some cases inspired by functional models of the ventral stream of the visual cortex. These architectures are characterized by the construction of a hierarchy of â€œlocalâ€ feature representations of the visual stimulus. We show that our framework includes some well-known techniques, and that it is suitable for the analysis of dynamic visual stimuli, presenting a quantitative error analysis in this setting.",MIT-CSAIL-TR-2008-030; CBCL-272,20 p.,,,Learning Theory; Hierarchical Architecture Theory; Unsupervised Learning; Theory of the Visual Cortex,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Trevor Darrell,"Lee, John J.",2008-04-07T20:45:20Z,2008-04-07T20:45:20Z,2008-04-07,http://hdl.handle.net/1721.1/41070,,LIBPMK: A Pyramid Match Toolkit,"LIBPMK is a C++ implementation of Grauman and Darrell's pyramid match algorithm. This toolkit provides a flexible framework with which developers can quickly match sets of image features and run experiments. LIBPMK provides functionality for $k$-means and hierarchical clustering, dealing with data sets too large to fit in memory, building multi-resolution histograms, quickly performing pyramid matches, and training and testing support vector machines (SVMs). This report provides a tutorial on how to use the LIBPMK code, and gives the specifications of the LIBPMK API.",MIT-CSAIL-TR-2008-017,217 p.,,,pmk; vgpmk; pyramid match kernel,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Dina Katabi,"Katabi, Dina; Gollakota, Shyamnath",2008-04-08T19:15:18Z,2008-04-08T19:15:18Z,2008-04-08,http://hdl.handle.net/1721.1/41084,,ZigZag Decoding: Combating Hidden Terminals in Wireless Networks,"This paper presents ZigZag, an 802.11 receiver that combats hidden terminals. ZigZag exploits 802.11 retransmissions which, in the case of hidden terminals, cause successive collisions. Due to asynchrony, these collisions have different interference-free stretches at their start, which ZigZag uses to bootstrap its decoding.  ZigZag makes no changes to the 802.11 MAC and introduces no overhead when there are no collisions. But, when senders collide, ZigZag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. We build a prototype of ZigZag in GNU Radio. In a testbed of 14 USRP nodes, ZigZag reduces the average packet loss rate at hidden terminals from 82.3% to about 0.7%.",MIT-CSAIL-TR-2008-018,14 p.,,,Hidden Terminals; Software Radios; Wireless Networks,Networks & Mobile Systems,,,,,,,,,http://hdl.handle.net/1721.1/42842,http://hdl.handle.net/1721.1/42842,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Tomaso Poggio,"Bileschi, Stanley M",2008-04-09T20:15:10Z,2008-04-09T20:15:10Z,2008-04-09,http://hdl.handle.net/1721.1/41093,,A Multi-Scale Generalization of the HoG and HMAX Image Descriptors for Object Detection,"Recently, several powerful image features have been proposed whichcan be described as spatial histograms of oriented energy. Forinstance, the HoG, HMAX C1, SIFT, and Shape Context feature allrepresent an input image using with a discrete set of bins whichaccumulate evidence for oriented structures over a spatial regionand a range of orientations. In this work, we generalize thesetechniques to allow for a foveated input image, rather than arectilinear raster. It will be shown that improved object detectionaccuracy can be achieved via inputting a spectrum of imagemeasurements, from sharp, fine-scale image sampling within a smallspatial region within the target to coarse-scale sampling of a widefield of view around the target. Several alternative featuregeneration algorithms are proposed and tested which suitably makeuse of foveated image inputs. In the experiments we show thatfeatures generated from the foveated input format produce detectorsof greater accuracy, as measured for four object types from commonlyavailable data-sets. Finally, a flexible algorithm for generatingfeatures is described and tested which is independent of inputtopology and uses ICA to learn appropriate filters.",MIT-CSAIL-TR-2008-019; CBCL-271,8 p.,,,"Object Detection, ICA, Multi-Scale, Image Features",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Trevor Darrell,"Urtasun, Raquel; Quattoni, Ariadna; Lawrence, Neil; Darrell, Trevor",2008-05-05T15:46:05Z,2008-05-05T15:46:05Z,2008-04-11,http://hdl.handle.net/1721.1/41517,,Transferring Nonlinear Representations using Gaussian Processes with a Shared Latent Space,"When a series of problems are related, representations derived from learning earlier tasks may be useful in solving later problems. In this paper we propose a novel approach to transfer learning with low-dimensional, non-linear latent spaces. We show how such representations can be jointly learned across multiple tasks in a Gaussian Process framework. When transferred to new tasks with relatively few training examples, learning can be faster and/or more accurate. Experiments on digit recognition and newsgroup classification tasks show significantly improved performance when compared to baseline performance with a representation derived from a semi-supervised learning approach or with a discriminative approach that uses only the target data.",MIT-CSAIL-TR-2008-020,10 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
William Freeman,"Levin, Anat; Freeman, William T.; Durand, Fredo",2008-05-05T15:45:16Z,2008-05-05T15:45:16Z,2008-04-16,http://hdl.handle.net/1721.1/41513,,Understanding camera trade-offs through a Bayesian analysis of light field projections,"Computer vision has traditionally focused on extracting structure,such as depth, from images acquired using thin-lens or pinhole optics. The development of computational imaging is broadening this scope; a variety of unconventional cameras do not directly capture a traditional image anymore, but instead require the joint reconstruction of structure and image information. For example, recent coded aperture designs have been optimized to facilitate the joint reconstruction of depth and intensity. The breadth of imaging designs requires new tools to understand the tradeoffs implied by different strategies.This paper introduces a unified framework for analyzing computational imagingapproaches. Each sensor element is modeled as an inner product over the 4D light field. The imaging task is then posed as Bayesian inference: given the observed noisy light field projections and a new prior on light field signals, estimatethe original light field. Under common imaging conditions, we compare the performance of various camera designs using 2D light field simulations. This framework allows us to better understand the tradeoffs of each camera type andanalyze their limitations.",MIT-CSAIL-TR-2008-021,21 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Shafi Goldwasser,"Pass, Rafael; Vaikuntanathan, Vinod",2008-05-05T15:46:14Z,2008-05-05T15:46:14Z,2008-04-16,http://hdl.handle.net/1721.1/41518,,New-Age Cryptography,"We introduce new and general complexity theoretic hardness assumptions. These assumptions abstract out concrete properties of a random oracle and are significantly stronger than traditional cryptographic hardness assumptions; however, assuming their validity we can resolve a number of longstandingopen problems in cryptography.",MIT-CSAIL-TR-2008-022,28 pp.,,,"Cryptographic Assumptions, Non-malleable Commitment, Non-malleable Zero-knowledge",Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing",2008-05-05T15:45:41Z,2008-05-05T15:45:41Z,2008-05-01,http://hdl.handle.net/1721.1/41515,,Generalization of the MV Mechanism,"Micali and Valiant proposed a mechanism for combinatorial auctions that is dominant-strategy truthful, guarantees reasonably high revenue, and is very resilient against collusions. Their mechanism, however, uses as a subroutine the VCG mechanism, that is not polynomial time.We propose a modification of their mechanism that is efficient, while retaining their collusion resilience and a good fraction of their revenue, if given as a subroutine an efficient approximation of the VCG mechanism.",MIT-CSAIL-TR-2008-023,7 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Piotr Indyk,"Andoni, Alexandr; Ba, Khanh Do; Indyk, Piotr",2008-05-05T15:45:27Z,2008-05-05T15:45:27Z,2008-05-02,http://hdl.handle.net/1721.1/41514,,Block Heavy Hitters,"e study a natural generalization of the heavy hitters problem in thestreaming context. We term this generalization *block heavy hitters* and define it as follows. We are to stream over a matrix$A$, and report all *rows* that are heavy, where a row is heavy ifits ell_1-norm is at least phi fraction of the ell_1 norm ofthe entire matrix $A$. In comparison, in the standard heavy hittersproblem, we are required to report the matrix *entries* that areheavy. As is common in streaming, we solve the problem approximately:we return all rows with weight at least phi, but also possibly someother rows that have weight no less than (1-eps)phi. To solve theblock heavy hitters problem, we show how to construct a linear sketchof A from which we can recover the heavy rows of A.The block heavy hitters problem has already found applications forother streaming problems. In particular, it is a crucial buildingblock in a streaming algorithm that constructs asmall-size sketch for the Ulam metric, a metric on non-repetitivestrings under the edit (Levenshtein) distance.",MIT-CSAIL-TR-2008-024,3 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Leslie Kaelbling,"McAllester, David; Milch, Brian; Goodman, Noah D.",2008-05-05T15:45:52Z,2008-05-05T15:45:52Z,2008-05-03,http://hdl.handle.net/1721.1/41516,,Random-World Semantics and Syntactic Independence for Expressive Languages,"We consider three desiderata for a language combining logic and probability: logical expressivity, random-world semantics, and the existence of a useful syntactic condition for probabilistic independence. Achieving these three desiderata simultaneously is nontrivial. Expressivity can be achieved by using a formalism similar to a programming language, but standard approaches to combining programming languages with probabilities sacrifice random-world semantics. Naive approaches to restoring random-world semantics undermine syntactic independence criteria. Our main result is a syntactic independence criterion that holds for a broad class of highly expressive logics under random-world semantics. We explore various examples including Bayesian networks, probabilistic context-free grammars, and an example from Mendelian genetics. Our independence criterion supports a case-factor inference technique that reproduces both variable elimination for BNs and the inside algorithm for PCFGs.",MIT-CSAIL-TR-2008-025,6 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Trevor Darrell,"Lee, John J.",2008-05-06T23:00:12Z,2008-05-06T23:00:12Z,2008-05-06,http://hdl.handle.net/1721.1/41519,,Efficient Object Recognition and Image Retrieval for Large-Scale Applications,"Algorithms for recognition and retrieval tasks generally call for both speed and accuracy. When scaling up to very large applications, however, we encounter additional significant requirements: adaptability and scalability. In many real-world systems, large numbers of images are constantly added to the database, requiring the algorithm to quickly tune itself to recent trends so it can serve queries more effectively. Moreover, the systems need to be able to meet the demands of simultaneous queries from many users. In this thesis, I describe two new algorithms intended to meet these requirements and give an extensive experimental evaluation for both. The first algorithm constructs an adaptive vocabulary forest, which is an efficient image-database model that grows and shrinks as needed while adapting its structure to tune itself to recent trends. The second algorithm is a method for efficiently performing classification tasks by comparing query images to only afixed number of training examples, regardless of the size of the image database. These two methods can be combined to create a fast, adaptable, and scalable vision system suitable for large-scale applications. I also introduce LIBPMK, a fast implementation of common computer vision processing pipelines such as that of the pyramid match kernel. This implementation was used to build several successful interactive applications as well as batch experiments for research settings. This implementation, in addition to the two new algorithms introduced by this thesis, are a step toward meeting the speed, adaptability, and scalability requirements of practical large-scale vision systems.",MIT-CSAIL-TR-2008-026,93 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Randall Davis,"Eisenstein, Jacob",2008-05-08T16:30:14Z,2008-05-08T16:30:14Z,2008-05-07,http://hdl.handle.net/1721.1/41526,,Gesture in Automatic Discourse Processing,"Computers cannot fully understand spoken language without access to the wide range of modalities that accompany speech. This thesis addresses the particularly expressive modality of hand gesture, and focuses on building structured statistical models at the intersection of speech, vision, and meaning.My approach is distinguished in two key respects. First, gestural patterns are leveraged to discover parallel structures in the meaning of the associated speech. This differs from prior work that attempted to interpret individual gestures directly, an approach that was prone to a lack of generality across speakers. Second, I present novel, structured statistical models for multimodal language processing, which enable learning about gesture in its linguistic context, rather than in the abstract.These ideas find successful application in a variety of language processing tasks: resolving ambiguous noun phrases, segmenting speech into topics, and producing keyframe summaries of spoken language. In all three cases, the addition of gestural features -- extracted automatically from video -- yields significantly improved performance over a state-of-the-art text-only alternative. This marks the first demonstration that hand gesture improves automatic discourse processing.",MIT-CSAIL-TR-2008-027,153 p.,,,,Natural Language Processing,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Milch, Brian; Koller, Daphne",2008-05-13T16:00:19Z,2008-05-13T16:00:19Z,2008-05-12,http://hdl.handle.net/1721.1/41530,,Ignorable Information in Multi-Agent Scenarios,"In some multi-agent scenarios, identifying observations that an agent can safely ignore reduces exponentially the size of the agent's strategy space and hence the time required to find a Nash equilibrium. We consider games represented using the multi-agent influence diagram (MAID) framework of Koller and Milch [2001], and analyze the extent to which information edges can be eliminated. We define a notion of a safe edge removal transformation, where all equilibria in the reduced model are also equilibria in the original model. We show that existing edge removal algorithms for influence diagrams are safe, but limited, in that they do not detect certain cases where edges can be removed safely. We describe an algorithm that produces the ""minimal"" safe reduction, which removes as many edges as possible while still preserving safety. Finally, we note that both the existing edge removal algorithms and our new one can eliminate equilibria where agents coordinate their actions by conditioning on irrelevant information. Surprisingly, in some games these ""lost"" equilibria can be preferred by all agents in the game.",MIT-CSAIL-TR-2008-029,16 p.,,,Multi-agent influence diagrams; Irrelevance,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-07-09T22:15:19Z,2008-07-09T22:15:19Z,2008-06,http://hdl.handle.net/1721.1/41877,,Knowledge Benchmarks in Adversarial Mechanism Design and Implementation in Surviving Strategies (Part I),"We put forward new benchmarks and solution concepts for Adversarial Mechanism Design, as defined by [MV07.a], and we exemplify them in the case of truly combinatorial auctions.We benchmark the combined performance (the sum of the auction's efficiency and revenue) of a truly combinatorial auction against a very relevant but private knowledge of the players: essentially, the maximum revenue that the best informed player could guarantee if he were the seller. (I.e., by offering each other player a subset of the goods for a take-it-or-leave-it price.)We achieve this natural benchmark within a factor of 2, by means of a new and probabilistic auction mechanism, in surviving strategies. That is, the above performance of our mechanism is guaranteed in any rational play, independent of any possible beliefs of the players. Indeed, our performance guarantee holds for any possible choice of strategies, so long as each player chooses a strategy among those surviving iterated elimination of dominated strategies.Our mechanism is extremely robust. Namely, its performance guarantees hold even if all but one of the players collude (together or in separate groups) in any possible but reasonable way. Essentially, the only restriction for the collective utility function of a collusive subset S of the players is the following: the collective utility increases when one member of S is allocated a ubset of the goods ""individually better"" for him and/or his ""individual price"" is smaller, while the allocations and prices of all other members of S stay the same.Our results improve on the yet unpublished ones of [MV07.b]. The second part of this paper, dealing with a more aggressive benchmark (essentially, the maximum welfare privately known to the players) is forthcoming.",MIT-CSAIL-TR-2008-041,17 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Hari Balakrishnan,"Jamieson, Kyle",2008-06-05T18:00:27Z,2008-06-05T18:00:27Z,2008-06-03,http://hdl.handle.net/1721.1/41857,,The SoftPHY Abstraction: from Packets to Symbols in Wireless Network Design,"At ever-increasing rates, we are using wireless systems to communicatewith others and retrieve content of interest to us. Current wirelesstechnologies such as WiFi or Zigbee use forward error correction todrive bit error rates down when there are few interferingtransmissions. However, as more of us use wireless networks toretrieve increasingly rich content, interference increases inunpredictable ways. This results in errored bits, degradedthroughput, and eventually, an unusable network. We observe that thisis the result of higher layers working at the packet granularity,whereas they would benefit from a shift in perspective from wholepackets to individual symbols.From real-world experiments on a 31-node testbed of Zigbee andsoftware-defined radios, we find that often, not all of the bitsin corrupted packets share fate. Thus, today's wireless protocolsretransmit packets where only a small number of the constituent bitsin a packet are in error, wasting network resources. In thisdissertation, we will describe a physical layer that passesinformation about its confidence in each decoded symbol up to higherlayers. These SoftPHY hints have many applications, one ofwhich, more efficient link-layer retransmissions, we will describe indetail. PP-ARQ is a link-layer reliable retransmission protocolthat allows a receiver to compactly encode a request forretransmission of only the bits in a packet that are likely in error.Our experimental results show that PP-ARQ increases aggregate networkthroughput by a factor of approximately 2x under variousconditions. Finally, we will place our contributions in the contextof related work and discuss other uses of SoftPHY throughout thewireless networking stack.",MIT-CSAIL-TR-2008-031,153 p.,,,802.11; 802.15.4,Networks & Mobile Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Seth Teller,"Huang, Albert S.; Teller, Seth",2008-06-11T20:15:09Z,2008-06-11T20:15:09Z,2008-06-06,http://hdl.handle.net/1721.1/41860,,Non-Metrical Navigation Through Visual Path Control,"We describe a new method for wide-area, non-metrical robot navigationwhich enables useful, purposeful motion indoors. Our method has twophases: a training phase, in which a human user directs a wheeledrobot with an attached camera through an environment while occasionallysupplying textual place names; and a navigation phase in which theuser specifies goal place names (again as text), and the robot issueslow-level motion control in order to move to the specified place. We show thatdifferences in the visual-field locations and scales of features matched acrosstraining and navigation can be used to construct a simple and robust controlrule that guides the robot onto and along the training motion path.Our method uses an omnidirectional camera, requires approximateintrinsic and extrinsic camera calibration, and is capable of effective motioncontrol within an extended, minimally-prepared building environment floorplan.We give results for deployment within a single building floor with 7 rooms, 6corridor segments, and 15 distinct place names.",MIT-CSAIL-TR-2008-032,8 p.,,,,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Trevor Darrell,"Yeh, Tom; Lee, John J.; Darrell, Trevor",2008-06-11T20:15:30Z,2008-06-11T20:15:30Z,2008-06-10,http://hdl.handle.net/1721.1/41862,,Fast concurrent object classification and localization,"Object localization and classification are important problems incomputer vision. However, in many applications, exhaustive searchover all class labels and image locations is computationallyprohibitive. While several methods have been proposed to makeeither classification or localization more efficient, few havedealt with both tasks simultaneously. This paper proposes anefficient method for concurrent object localization andclassification based on a data-dependent multi-classbranch-and-bound formalism. Existing bag-of-featuresclassification schemes, which can be expressed as weightedcombinations of feature counts can be readily adapted to ourmethod. We present experimental results that demonstrate the meritof our algorithm in terms of classification accuracy, localizationaccuracy, and speed, compared to baseline approaches includingexhaustive search, the ISM method, and single-class branch andbound.",MIT-CSAIL-TR-2008-033,9 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Karen Sollins,"Li, Ji",2008-06-11T20:15:18Z,2008-06-11T20:15:18Z,2008-06-11,http://hdl.handle.net/1721.1/41861,,Agent Organization in the Knowledge Plane,"In designing and building a network like the Internet, we continue to face the problems of scale and distribution. With the dramatic expansion in scale and heterogeneity of the Internet, network management has become an increasingly difficult task. Furthermore, network applications often need to maintain efficient organization among the participants by collecting information from the underlying networks. Such individual information collection activities lead to duplicate efforts and contention for network resources.The Knowledge Plane (KP) is a new common construct that provides knowledge and expertise to meet the functional, policy and scaling requirements of network management, as well as to create synergy and exploit commonality among many network applications. To achieve these goals, we face many challenging problems, including widely distributed data collection, efficient processing of that data, wide availability of the expertise, etc.In this thesis, to provide better support for network management and large-scale network applications, I propose a knowledge plane architecture that consists of a network knowledge plane (NetKP) at the network layer, and on top of it, multiple specialized KPs (spec-KPs). The NetKP organizes agents to provide valuable knowledge and facilities about the Internet to the spec-KPs. Each spec-KP is specialized in its own area of interest. In both the NetKP and the spec-KPs, agents are organized into regions based on different sets of constraints. I focus on two key design issues in the NetKP: (1) a regionbased architecture for agent organization, in which I design an efficient and non-intrusive organization among regions that combines network topology and a distributed hash table; (2) request and knowledge dissemination, in which I design a robust and efficient broadcast and aggregation mechanism using a tree structure among regions. In the spec-KPs, I build two examples: experiment management on the PlanetLab testbed and distributed intrusion detection on the DETER testbed. The experiment results suggest a common approach driven by the design principles of the Internet and more specialized constraints can derive productive organization for network management and applications.",MIT-CSAIL-TR-2008-034,191 p.,,,broadcast; region; knowledge plane; network pnowledge plane; agent; intrusion detection; specialized knowledge plane; distributed hash table; aggregation,Advanced Network Architecture,,,,,,,,,,,,; Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,,,,,,,,,,,,,
Chris Terman,"Carli, Roberto",2008-07-02T06:00:36Z,2008-07-02T06:00:36Z,2008-06-16,http://hdl.handle.net/1721.1/41874,,Flexible MIPS Soft Processor Architecture,"The flexible MIPS soft processor architecture borrows selected technologies from high-performance computing to deliver a modular, highly customizable CPU targeted towards FPGA implementations for embedded systems; the objective is to provide a more flexible architectural alternative to coprocessor-based solutions. The processor performs out-of-order execution on parallel functional units, it delivers in-order instruction commit and it is compatible with the MIPS-1 Instruction Set Architecture. Amongst many available options, the user can introduce custom instructions and matching functional units; modify existing units; change the pipelining depth within functional units to any fixed or variable value; customize instruction definitions in terms of operands, control signals and register file interaction; insert multiple redundant functional units for improved performance. The flexibility provided by the architecture allows the user to expand the processor functionality to implement instructions of coprocessor-level complexity through additional functional units. The processor design was implemented and simulated on two FPGA platforms, tested on multiple applications, and compared to three commercially available soft processor solutions in terms of features, area, clock frequency and benchmark performance.",MIT-CSAIL-TR-2008-036,49 p.,,,,Computer Architecture,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-07-02T06:00:46Z,2008-07-02T06:00:46Z,2008-06-17,http://hdl.handle.net/1721.1/41875,,Leveraging Player Knowledge in Combinatorial Auctions (and Implementation in Surviving Strategies),,MIT-CSAIL-TR-2008-037,6 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Eric Grimson,"Grimson, Eric; Wang, Xiaogang; Ng, Gee-Wah; Ma, Keng Teck",2008-03-17T19:15:17Z,2008-03-17T19:15:17Z,2008-06-24,http://hdl.handle.net/1721.1/40808,,Trajectory Analysis and Semantic Region Modeling Using A Nonparametric Bayesian Model,"We propose a novel nonparametric Bayesian model, Dual Hierarchical Dirichlet Processes (Dual-HDP), for trajectory analysis and semantic region modeling in surveillance settings, in an unsupervised way. In our approach, trajectories are treated as documents and observations of an object on a trajectory are treated as words in a document. Trajectories are clustered into different activities. Abnormal trajectories are detected as samples with low likelihoods. The semantic regions, which are intersections of paths commonly taken by objects, related to activities in the scene are also modeled. Dual-HDP advances the existing Hierarchical Dirichlet Processes (HDP) language model. HDP only clusters co-occurring words from documents into topics and automatically decides the number of topics. Dual-HDP co-clusters both words and documents. It learns both the numbers of word topics and document clusters from data. Under our problem settings, HDP only clusters observations of objects, while Dual-HDP clusters both observations and trajectories. Experiments are evaluated on two data sets, radar tracks collected from a maritime port and visual tracks collected from a parking lot.",MIT-CSAIL-TR-2008-015,12 p.,,,"hierarchical Dirichlet processes, activity analysis, clustering, visual surveillance",Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Barbara Liskov,"Vandiver, Benjamin Mead",2008-07-02T06:00:10Z,2008-07-02T06:00:10Z,2008-06-30,http://hdl.handle.net/1721.1/41873,,Detecting and Tolerating Byzantine Faults in Database Systems,"This thesis describes the design, implementation, and evaluation of a replication scheme to handle Byzantine faults in transaction processing database systems. The scheme compares answers from queries and updates on multiple replicas which are off-the-shelf database systems, to provide a single database that is Byzantine fault tolerant. The scheme works when the replicas are homogeneous, but it also allows heterogeneous replication in which replicas come from different vendors. Heterogeneous replicas reduce the impact of bugs and security compromises because they are implemented independently and are thus less likely to suffer correlated failures. A final component of the scheme is a repair mechanism that can correct the state of a faulty replica, ensuring the longevity of the scheme.The main challenge in designing a replication scheme for transaction processingsystems is ensuring that the replicas state does not diverge while allowing a high degree of concurrency. We have developed two novel concurrency control protocols, commit barrier scheduling (CBS) and snapshot epoch scheduling (SES) that provide strong consistency and good performance. The two protocols provide different types of consistency: CBS provides single-copy serializability and SES provides single-copy snapshot isolation. We have implemented both protocols in the context of a replicated SQL database. Our implementation has been tested with production versions of several commercial and open source databases as replicas. Our experiments show a configuration that can tolerate one faulty replica has only a modest performance overhead (about 10-20% for the TPC-C benchmark). Our implementation successfully masks several Byzantine faults observed in practice and we have used it to find a new bug in MySQL.",MIT-CSAIL-TR-2008-040,174 p.,,,,Programming Methodology,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-07-14T19:45:18Z,2008-07-14T19:45:18Z,2008-07,http://hdl.handle.net/1721.1/41878,,Knowledge Benchmarks in Adversarial Mechanism Design (Part I) and Implementation in Surviving Strategies (Part I),"We put forward new benchmarks and solution concepts for Adversarial Mechanism Design, as defined by [MV07.a], and we exemplify them in the case of truly combinatorial auctions.We benchmark the combined performance (the sum of the auction's effciency and revenue)of a truly combinatorial auction against a very relevant but private knowledge of the players: essentially, the maximum revenue that the best informed player could guarantee if he were the seller. (I.e., by offering each other player a subset of the goods for a take-it-or-leave-it price.) We achieve this natural benchmark within a factor of 2, by means of a new and probabilisticauction mechanism, in KNOWLINGLY SURVIVING STRATEGIES. That is, the above performance of our mechanism is guaranteed in any rational play, independent of any possible beliefs of the players. Indeed, our performance guarantee holds for any possible choice of strategies, so long as each player chooses a strategy among those surviving iterated elimination of knowingly dominated strategies.Our mechanism is extremely robust. Namely, its performance guarantees hold even if all but one of the players collude (together or in separate groups) in any possible but reasonable way. Essentially, the only restriction for the collective utility function of a collusive subset S of the players is the following: the collective utility increases when one member of S is allocated asubset of the goods ""individually better"" for him and/or his ""individual price"" is smaller, while the allocations and prices of all other members of S stay the same.Our results improve on the yet unpublished ones of [MV07.b]. The second part of this paper, dealing with a more aggressive benchmark (essentially, the maximum welfare privately known to the players) is forthcoming.",MIT-CSAIL-TR-2008-042,30 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Gerald Sussman,"Qumsiyeh, Dany M.",2008-07-16T20:15:24Z,2008-07-16T20:15:24Z,2008-07-14,http://hdl.handle.net/1721.1/41879,,A Distributed Building Evacuation System,"This thesis investigates the feasibility of a smart building evacuation system, capable of guiding occupants along safe paths to exits and responding to changing threats. Inspired by developments in amorphous computing, the design presented is scalable to large networks, robust to hardware and communication failure, and based on simple low-cost components. A simulation and hardware prototype demonstrate that this distributed building evacuation system is both feasible and cost effective.",MIT-CSAIL-TR-2008-043,82 p.,,,sensor networks; gradient; distributed control; synchronization; threat avoidance; fire; emergency,Mathematics and Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Trevor Darrell,"Quattoni, Ariadna; Carreras, Xavier; Collins, Michael; Darrell, Trevor",2008-07-24T20:00:14Z,2008-07-24T20:00:14Z,2008-07-23,http://hdl.handle.net/1721.1/41888,,A Projected Subgradient Method for Scalable Multi-Task Learning,"Recent approaches to multi-task learning have investigated the use of a variety of matrix norm regularization schemes for promoting feature sharing across tasks.In essence, these approaches aim at extending the l1 framework for sparse single task approximation to the multi-task setting. In this paper we focus on the computational complexity of training a jointly regularized model and propose an optimization algorithm whose complexity is linear with the number of training examples and O(n log n) with n being the number of parameters of the joint model. Our algorithm is based on setting jointly regularized loss minimization as a convex constrained optimization problem for which we develop an efficient projected gradient algorithm. The main contribution of this paper is the derivation of a gradient projection method with l1âˆ’âˆž constraints that can be performed efficiently and which has convergence rates.",MIT-CSAIL-TR-2008-045,8 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Patrick Winston,"Bonawitz, Keith A",2008-07-23T19:30:16Z,2008-07-23T19:30:16Z,2008-07-23,http://hdl.handle.net/1721.1/41887,,Composable Probabilistic Inference with Blaise,"Probabilistic inference provides a unified, systematic framework for specifying and solving these problems. Recent work has demonstrated the great value of probabilistic models defined over complex, structured domains. However, our ability to imagine probabilistic models has far outstripped our ability to programmatically manipulate them and to effectively implement inference, limiting the complexity of the problems that we can solve in practice.This thesis presents Blaise, a novel framework for composable probabilistic modeling and inference, designed to address these limitations. Blaise has three components: * The Blaise State-Density-Kernel (SDK) graphical modeling language that generalizes factor graphs by: (1) explicitly representing inference algorithms (and their locality) using a new type of graph node, (2) representing hierarchical composition and repeated substructures in the state space, the interest distribution, and the inference procedure, and (3) permitting the structure of the model to change during algorithm execution. * A suite of SDK graph transformations that may be used to extend a model (e.g. to construct a mixture model from a model of a mixture component), or to make inference more effective (e.g. by automatically constructing a parallel tempered version of an algorithm or by exploiting conjugacy in a model). * The Blaise Virtual Machine, a runtime environment that can efficiently execute the stochastic automata represented by Blaise SDK graphs. Blaise encourages the construction of sophisticated models by composing simpler models, allowing the designer to implement and verify small portions of the model and inference method, and to reuse model components from one task to another. Blaise decouples the implementation of the inference algorithm from the specification of the interest distribution, even in cases (such as Gibbs sampling) where the shape of the interest distribution guides the inference. This gives modelers the freedom to explore alternate models without slow, error-prone reimplementation. The compositional nature of Blaise enables novel reinterpretations of advanced Monte Carlo inference techniques (such as parallel tempering) as simple transformations of Blaise SDK graphs.In this thesis, I describe each of the components of the Blaise modeling framework, as well as validating the Blaise framework by highlighting a variety of contemporary sophisticated models that have been developed by the Blaise user community. I also present several surprising findings stemming from the Blaise modeling framework, including that an Infinite Relational Model can be built using exactly the same inference methods as a simple mixture model, that constructing a parallel tempered inference algorithm should be a point-and-click/one-line-of-code operation, and that Markov chain Monte Carlo for probabilistic models with complicated long-distance dependencies, such as a stochastic version of Scheme, can be managed using standard Blaise mechanisms.",MIT-CSAIL-TR-2008-044,190 p.,,,Bayesian; MCMC,Genesis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Tomaso Poggio,"De Mol, Christine; Rosasco, Lorenzo; De Vito, Ernesto",2008-07-24T20:00:33Z,2008-07-24T20:00:33Z,2008-07-24,http://hdl.handle.net/1721.1/41889,MIT-CSAIL-TR-2008-046; CBCL-273,Elastic-Net Regularization in Learning Theory,"Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie [""Regularization and variable selection via the elastic net"" J. R. Stat. Soc. Ser. B, 67(2):301-320, 2005] for the selection of groups of correlated variables. To investigate on the statistical properties of this scheme and in particular on its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combination of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular ""elastic-net representation"" of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in ""Regularization and variable selection via the elastic net"".",,32 p.,,,machine learning; regularization; feature selection,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
William Freeman,"Levin, Anat; Freeman, William; Durand, Fredo",2008-07-28T16:30:15Z,2008-07-28T16:30:15Z,2008-07-28,http://hdl.handle.net/1721.1/41892,,Understanding camera trade-offs through a Bayesian analysis of light field projections - A revision,"Computer vision has traditionally focused on extracting structure,such as depth, from images acquired using thin-lens or pinholeoptics. The development of computational imaging is broadening thisscope; a variety of unconventional cameras do not directly capture atraditional image anymore, but instead require the jointreconstruction of structure and image information. For example, recentcoded aperture designs have been optimized to facilitate the jointreconstruction of depth and intensity. The breadth of imaging designs requires new tools to understand the tradeoffs implied bydifferent strategies. This paper introduces a unified framework for analyzing computational imaging approaches.Each sensor element is modeled as an inner product over the 4D light field.The imaging task is then posed as Bayesian inference: giventhe observed noisy light field projections and a new prior on light field signals, estimate the original light field. Under common imaging conditions, we compare theperformance of various camera designs using 2D light field simulations. Thisframework allows us to better understand the tradeoffs of each camera type and analyze their limitations.",MIT-CSAIL-TR-2008-049,26 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
Nancy Lynch,"Umeno, Shinya",2008-07-28T13:30:25Z,2008-07-28T13:30:25Z,2008-07-28,http://hdl.handle.net/1721.1/41891,,Event Order Abstraction for Parametric Real-Time System Verification,"We present a new abstraction technique, event order abstraction (EOA), for parametric safety verification of real-time systems in which ``correct orderings of events'' needed for system correctness are maintained by timing constraints on the systems' behavior. By using EOA, one can separate the task of verifying a real-time system into two parts: 1. Safety property verification of the system given that only correct event orderings occur; and 2. Derivation of timing parameter constraints for correct orderings of events in the system.The user first identifies a candidate set of bad event orders.Then, by using ordinary untimed model-checking, the user examines whether a discretized system model in which all timing constraints are abstracted away satisfies a desirable safety property under the assumption that the identified bad event orders occur in no system execution. The user uses counterexamples obtained from the model-checker to identify additional bad event orders, and repeats the process until the model-checking succeeds. In this step, the user obtains a sufficient set of bad event orders that must be excluded by timing synthesis for system correctness.Next, the algorithm presented in the paper automatically derives a set of timing parameter constraints under which the system does not exhibit the identified bad event orderings. From this step combined with the untimed model-checking step,the user obtains a sufficient set of timing parameter constraints under which the system executes correctly with respect to a given safety property.We illustrate the use of EOA with a train-gate example inspired by the general railroad crossing problem. We also summarize three other case studies, a biphase mark protocol, the IEEE 1394 root contention protocol, and the Fischer mutual exclusion algorithm.",MIT-CSAIL-TR-2008-048,19 p.,,,parametric verification; event-based approach; counter-example guided abstraction refinement (CEGAR); automatic timing synthesis,Theory of Computation,,,,,,,,,,,,; Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Gardiol, Natalia H.; Kaelbling, Leslie Pack",2008-08-01T21:30:16Z,2008-08-01T21:30:16Z,2008-07-29,http://hdl.handle.net/1721.1/41920,,Adaptive Envelope MDPs for Relational Equivalence-based Planning,"We describe a method to use structured representations of the environmentâ€™s dynamics to constrain and speed up the planning process. Given a problem domain described in a probabilistic logical description language, we develop an anytime technique that incrementally improves on an initial, partial policy. This partial solution is found by ï¬rst reducing the number of predicates needed to represent a relaxed version of the problem to a minimum, and then dynamically partitioning the action space into a set of equivalence classes with respect to this minimal representation. Our approach uses the envelope MDP framework, which creates a Markov decision process out of a subset of the full state space as de- termined by the initial partial solution. This strategy permits an agent to begin acting within a restricted part of the full state space and to expand its envelope judiciously as resources permit.",MIT-CSAIL-TR-2008-050,17 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,
,"Silva, Marco da; Popovic, Jovan; Abe, Yeuhi",2008-08-28T18:45:23Z,2008-08-28T18:45:23Z,2008-08-01,http://hdl.handle.net/1721.1/42003,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Interactive Simulation of Stylized Human Locomotion,"Animating natural human motion in dynamic environments is difficult because of complex geometric and physical interactions. Simulation provides an automatic solution to parts of this problem, but it needs control systems to produce lifelike motions. This paper describes the systematic computation of controllers that can reproduce a range of locomotion styles in interactive simulations. Given a reference motion that describes the desired style, a derived control system can reproduce that style in simulation and in new environments. Because it produces high-quality motions that are both geometrically and physically consistent with simulated surroundings, interactive animation systems could begin to use this approach with more established kinematic methods.",,,,,animation; control; robotics,,,,,,"ACM Transactions on Graphics, Volume 27, Issue 3",Jovan Popovic; Computer Graphics,,,,,,,,,,,,,,,,,,,,,,
Ronitt Rubinfeld,"Agarwal, Shivani",2008-08-14T17:15:02Z,2008-08-14T17:15:02Z,2008-08-07,http://hdl.handle.net/1721.1/41938,MIT-CSAIL-TR-2008-051,Transductive Ranking on Graphs,"In ranking, one is given examples of order relationships among objects, and the goal is to learn from these examples a real-valued ranking function that induces a ranking or ordering over the object space. We consider the problem of learning such a ranking function in a transductive, graph-based setting, where the object space is finite and is represented as a graph in which vertices correspond to objects and edges encode similarities between objects. Building on recent developments in regularization theory for graphs and corresponding Laplacian-based learning methods, we develop an algorithmic framework for learning ranking functions on graphs. We derive generalization bounds for our algorithms in transductive models similar to those used to study other transductive learning problems, and give experimental evidence of the potential benefits of our framework.",,27 p.,,,Graph-based learning; Transductive learning; Ranking; Graph regularization,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Sodini, Charles; Edalat, Farinaz; Katabi, Dina; Kushman, Nate; Rahul, Hariharan",2010-02-02T23:15:08Z,2010-02-02T23:15:08Z,2008-08-17/2008-08-22,http://hdl.handle.net/1721.1/51335,MIT-CSAIL-TR-2010-001,SWIFT: A Narrowband-Friendly Cognitive Wideband Network,"Wideband technologies in the unlicensed spectrum can satisfy the ever-increasing demands for wireless bandwidth created by emerging rich media applications. The key challenge for such systems, however, is to allow narrowband technologies that share these bands (say, 802.11 a/b/g/n, Zigbee) to achieve their normal performance, without compromising the throughput or range of the wideband network.This paper presents SWIFT, the first system where high-throughput wideband nodes are shown in a working deployment to coexist with unknown narrowband devices, while forming a network of their own. Prior work avoids narrowband devices by operating below the noise level and limiting itself to a single contiguous unused band. While this achieves coexistence, it sacrifices the throughput and operating distance of the wideband device. In contrast, SWIFT creates high throughput wireless links by weaving together non-contiguous unused frequency bands that change as narrowband devices enter or leave the environment. This design principle of cognitive aggregation allows SWIFT to achieve coexistence, while operating at normal power, and thereby obtaining higher throughput and greater operating range. We implement SWIFT on a wideband hardware platform, and evaluate it in the presence of 802.11 devices. In comparison to a baseline that coexists with narrowband devices by operating below their noise level, SWIFT is equally narrowband-friendly but achieves 3.6x-10.5x higher throughput and 6x greater range.",,13 p.,,,Cognitive Radios; White Spaces; Cognitive Aggregation; Wideband; Wireless Networks,Networks & Mobile Systems,,,,,"""Learning to Share: Narrowband-Friendly Wideband Networks"", ACM SIGCOMM 2008",,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"De Vito, Ernesto; Belkin, Mikhail; Rosasco, Lorenzo",2008-08-20T19:15:07Z,2008-08-20T19:15:07Z,2008-08-19,http://hdl.handle.net/1721.1/41940,MIT-CSAIL-TR-2008-052; CBCL-274,A Note on Perturbation Results for Learning Empirical Operators,"A large number of learning algorithms, for example, spectral clustering, kernel Principal Components Analysis and many manifold methods are based on estimating eigenvalues and eigenfunctions of operators defined by a similarity function or a kernel, given empirical data. Thus for the analysis of algorithms, it is an important problem to be able to assess the quality of such approximations. The contribution of our paper is two-fold:  1. We use a technique based on a concentration inequality for Hilbert spaces to provide new much simplified proofs for a number of results in spectral approximation.  2. Using these methods we provide several new results for estimating spectral properties of the graph Laplacian operator extending and strengthening results from [26].",,22 p.,,,perturbation theory; statistical learning theory; kernel methods; spectral methods,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dig, Danny; Marrero, John; Ernst, Michael D.",2008-09-05T21:00:10Z,2008-09-05T21:00:10Z,2008-09-05,http://hdl.handle.net/1721.1/42832,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,How do programs become more concurrent? A story of program transformations,"For several decades, programmers have relied onMooreâ  s Law to improve the performance of their softwareapplications. From now on, programmers need to programthe multi-cores if they want to deliver efficient code. Inthe multi-core era, a major maintenance task will be tomake sequential programs more concurrent. What are themost common transformations to retrofit concurrency intosequential programs?We studied the source code of 5 open-source Javaprojects. We analyzed qualitatively and quantitatively thechange patterns that developers have used in order toretrofit concurrency. We found that these transformationsbelong to four categories: transformations that improve thelatency, the throughput, the scalability, or correctness of theapplications. In addition, we report on our experience ofparallelizing one of our own programs. Our findings caneducate software developers on how to parallelize sequentialprograms, and can provide hints for tool vendors aboutwhat transformations are worth automating.",,10 p.,,,concurrency; program transformation; refactoring,,,,,,,Michael Ernst; Program Analysis,,,,,,,,local: MIT-CSAIL-TR-2008-053,,,,,,,,,,,,,,
,"Kiezun, Adam; Guo, Philip J.; Jayaraman, Karthick; Ernst, Michael D.",2008-09-25T19:00:06Z,2008-09-25T19:00:06Z,2008-09-10,http://hdl.handle.net/1721.1/42836,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Creation of SQL Injection and Cross-Site Scripting Attacks,"We present a technique for finding security vulnerabilitiesin Web applications. SQL Injection (SQLI) and cross-sitescripting (XSS) attacks are widespread forms of attackin which the attacker crafts the input to the application toaccess or modify user data and execute malicious code. Inthe most serious attacks (called second-order, or persistent,XSS), an attacker can corrupt a database so as to causesubsequent users to execute malicious code.This paper presents an automatic technique for creatinginputs that expose SQLI and XSS vulnerabilities. The techniquegenerates sample inputs, symbolically tracks taintsthrough execution (including through database accesses),and mutates the inputs to produce concrete exploits. Oursis the first analysis of which we are aware that preciselyaddresses second-order XSS attacks.Our technique creates real attack vectors, has few falsepositives, incurs no runtime overhead for the deployed application,works without requiring modification of applicationcode, and handles dynamic programming-languageconstructs. We implemented the technique for PHP, in a toolArdilla. We evaluated Ardilla on five PHP applicationsand found 68 previously unknown vulnerabilities (23 SQLI,33 first-order XSS, and 12 second-order XSS).",,11 p.,,,reliability; dynamic analysis; dynamic taint,,,,,,,Michael Ernst; Program Analysis,,,,,,,,local: MIT-CSAIL-TR-2008-054,,,,,,,,,,,,,,
David Karger,"Nikolova, Evdokia",2008-09-25T19:00:16Z,2008-09-25T19:00:16Z,2008-09-13,http://hdl.handle.net/1721.1/42837,MIT-CSAIL-TR-2008-055,Stochastic Combinatorial Optimization with Risk,"We consider general combinatorial optimization problems that can be formulated as minimizing the weight of a feasible solution wT x over an arbitrary feasible set. For these problems we describe a broad class of corresponding stochastic problems where the weight vector W has independent random components, unknown at the time of solution. A natural and important objective which incorporates risk in this stochastic setting, is to look for a feasible solution whose stochastic weight has a small tail or a small linear combination of mean and standard deviation. Our models can be equivalently reformulated as deterministic nonconvex programs for which no efficient algorithms are known. In this paper, we make progress on these hard problems.  Our results are several efficient general-purpose approximation schemes. They use as a black-box (exact or approximate) the solution to the underlying deterministic combinatorial problem and thus immediately apply to arbitrary combinatorial problems. For example, from an available ?-approximation algorithm to the deterministic problem, we construct a ?(1 + ?)-approximation algorithm that invokes the deterministic algorithm only a logarithmic number of times in the input and polynomial in 1/?, for any desired accuracy level ? > 0. The algorithms are based on a geometric analysis of the curvature and approximability of the nonlinear level sets of the objective functions.",,25 p.,,,"approximation algorithms, combinatorial optimization, stochastic optimization, risk, nonconvex optimization, concave programming",Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trevor Darrell,"Stiefelhagen, Rainer; Darrell, Trevor; Urtasun, Raquel; Geiger, Andreas",2008-09-29T20:15:10Z,2008-09-29T20:15:10Z,2008-09-26,http://hdl.handle.net/1721.1/42840,MIT-CSAIL-TR-2008-056,Rank Priors for Continuous Non-Linear Dimensionality Reduction,"Non-linear dimensionality reduction methods are powerful techniques to deal with high-dimensional datasets. However, they often are susceptible to local minima and perform poorly when initialized far from the global optimum, even when the intrinsic dimensionality is known a priori. In this work we introduce a prior over the dimensionality of the latent space, and simultaneously optimize both the latent space and its intrinsic dimensionality. Ad-hoc initialization schemes are unnecessary with our approach; we initialize the latent space to the observation space and automatically infer the latent dimensionality using an optimization scheme that drops dimensions in a continuous fashion. We report results applying our prior to various tasks involving probabilistic non-linear dimensionality reduction, and show that our method can outperform graph-based dimensionality reduction techniques as well as previously suggested ad-hoc initialization strategies.",,8 p.,,,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Ernst, Michael D.; Marrero, John; Dig, Danny",2008-09-30T19:15:06Z,2008-09-30T19:15:06Z,2008-09-30,http://hdl.handle.net/1721.1/42841,MIT-CSAIL-TR-2008-057,Refactoring Sequential Java Code for Concurrency via Concurrent Libraries,"Parallelizing existing sequential programs to run efficiently on multicores is hard. The Java 5 packagejava.util.concurrent (j.u.c.) supports writing concurrent programs: much of the complexity of writing threads-safe and scalable programs is hidden in the library.  To use this package, programmers still need to reengineer existing code. This is tedious because it requires changing many lines of code, is error-prone because programmers can use the wrong APIs, and is omission-prone because programmers can miss opportunities to use the enhanced APIs.  This paper presents our tool, CONCURRENCER, which enables programmers to refactor sequential code into parallel code that uses j.u.c. concurrent utilities. CONCURRENCER does not require any program annotations, although the transformations are very involved: they span multiple program statements and use custom program analysis.  A find-and-replace tool can not perform such transformations.  Empirical evaluation shows that CONCURRENCER refactors code effectively: CONCURRENCER correctly identifies and applies transformations that some open-source developers overlooked, and the converted code exhibits good speedup.",,12 p.,,,program transformations; concurrency; refactoring; library,Program Analysis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Katabi, Dina; Gollakota, Shyamnath",2008-10-02T15:45:10Z,2008-10-02T15:45:10Z,2008-10-01,http://hdl.handle.net/1721.1/42842,MIT-CSAIL-TR-2008-058,ZigZag Decoding: Combating Hidden Terminals In Wireless Networks,"This paper presents ZigZag, an 802.11 receiver design that combats hidden terminals. ZigZag's core contribution is a new form of interference cancellation that exploits asynchrony across successive collisions. Specifically, 802.11 retransmissions, in the case of hidden terminals, cause successive collisions. These collisions have different interference-free stretches at their start, which ZigZag exploits to bootstrap its decoding. ZigZag makes no changes to the 802.11 MAC and introduces no overhead when there are no collisions. But, when senders collide, ZigZag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. We build a prototype of ZigZag in GNU Radio. In a testbed of 14 USRP nodes, ZigZag reduces the average packet loss rate at hidden terminals from 72.6% to about 0.7%.",,14 p.,,,,Networks & Mobile Systems,,,,,,,,,,http://hdl.handle.net/1721.1/41084,http://hdl.handle.net/1721.1/41084,,,,,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Chen, Jing",2008-12-03T16:15:09Z,2008-12-03T16:15:09Z,2008-10-08,http://hdl.handle.net/1721.1/43715,MIT-CSAIL-TR-2008-071,Resilient Knowledge-Based  Mechanisms  For Truly Combinatorial Auctions (And Implementation in Surviving Strategies),"We put forward a new mechanism achieving a high benchmark for (both revenue and) the sum of revenue and efficiency in truly combinatorial auctions. Notably, our mechanism guarantees its performance (1) in a very adversarial collusion model; (2) for any profile of strategies surviving the iterated elimination of dominated strategies; and (3) by leveraging the knowledge that the players have about each other (in a non-Bayesian setting).Our mechanism also is computationally efficient, and preserves the players' privacy to an unusual extent.",,18 p.,,,Implementation in surviving strategies; Resilient Mechanism Design; Privacy-preserving mechanisms; Equilibrium-less mechanism design; Knowledge benchmarks,Theory of Computation,,,,,,Silvio Micali; Theory of Computation,,,,,,MIT-CSAIL-TR-2008-059,,,,,,,,,,,,,,,,
Anant Agarwal,"Agarwal, Anant; Wentzlaff, David",2008-10-08T23:15:04Z,2008-10-08T23:15:04Z,2008-10-08,http://hdl.handle.net/1721.1/42894,MIT-CSAIL-TR-2008-060,The Case for a Factored Operating System (fos),"The next decade will afford us computer chips with 1,000 - 10,000 cores on a single piece of silicon. Contemporary operating systems have been designed to operate on a single core or small number of cores and hence are not well suited to manage and provide operating system services at such large scale. Managing 10,000 cores is so fundamentally different from managing two cores that the traditional evolutionary approach of operating system optimization will cease to work. The fundamental design of operating systems and operating system data structures must be rethought. This work begins by documenting the scalability problems of contemporary operating systems. These studies are used to motivate the design of a factored operating system (fos). fos is a new operating system targeting 1000+ core multicore systems where space sharing replaces traditional time sharing to increase scalability. fos is built as a collection of Internet inspired services. Each operating system service is factored into a fleet of communicating servers which in aggregate implement a system service. These servers are designed much in the way that distributed Internet services are designed, but instead of providing high level Internet services, these servers provide traditional kernel services and manage traditional kernel data structures in a factored, spatially distributed manner. The servers are bound to distinct processing cores and by doing so do not fight with end user applications for implicit resources such as TLBs and caches. Also, spatial distribution of these OS services facilitates locality as many operations only need to communicate with the nearest server for a given service.",,12 p.,,,multicore; operating system design; manycore,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-10-08T20:15:07Z,2008-10-08T20:15:07Z,2008-10-08,http://hdl.handle.net/1721.1/42893,MIT-CSAIL-TR-2008-059,New Resiliency in  Truly Combinatorial Auctions  (and Implementation in Surviving Strategies),"Following Micali and Valiant [MV07.a], a mechanism is resilient if it achieves its objective without any problem of (1) equilibrium selection and (2) player collusion. To advance resilient mechanism design,We put forward a new meaningful benchmark for the COMBINED social welfare-revenue performance of any mechanism in truly combinatorial auctions.We put forward a NEW notion of implementation, much more general than the ones used so far, which we believe to be of independent interest.We put forward a new RESILIENT mechanism that, by leveraging the knowledge that the players have about each other, guarantees at least one half of our benchmark under a very general collusion model.",,32 p.,,,knowledge benchmarks; implementation in surviving strategies; equilibrium-less implementation; combinatorial auctions; resilient mechanisms; collusion; truly combinatorial auctions,Theory of Computation,,,,,,,,,,,,MIT-CSAIL-TR-2008-041,,,,,,,,,,,,,,,,
Daniel Jackson,"Edwards, Jonathan",2008-10-10T21:15:05Z,2008-10-10T21:15:05Z,2008-10-10,http://hdl.handle.net/1721.1/42895,MIT-CSAIL-TR-2008-061,Modular Generation and Customization,"Modularity and flexibility can conflict in multi-language systems. For example, the templates commonly used to generate web pages must be manually updated when the database schema changes. Modularity can be improved by generating web pages automatically from the database schema, but it is hard for such a generator to produce the same variety of outputs that are easily achieved by ad hoc edits to a template. Ideally, such ad hoc edits would be abstracted into transformations that compose with the generator, offering both modularity and flexibility. However common customizations cannot be abstracted using the standard techniques of textual identifiers and ordinal positions. These difficulties are distilled into a challenge problem to evaluate potential solutions. A solution is proposed based on field trees, a new data model for software artifacts that provides persistent identifiers and unshifting positions within sequences. But using field trees with conventional programming languages and development environments requires more effort than the ad hoc editing they seek to supplant. Field trees are therefore extended into differential trees, which integrate artifacts and their transformations into a unified representation.",,13 p.,,,,Software Design,,Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Rosasco, Lorenzo; Pereverzyev, Sergei; De Vito, Ernesto",2008-10-17T15:30:10Z,2008-10-17T15:30:10Z,2008-10-16,http://hdl.handle.net/1721.1/42896,MIT-CSAIL-TR-2008-062; CBCL-275,Adaptive Kernel Methods Using the Balancing Principle,"The regularization parameter choice is a fundamental problem in supervised learning since the performance of most algorithms crucially depends on the choice of one or more of such parameters. In particular a main theoretical issue regards the amount of prior knowledge on the problem needed to suitably choose the regularization parameter and obtain learning rates. In this paper we present a strategy, the balancing principle, to choose the regularization parameter without knowledge of the regularity of the target function. Such a choice adaptively achieves the best error rate. Our main result applies to regularization algorithms in reproducing kernel Hilbert space with the square loss, though we also study how a similar principle can be used in other situations. As a straightforward corollary we can immediately derive adaptive parameter choice for various kernel methods recently studied. Numerical experiments with the proposed parameter choice rules are also presented.",,24 p.,,,Adaptive Model Selection; Learning Theory; Inverse Problems; Regularization,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Woo, Grace; Katabi, Dina; Chachulski, Szymon",2008-10-20T15:00:05Z,2008-10-20T15:00:05Z,2008-10-18,http://hdl.handle.net/1721.1/42897,MIT-CSAIL-TR-2008-063,One Video Stream to Serve Diverse Receivers,"The fundamental problem of wireless video multicast is to scalably serve multiple receivers which may have very different channel characteristics. Ideally, one would like to broadcast a single stream that allows each receiver to benefit from all correctly received bits to improve its video quality. We introduce Digital Rain, a new approach to wireless video multicast that adapts to channel characteristics without any need for receiver feedback or variable codec rates. Users that capture more packets or have fewer bit errors naturally see higher video quality. Digital Rain departs from current approaches in two ways: 1) It allows a receiver to exploit video packets that may contain bit errors; 2) It builds on the theory of compressed sensing to develop robust video encoding and decoding algorithms that degrade smoothly with bit errors and packet loss. Implementation results from an indoor wireless testbed show that Digital Rain significantly improves the received video quality and the number of supported receivers.",,12 p.,,,Wireless networks; Video streaming; Multicast,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Agarwal, Anant; Psota, James; Eastep, Jonathan; Konstantakopoulos, Theodoros",2008-11-14T05:00:09Z,2008-11-14T05:00:09Z,2008-11-11,http://hdl.handle.net/1721.1/43707,MIT-CSAIL-TR-2008-066,Energy Scalability of On-Chip Interconnection Networks in Multicore Architectures,"On-chip interconnection networks (OCNs) such as point-to-point networks and buses form the communication backbone in systems-on-a-chip, multicore processors, and tiled processors. OCNs can consume significant portions of a chip's energy budget, so analyzing their energy consumption early in the design cycle becomes important for architectural design decisions. Although numerous studies have examined OCN implementation and performance, few have examined energy. This paper develops an analytical framework for energy estimation in OCNs and presents results based on both analytical models of communication patterns and real network traces from applications running on a tiled multicore processor. Our analytical framework supports arbitrary OCN topologies under arbitrary communication patterns while accounting for wire length, switch energy, and network contention. It is the first to incorporate the effects of communication locality and network contention, and use real traces extensively. This paper compares the energy of point-to-point networks against buses under varying degrees of communication locality. The results indicate that, for 16 or more processors, a one-dimensional and a two-dimensional point-to-point network provide 66% and 82% energy savings, respectively, over a bus assuming that processors communicate with equal likelihood. The energy savings increase for patterns which exhibit locality. For the two-dimensional point-to-point OCN of the Raw tiled microprocessor, contention contributes a maximum of just 23% of the OCN energy, using estimated values for channel, switch control logic, and switch queue buffer energy of 34.5pJ, 17pJ, and 12pJ, respectively. Our results show that the energy-delay product per message decreases with increasing processor message injection rate.",,24 p.,,,on-chip networks; multicore; energy scalability,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Benjamin, Michael R.",2008-11-14T05:00:20Z,2008-11-14T05:00:20Z,2008-11-11,http://hdl.handle.net/1721.1/43708,MIT-CSAIL-TR-2008-065,MOOS-IvP Autonomy Tools Users Manual,This document describes seven common MOOS-IvP autonomy tools. The uHelmScope application provides a run-time scoping window into the state of an active IvP Helm executing its mission. The pMarineViewer application is a geo-based GUI tool for rendering marine vehicles and certain autonomy properties in their operational area. The uXMS application is a terminal based tool for live scoping on a MOOSDB process. The uTermCommand application is a terminal based tool for poking the MOOSDB with a set of MOOS file pre-defined variable-value pairs selectable with tab-completion of aliases from the command-line. The pEchoVar application provides a way of echoing an observed write to a variable with a new write with the same value to a different variable name. The uProcessWatch application is a way of monitoring the presence or absence of a set of MOOS processes and summarizing the collective status in a single MOOS variable. The uPokeDB application is a way of poking a MOOSDB from the command line with one or more variable-value pairs without any pre-existing configuration of a MOOS file.,,46 p.,,,MOOS Poke; Marine Robotics; IvP Helm; MOOS; MOOS Scope; Helm Scope,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Valiant, Paul",2008-11-14T05:00:26Z,2008-11-14T05:00:26Z,2008-11-13,http://hdl.handle.net/1721.1/43709,MIT-CSAIL-TR-2008-067,Resilient Mechanisms For Truly Combinatorial Auctions,"Dominant-strategy truthfulness is traditionally considered the best possible solution concept in mechanism design, as it enables one to predict with confidence which strategies INDEPENDENT players will actually choose. Yet, as with any other form of equilibrium, it too can be extremely vulnerable to COLLUSION. The problem of collusion is particularly evident for UNRESTRICTED combinatorial auctions}, arguably the hardest type of auctions.We thus investigate how much revenue can be guaranteed, in unrestricted combinatorial auctions, by dominant-strategy-truthful mechanisms that are COLLUSION-RESILIENT in a very strong sense; and obtain almost matching upper- and lower-bounds.",,18 p.,,,Resilient mechanism design,Theory of Computation,,,,,,,,,,,,MIT-CSAIL-TR-2008-039; MIT-CSAIL-TR-2007-052,,,,,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Pereira, Olivier; Kaynar, Dilsun; Cheung, Ling; Canetti, Ran",2008-11-24T06:00:04Z,2008-11-24T06:00:04Z,2008-11-22,http://hdl.handle.net/1721.1/43711,MIT-CSAIL-TR-2008-068,"Modeling Computational Security in Long-Lived Systems, Version 2","For many cryptographic protocols, security relies on the assumption that adversarial entities have limited computational power. This type of security degrades progressively over the lifetime of a protocol. However, some cryptographic services, such as timestamping services or digital archives, are long-lived in nature; they are expected to be secure and operational for a very long time (i.e., super-polynomial). In such cases, security cannot be guaranteed in the traditional sense: a computationally secure protocol may become insecure if the attacker has a super-polynomial number of interactions with the protocol. This paper proposes a new paradigm for the analysis of long-lived security protocols. We allow entities to be active for a potentially unbounded amount of real time, provided they perform only a polynomial amount of work per unit of real time. Moreover, the space used by these entities is allocated dynamically and must be polynomially bounded. We propose a new notion of long-term implementation, which is an adaptation of computational indistinguishability to the long-lived setting. We show that long-term implementation is preserved under polynomial parallel composition and exponential sequential composition. We illustrate the use of this new paradigm by analyzing some security properties of the long-lived timestamping protocol of Haber and Kamat.",,27 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joshua Tenenbaum,"Tenenbaum, Joshua B.; Jonas, Eric M.; Mansinghka, Vikash K.",2008-11-24T16:30:26Z,2008-11-24T16:30:26Z,2008-11-24,http://hdl.handle.net/1721.1/43712,MIT-CSAIL-TR-2008-069,Stochastic Digital Circuits for Probabilistic Inference,"We introduce combinational stochastic logic, an abstraction that generalizes deterministic digital circuit design (based on Boolean logic gates) to the probabilistic setting. We show how this logic can be combined with techniques from contemporary digital design to generate stateless and stateful circuits for exact and approximate sampling from a range of probability distributions. We focus on Markov chain Monte Carlo algorithms for Markov random fields, using massively parallel circuits. We implement these circuits on commodity reconfigurable logic and estimate the resulting performance in time, space and price. Using our approach, these simple and general algorithms could be affordably run for thousands of iterations on models with hundreds of thousands of variables in real time.",,10 p.,,,cognitive science; robustness; Bayesian inference; artificial intelligence,Computational Cognitive Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Caponnetto, Andrea; Poggio, Tomaso; Bouvrie, Jake; Rosasco, Lorenzo; Smale, Steve",2008-11-27T01:30:05Z,2008-11-27T01:30:05Z,2008-11-26,http://hdl.handle.net/1721.1/43713,MIT-CSAIL-TR-2008-070; CBCL-276,Mathematics of the Neural Response,"We propose a natural image representation, the neural response, motivated by the neuroscience of the visual cortex. The inner product defined by the neural response leads to a similarity measure between functions which we call the derived kernel. Based on a hierarchical architecture, we give a recursive definition of the neural response and associated derived kernel. The derived kernel can be used in a variety of application domains such as classification of images, strings of text and genomics data.",,25 p.,,,neuroscience; computer vision; kernels,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-12-16T19:15:07Z,2008-12-16T19:15:07Z,2008-12-02,http://hdl.handle.net/1721.1/43946,MIT-CSAIL-TR-2008-073,"Resilient  Provision of a Public and/or Private Good,  or: Resilient Auctions of One Good in Unlimited Supply","We present two resilient mechanisms: the first for the provision of a public good, and the second for the provision of a private good. Both mechanisms adopt a knowledge-based benchmark.",,3 p.,,,"Single-good, unlimited-supply auctions; Resilient mechanism design; Provision of a public good; Knowledge-Based Benchmarks",Theory of Computation,,,,,,,,,,http://hdl.handle.net/1721.1/43716,,MIT-CSAIL-TR-2008-072,,,,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Chen, Jing",2008-12-03T16:15:18Z,2008-12-03T16:15:18Z,2008-12-02,http://hdl.handle.net/1721.1/43716,MIT-CSAIL-TR-2008-072,Resilient Provision of a Public Good,We present two resilient mechanisms for the provision of a public good.  Both mechanisms adopt a knowledge-based benchmark.,,2 p.,,,Resilient Mechanism Design; Equilibrium-less Mechanism Design; Knowledge Benchmarks,Theory of Computation,,,,,,,,,,,,,,,http://hdl.handle.net/1721.1/43946,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Chen, Jing",2008-12-17T21:00:03Z,2008-12-17T21:00:03Z,2008-12-17,http://hdl.handle.net/1721.1/43947,MIT-CSAIL-TR-2008-074,Resilient Auctions of One Good in Limited Supply,We present various resilient auction mechanisms for a good in limited supply. Our mechanisms achieve both player-knowledge and aggregated player-knowledge benchmarks.,,3 p.,,,Resilient mechanism design; Knowledge-Based Benchmarks; Player-Knowledge Benchmarks; Aggregated Knowledge-Based Benchmarks; Aggregated Player-Knowledge Benchmarks,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Seth Teller,"Hicks, Jamey; Curtis, Dorothy; Teller, Seth; Charrow, Ben; Ryan, Russell; Ledlie, Jonathan; Battat, Jonathan",2008-12-30T21:45:11Z,2008-12-30T21:45:11Z,2008-12-30,http://hdl.handle.net/1721.1/43951,MIT-CSAIL-TR-2008-075,Organic Indoor Location Discovery,"We describe an indoor, room-level location discovery method based on spatial variations in ""wifi signatures,"" i.e., MAC addresses and signal strengths of existing wireless access points. The principal novelty of our system is its organic nature; it builds signal strength maps from the natural mobility and lightweight contributions of ordinary users, rather than dedicated effort by a team of site surveyors. Whenever a user's personal device observes an unrecognized signature, a GUI solicits the user's location. The resulting location-tagged signature or ""bind"" is then shared with other clients through a common database, enabling devices subsequently arriving there to discover location with no further user contribution.  

Realizing a working system deployment required three novel elements: (1) a human-computer interface for indicating location over intervals of varying duration; (2) a client-server protocol for pre-fetching signature data for use in localization; and (3) a location-estimation algorithm incorporating highly variable signature data. We describe an experimental deployment of our method in a nine-story building with more than 1,400 distinct spaces served by more than 200 wireless access points. At the conclusion of the deployment, users could correctly localize to within 10 meters 92 percent of the time.",,14 p.,,,Shared Sensing; Computer Communication Networks; Localization; Location-Based Services; Geo-Tagging; Collaborative Computing; Distributed Systems; Distributed Applications; Pervasive Computing; Algorithms; Experiments; Groups and Organization Interfaces; Measurement; Crowd-Sourcing; Information Interfaces and Presentation; Human Factors,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Jay Sussman,"Sussman, Gerald Jay; Radul, Alexey",2009-01-27T06:15:12Z,2009-01-27T06:15:12Z,2009-01-26,http://hdl.handle.net/1721.1/44215,MIT-CSAIL-TR-2009-002,The Art of the Propagator,"We develop a programming model built on the idea that the basic computational elements are autonomous machines interconnected by shared cells through which they communicate. Each machine continuously examines the cells it is interested in, and adds information to some based on deductions it can make from information from the others. This model makes it easy to smoothly combine expression-oriented and constraint-based programming; it also easily accommodates implicit incremental distributed search in ordinary programs.  This work builds on the original research of Guy Lewis Steele Jr. and was developed more recently with the help of Chris Hanson.",,50 p.,,,Constraint-based programming; Programming models; Distributed search,Mathematics and Computation,,Creative Commons Attribution-Share Alike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Lahiani, Limor; Dolev, Shlomi; Nolte, Tina",2009-01-30T17:00:07Z,2009-01-30T17:00:07Z,2009-01-28,http://hdl.handle.net/1721.1/44516,MIT-CSAIL-TR-2009-003,Self-Stabilizing Message Routing in Mobile ad hoc Networks,"We present a self-stabilizing algorithm for routing messages between arbitrary pairs of nodes in a mobile ad hoc network. Our algorithm assumes the availability of a reliable GPS service, which supplies mobile nodes with accurate information about real time and about their own geographical locations. The GPS service provides an external, shared source of consistency for mobile nodes, allowing them to label and timestamp messages, and thereby aiding in recovery from failures. Our algorithm utilizes a Virtual Infrastructure programming abstraction layer, consisting of mobile client nodes, virtual stationary timed machines called Virtual Stationary Automata (VSAs), and a local broadcast service connecting VSAs and mobile clients. VSAs are associated with predetermined regions in the plane, and are emulated in a self-stabilizing manner by the mobile nodes. VSAs are relatively stable in the face of node mobility and failure, and can be used to simplify algorithm development for mobile networks. Our routing algorithm consists of three subalgorithms: [(1)] a VSA-to-VSA geographical routing algorithm, [2] a mobile client location management algorithm, and [3] the main algorithm, which utilizes both location management and geographical routing. All three subalgorithms are self-stabilizing, and consequently, the entire algorithm is also self-stabilizing.",,50 p.,,,Self-stabilizing algorithms; Mobile ad-hoc networks; Routing algorithms,Theory of Computation,,,,,"Closely based on chapters 12-14 of Tina Nolte's MIT PhD Thesis, 2009.",,,,,,,,,,,,,,,,,,,,,,,
Michael Ernst,"Ernst, Michael D.; Kiezun, Adam; Ganesh, Vijay; Guo, Philip J.; Hooimeijer, Pieter",2009-02-04T19:00:04Z,2009-02-04T19:00:04Z,2009-02-04,http://hdl.handle.net/1721.1/44584,MIT-CSAIL-TR-2009-004,HAMPI: A Solver for String Constraints,"Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable tools. The increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few, if any, effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis techniques for string-manipulating programs. We designed and implemented Hampi, a solver for string constraints over bounded string variables. Hampi constraints express membership in regular languages and bounded context-free languages. Hampi constraints may contain context-free-language definitions, regular-language definitions and operations, and the membership predicate. Given a set of constraints, Hampi outputs a string that satisfies all the constraints, or reports that the constraints are unsatisfiable. Hampi is expressive and efficient, and can be successfully applied to testing and analysis of real programs. Our experiments use Hampi in: static and dynamic analyses for finding SQL injection vulnerabilities in Web applications; automated bug finding in C programs using systematic testing; and compare Hampi with another string solver. Hampi's source code, documentation, and the experimental data are available at http://people.csail.mit.edu/akiezun/hampi.",,11 p.,,,Constraint solvers; Automated testing,Program Analysis,,,,,,,,,,http://people.csail.mit.edu/akiezun/hampi,,,,,,,,,,,,,,,,,,
Dina Katabi,"Katabi, Dina; Rahul, Hariharan; Jakubczak, Szymon",2009-02-09T16:30:14Z,2009-02-09T16:30:14Z,2009-02-07,http://hdl.handle.net/1721.1/44585,MIT-CSAIL-TR-2009-005,SoftCast: One Video to Serve All Wireless Receivers,"The main challenge in wireless video multicast is to scalably serve multiple receivers who have different channel characteristics. Current wireless transmission schemes, however, cannot support smooth degradation. Specifically, each packet is transmitted at a particular bitrate and is decodable only by receivers that support the chosen bitrate. Broadcasting a video stream to all receivers requires transmitting at the lowest bitrate, and hence reduces everyone to the performance of the worst receiver in the multicast group.This paper introduces SoftCast, an alternative design for wireless video multicast, in which a sender broadcasts a single stream and each receiver watches a video quality that matches its channel quality. SoftCast achieves this by making the magnitude of the transmitted signal proportional to the pixel value. Hence, channel noise directly translates to a small perturbation in pixel values, allowing graceful degradation with increasing noise. SoftCast introduces a novel power allocation scheme that allows the transmission of real-valued video signals in a compact and resilient manner. We implement SoftCast in the WARP radio platform. Our results show that SoftCast improves the average video quality across multicast receivers by 3-7dB over the current approach. Further, it stays competitive with the current approach even for regular unicast.",,14 p.,,,Wireless networks; Multicast; Wireless video; Video coding,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Benjamin, Michael R.; Leonard, John J.; Schmidt, Henrik; Newman, Paul M.",2009-02-13T22:30:07Z,2009-02-13T22:30:07Z,2009-02-13,http://hdl.handle.net/1721.1/44590,MIT-CSAIL-TR-2009-006,A Tour of MOOS-IvP Autonomy Software Modules,"This paper provides an overview of the MOOS-IvP autonomy software modules. The MOOS-IvP collection of software, i.e., codebase, described here has been developed and is currently maintained by three organizations - Oxford University, Massachusetts Institute of Technology (MIT), and the Naval Undersea Warfare Center (NUWC) Division Newport Rhode Island. The objective of this paper is to provide a comprehensive list of modules and provide for each (a) a general description of functionality, (b) dependency relationships to other modules, (c) rough order of magnitude in complexity or size, (d) authorship, and (e) current and planned distribution access.",,38 p.,,,marine vehicles; middleware; autonomous helm; unmanned underwater vehicle; IvP helm; IvP; unmanned surface vehicles; dackseat driver; behaviors; behavior-based; UUV; multi-objective optimization; publish-subscribe; interval programming; USV; Mission Oriented Operating Suite; MOOSDB; MOOS; pHelmIvP; decision making,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomas Lozano-Perez,"Lozano-Perez, Tomas; Kaelbling, Leslie Pack; Chiu, Han-Pang",2009-02-19T19:15:05Z,2009-02-19T19:15:05Z,2009-02-18,http://hdl.handle.net/1721.1/44615,MIT-CSAIL-TR-2009-008,Automatic Class-Specific 3D Reconstruction from a Single Image,"Our goal is to automatically reconstruct 3D objects from a single image, by using prior 3D shape models of classes. The shape models, defined as a collection of oriented primitive shapes centered at fixed 3D positions, can be learned from a few labeled images for each class. The 3D class model can then be used to estimate the 3D shape of an object instance, including occluded parts, from a single image. We provide a quantitative evaluation of the shape estimation process on real objects and demonstrate its usefulness in three applications: robot manipulation, object detection, and generating 3D 'pop-up' models from photos.",,9 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Perli, Samuel David; Gollakota, Shyamnath; Katabi, Dina",2009-02-19T19:15:22Z,2009-02-19T19:15:22Z,2009-02-18,http://hdl.handle.net/1721.1/44616,MIT-CSAIL-TR-2009-007,Overcoming the Antennas-Per-Node Throughput Limit in MIMO LANs,"Today, the number of concurrent packets in a MIMO LAN is limited by the number of antennas on the AP. This paper shows how to overcome this limit. It presents a new design where multiple client-AP pairs can communicate concurrently, on the same 802.11 channel. We demonstrate both analytically and experimentally that our design almost doubles the throughput of a MIMO LAN.  The key idea underlying our approach is Interference Alignment and Cancellation (IAC), a novel technique for decoding concurrent sender-receiver pairs in MIMO LANs. It exploits two basic properties of MIMO LANs. First, MIMO transmitters can control the alignment of their signals at a receiver. Second, APs are typically connected to a backend Ethernet, which they can use for coordination. Hence, in IAC, transmitters align their signals such that the first AP can decode at least one of the concurrent packets. Once a packet is decoded, it is sent over the Ethernet to the second AP, which subtracts it from its received signal to decode a second packet, which it sends to the third AP to decode the next packet, and so on. We implement our technique in 2x2 MIMO GNU Radios, and demonstrate via wireless experiments that IAC increases the average throughput of a MIMO LAN by 1.5x on the downlink and 2x on the uplink.",,15 p.,,,Interference alignment; Interference cancellation; Wireless networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Newport, Calvin; Lynch, Nancy; Kuhn, Fabian",2009-02-23T16:30:04Z,2009-02-23T16:30:04Z,2009-02-21,http://hdl.handle.net/1721.1/44620,MIT-CSAIL-TR-2009-009,The Abstract MAC Layer,"A diversity of possible communication assumptions complicates the study of algorithms and lower bounds for radio networks. We address this problem by defining an Abstract MAC Layer. This service provides reliable local broadcast communication, with timing guarantees stated in terms of a collection of abstract delay functions applied to the relevant contention. Algorithm designers can analyze their algorithms in terms of these functions, independently of specific channel behavior. Concrete implementations of the Abstract MAC Layer over basic radio network models generate concrete definitions for these delay functions, automatically adapting bounds proven for the abstract service to bounds for the specific radio network under consideration. To illustrate this approach, we use the Abstract MAC Layer to study the new problem of Multi-Message Broadcast, a generalization of standard single-message broadcast, in which any number of messages arrive at any processes at any times.We present and analyze two algorithms for Multi-Message Broadcast in static networks: a simple greedy algorithm and one that uses regional leaders. We indicate how these results can be extended to mobile networks.",,26 p.,,,,Theory of Computation,,,,,,,,,MIT-CSAIL-TR-2009-021,http://hdl.handle.net/1721.1/45515,,,,,,,,,,,,,,,,,,
Michael Ernst,"Tip, Frank; Ernst, Michael D.; Dig, Danny; Dolby, Julian; Kiezun, Adam; Artzi, Shay; Paradkar, Amit",2009-03-27T16:00:07Z,2009-03-27T16:00:07Z,2009-03-26,http://hdl.handle.net/1721.1/44956,MIT-CSAIL-TR-2009-010,Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit State Model Checking,"Web script crashes and malformed dynamically-generated web pages are common errors, and they seriously impact the usability of web applications. Current tools for web-page validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests, so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 302 faults in 6 PHP web applications.",,17 p.,,,Software Testing; PHP; Dynamic Analysis,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Srinivas Devadas,"Kinsy, Michel; Wen, Tina; Shim, Keun Sup; Lis, Mieszko; Cho, Myong Hyon; Devadas, Srinivas",2009-03-30T18:00:06Z,2009-03-30T18:00:06Z,2009-03-27,http://hdl.handle.net/1721.1/44958,MIT-CSAIL-TR-2009-011,Oblivious Routing in On-Chip Bandwidth-Adaptive Networks,"Oblivious routing can be implemented on simple router hardware, but network performance suffers when routes become congested. Adaptive routing attempts to avoid hot spots by re-routing flows, but requires more complex hardware to determine and configure new routing paths. We propose on-chip bandwidth-adaptive networks to mitigate the performance problems of oblivious routing and the complexity issues of adaptive routing.In a bandwidth-adaptive network, the bisection bandwidth of a network can adapt to changing network conditions.  We describe one implementation of a bandwidth-adaptive network in the form of a two-dimensional mesh with adaptive bidirectional links, where the bandwidth of the link in one direction can be increased at the expense of the other direction. Efficient local intelligence is used to reconfigure each link, and this reconfiguration can be done very rapidly in response to changing traffic demands.  We compare the hardware designs of a unidirectional and bidirectional link and evaluate the performance gains provided by a bandwidth-adaptive network in comparison to a conventional network under uniform and bursty traffic when oblivious routing is used.",,10 p.,,,bidirectional link; network router; reconfigurable network; adaptive link,Computation Structures,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Whitman Richards,"Wormald, Nicholas; Richards, Whitman",2009-03-30T18:00:14Z,2009-03-30T18:00:14Z,2009-03-30,http://hdl.handle.net/1721.1/44959,MIT-CSAIL-TR-2009-012,Representing Small Group Evolution,"Understanding the dynamics of network evolution rests in part on the representation chosen to characterize the evolutionary process. We offer a simple, three-parameter representation based on subgraphs that capture three important properties of social networks: leadership, team alignment or bonding among members, and diversity of expertise. When plotted on this representation, the evolution of a typical small group such as start-ups or street gangs has a spiral trajectory, moving toward a tentative fixed point as membership increases to two dozen or so. We show that a simple probabilistic model for recruitment and bonding can not explain these observations, and suggest that strategic moves among group members may come into play.",,15 p.,,,simplex representation; network evolution,Belief Dynamics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
William Freeman,"Freeman, William; Durand, Fredo; Weiss, Yair; Levin, Anat",2009-03-31T18:00:05Z,2009-03-31T18:00:05Z,2009-03-31,http://hdl.handle.net/1721.1/44964,MIT-CSAIL-TR-2009-014,Understanding and evaluating blind deconvolution algorithms,"Blind deconvolution is the recovery of a sharp version of a blurred image when the blur kernel is unknown. Recent algorithms have afforded dramatic progress, yet many aspects of the problem remain challenging and hard to understand.The goal of this paper is to analyze and evaluate recent blind deconvolution algorithms both theoretically and experimentally. We explain the previously reported failure of the naive MAP approach by demonstrating that it mostly favors no-blur explanations. On the other hand we show that since the kernel size is often smaller than the image size a MAP estimation of the kernel alone can be well constrained and accurately recover the true blur. The plethora of recent deconvolution techniques makes an experimental evaluation on ground-truth data important. We have collected blur data with ground truth and compared recent algorithms under equal settings. Additionally, our data demonstrates that the shift-invariant blur assumption made by most algorithms is often violated.",,44 p.,,,deconvolution,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joshua Tenenbaum,"O'Donnell, Timothy J.; Tenenbaum, Joshua B.; Goodman, Noah D.",2009-03-31T05:00:03Z,2009-03-31T05:00:03Z,2009-03-31,http://hdl.handle.net/1721.1/44963,MIT-CSAIL-TR-2009-013,Fragment Grammars: Exploring Computation and Reuse in Language,"Language relies on a division of labor between stored units and structure building operations which combine the stored units into larger structures. This division of labor leads to a tradeoff: more structure-building means less need to store while more storage means less need to compute structure. We develop a hierarchical Bayesian model called fragment grammar to explore the optimum balance between structure-building and reuse. The model is developed in the context of stochastic functional programming (SFP) and in particular using a probabilistic variant of Lisp known as the Church programming language (Goodman, Mansinghka, Roy, Bonawitz, & Tenenbaum, 2008). We show how to formalize several probabilistic models of language structure using Church, and how fragment grammar generalizes one of them---adaptor grammars (Johnson, Griffiths, & Goldwater, 2007). We conclude with experimental data with adults and preliminary evaluations of the model on natural language corpus data.",,63 p.,,,Language; Stochastic Functional Programming; Stochastic Memoization; Reuse; Lexicon; Hierarchical Bayes,Computational Cognitive Science,,,,,"O'DONNELL, T., GOODMAN, N., and TENENBAUM, J. 2009. Fragment Grammars: Exploring Computation and Reuse in Language. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series, MIT-CSAIL-TR-2009-013.",,,,,,,,,,,,,,,,,,,,,,,
Barbara Liskov,"Zhou, You",2009-04-23T17:15:06Z,2009-04-23T17:15:06Z,2009-04-16,http://hdl.handle.net/1721.1/45141,MIT-CSAIL-TR-2009-015,Computing Network Coordinates in the Presence of Byzantine Faults,"Network coordinate systems allow for efficient construction of large-scale distributed systems on the Internet. Coordinates provide locality information in a compact way, without requiring each node to contact every potential neighbor; distances between two nodes' coordinates represent estimates of the network latency between them. Past work on network coordinates has assumed that all nodes in the system behave correctly. The techniques in these systems do not behave well when nodes are Byzantine. These Byzantine failures, wherein a faulty node can behave arbitrarily, can make the coordinate-based distance estimates meaningless. For example, a Byzantine node can delay responding to some other node, thus distorting that node's computation of its own location. We present a network coordinate system based on landmarks, reference nodes that are used for measurements, some of which may be Byzantine faulty. It scales linearly in the number of clients computing their coordinates and does not require excessive network traffic to allow clients to do so. Our results show that our system is able to compute accurate coordinates even when some landmarks are exhibiting Byzantine faults.",,60 p.,,,,Programming Methodology,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,M.Eng.,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Williams, Brian C.; Ono, Masahiro",2009-04-23T17:15:13Z,2009-04-23T17:15:13Z,2009-04-22,http://hdl.handle.net/1721.1/45142,MIT-CSAIL-TR-2009-016,Risk Allocation for Multi-agent Systems using Tatonnement,"This paper proposes a new market-based distributed planning algorithm for multi-agent systems under uncertainty, called MIRA (Market-based Iterative Risk Allocation). In large coordination problems, from power grid management to multi-vehicle missions, multiple agents act collectively in order to optimize the performance of the system, while satisfying mission constraints. These optimal plans are particularly susceptible to risk when uncertainty is introduced. We present a distributed planning algorithm that minimizes the system cost while ensuring that the probability of violating mission constraints is below a user-specified level. We build upon the paradigm of risk allocation (Ono and Williams, AAAI-08), in which the planner optimizes not only the sequence of actions, but also its allocation of risk among each constraint at each time step. We extend the concept of risk allocation to multi-agent systems by highlighting risk as a good that is traded in a computational market. The equilibrium price of risk that balances the supply and demand is found by an iterative price adjustment process called tatonnement (also known as Walrasian auction). The simulation results demonstrate the efficiency and optimality of the proposed distributed planner.",,18 p.,,,Robust MPC; Chance constraint; RMPC; Brent's method; Grouping,Model-based Embedded and Robotic Systems,This research is funded by The Boeing Company grant MIT-BA-GTA-1.,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Wentzlaff, David; Agarwal, Anant; Hoffmann, Henry",2009-05-06T15:30:08Z,2009-05-06T15:30:08Z,2009-05-05,http://hdl.handle.net/1721.1/45509,MIT-CSAIL-TR-2009-017,Remote Store Programming: Mechanisms and Performance,"This paper presents remote store programming (RSP). This paradigm combines usability and efficiency through the exploitation of a simple hardware mechanism, the remote store, which can easily be added to existing multicores.Remote store programs are marked by fine-grained and one-sided communication which results in a stream of data flowing from the registers of a sending process to the cache of a destination process. The RSP model and its hardware implementation trade a relatively high store latency for a low load latency because loads are more common than stores, and it is easier to tolerate store latency than load latency. This paper demonstrates the performance advantages of remote store programming by comparing it to both cache-coherent shared memory and direct memory access (DMA) based approaches using the TILEPro64 processor. The paper studies two applications: a two-dimensional Fast Fourier Transform (2D FFT) and an H.264 encoder for high-definition video. For a 2D FFT using 56 cores, RSP is 1.64x faster than DMA and 4.4x faster than shared memory. For an H.264 encoder using 40 cores, RSP achieves the same performance as DMA and 4.8x the performance of shared memory. Along with these performance advantages, RSP requires the least hardware support of the three. RSP's features, performance, and hardware simplicity make it well suited to the embedded processing domain.",,11 p.,,,Programming models; Multicore architecture,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Liu, Jifeng; Psota, James; Beckmann, Nathan; Miller, Jason; Michel, Jurgen; Eastep, Jonathan; Kurian, George; Kimerling, Lionel; Agarwal, Anant; Beals, Mark",2009-05-06T18:30:04Z,2009-05-06T18:30:04Z,2009-05-05,http://hdl.handle.net/1721.1/45510,MIT-CSAIL-TR-2009-018,ATAC: A Manycore Processor with On-Chip Optical Network,"Ever since industry has turned to parallelism instead of frequency scaling to improve processor performance, multicore processors have continued to scale to larger and larger numbers of cores. Some believe that multicores will have 1000 cores or more by the middle of the next decade. However, their promise of increased performance will only be reached if their inherent scaling and programming challenges are overcome. Meanwhile, recent advances in nanophotonic device manufacturing are making chip-stack optics a reality; interconnect technology which can provide significantly more bandwidth at lower power than conventional electrical analogs. Perhaps more importantly, optical interconnect also has the potential to enable new, easy-to-use programming models enabled by an inexpensive broadcast mechanism. This paper introduces ATAC, a new manycore architecture that capitalizes on the recent advances in optics to address a number of the challenges that future manycore designs will face. The new constraints and opportunities associated with on-chip optical interconnect are presented and explored in the design of ATAC. Furthermore, this paper introduces ACKwise, a novel directory-based cache coherence protocol that takes advantage of the special properties of ATAC to achieve high performance and scalability on large-scale manycores. Early performance results show that a 1000-core ATAC chip achieves a speedup of as much as 39% when compared with a similarly sized manycore with an electrical mesh network.",,14 p.,,,Many-core processors; Processor interconnects; Optical interconnects,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
William Freeman,"Levin, Anat; Hasinoff, Samuel W.; Freeman, William T.; Green, Paul; Durand, Fredo",2009-05-08T15:45:09Z,2009-05-08T15:45:09Z,2009-05-08,http://hdl.handle.net/1721.1/45513,MIT-CSAIL-TR-2009-019,4D Frequency Analysis of Computational Cameras for Depth of Field Extension,"Depth of field (DOF), the range of scene depths that appear sharp in a photograph, poses a fundamental tradeoff in photography---wide apertures are important to reduce imaging noise, but they also increase defocus blur. Recent advances in computational imaging modify the acquisition process to extend the DOF through deconvolution. Because deconvolution quality is a tight function of the frequency power spectrum of the defocus kernel, designs with high spectra are desirable. In this paper we study how to design effective extended-DOF systems, and show an upper bound on the maximal power spectrum that can be achieved. We analyze defocus kernels in the 4D light field space and show that in the frequency domain, only a low-dimensional 3D manifold contributes to focus. Thus, to maximize the defocus spectrum, imaging systems should concentrate their limited energy on this manifold. We review several computational imaging systems and show either that they spend energy outside the focal manifold or do not achieve a high spectrum over the DOF. Guided by this analysis we introduce the lattice-focal lens, which concentrates energy at the low-dimensional focal manifold and achieves a higher power spectrum than previous designs. We have built a prototype lattice-focal lens and present extended depth of field results.",,18 p.,,,light fields; Fourier analysis; Computational cameras; depth of field,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Terashima, Yoshito",2009-05-11T17:30:10Z,2009-05-11T17:30:10Z,2009-05-10,http://hdl.handle.net/1721.1/45516,CBCL-277; MIT-CSAIL-TR-2009-020,Scene Classification with a Biologically Inspired Method,"We present a biologically motivated method for scene image classification. The core of the method is to use shape based image property that is provided by a hierarchical feedforward model of the visual cortex [18]. Edge based and color based image properties are additionally used to improve the accuracy. The method consists of two stages of image analysis. In the first stage, each of three paths of classification uses each image property (i.e. shape, edge or color based features) independently. In the second stage, a single classifier assigns the category of an image based on the probability distributions of the first stage classifier outputs. Experiments show that the method boosts the classification accuracy over the shape based model. We demonstrate that this method achieves a high accuracy comparable to other reported methods on publicly available color image dataset.",,8 p.,,,image classification; vision,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Kuhn, Fabian; Newport, Calvin; Lynch, Nancy",2009-05-11T17:30:04Z,2009-05-11T17:30:04Z,2009-05-11,http://hdl.handle.net/1721.1/45515,MIT-CSAIL-TR-2009-021,The Abstract MAC Layer,"A diversity of possible communication assumptions complicates the study of algorithms and lower bounds for radio networks. We address this problem by defining an Abstract MAC Layer. This service provides reliable local broadcast communication, with timing guarantees stated in terms of a collection of abstract \emph{delay functions} applied to the relevant contention. Algorithm designers can analyze their algorithms in terms of these functions, independently of specific channel behavior. Concrete implementations of the Abstract MAC Layer over basic radio network models generate concrete definitions for these delay functions, automatically adapting bounds proven for the abstract service to bounds for the specific radio network under consideration. To illustrate this approach, we use the Abstract MAC Layer to study the new problem of Multi-Message Broadcast, a generalization of standard single-message broadcast, in which any number of messages arrive at any processes at any times.We present and analyze two algorithms for Multi-Message Broadcast in static networks: a simple greedy algorithm and one that uses regional leaders. We then indicate how these results can be extended to mobile networks.",,27 p.,,en,network modeling; mobile networks; wireless networks; medium-acccess protocols,Theory of Computation,,,,,,,,,,http://hdl.handle.net/1721.1/44620,MIT-CSAIL-TR-2009-009,,,,,,,,,,,,,,,,,
Boris Katz,"Marton, Gregory Adam; Westrick, Linda Brown",2009-05-28T19:00:05Z,2009-05-28T19:00:05Z,2009-05-28,http://hdl.handle.net/1721.1/45548,,Sepia: a Framework for Natural Language Semantics,"To help explore linguistic semantics in the context of computational natural language understanding, Sepia provides a realization the central theoretical idea of categorial grammar: linking words and phrases to compositional lambda semantics.  The Sepia framework provides a language in 
which to express complex transformations from text to data structures, and tools surrounding that language for parsing and machine learning.  Lambda semantics are expressed as arbitrary Scheme programs, unlimited in the semantic representations they may build, and the rules for transformation are expressed in Combinatory Categorial Grammar, though the details of grammar formalism may be easily changed.  This report explains the major design decisions, and is meant to teach the reader how to understand Sepia semantics and how to create lexical items for a new language understanding task.",,25 p.,,,,Infolab,,Creative Commons Attribution-Share Alike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,Source code and technical description,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Locher, Thomas; Kuhn, Fabian; Oshman, Rotem",2009-05-29T16:00:04Z,2009-05-29T16:00:04Z,2009-05-29,http://hdl.handle.net/1721.1/45549,MIT-CSAIL-TR-2009-022,Gradient Clock Synchronization in Dynamic Networks,"Over the last years, large-scale decentralized computer networks such as peer-to-peer and mobile ad hoc networks have become increasingly prevalent. The topologies of many of these networks are often highly dynamic. This is especially true for ad hoc networks formed by mobile wireless devices. In this paper, we study the fundamental problem of clock synchronization in dynamic networks. We show that there is an inherent trade-off between the skew S guaranteed along sufficiently old links and the time needed to guarantee a small skew along new links. For any sufficiently large initial skew on a new link, there are executions in which the time required to reduce the skew on the link to O(S) is at least Omega(n/S). We show that this bound is tight for moderately small values of S. Assuming a fixed set of n nodes and an arbitrary pattern of edge insertions and removals, a weak dynamic connectivity requirement suffices to prove the following results. We present an algorithm that always maintains a skew of O(n) between any two nodes in the network. For a parameter S = Omega(sqrt{rho n}), where rho is the maximum hardware clock drift, it is further guaranteed that if a communication link between two nodes u, v persists in the network for at least Omega(n/S) time, the clock skew between u and v is reduced to no more than O(S).",,33 p.,,,time synchronization; distributed algorithms; lower bound,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Newport, Calvin",2009-06-05T15:30:06Z,2009-06-05T15:30:06Z,2009-06-04,http://hdl.handle.net/1721.1/45553,MIT-CSAIL-TR-2009-023,Modeling Radio Networks,"We describe a modeling framework and collection of foundational composition results for the study of probabilistic distributed algorithms in synchronous radio networks. Existing results in this setting rely on informal descriptions of the channel behavior and therefore lack easy comparability and are prone to error caused by definition subtleties. Our framework rectifies these issues by providing: (1) a method to precisely describe a radio channel as a probabilistic automaton; (2) a mathematical notion of implementing one channel using another channel, allowing for direct comparisons of channel strengths and a natural decomposition of problems into implementing a more powerful channel and solving the problem on the powerful channel; (3) a mathematical definition of a problem and solving a problem; (4) a pair of composition results that simplify the tasks of proving properties about channel implementation algorithms and combining problems with channel implementations. Our goal is to produce a model streamlined for the needs of the radio network algorithms community.",,21 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Daniel Jackson,"Edwards, Jonathan",2009-06-12T21:30:04Z,2009-06-12T21:30:04Z,2009-06-12,http://hdl.handle.net/1721.1/45563,MIT-CSAIL-TR-2009-024,Coherent Reaction,"Side effects are both the essence and bane of imperative programming. The programmer must carefully coordinate actions to manage their side effects upon each other. Such coordination is complex, error-prone, and fragile. Coherent reaction is a new model of change-driven computation that coordinates effects automatically. State changes trigger events called reactions that in turn change other states. A coherent execution order is one in which each reaction executes before any others that are affected by its changes. A coherent order is discovered iteratively by detecting incoherencies as they occur and backtracking their effects. Unlike alternative solutions, much of the power of imperative programming is retained, as is the common sense notion of mutable state. Automatically coordinating actions lets the programmer express what to do, not when to do it. Coherent reactions are embodied in the Coherence language, which is specialized for interactive applications like those common on the desktop and web. The fundamental building block of Coherence is the dynamically typed mutable tree. The fundamental abstraction mechanism is the virtual tree, whose value is lazily computed, and whose behavior is generated by coherent reactions.",,15 p.,,,reactive systems; synchronous reactive programming; interactive systems; functional reactive programming; bidirectional functions,Software Design,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Katabi, Dina; Raskar, Ramesh; Mohan, Ankit; Woo, Grace",2009-06-15T23:15:04Z,2009-06-15T23:15:04Z,2009-06-15,http://hdl.handle.net/1721.1/45565,,Simple LCD Transmitter Camera Receiver Data Link,"We demonstrate a freespace optical system using a consumer camera and projector in indoor environments using available devices for visual computing. Through design, prototype and experimentation with this commodity hardware, we analyze a practical optical solution as well as the drawbacks for current wireless challenges unmet by classic RF wireless communication. We summarize and introduce some new applications enabled by such similar setups.",,5 p.,,,Visible Light Communication,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Micali, Silvio",2009-06-16T15:00:04Z,2009-06-16T15:00:04Z,2009-06-15,http://hdl.handle.net/1721.1/45566,MIT-CSAIL-TR-2009-025,A Useful Homomorphic Encryption Method,,,1 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"Devadas, Srinivas; Agarwal, Anant; Hoffmann, Henry",2009-06-16T17:15:03Z,2009-06-16T17:15:03Z,2009-06-16,http://hdl.handle.net/1721.1/45567,MIT-CSAIL-TR-2009-026,Partitioning Strategies for Concurrent Programming,"This work presents four partitioning strategies, or patterns, useful for decomposing a serial application into multiple concurrently executing parts. These partitioning strategies augment the commonly used task and data parallel design patterns by recognizing that applications are spatiotemporal in nature. Therefore, data and instruction decomposition are further distinguished by whether the partitioning is done in the spatial or in temporal dimension. Thus, this work describes four decomposition strategies: spatial data partitioning (SDP), temporal data partitioning (TDP), spatial instruction partitioning (SIP), and temporal instruction partitioning (TIP), while cataloging the benefits and drawbacks of each. In addition, the practical use of these strategies is demonstrated through a case study in which they are applied to implement several different parallelizations of a multicore H.264 encoder for HD video. This case study illustrates both the application of the patterns and their effects on the performance of the encoder.",,16 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Ley-Wild, Ruy; Kuhn, Fabian; Cornejo, Alejandro",2009-06-17T21:15:03Z,2009-06-17T21:15:03Z,2009-06-17,http://hdl.handle.net/1721.1/45568,MIT-CSAIL-TR-2009-027,Keeping Mobile Robots Connected,"Designing robust algorithms for mobile agents with reliable communication is difficult due to the distributed nature of computation, in mobile ad hoc networks (MANETs) the matter is exacerbated by the need to ensure connectivity. Existing distributed algorithms provide coordination but typically assume connectivity is ensured by other means. We present a connectivity service that encapsulates an arbitrary motion planner and can refine any plan to preserve connectivity (the graph of agents remains connected) and ensure progress (the agents advance towards their goal). The service is realized by a distributed algorithm that is modular in that it makes no assumptions of the motion-planning mechanism except the ability for an agent to query its position and intended goal position, local in that it uses 1-hop broadcast to communicate with nearby agents but doesn't need any network routing infrastructure, and \emph{oblivious} in that it does not depend on previous computations. We prove the progress of the algorithm in one round is at least Omega(min(d,r)), where d is the minimum distance between an agent and its target and r is the communication radius. We characterize the worst case configuration and show that when d >= r this bound is tight and the algorithm is optimal, since no algorithm can guarantee greater progress. Finally we show all agents get epsilon-close to their targets within O(D_0/r+n^2/epsilon) rounds where n is the number of agents and D_0 is the initial distance to the targets.",,21 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Benjamin, Michael R.; Leonard, John J.; Schmidt, Henrik; Newman, Paul M.",2009-06-18T17:45:13Z,2009-06-18T17:45:13Z,2009-06-18,http://hdl.handle.net/1721.1/45569,MIT-CSAIL-TR-2009-028,An Overview of MOOS-IvP and a Brief Users Guide to the IvP Helm Autonomy Software,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",,168 p.,,,UUV; AUV; Behavior Based Architecture; Unmanned Surface Vehicles; Behavior Based Control; Unmanned Marine Vehicles; Action Selection; Collision Avoidance; Multi-Objective Optimization; Unmanned Vehicles; MOOS; USV; Autonomous Decision Making; Autonomous Marine Vehicles; Autonomous Helm; Arbitration; Autonomous Vehicles; Underwater Vehicles; AI; MOOSDB; Behaviors,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Poggio, Tomaso; Serre, Thomas; Tan, Cheston; Chikkerur, Sharat",2009-06-22T17:15:20Z,2009-06-22T17:15:20Z,2009-06-20,http://hdl.handle.net/1721.1/45598,CBCL-278; MIT-CSAIL-TR-2009-029,An integrated model of visual attention using shape-based features,"Apart from helping shed some light on human perceptual mechanisms, modeling visual attention has important applications in computer vision. It has been shown to be useful in priming object detection, pruning interest points, quantifying visual clutter as well as predicting human eye movements. Prior work has either relied on purely bottom-up approaches or top-down schemes using simple low-level features. In this paper, we outline a top-down visual attention model based on shape-based features. The same shape-based representation is used to represent both the objects and the scenes that contain them. The spatial priors imposed by the scene and the feature priors imposed by the target object are combined in a Bayesian framework to generate a task-dependent saliency map. We show that our approach can predict the location of objects as well as match eye movements (92% overlap with human observers). We also show that the proposed approach performs better than existing bottom-up and top-down computational models.",,10 p.,,,attention; bayesian network,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Scull, Craig; Johnson, Steve; Aliaga, Frederick; Paris, Sylvain; Su, Sara L.; Durand, Fredo",2009-06-24T23:00:06Z,2009-06-24T23:00:06Z,2009-06-24,http://hdl.handle.net/1721.1/45600,MIT-CSAIL-TR-2009-031,Interactive Visual Histories for Vector Graphics,"Presentation and graphics software enables users to experiment with variations of illustrations. They can revisit recent editing operations using the ubiquitous undo command, but they are limited to sequential exploration. We propose a new interaction metaphor and visualization for operation history. While editing, a user can access a history mode in which actions are denoted by graphical depictions appearing on top of the document. Our work is inspired by the visual language of film storyboards and assembly instructions. Our storyboard provides an interactive visual history, summarizing the editing of a document or a selected object. Each view is composed of action depictions representing the userâ  s editing actions and enables the user to consider the operation history in context rather than in a disconnected list view. This metaphor provides instant access to any past action and we demonstrate that this is an intuitive interface to a selective undo mechanism.",,12 p.,,,interaction techniques; vector graphics; visualization; undo; storyboard; operation history,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Su, Sara L.",2009-06-24T21:45:39Z,2009-06-24T21:45:39Z,2009-06-24,http://hdl.handle.net/1721.1/45599,MIT-CSAIL-TR-2009-030,Enhanced Visual Authoring Using Operation History,"Graphical editors have introduced great flexibility to the designer's workflow, providing powerful digital tools and enabling the creation of complex and compelling designs. This thesis presents methods for improving these interactions by leveraging operation history. Much instrumentation and activity logging in software has been for the purpose of debugging, that is, for the benefit of the programmer or analyst. Our work addresses the mining of operation history for the benefit of the end user. We present three main contributions in this area. First, we introduce selection expansion, a method for facilitating the reuse of complex multiple-item selections by identifying items that are likely to be edited together. We then discuss an extension of this work, soft grouping, which gives users more control than standard selection and more flexibility than standard grouping. Finally, we present an interactive visualization of operation history, interactive storyboards, which enables in-context browsing and manipulation of operation history. We demonstrate these approaches in the context of vector graphics editing and present the results of pilot studies using our software implementation. While this thesis focuses on the usage patterns of graphic designers, many of the strategies could be generalized to other domains.",,123 p.,,,operation history; storyboard; human computer interaction; selection; grouping; computer graphics; 2D drawing; interactive techniques; vector graphics,Computer Graphics,,,,PhD thesis,,,,Ph.D.,,,,,,,,Electrical Engineering and Computer Science,,,,,,,,,,,,
Daniel Jackson,"Jackson, Daniel; Estler, H.-Christian; Rayside, Derek",2009-07-03T19:15:04Z,2009-07-03T19:15:04Z,2009-07-03,http://hdl.handle.net/1721.1/46322,MIT-CSAIL-TR-2009-033,"The Guided Improvement Algorithm for Exact, General-Purpose, Many-Objective Combinatorial Optimization","This paper presents a new general-purpose algorithm for exact solving of combinatorial many-objective optimization problems. We call this new algorithm the guided improvement algorithm. The algorithm is implemented on top of the non-optimizing relational constraint solver Kodkod. We compare the performance of this new algorithm against two algorithms from the literature [Gavanelli 2002, Lukasiewycz et alia 2007, Laumanns et alia 2006]) on three micro-benchmark problems (n-Queens, n-Rooks, and knapsack) and on two aerospace case studies. Results indicate that the new algorithm is better for the kinds of many-objective problems that our aerospace collaborators are interested in solving. The new algorithm returns Pareto-optimal solutions as it computes.",,20 p.,,,,Software Design,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Ted Adelson,"Pfister, Hanspeter; Freeman, William T.; Avidan, Shai; Dale, Kevin; Johnson, Micah K.; Matusik, Wojciech",2009-07-21T21:30:34Z,2009-07-21T21:30:34Z,2009-07-15,http://hdl.handle.net/1721.1/46335,MIT-CSAIL-TR-2009-034,CG2Real: Improving the Realism of Computer Generated Images using a Large Collection of Photographs,"Computer Graphics (CG) has achieved a high level of realism, producing strikingly vivid images. This realism, however, comes at the cost of long and often expensive manual modeling, and most often humans can still distinguish between CG images and real images. We present a novel method to make CG images look more realistic that is simple and accessible to novice users. Our system uses a large collection of photographs gathered from online repositories. Given a CG image, we retrieve a small number of real images with similar global structure. We identify corresponding regions between the CG and real images using a novel mean-shift cosegmentation algorithm. The user can then automatically transfer color, tone, and texture from matching regions to the CG image. Our system only uses image processing operations and does not require a 3D model of the scene, making it fast and easy to integrate into digital content creation workflows. Results of a user study show that our improved CG images appear more realistic than the originals.",,10 p.,,,image databases; image synthesis; computer graphics,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Miller, Jason; Agarwal, Anant; Santambrogio, Marco; Eastep, Jonathan; Hoffmann, Henry",2009-08-07T17:45:05Z,2009-08-07T17:45:05Z,2009-08-07,http://hdl.handle.net/1721.1/46351,MIT-CSAIL-TR-2009-035,Application Heartbeats for Software Performance and Health,"Adaptive, or self-aware, computing has been proposed as one method to help application programmers confront the growing complexity of multicore software development. However, existing approaches to adaptive systems are largely ad hoc and often do not manage to incorporate the true performance goals of the applications they are designed to support. This paper presents an enabling technology for adaptive computing systems: Application Heartbeats. The Application Heartbeats framework provides a simple, standard programming interface that applications can use to indicate their performance and system software (and hardware) can use to query an applicationâ  s performance. Several experiments demonstrate the simplicity and efficacy of the Application Heartbeat approach. First the PARSEC benchmark suite is instrumented with Application Heartbeats to show the broad applicability of the interface. Then, an adaptive H.264 encoder is developed to show how applications might use Application Heartbeats internally. Next, an external resource scheduler is developed which assigns cores to an application based on its performance as specified with Application Heartbeats. Finally, the adaptive H.264 encoder is used to illustrate how Application Heartbeats can aid fault tolerance.",,10 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"Devadas, Srinivas; Cho, Myong Hyon; Shim, Keun Sup; Lis, Mieszko",2009-08-18T22:30:03Z,2009-08-18T22:30:03Z,2009-08-18,http://hdl.handle.net/1721.1/46353,MIT-CSAIL-TR-2009-036,Guaranteed in-order packet delivery using Exclusive Dynamic Virtual Channel Allocation,"In-order packet delivery, a critical abstraction for many higher-level protocols, can severely limit the performance potential in low-latency networks (common, for example, in network-on-chip designs with many cores). While basic variants of dimension-order routing guarantee in-order delivery, improving performance by adding multiple dynamically allocated virtual channels or using other routing schemes compromises this guarantee. Although this can be addressed by reordering out-of-order packets at the destination core, such schemes incur significant overheads, and, in the worst case, raise the specter of deadlock or require expensive retransmission. We present Exclusive Dynamic VCA, an oblivious virtual channel allocation scheme which combines the performance advantages of dynamic virtual allocation with in-network, deadlock-free in-order delivery. At the same time, our scheme reduces head-of-line blocking, often significantly improving throughput compared to equivalent baseline (out-of-order) dimension-order routing when multiple virtual channels are used, and so may be desirable even when in-order delivery is not required. Implementation requires only minor, inexpensive changes to traditional oblivious dimension-order router architectures, more than offset by the removal of packet reorder buffers and logic.",,10 p.,,,,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Robotics, Vision & Sensor Networks; John Leonard","Benjamin, Michael R.; Newman, Paul M.; Schmidt, Henrik; Leonard, John J.",2009-08-20T18:30:06Z,2009-08-20T18:30:06Z,2009-08-20,http://hdl.handle.net/1721.1/46361,MIT-CSAIL-TR-2009-037,Extending a MOOS-IvP Autonomy System and Users Guide to the IvPBuild Toolbox,"This document describes how to extend the suite of MOOS applications and IvP Helm behaviors distributed with the MOOS-IvP software bundle from www.moos-ivp.org. It covers (a) a straw-man repository with a place-holder MOOS application and IvP Behavior, with a working CMake build structure, (b) a brief overview of the MOOS application class with an example application, (c) an overview of the IvP Behavior class with an example behavior, and (d) the IvPBuild Toolbox for generation of objective functions within behaviors.",,102 p.,,,UUV; Behavior Based Control; Unmanned Vehicles; Multi-objective Optimization; Autonomous Marine Vehicles; Behavior Based Architecture; Autonomous Vehicles; AUV; Arbitration; MOOSDB; Unmanned Marine Vehicles; Action Selection; Multi-Objective Optimization; Autonomous Helm; USV; Unmanned Surface Vehicles; MOOS; Behaviors; Artificial Intelligence; Autonomous Decision Making; Underwater Vehicles; ZAIC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Peter Szolovits,"Hug, Caleb",2009-08-26T18:15:07Z,2009-08-26T18:15:07Z,2009-08-26,http://hdl.handle.net/1721.1/46690,MIT-CSAIL-TR-2009-038,Detecting Hazardous Intensive Care Patient Episodes Using Real-time Mortality Models,"The modern intensive care unit (ICU) has become a complex, expensive, data-intensive environment. Caregivers maintain an overall assessment of their patients based on important observations and trends. If an advanced monitoring system could also reliably provide a systemic interpretation of a patient's observations it could help caregivers interpret these data more rapidly and perhaps more accurately. In this thesis I use retrospective analysis of mixed medical/surgical intensive care patients to develop predictive models. Logistic regression is applied to 7048 development patients with several hundred candidate variables. These candidate variables range from simple vitals to long term trends and baseline deviations. Final models are selected by backward elimination on top cross-validated variables and validated on 3018 additional patients. The real-time acuity score (RAS) that I develop demonstrates strong discrimination ability for patient mortality, with an ROC area (AUC) of 0.880. The final model includes a number of variables known to be associated with mortality, but also computationally intensive variables absent in other severity scores. In addition to RAS, I also develop secondary outcome models that perform well at predicting pressor weaning (AUC=0.825), intraaortic balloon pump removal (AUC=0.816), the onset of septic shock (AUC=0.843), and acute kidney injury (AUC=0.742). Real-time mortality prediction is a feasible way to provide continuous risk assessment for ICU patients. RAS offers similar discrimination ability when compared to models computed once per day, based on aggregate data over that day. Moreover, RAS mortality predictions are better at discrimination than a customized SAPS II score (Day 3 AUC=0.878 vs AUC=0.849, p < 0.05). The secondary outcome models also provide interesting insights into patient responses to care and patient risk profiles. While models trained for specifically recognizing secondary outcomes consistently outperform the RAS model at their specific tasks, RAS provides useful baseline risk estimates throughout these events and in some cases offers a notable level of predictive utility.",,315 p.,,,"Severity of Illness Index; Real-Time Systems; Decision Support Systems, Clinical; Pattern Recognition, Automated; Intensive Care; Hospital Mortality; Data Interpretation, Statistical; Artificial Intelligence; Decision Making, Computer-Assisted",Clinical Decision-Making,,,,PhD thesis,,,,Ph.D. in Computer Science,,,,,,,,"Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science",Thesis,,,,,,,,,,,
Martin Rinard,"Ganesh, Vijay; Singh, Rishabh; Near, Joseph P.; Rinard, Martin",2009-08-26T22:00:06Z,2009-08-26T22:00:06Z,2009-08-26,http://hdl.handle.net/1721.1/46691,MIT-CSAIL-TR-2009-039,AvatarSAT: An Auto-tuning Boolean SAT Solver,"We present AvatarSAT, a SAT solver that uses machine-learning classifiers to automatically tune the heuristics of an off-the-shelf SAT solver on a per-instance basis. The classifiers use features of both the input and conflict clauses to select parameter settings for the solver's tunable heuristics. On a randomly selected set of SAT problems chosen from the 2007 and 2008 SAT competitions, AvatarSAT is, on average, over two times faster than MiniSAT based on the geometric mean speedup measure and 50% faster based on the arithmeticmean speedup measure. Moreover, AvatarSAT is hundreds to thousands of times faster than MiniSAT on many hard SAT instances and is never more than twenty times slower than MiniSAT on any SAT instance.",,7 p.,,,self-tuning; machine learning; SAT solvers,Computer Architecture,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Barbara Liskov,"Cheng, Winnie Wing-Yee",2009-08-27T22:00:05Z,2009-08-27T22:00:05Z,2009-08-27,http://hdl.handle.net/1721.1/46700,MIT-CSAIL-TR-2009-040,Information Flow for Secure Distributed Applications,"Private and confidential information is increasingly stored online and increasingly being exposed due to human errors as well as malicious attacks. Information leaks threaten confidentiality, lead to lawsuits, damage enterprise reputations, and cost billion of dollars. While distributed computing architectures provide data and service integration, they also create information flow control problems due to the interaction complexity among service providers. A main problem is the lack of an appropriate programming model to capture expected information flow behaviors in these large distributed software infrastructures. This research tackles this problem by proposing a programming methodology and enforcement platform for application developers to protect and share their sensitive data. We introduce Aeolus, a new platform intended to make it easier to build distributed applications that avoid the unauthorized release of information. The Aeolus security model is based on information flow control but differs from previous work in ways that we believe make it easier to use and understand. In addition, Aeolus provides a number of new mechanisms (anonymous closures, compound tags, boxes, and shared volatile state) to ease the job of writing applications. This thesis provides examples to show how Aeolus features support secure distributed applications. It describes the system design issues and solutions in designing a prototype implementation and presents performance results that show our platform has low overhead.",,177 p.,,,privacy; security; distributed systems; information flow control; integrity; secrecy; confidentiality,Programming Methodology,,,,PhD thesis,,,,,,,,,,,,"Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science",Thesis,,,,,,,,,,,
Seth Teller,"Moore, David; Olson, Edwin; Huang, Albert",2009-09-04T15:45:08Z,2009-09-04T15:45:08Z,2009-09-02,http://hdl.handle.net/1721.1/46708,MIT-CSAIL-TR-2009-041,Lightweight Communications and Marshalling for Low-Latency Interprocess Communication,"We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, targeted at real-time robotics applications. LCM is comprised of several components: a data type specification language, a message passing system, logging/playback tools, and real-time analysis tools. LCM provides a platform- and language-independent type specification language. These specifications can be compiled into platform and language specific implementations, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. Messages can be transmitted between different processes using LCM's message-passing system, which implements a publish/subscribe model. LCM's implementation is notable in providing low-latency messaging and eliminating the need for a central communications ""hub"". This architecture makes it easy to mix simulated, recorded, and live data sources. A number of logging, playback, and traffic inspection tools simplify common development and debugging tasks. LCM is targeted at robotics and other real-time systems where low latency is critical; its messaging model permits dropping messages in order to minimize the latency of new messages. In this paper, we explain LCM's design, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots.",,15 p.,,,message passing; interprocess communication; robotics middleware; real-time systems,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Hoffmann, Henry; Misailovic, Sasa; Sidiroglou, Stelios; Agarwal, Anant; Rinard, Martin",2009-09-04T15:45:14Z,2009-09-04T15:45:14Z,2009-09-03,http://hdl.handle.net/1721.1/46709,MIT-CSAIL-TR-2009-042,"Using Code Perforation to Improve Performance, Reduce Energy Consumption, and Respond to Failures","Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are designed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. We present a new general technique, code perforation, for automatically augmenting existing computations with the capability of trading off accuracy in return for performance. In contrast to existing approaches, which typically require the manual development of new algorithms, our implemented SpeedPress compiler can automatically apply code perforation to existing computations with no developer intervention whatsoever. The result is a transformed computation that can respond almost immediately to a range of increased performancedemands while keeping any resulting output distortion within acceptable user-defined bounds. We have used SpeedPress to automatically apply code perforation to applications from the PARSEC benchmark suite. The results show that the transformed applications can run as much as two to three times faster than the original applications while distorting the output by less than 10%. Because the transformed applications can operate successfully at many points in the performance/accuracy tradeoff space, they can (dynamically and on demand) navigate the tradeoff space to either maximize performance subject to a given accuracy constraint, or maximize accuracy subject to a given performance constraint. We also demonstrate the SpeedGuard runtime system which uses code perforation to enable applications to automatically adapt to challenging execution environments such as multicore machines that suffer core failures or machines that dynamically adjust the clock speed to reduce power consumption or to protect the machine from overheating.",,19 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,"Hoffmann, Henry; Misailovic, Sasa; Sidiroglou, Stelios; Agarwal, Anant; Rinard, Martin",,,,,,,,,,
Gerald Sussman,"Beal, Jacob; Indurkhya, Sagar",2009-09-04T21:30:14Z,2009-09-04T21:30:14Z,2009-09-04,http://hdl.handle.net/1721.1/46710,,Code for LOLCAT Method (Variant of Gillespie Algorithm),"This code and data is publicly listed code for the LOLCAT Method developed by Sagar Indurkhya and Jacob Beal, in the paper: ""Reaction factoring and bipartite update graphs accelerate the Gillespie algorithm for large-scale biochemical systems.""",,,,,Computational Systems Biology; Gillespie Algorithm; Stochastic Simulation Algorithm; Bioinformatics,Mathematics and Computation,,Creative Commons Attrbution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2009-09-17T18:30:10Z,2009-09-17T18:30:10Z,2009-09-12,http://hdl.handle.net/1721.1/46722,MIT-CSAIL-TR-2009-043,Finding aircraft collision-avoidance strategies using policy search methods,A progress report describing the application of policy gradient and policy search by dynamic programming methods to an aircraft collision avoidance problem inspired by the requirements of next-generation TCAS.,,11 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Nicholas Roy,"Roy, Nicholas; He, Ruijie",2009-09-28T21:00:15Z,2009-09-28T21:00:15Z,2009-09-23,http://hdl.handle.net/1721.1/46820,MIT-CSAIL-TR-2009-044,Efficient POMDP Forward Search by Predicting the Posterior Belief Distribution,"Online, forward-search techniques have demonstrated promising results for solving problems in partially observable environments. These techniques depend on the ability to efficiently search and evaluate the set of beliefs reachable from the current belief. However, enumerating or sampling action-observation sequences to compute the reachable beliefs is computationally demanding; coupled with the need to satisfy real-time constraints, existing online solvers can only search to a limited depth. In this paper, we propose that policies can be generated directly from the distribution of the agent's posterior belief. When the underlying state distribution is Gaussian, and the observation function is an exponential family distribution, we can calculate this distribution of beliefs without enumerating the possible observations. This property not only enables us to plan in problems with large observation spaces, but also allows us to search deeper by considering policies composed of multi-step action sequences. We present the Posterior Belief Distribution (PBD) algorithm, an efficient forward-search POMDP planner for continuous domains, demonstrating that better policies are generated when we can perform deeper forward search.",,12 p.,,,,"Robotics, Vision & Sensor Networks",,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Frans Kaashoek,"Lesniewski-Laas, Chris; Kaashoek, M. Frans",2009-09-28T21:00:08Z,2009-09-28T21:00:08Z,2009-09-24,http://hdl.handle.net/1721.1/46819,MIT-CSAIL-TR-2009-045,Whanaungatanga: Sybil-proof routing with social networks,"Decentralized systems, such as distributed hash tables, are subject to the Sybil attack, in which an adversary creates many false identities to increase its influence. This paper proposes a routing protocol for a distributed hash table that is strongly resistant to the Sybil attack. This is the first solution to this problem with sublinear run time and space usage. The protocol uses the social connections between users to build routing tables that enable Sybil-resistant distributed hash table lookups. With a social network of N well-connected honest nodes, the protocol can tolerate up to O(N/log N) ""attack edges"" (social links from honest users to phony identities). This means that an adversary has to fool a large fraction of the honest users before any lookups will fail. The protocol builds routing tables that contain O(N log^(3/2) N) entries per node. Lookups take O(1) time. Simulation results, using social network graphs from LiveJournal, Flickr, and YouTube, confirm the analytical results.",,14 p.,,,dht; security; sybil; overlay; distributed hash table,Parallel and Distributed Operating Systems,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Chikkerur, Sharat; Poggio, Tomaso; Serre, Thomas",2009-10-06T22:45:05Z,2009-10-06T22:45:05Z,2009-10-02,http://hdl.handle.net/1721.1/49415,CBCL-279; MIT-CSAIL-TR-2009-046,Attentive processing improves object recognition,"The human visual system can recognize several thousand object categories irrespective of their position and size. This combination of selectivity and invariance is built up gradually across several stages of visual processing. However, the recognition of multiple objects in cluttered visual scenes presents a difficult problem for human as well as machine vision systems. The human visual system has evolved to perform two stages of visual processing: a pre-attentive parallel processing stage, in which the entire visual field is processed at once and a slow serial attentive processing stage, in which aregion of interest in an input image is selected for ""specialized"" analysis by an attentional spotlight. We argue that this strategy evolved to overcome the limitation of purely feed forward processing in the presence of clutter and crowding. Using a Bayesian model of attention along with a hierarchical model of feed forward recognition on a data set of real world images, we show that this two stage attentive processing can improve recognition in cluttered and crowded conditions.",,12 p.,,,,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Chikkerur, Sharat; Serre, Thomas; Poggio, Tomaso",2009-10-06T22:45:10Z,2009-10-06T22:45:10Z,2009-10-03,http://hdl.handle.net/1721.1/49416,CBCL-280; MIT-CSAIL-TR-2009-047,A Bayesian inference theory of attention: neuroscience and algorithms,"The past four decades of research in visual neuroscience has generated a large and disparate body of literature on the role of attention [Itti et al., 2005]. Although several models have been developed to describe specific properties of attention, a theoretical framework that explains the computational role of attention and is consistent with all known effects is still needed. Recently, several authors have suggested that visual perception can be interpreted as a Bayesian inference process [Rao et al., 2002, Knill and Richards, 1996, Lee and Mumford, 2003]. Within this framework, topdown priors via cortical feedback help disambiguate noisy bottom-up sensory input signals. Building on earlier work by Rao [2005], we show that this Bayesian inference proposal can be extended to explain the role and predict the main properties of attention: namely to facilitate the recognition of objects in clutter. Visual recognition proceeds by estimating the posterior probabilities for objects and their locations within an image via an exchange of messages between ventral and parietal areas of the visual cortex. Within this framework, spatial attention is used to reduce the uncertainty in feature information; feature-based attention is used to reduce the uncertainty in location information. In conjunction, they are used to recognize objects in clutter. Here, we find that several key attentional phenomena such such as pop-out, multiplicative modulation and change in contrast response emerge naturally as a property of the network. We explain the idea in three stages. We start with developing a simplified model of attention in the brain identifying the primary areas involved and their interconnections. Secondly, we propose a Bayesian network where each node has direct neural correlates within our simplified biological model. Finally, we elucidate the properties of the resulting model, showing that the predictions are consistent with physiological and behavioral evidence.",,18 p.,,,,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rob Miller,"Miller, Rob; Karger, David; Marcus, Adam; Bernstein, Michael",2009-10-13T12:31:44Z,2009-10-13T12:31:44Z,2009-10-07,http://hdl.handle.net/1721.1/49426,MIT-CSAIL-TR-2009-048,Understanding and Supporting Directed Content Sharing on the Web,"To find interesting, personally relevant web content, we often rely on friends and colleagues to pass links along as they encounter them. In this paper, we study and augment link-sharing via e-mail, the most popular means of sharing web content today. Armed with survey data indicating that active sharers of novel web content are often those that actively seek it out, we present FeedMe, a plug-in for Google Reader that makes directed sharing of content a more salient part of the user experience. Our survey research indicates that sharing is moderated by concern about relevancy to the recipient, a desire to send only novel content to the recipient, and the effort required to share. FeedMe allays these concerns by recommending friends who may be interested in seeing the content, providing information on what the recipient has seen and how many emails they have received recently, and giving recipients the opportunity to provide lightweight feedback when they appreciate shared content. FeedMe introduces a novel design space for mixed-initiative social recommenders: friends who know the user voluntarily vet the material on the userâ  s behalf. We present a two week field experiment (N=60) demonstrating that FeedMeâ  s recommendations and social awareness features made it easier and more enjoyable to share content that recipients appreciated and would not have found otherwise.",,10 p.,,,friendsourcing; Social link sharing; blogs; RSS,User Interface Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Shakhnarovich, Greg; Bouvrie, Jake; Rosasco, Lorenzo; Smale, Steve",2009-10-13T12:31:06Z,2009-10-13T12:31:06Z,2009-10-09,http://hdl.handle.net/1721.1/49425,CBCL-281; MIT-CSAIL-TR-2009-049,Notes on the Shannon Entropy of the Neural Response,"In these notes we focus on the concept of Shannon entropy in an attempt to provide a systematic way of assessing the discrimination properties of the neural response, and quantifying the role played by the number of layers and the number of templates.",,6 p.,,,computer vision; artificial intelligence; neuroscience; computation,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Rosasco, Lorenzo; Verri, Alessandro; Santoro, Matteo; Mosci, Sofia; Villa, Silvia",2009-10-14T21:00:10Z,2009-10-14T21:00:10Z,2009-10-14,http://hdl.handle.net/1721.1/49428,MIT-CSAIL-TR-2009-050; CBCL-282,Iterative Projection Methods for Structured Sparsity Regularization,"In this paper we propose a general framework to characterize and solve the optimization problems underlying a large class of sparsity based regularization algorithms. More precisely, we study the minimization of learning functionals that are sums of a differentiable data term and a convex non differentiable penalty. These latter penalties have recently become popular in machine learning since they allow to enforce various kinds of sparsity properties in the solution. Leveraging on the theory of Fenchel duality and subdifferential calculus, we derive explicit optimality conditions for the regularized solution and propose a general iterative projection algorithm whose convergence to the optimal solution can be proved. The generality of the framework is illustrated, considering several examples of regularization schemes, including l1 regularization (and several variants), multiple kernel learning and multi-task learning. Finally, some features of the proposed framework are empirically studied.",,28 p.,,,computation; learning,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ted Adelson,"Adelson, Edward H.; Torralba, Antonio; Fleming, Roland W.",2009-10-22T17:30:07Z,2009-10-22T17:30:07Z,2009-10-22,http://hdl.handle.net/1721.1/49511,MIT-CSAIL-TR-2009-051,Shape from Sheen,,,56 p.,,,specular; shape perception; Vision,Vision,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Yu, Xinlin; Steele, Andrew D.; Khilnani, Vinita; Garrote, Estibaliz; Jhuang, Hueihan; Serre, Thomas; Poggio, Tomaso",2009-11-03T20:30:21Z,2009-11-03T20:30:21Z,2009-10-26,http://hdl.handle.net/1721.1/49527,CBCL-283; MIT-CSAIL-TR-2009-052,Automated home-cage behavioral phenotyping of mice,"We describe a trainable computer vision system enabling the automated analysis of complex mouse behaviors. We provide software and a very large manually annotated video database used for training and testing the system. Our system outperforms leading commercial software and performs on par with human scoring, as measured from the ground-truth manual annotations of thousands of clips of freely behaving animals. We show that the home-cage behavior profiles provided by the system is sufficient to accurately predict the strain identity of individual animals in the case of two standard inbred and two non-standard mouse strains. Our software should complement existing sensor-based automated approaches and help develop an adaptable, comprehensive, high-throughput, fine-grained, automated analysis of rodent behavior.",,27 p.,,,animal monitoring; rodents,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-Noncommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Polina Golland,"Golland, Polina; Lashkari, Danial",2009-11-03T20:30:11Z,2009-11-03T20:30:11Z,2009-11-03,http://hdl.handle.net/1721.1/49526,MIT-CSAIL-TR-2009-054,Co-Clustering with Generative Models,"In this paper, we present a generative model for co-clustering and develop algorithms based on the mean field approximation for the corresponding modeling problem. These algorithms can be viewed as generalizations of the traditional model-based clustering; they extend hard co-clustering algorithms such as Bregman co-clustering to include soft assignments. We show empirically that these model-based algorithms offer better performance than their hard-assignment counterparts, especially with increasing problem complexity.",,9 p.,,,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gerald Sussman,"Radul, Alexey",2009-11-03T20:30:05Z,2009-11-03T20:30:05Z,2009-11-03,http://hdl.handle.net/1721.1/49525,MIT-CSAIL-TR-2009-053,Propagation Networks: A Flexible and Expressive Substrate for Computation,"I propose a shift in the foundations of computation. Practically all ideas of general-purpose computation today are founded either on execution of sequences of atomic instructions, i.e., assembly languages, or on evaluation of tree-structured expressions, i.e., most higher level programming languages. Both have served us well in the past, but it is increasingly clear that we need something more. I suggest that we can build general-purpose computation on propagation of information through networks of stateful cells interconnected with stateless autonomous asynchronous computing elements. Various forms of this general idea have been used with great success for various special purposes; perhaps the most immediate example is constraint propagation in constraint satisfaction systems. These special-purpose systems, however, are all complex and all different, and neither compose well, nor interoperate well, nor generalize well. A foundational layer is missing. The key insight in this work is that a cell should not be seen as storing a value, but as accumulating information about a value. The cells should never forget information -- such monotonicity prevents race conditions in the behavior of the network. Monotonicity of information need not be a severe restriction: for example, carrying reasons for believing each thing makes it possible to explore but thenpossibly reject tentative hypotheses, thus appearing to undo something, while maintaining monotonicity. Accumulating information is a broad enough design principle to encompass arbitrary computation. The object of this dissertation is therefore to architect a general-purpose computing system based on propagation networks; to subsume expression evaluation under propagation just as instruction execution is subsumed under expression evaluation; to demonstrate that a general-purpose propagation system can recover all the benefits that have been derived from special-purpose propagation systems, allow them to compose andinteroperate, and offer further expressive power beyond what we have known in the past; and finally to contemplate the lessons that such a fundamental shift can teach us about the deep nature of computation.",,174 p.,,,,Mathematics and Computation,"My graduate career in general, and this work in particular, have been sponsored in part by a National Science Foundation Graduate Research Fellowship, by the Disruptive Technology Office as part of the AQUAINT Phase 3 research program, by the Massachusetts Institute of Technology, by Google, Inc., and by the National Science Foundation Cybertrust (05-518) program.",Creative Commons Attribution-Noncommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,PhD thesis,,,,Doctor of Philosophy,,,,,,,,Massachusetts Institute of Technology Department of Electrical Engineering and Computer Science,,,2009-09,,,,,,,,,
Anant Agarwal,"Agarwal, Anant; Santambrogio, Marco D.; Wingate, David; Eastep, Jonathan",2009-11-09T21:30:03Z,2009-11-09T21:30:03Z,2009-11-09,http://hdl.handle.net/1721.1/49808,MIT-CSAIL-TR-2009-055,Smartlocks: Self-Aware Synchronization through Lock Acquisition Scheduling,"As multicore processors become increasingly prevalent, system complexity is skyrocketing. The advent of the asymmetric multicore compounds this -- it is no longer practical for an average programmer to balance the system constraints associated with today's multicores and worry about new problems like asymmetric partitioning and thread interference. Adaptive, or self-aware, computing has been proposed as one method to help application and system programmers confront this complexity. These systems take some of the burden off of programmers by monitoring themselves and optimizing or adapting to meet their goals. This paper introduces an open-source self-aware synchronization library for multicores and asymmetric multicores called Smartlocks. Smartlocks is a spin-lock library that adapts its internal implementation during execution using heuristics and machine learning to optimize toward a user-defined goal, which may relate to performance, power, or other problem-specific criteria. Smartlocks builds upon adaptation techniques from prior work like reactive locks, but introduces a novel form of adaptation designed for asymmetric multicores that we term lock acquisition scheduling. Lock acquisition scheduling is optimizing which waiter will get the lock next for the best long-term effect when multiple threads (or processes) are spinning for a lock. Our results demonstrate empirically that lock scheduling is important for asymmetric multicores and that Smartlocks significantly outperform conventional and reactive locks for asymmetries like dynamic variations in processor clock frequencies caused by thermal throttling events.",,16 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Beckmann, Nathan; Eastep, Jonathan; Gruenwald, Charles, III; Kurian, George; Kasture, Harshad; Miller, Jason E.; Celio, Christopher; Agarwal, Anant",2009-11-10T18:15:03Z,2009-11-10T18:15:03Z,2009-11-09,http://hdl.handle.net/1721.1/49809,MIT-CSAIL-TR-2009-056,Graphite: A Distributed Parallel Simulator for Multicores,"This paper introduces the open-source Graphite distributed parallel multicore simulator infrastructure. Graphite is designed from the ground up for exploration of future multicore processors containing dozens, hundreds, or even thousands of cores. It provides high performance for fast design space exploration and software development for future processors. Several techniques are used to achieve this performance including: direct execution, multi-machine distribution, analytical modeling, and lax synchronization. Graphite is capable of accelerating simulations by leveraging several machines. It can distribute simulation of an off-the-shelf threaded application across a cluster of commodity Linux machines with no modification to the source code. It does this by providing a single, shared address space and consistent single-process image across machines. Graphite is designed to be a simulation framework, allowing different component models to be easily replaced to either model different architectures or tradeoff accuracy for performance. We evaluate Graphite from a number of perspectives and demonstrate that it can simulate target architectures containing over 1000 cores on ten 8-core servers. Performance scales well as more machines are added with near linear speedup in many cases. Simulation slowdown is as low as 41x versus native execution for some applications. The Graphite infrastructure and existing models will be released as open-source software to allow the community to simulate their own architectures and extend and improve the framework.",,17 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Oshman, Rotem; Lynch, Nancy; Kuhn, Fabian",2009-11-12T19:30:03Z,2009-11-12T19:30:03Z,2009-11-10,http://hdl.handle.net/1721.1/49814,MIT-CSAIL-TR-2009-058,Distributed Computation in Dynamic Networks,"In this report we investigate distributed computation in dynamic networks in which the network topology changes from round to round. We consider a worst-case model in which the communication links for each round are chosen by an adversary, and nodes do not know who their neighbors for the current round are before they broadcast their messages. The model is intended to capture mobile networks and wireless networks, in which mobility and interference render communication unpredictable. The model allows the study of the fundamental computation power of dynamic networks. In particular, it captures mobile networks and wireless networks, in which mobility and interference render communication unpredictable. In contrast to much of the existing work on dynamic networks, we do not assume that the network eventually stops changing; we require correctness and termination even in networks that change continually. We introduce a stability property called T-interval connectivity (for T >= 1), which stipulates that for every T consecutive rounds there exists a stable connected spanning subgraph. For T = 1 this means that the graph is connected in every round, but changes arbitrarily between rounds. Algorithms for the dynamic graph model must cope with these unceasing changes. We show that in 1-interval connected graphs it is possible for nodes to determine the size of the network and compute any computable function of their initial inputs in O(n^2) rounds using messages of size O(log n + d), where d is the size of the input to a single node. Further, if the graph is T-interval connected for T > 1, the computation can be sped up by a factor of T, and any function can be computed in O(n + n^2 / T) rounds using messages of size O(log n + d). We also give two lower bounds on the gossip problem, which requires the nodes to disseminate k pieces of information to all the nodes in the network. We show an Omega(n log k) bound on gossip in 1-interval connected graphs against centralized algorithms, and an Omega(n + nk / T) bound on exchanging k pieces of information in T-interval connected graphs for a restricted class of randomized distributed algorithms. The T-interval connected dynamic graph model is a novel model, which we believe opens new avenues for research in the theory of distributed computing in wireless, mobile and dynamic networks.",,40 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Chen, Jing",2009-11-10T18:15:12Z,2009-11-10T18:15:12Z,2009-11-10,http://hdl.handle.net/1721.1/49810,MIT-CSAIL-TR-2009-057,Rational Robustness for Mechanism Design,"The currently prevailing equilibrium-based approach to mechanism design suffers from a plurality of fundamental problems, and new conceptual frameworks are needed to solve or sufficiently alleviate them. In this paper, we put forward rational robustness, a new solution concept/implementation notion that is not equilibrium-based; prove its fundamental structural theorems; and compare it with prior notions. Our notion of implementation is specifically built so as to be robust against the problem of equilibrium selection. We prove it robust against other fundamental problems as well in different papers.",,22 p.,,,"Mechanism Design, Implementation, Rational Robustness, Distinguishably Dominated Strategies",,Work partially supported by ONR Contract Number N00014-09-1-0597.,,,first draft,,,,,,,,,,,,,,,,Theory of Computation,,,,,,,,
Anant Agarwal,"Modzelewski, Kevin; Miller, Jason; Belay, Adam; Beckmann, Nathan; Gruenwald, Charles, III; Wentzlaff, David; Youseff, Lamia; Agarwal, Anant",2009-11-20T23:45:04Z,2009-11-20T23:45:04Z,2009-11-20,http://hdl.handle.net/1721.1/49844,MIT-CSAIL-TR-2009-059,A Unified Operating System for Clouds and Manycore: fos,"Single chip processors with thousands of cores will be available in the next ten years and clouds of multicore processors afford the operating system designer thousands of cores today. Constructing operating systems for manycore and cloud systems face similar challenges. This work identifies these shared challenges and introduces our solution: a factored operating system (fos) designed to meet the scalability, faultiness, variability of demand, and programming challenges of OSâ  s for single-chip thousand-core manycore systems as well as current day cloud computers. Current monolithic operating systems are not well suited for manycores and clouds as they have taken an evolutionary approach to scaling such as adding fine grain locks and redesigning subsystems, however these approaches do not increase scalability quickly enough. fos addresses the OS scalability challenge by using a message passing design and is composed out of a collection of Internet inspired servers. Each operating system service is factored into a set of communicating servers which in aggregate implement a system service. These servers are designed much in the way that distributed Internet services are designed, but provide traditional kernel services instead of Internet services. Also, fos embraces the elasticity of cloud and manycore platforms by adapting resource utilization to match demand. fos facilitates writing applications across the cloud by providing a single system image across both future 1000+ core manycores and current day Infrastructure as a Service cloud computers. In contrast, current cloud environments do not provide a single system image and introduce complexity for the user by requiring different programming models for intra- vs inter-machine communication, and by requiring the use of non-OS standard management tools.",,11 p.,,,Infrastructure as a Service; Cloud Computing; Manycore; Operating System; Multicore,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Poggio, Tomaso; Rosasco, Lorenzo; Wibisono, Andre",2009-12-01T21:15:05Z,2009-12-01T21:15:05Z,2009-12-01,http://hdl.handle.net/1721.1/49868,CBCL-284; MIT-CSAIL-TR-2009-060,Sufficient Conditions for Uniform Stability of Regularization Algorithms,"In this paper, we study the stability and generalization properties of penalized empirical-risk minimization algorithms. We propose a set of properties of the penalty term that is sufficient to ensure uniform ?-stability: we show that if the penalty function satisfies a suitable convexity property, then the induced regularization algorithm is uniformly ?-stable. In particular, our results imply that regularization algorithms with penalty functions which are strongly convex on bounded domains are ?-stable. In view of the results in [3], uniform stability implies generalization, and moreover, consistency results can be easily obtained. We apply our results to show that â  p regularization for 1 < p <= 2 and elastic-net regularization are uniformly ?-stable, and therefore generalize.",,16 p.,,,artificial intelligence; theory; computation; learning,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Chen, Jing",2009-12-09T21:15:08Z,2009-12-09T21:15:08Z,2009-12-04,http://hdl.handle.net/1721.1/49869,MIT-CSAIL-TR-2009-061,Perfect and General Virtual Implementation For Perfectly Informed Players,"We show that, when the players are perfectly informed about each other, essentially all social-choice functions can be rationally robustly implemented via an extensive-form public-action mechanism that (1) is perfectly robust against collusion, (2) requires only a linear number of computation steps and communication bits, and (3) preserves the privacy of the players' types to a very high extent.",,4 p.,,,rationally robust implementation; Virtual implementation; perfectly informed players,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Durand, Fredo; Cohen, Michael; Chen, Jiawen; Paris, Sylvain; Wang, Jue; Matusik, Wojciech",2009-12-17T19:15:09Z,2009-12-17T19:15:09Z,2009-12-16,http://hdl.handle.net/1721.1/50231,MIT-CSAIL-TR-2009-062,The Video Mesh: A Data Structure for Image-based Video Editing,"This paper introduces the video mesh, a data structure for representing video as 2.5D ""paper cutouts."" The video mesh allows interactive editing of moving objects and modeling of depth, which enables 3D effects and post-exposure camera control. The video mesh sparsely encodes optical flow as well as depth, and handles occlusion using local layering and alpha mattes. Motion is described by a sparse set of points tracked over time. Each point also stores a depth value. The video mesh is a triangulation over this point set and per-pixel information is obtained by interpolation. The user rotoscopes occluding contours and we introduce an algorithm to cut the video mesh along them. Object boundaries are refined with perpixel alpha values. The video mesh is at its core a set of texture mapped triangles, we leverage graphics hardware to enable interactive editing and rendering of a variety of effects. We demonstrate the effectiveness of our representation with a number of special effects including 3D viewpoint changes, object insertion, and depth-of-field manipulation.",,9 p.,,,feature tracking; compositing; video editing; optical flow; triangle mesh,Computer Graphics,,Creative Commons Attribution-Noncommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,,"CHEN, J., PARIS, S., WANG, J., MATUSIK, W., COHEN, M., and DURAND, F. 2009. The Video Mesh: A Data Structure for Image-based Video Editing. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series, MIT-CSAIL-TR-2009-062.",,,,,,,,,,,,,,,,,,,,,,,
Whitman Richards,"Richards, Whitman; Winston, Patrick Henry; Finlayson, Mark Alan",2009-12-17T21:15:04Z,2009-12-17T21:15:04Z,2009-12-17,http://hdl.handle.net/1721.1/50232,MIT-CSAIL-TR-2009-063,Advancing Computational Models of Narrative,"Report of a Workshop held at the Wylie Center, Beverly, MA, Oct 8-10 2009",,32 p.,,en,literary theory; stories; narrative; computational linguistics; natural language generation; text understanding; discourse parsing,Belief Dynamics,Sponsored by the AFOSR under MIT-MURI contract #FA9550-05-1-0321,Creative Commons Attribution-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,Belief Dynamics; Whitman Richards,,,,,,,,,,,,,,,"Beverly, Massachusetts, United States",2009-10-08/2009-10-10,FA9550-05-1-0321,,,,,
Saman Amarasinghe,"Amarasinghe, Saman; Rabbah, Rodric; Larsen, Samuel",2009-12-18T19:30:12Z,2009-12-18T19:30:12Z,2009-12-18,http://hdl.handle.net/1721.1/50235,MIT-CSAIL-TR-2009-064,Selective Vectorization for Short-Vector Instructions,"Multimedia extensions are nearly ubiquitous in today's general-purpose processors. These extensions consist primarily of a set of short-vector instructions that apply the same opcode to a vector of operands. Vector instructions introduce a data-parallel component to processors that exploit instruction-level parallelism, and present an opportunity for increased performance. In fact, ignoring a processor's vector opcodes can leave a significant portion of the available resources unused. In order for software developers to find short-vector instructions generally useful, however, the compiler must target these extensions with complete transparency and consistent performance. This paper describes selective vectorization, a technique for balancing computation across a processor's scalar and vector units. Current approaches for targeting short-vector instructions directly adopt vectorizing technology first developed for supercomputers. Traditional vectorization, however, can lead to a performance degradation since it fails to account for a processor's scalar resources. We formulate selective vectorization in the context of software pipelining. Our approach creates software pipelines with shorter initiation intervals, and therefore, higher performance. A key aspect of selective vectorization is its ability to manage transfer of operands between vector and scalar instructions. Even when operand transfer is expensive, our technique is sufficiently sophisticated to achieve significant performance gains. We evaluate selective vectorization on a set of SPEC FP benchmarks. On a realistic VLIW processor model, the approach achieves whole-program speedups of up to 1.35x over existing approaches. For individual loops, it provides speedups of up to 1.75x.",,25 p.,,,SIMD; Vectorization; Compiler,Computer Architecture,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2013-04-11T20:45:03Z,2013-04-11T20:45:03Z,2009.,http://hdl.handle.net/1721.1/78359,MIT-CSAIL-TR-2013-006,Task-Structured Probabilistic I/O Automata,"Modeling frameworks such as Probabilistic I/O Automata (PIOA) and Markov Decision Processes permit both probabilistic and nondeterministic choices. In order to use these frameworks to express claims about probabilities of events, one needs mechanisms for resolving nondeterministic choices. For PIOAs, nondeterministic choices have traditionally been resolved by schedulers that have perfect information about the past execution. However, these schedulers are too powerful for certain settings, such as cryptographic protocol analysis, where information must sometimes be hidden. Here, we propose a new, less powerful nondeterminism-resolution mechanism for PIOAs, consisting of tasks and local schedulers. Tasks are equivalence classes of system actions that are scheduled by oblivious, global task sequences. Local schedulers resolve nondeterminism within system components, based on local information only. The resulting task-PIOA framework yields simple notions of external behavior and implementation, and supports simple compositionality results. We also define a new kind of simulation relation, and show it to be sound for proving implementation. We illustrate the potential of the task-PIOAframework by outlining its use in verifying an Oblivious Transfer protocol.",,46 p.,,,,Theory of Computation,,,,"""May 28, 2009.""",,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Kaess, Michael; Dellaert, Frank; Roberts, Richard; Ila, Viorela",2010-05-05T18:15:02Z,2010-05-05T18:15:02Z,2010-01-29,http://hdl.handle.net/1721.1/54717,MIT-CSAIL-TR-2010-021,The Bayes Tree: Enabling Incremental Reordering and Fluid Relinearization for Online Mapping,"In this paper we present a novel data structure, the Bayes tree, which exploits the connections between graphical model inference and sparse linear algebra. The proposed data structure provides a new perspective on an entire class of simultaneous localization and mapping (SLAM) algorithms. Similar to a junction tree, a Bayes tree encodes a factored probability density, but unlike the junction tree it is directed and maps more naturally to the square root information matrix of the SLAM problem. This makes it eminently suited to encode the sparse nature of the problem, especially in a smoothing and mapping (SAM) context. The inherent sparsity of SAM has already been exploited in the literature to produce efficient solutions in both batch and online mapping. The graphical model perspective allows us to develop a novel incremental algorithm that seamlessly incorporates reordering and relinearization. This obviates the need for expensive periodic batch operations from previous approaches, which negatively affect the performance and detract from the intended online nature of the algorithm. The new method is evaluated using simulated and real-world datasets in both landmark and pose SLAM settings.",,18 p.,,,sparse nonlinear optimization; sparse linear algebra; iSAM; SLAM; junction tree; probabilistic inference; graphical models; mobile robotics; smoothing and mapping,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Erik Demaine,"Zadimoghaddam, Morteza; Hajiaghayi, MohammadTaghi; Bateni, MohammadHossein",2010-02-02T23:30:07Z,2010-02-02T23:30:07Z,2010-02-01,http://hdl.handle.net/1721.1/51336,MIT-CSAIL-TR-2010-002,Submodular Secretary Problem and Extensions,"Online auction is an essence of many modern markets, particularly networked markets, in which information about goods, agents, and outcomes is revealed over a period of time, and the agents must make irrevocable decisions without knowing future information. Optimal stopping theory, especially the classic ""secretary problem"", is a powerful tool for analyzing such online scenarios which generally require optimizing an objective function over the input. The secretary problem and its generalization the ""multiple-choice secretary problem"" were under a thorough study in the literature. In this paper, we consider a very general setting of the latter problem called the ""submodular secretary problem"", in which the goal is to select k secretaries so as to maximize the expectation of a (not necessarily monotone) submodular function which defines efficiency of the selected secretarial group based on their overlapping skills. We present the first constant-competitive algorithm for this case. In a more general setting in which selected secretaries should form an independent (feasible) set in each of l given matroids as well, we obtain an O(l log^2 r)-competitive algorithm generalizing several previous results, where r is the maximum rank of the matroids. Another generalization is to consider l knapsack constraints instead of the matroid constraints, for which we present an O(l)-competitive algorithm. In a sharp contrast, we show for a more general setting of ""subadditive secretary problem, there is no o~(sqrt(n))-competitive algorithm and thus submodular functions are the most general functions to consider for constant competitiveness in our setting. We complement this result by giving a matching O(sqrt(n))-competitive algorithm for the subadditive case. At the end, we consider some special cases of our general setting as well.",,19 p.,,,,Theory of Computation,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Modzelewski, Kevin; Miller, Jason; Belay, Adam; Beckmann, Nathan; Gruenwald, Charles, III; Wentzlaff, David; Youseff, Lamia; Agarwal, Anant",2010-02-08T20:00:07Z,2010-02-08T20:00:07Z,2010-02-08,http://hdl.handle.net/1721.1/51381,MIT-CSAIL-TR-2010-003,An Operating System for Multicore and Clouds: Mechanisms and Implementation,"Cloud computers and multicore processors are two emerging classes of computational hardware that have the potential to provide unprecedented compute capacity to the average user. In order for the user to effectively harness all of this computational power, operating systems (OSes) for these new hardware platforms are needed.  Existing multicore operating systems do not scale to large numbers of cores, and do not support clouds. Consequently, current-day cloud systems push much complexity onto the user, requiring the user to manage individual Virtual Machines (VMs) and deal with many system-level concerns. In this work we describe the mechanisms and implementation of a factored operating system named fos. fos is a single system image operating system across both multicore and Infrastructure as a Service (IaaS) cloud systems.  fos tackles OS scalability challenges by factoring the OS into its component system services. Each system service is further factored into a collection of Internet-inspired servers which communicate via messaging. Although designed in a manner similar to distributed Internet services, OS services instead provide traditional kernel services such as file systems, scheduling, memory management,and access to hardware. fos also implements new classes of OS services like fault tolerance and demand elasticity. In this work, we describe our working fos implementation, and provide early performance measurements of fos for both intra-machine and inter-machine operations.",,12 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Kuhn, Fabian; Kowalski, Dariusz; Khabbazian, Majid",2010-02-09T18:30:02Z,2010-02-09T18:30:02Z,2010-02-09,http://hdl.handle.net/1721.1/51667,MIT-CSAIL-TR-2010-005,The Cost of Global Broadcast Using Abstract MAC Layers,"We analyze greedy algorithms for broadcasting messages throughout a multi-hop wireless network, using a slot-based model that includes message collisions without collision detection. Our algorithms are split formally into two pieces: a high-level piece for broadcast and a low-level piece for contention management. We accomplish the split using abstract versions of the MAC layer to encapsulate the contention management. We use two different abstract MAC layers: a basic non-probabilistic one, which our contention management algorithm implements with high probability, and a probabilistic one, which our contention management algorithm implements precisely. Using this approach, we obtain the following complexity bounds: Single-message broadcast, using the basic abstract MAC layer, takes time O(D log(n/epsilon) log(Delta)) to deliver the message everywhere with probability 1 - epsilon, where D is the network diameter, n is the number of nodes, and Delta is the maximum node degree. Single-message broadcast, using the probabilistic abstract MAC layer, takes time only O((D + log(n/epsilon)) log(Delta)). For multi-message broadcast, the bounds are O((D + k' Delta) log(n/epsilon) log(Delta)) using the basic layer and O((D + k' Delta log(n/epsilon)) log(Delta)) using the probabilistic layer,for the time to deliver a single message everywhere in the presence of at most k' concurrent messages.",,42 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Kim, Deokhwan; Misailovic, Sasa; Rinard, Martin",2010-02-10T18:15:03Z,2010-02-10T18:15:03Z,2010-02-10,http://hdl.handle.net/1721.1/51680,MIT-CSAIL-TR-2010-007,Automatic Parallelization With Statistical Accuracy Bounds,"Traditional parallelizing compilers are designed to generate parallel programs that produce identical outputs as the original sequential program. The difficulty of performing the program analysis required to satisfy this goal and the restricted space of possible target parallel programs have both posed significant obstacles to the development of effective parallelizing compilers. The QuickStep compiler is instead designed to generate parallel programs that satisfy statistical accuracy guarantees. The freedom to generate parallel programs whose output may differ (within statistical accuracy bounds) from the output of the sequential program enables a dramatic simplification of the compiler and a significant expansion in the range of parallel programs that it can legally generate. QuickStep exploits this flexibility to take a fundamentally different approach from traditional parallelizing compilers. It applies a collection of transformations (loop parallelization, loop scheduling, synchronization introduction, and replication introduction) to generate a search space of parallel versions of the original sequential program. It then searches this space (prioritizing the parallelization of the most time-consuming loops in the application) to find a final parallelization that exhibits good parallel performance and satisfies the statistical accuracy guarantee. At each step in the search it performs a sequence of trial runs on representative inputs to examine the performance, accuracy, and memory accessing characteristics of the current generated parallel program. An analysis of these characteristics guides the steps the compiler takes as it explores the search space of parallel programs. Results from our benchmark set of applications show that QuickStep can automatically generate parallel programs with good performance and statistically accurate outputs. For two of the applications, the parallelization introduces noise into the output, but the noise remains within acceptable statistical bounds. The simplicity of the compilation strategy and the performance and statistical acceptability of the generated parallel programs demonstrate the advantages of the QuickStep approach.",,12 p.,,,,Computer Architecture,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Agarwal, Anant; Miller, Jason; Beckmann, Nathan; Wentzlaff, David",2010-02-12T07:15:04Z,2010-02-12T07:15:04Z,2010-02-11,http://hdl.handle.net/1721.1/51733,MIT-CSAIL-TR-2010-008,Core Count vs Cache Size for Manycore Architectures in the Cloud,"The number of cores which fit on a single chip is growing at an exponential rate while off-chip main memory bandwidth is growing at a linear rate at best. This core count to off-chip bandwidth disparity causes per-core memory bandwidth to decrease as process technology advances. Continuing per-core off-chip bandwidth reduction will cause multicore and manycore chip architects to rethink the optimal grain size of a core and the on-chip cache configuration in order to save main memory bandwidth. This work introduces an analytic model to study the tradeoffs of utilizing increased chip area for larger caches versus more cores. We focus this study on constructing manycore architectures well suited for the emerging application space of cloud computing where many independent applications are consolidated onto a single chip. This cloud computing application mix favors small, power-efficient cores. The model is exhaustively evaluated across a large range of cache and core-count configurations utilizing SPEC Int 2000 miss rates and CACTI timing and area models to determine the optimal cache configurations and the number of cores across four process nodes. The model maximizes aggregate computational throughput and is applied to SRAM and logic process DRAM caches. As an example, our study demonstrates that the optimal manycore configuration in the 32nm node for a 200 mm^2 die uses on the order of 158 cores, with each core containing a 64KB L1I cache, a 16KB L1D cache, and a 1MB L2 embedded-DRAM cache. This study finds that the optimal cache size will continue to grow as process technology advances, but the tradeoff between more cores and larger caches is a complex tradeoff in the face of limited off-chip bandwidth and the non-linearities of cache miss rates and memory controller queuing delay.",,13 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Psota, James; Agarwal, Anant; Miller, Jason; Beckmann, Nathan; Kurian, George",2010-02-12T07:15:12Z,2010-02-12T07:15:12Z,2010-02-11,http://hdl.handle.net/1721.1/51734,MIT-CSAIL-TR-2010-009,Efficient Cache Coherence on Manycore Optical Networks,"Ever since industry has turned to parallelism instead of frequency scaling to improve processor performance, multicore processors have continued to scale to larger and larger numbers of cores. Some believe that multicores will have 1000 cores or more by the middle of the next decade. However, their promise of increased performance will only be reached if their inherent scaling challenges are overcome. One such major scaling challenge is the viability of efficient cache coherence with large numbers of cores. Meanwhile, recent advances in nanophotonic device manufacturing are making CMOS-integrated optics a realityâ  interconnect technology which can provide significantly more bandwidth at lower power than conventional electrical analogs. The contributions of this paper are two-fold. (1) It presents ATAC, a new manycore architecture that augments an electrical mesh network with an optical network that performs highly efficient broadcasts. (2) It introduces ACKwise, a novel directory-based cache coherence protocol that provides high performance and scalability on any large-scale manycore interconnection net- work with broadcast capability. Performance evaluation studies using analytical models show that (i) a 1024-core ATAC chip using ACKwise achieves a speedup of 3.9Ã  compared to a similarly-sized pure electrical mesh manycore with a conventional limited directory protocol; (ii) the ATAC chip with ACKwise achieves a speedup of 1.35Ã  compared to the electrical mesh chip with ACKwise; and (iii) a pure electrical mesh chip with ACKwise achieves a speedup of 2.9Ã  over the same chip using a conventional limited directory protocol.",,4 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Guttag,"Zeng, Qing; Curtis, Dorothy",2010-02-25T23:45:11Z,2010-02-25T23:45:11Z,2010-02-25,http://hdl.handle.net/1721.1/51833,MIT-CSAIL-TR-2010-012,Performance and error analysis of three part of speech taggers on health texts,"Increasingly, natural language processing (NLP) techniques are being developed and utilized in a variety of biomedical domains. Part of speech tagging is a critical step in many NLP applications. Currently, we are developing a NLP tool for text simplification. As part of this effort, we set off to evaluate several part of speech (POS) taggers. We selected 120 sentences (2375 tokens) from a corpus of six types of diabetes-related health texts and asked human reviewers to tag each word in these sentences to create a ""Gold Standard."" We then tested each of the three POS taggers against the ""Gold Standard."" One tagger (dTagger) had been trained on health texts and the other two (MaxEnt and Curran & Clark) were trained on general news articles. We analyzed the errors and placed them into five categories: systematic, close, subtle, difficult source, and other. The three taggers have relatively similar rates of success: dTagger, MaxEnt, and Curran & Clark had 87%, 89% and 90% agreement with the gold standard, respectively. These rates of success are lower than published rates for these taggers. This is probably due to our testing them on a corpus that differs significantly from their training corpora. The taggers made different errors: the dTagger, which had been trained on a set of medical texts (MedPost), made fewer errors on medical terms than MaxEnt and Curran & Clark. The latter two taggers performed better on non-medical terms and we found the difference between their performance and that of dTagger was statistically significant. Our findings suggest that the three POS taggers have similar correct tagging rates, though they differ in the types of errors they make. For the task of text simplification, we are inclined to perform additional training of the Curran & Clark tagger with the Medpost corpus because both the fine grained tagging provided by this tool and the correct recognition of medical terms are equally important.",,20 p.,,,,Networks & Mobile Systems,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Poggio, Tomaso; Knoblich, Ulf; Mutch, Jim",2010-02-26T20:30:02Z,2010-02-26T20:30:02Z,2010-02-26,http://hdl.handle.net/1721.1/51839,CBCL-286; MIT-CSAIL-TR-2010-013,CNS: a GPU-based framework for simulating cortically-organized networks,"Computational models whose organization is inspired by the cortex are increasing in both number and popularity. Current instances of such models include convolutional networks, HMAX, Hierarchical Temporal Memory, and deep belief networks. These models present two practical challenges. First, they are computationally intensive. Second, while the operations performed by individual cells, or units, are typically simple, the code needed to keep track of network connectivity can quickly become complicated, leading to programs that are difficult to write and to modify. Massively parallel commodity computing hardware has recently become available in the form of general-purpose GPUs. This helps address the first problem but exacerbates the second. GPU programming adds an extra layer of difficulty, further discouraging exploration. To address these concerns, we have created a programming framework called CNS ('Cortical Network Simulator'). CNS models are automatically compiled and run on a GPU, typically 80-100x faster than on a single CPU, without the user having to learn any GPU programming. A novel scheme for the parametric specification of network connectivity allows the user to focus on writing just the code executed by a single cell. We hope that the ability to rapidly define and run cortically-inspired models will facilitate research in the cortical modeling community. CNS is available under the GNU General Public License.",,11 p.,,,,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sam Madden,"Wu, Eugene; Madden, Samuel; Zhang, Yang; Jones, Evan; Curino, Carlo",2010-03-15T22:15:07Z,2010-03-15T22:15:07Z,2010-03-14,http://hdl.handle.net/1721.1/52606,MIT-CSAIL-TR-2010-014,Relational Cloud: The Case for a Database Service,"In this paper, we make the case for â  databases as a serviceâ   (DaaS), with two target scenarios in mind: (i) consolidation of data management functionality for large organizations and (ii) outsourcing data management to a cloud-based service provider for small/medium organizations. We analyze the many challenges to be faced, and discuss the design of a database service we are building, called Relational Cloud. The system has been designed from scratch and combines many recent advances and novel solutions. The prototype we present exploits multiple dedicated storage engines, provides high-availability via transparent replication, supports automatic workload partitioning and live data migration, and provides serializable distributed transactions. While the system is still under active development, we are able to present promising initial results that showcase the key features of our system. The tests are based on TPC benchmarks and real-world data from epinions.com, and show our partitioning, scalability and balancing capabilities.",,6 p.,,,cloud computing; database partitioning; distributed databases; database as a service,Database,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Ragan-Kelley, Jonathan; Doggett, Michael; Lehtinen, Jaakko; Chen, Jiawen; Durand, Fredo",2010-03-29T18:45:17Z,2010-03-29T18:45:17Z,2010-03-29,http://hdl.handle.net/1721.1/53330,MIT-CSAIL-TR-2010-015,Decoupled Sampling for Real-Time Graphics Pipelines,"We propose decoupled sampling, an approach that decouples shading from visibility sampling in order to enable motion blur and depth-of-field at reduced cost. More generally, it enables extensions of modern real-time graphics pipelines that provide controllable shading rates to trade off quality for performance. It can be thought of as a generalization of GPU-style multisample antialiasing (MSAA) to support unpredictable shading rates, with arbitrary mappings from visibility to shading samples as introduced by motion blur, depth-of-field, and adaptive shading. It is inspired by the Reyes architecture in offline rendering, but targets real-time pipelines by driving shading from visibility samples as in GPUs, and removes the need for micropolygon dicing or rasterization. Decoupled Sampling works by defining a many-to-one hash from visibility to shading samples, and using a buffer to memoize shading samples and exploit reuse across visibility samples. We present extensions of two modern GPU pipelines to support decoupled sampling: a GPU-style sort-last fragment architecture, and a Larrabee-style sort-middle pipeline. We study the architectural implications and derive end-to-end performance estimates on real applications through an instrumented functional simulator. We demonstrate high-quality motion blur and depth-of-field, as well as variable and adaptive shading rates.",,16 p.,,,Computer Graphics; Graphics Systems; Graphics Hardware,Computer Graphics,,Creative Commons Attribution-Share Alike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,"RAGAN-KELLEY, J., LEHTINEN, J., CHEN, J., DOGGETT, M., and DURAND, F. 2010. Decoupled Sampling for Real-Time Graphics Pipelines. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series, MIT-CSAIL-TR-2010-015.",,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Agarwala, Aseem; Bae, Soonmin; Durand, Fredo",2010-04-14T17:30:08Z,2010-04-14T17:30:08Z,2010-04-07,http://hdl.handle.net/1721.1/53705,MIT-CSAIL-TR-2010-016,Computational Re-Photography,"Rephotographers aim to recapture an existing photograph from the same viewpoint. A historical photograph paired with a well-aligned modern rephotograph can serve as a remarkable visualization of the passage of time. However, the task of rephotography is tedious and often imprecise, because reproducing the viewpoint of the original photograph is challenging. The rephotographer must disambiguate between the six degrees of freedom of 3D translation and rotation, and the confounding similarity between the effects of camera zoom and dolly. We present a real-time estimation and visualization technique for rephotography that helps users reach a desired viewpoint during capture. The input to our technique is a reference image taken from the desired viewpoint. The user moves through the scene with a camera and follows our visualization to reach the desired viewpoint. We employ computer vision techniques to compute the relative viewpoint difference. We guide 3D movement using two 2D arrows. We demonstrate the success of our technique by rephotographing historical images and conducting user studies.",,15 p.,,,Rephotography; Computational photography; Pose estimation,,,Creative Commons Attribution-Noncommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,,,,,,,,,,,,,Computer Graphics,,,,,,,,,,,,
Brian Williams,"Li, Hui X.",2010-04-16T16:15:05Z,2010-04-16T16:15:05Z,2010-04-09,http://hdl.handle.net/1721.1/53720,MIT-CSAIL-TR-2010-018,Kongming: A Generative Planner for Hybrid Systems with Temporally Extended Goals,"Most unmanned missions in space and undersea are commanded by a ""script"" that specifies a sequence of discrete commands and continuous actions. Currently such scripts are mostly hand-generated by human operators. This introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. For discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals to be accomplished, and the goal-directed systems automatically output the correct, executable command sequences. Increasingly, the control of autonomous systems involves performing actions with a mix of discrete and continuous effects. For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and take sample, and continuous actions, like descend and ascend, which are influenced by the dynamical model of the vehicle. A hybrid planner generates a sequence of discrete and continuous actions that achieve the mission goals. In this thesis, I present a novel approach to solve the generative planning problem for temporally extended goals for hybrid systems, involving both continuous and discrete actions. The planner, Kongming, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for continuous actions. Second, it engages novel reformulation schemes to handle temporally flexible actions and temporally extended goals. I have successfully demonstrated controlling an AUV in the Atlantic ocean using mission scripts solely generated by Kongming. I have also empirically evaluated Kongming on various real-world scenarios in the underwater domain and the air vehicle domain, and found it successfully and efficiently generates valid and optimal plans.",,237 p.,,,combinatorial optimization; AI planning; autonomous systems,Model-based Embedded and Robotic Systems,Funded by the Boeing Company under contract MIT-BA-GTA-1,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"Devadas, Srinivas; Lis, Mieszko; Khan, Omer",2010-04-23T17:15:07Z,2010-04-23T17:15:07Z,2010-04-17,http://hdl.handle.net/1721.1/53748,MIT-CSAIL-TR-2010-019,Instruction-Level Execution Migration,"We introduce the Execution Migration Machine (EM²), a novel data-centric multicore memory system architecture based on computation migration. Unlike traditional distributed memory multicores, which rely on complex cache coherence protocols to move the data to the core where the computation is taking place, our scheme always moves the computation to the core where the data resides. By doing away with the cache coherence protocol, we are able to boost the effectiveness of per-core caches while drastically reducing hardware complexity. To evaluate the potential of EM² architectures, we developed a series of PIN/Graphite-based models of an EM² multicore with 64 x86 cores and, under some simplifying assumptions (a timing model restricted to data memory performance, no instruction cache modeling, high-bandwidth fixed-latency interconnect allowing concurrent migrations), compared them against corresponding directory-based cache-coherent architecture models. We justify our assumptions and show that our conclusions are valid even if our assumptions are removed. Experimental results on a range of SPLASH-2 and PARSEC benchmarks indicate that EM2 can significantly improve per-core cache performance, decreasing overall miss rates by as much as 84% and reducing average memory latency by up to 58%.",,13 p.,,,,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Frans Kaashoek,"Kaashoek, Frans; Morris, Robert; Mao, Yandong",2010-05-03T23:15:05Z,2010-05-03T23:15:05Z,2010-05-02,http://hdl.handle.net/1721.1/54692,MIT-CSAIL-TR-2010-020,Optimizing MapReduce for Multicore Architectures,"MapReduce is a programming model for data-parallel programs originally intended for data centers. MapReduce simplifies parallel programming, hiding synchronization and task management. These properties make it a promising programming model for future processors with many cores, and existing MapReduce libraries such as Phoenix have demonstrated that applications written with MapReduce perform competitively with those written with Pthreads. This paper explores the design of the MapReduce data structures for grouping intermediate key/value pairs, which is often a performance bottleneck on multicore processors. The paper finds the best choice depends on workload characteristics, such as the number of keys used by the application, the degree of repetition of keys, etc. This paper also introduces a new MapReduce library, Metis, with a compromise data structure designed to perform well for most workloads. Experiments with the Phoenix benchmarks on a 16-core AMD-based servershow that Metisâ   data structure performs better than simpler alternatives, including Phoenix.",,13 p.,,,,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Igarashi, Takeo; Durand, Fredo; Rivers, Alec",2010-05-06T17:45:04Z,2010-05-06T17:45:04Z,2010-05-05,http://hdl.handle.net/1721.1/54731,MIT-CSAIL-TR-2010-023,A User Study Comparing 3D Modeling with Silhouettes and Google SketchUp,"We describe a user study comparing 3D Modeling with Silhouettes and Google SketchUp. In the user study, ten users were asked to create 3D models of three different objects, using either 3D Modeling with Silhouettes or Google SketchUp. Ten different users were then asked to rank images of the models produced by the first group. We show that the models made with 3D Modeling with Silhouettes were ranked significantly higher on average than those made with Google SketchUp.",,4 p.,,,sketch-based modeling,Computer Graphics,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Jayaraman, Karthick; Rinard, Martin C.; Tripunitara, Mahesh; Ganesh, Vijay; Chapin, Steve",2010-05-06T17:30:07Z,2010-05-06T17:30:07Z,2010-05-05,http://hdl.handle.net/1721.1/54730,MIT-CSAIL-TR-2010-022,Automatic Error Finding in Access-Control Policies,"Access-control policies are a key infrastructural technology for computer security. However, a significant problem is that system administrators need to be able to automatically verify whether their policies capture the intended security goals. To address this important problem, researchers have proposed many automated verification techniques. Despite considerable progress in verification techniques, scalability is still a significant issue. Hence, in this paper we propose that error finding complements verification, and is a fruitful way of checking whether or not access control policies implement the security intent of system administrators. Error finding is more scalable (at the cost of completeness), and allows for the use of a wider variety of techniques. In this paper, we describe an abstraction-refinement based technique and its implementation, the Mohawk tool, aimed at finding errors in ARBAC access-control policies. The key insight behind our abstraction-refinement technique is that it is more efficient to look for errors in an abstract policy (with successive refinements, if necessary) than its complete counterpart. Mohawk accepts as input an access-control policy and a safety question. If Mohawk finds an error in the input policy, it terminates with a sequence of actions that cause the error. We provide an extensive comparison of Mohawk with the current state-of-the-art analysis tools. We show that Mohawk scales very well as the size and complexity of the input policies increase, and is orders of magnitude faster than competing tools. The Mohawk tool is open source and available from the Google Code website: http://code.google.com/p/mohawk/",,12 p.,,,access-control policies; error finding; bounded model-checking; abstraction refinement,Computer Architecture,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,http://code.google.com/p/mohawk/,,,,,,,,,,,,,,,,,,
,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2010-05-12T23:00:07Z,2010-05-12T23:00:07Z,2010-05-07,http://hdl.handle.net/1721.1/54780,MIT-CSAIL-TR-2010-026,Hierarchical Task and Motion Planning in the Now,"In this paper we outline an approach to the integration of task planning and motion planning that has the following key properties: It is aggressively hierarchical. It makes choices and commits to them in a top-down fashion in an attempt to limit the length of plans that need to be constructed, and thereby exponentially decrease the amount of search required. Importantly, our approach also limits the need to project the effect of actions into the far future. It operates on detailed, continuous geometric representations and partial symbolic descriptions. It does not require a complete symbolic representation of the input geometry or of the geometric effect of the task-level operations.",,9 p.,,,,Learning and Intelligent Systems,This work was supported in part by the National Science Foundation under Grant No. 0712012.,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,"Workshop on Mobile Manipulation, IEEE International Conference on Robotics and Automation",,,,,,,,,,,,,,,,,,,,,,,,
William Freeman,"Freeman, William T.; Torralba, Antonio; Yuen, Jenny; Liu, Ce",2010-05-13T19:30:03Z,2010-05-13T19:30:03Z,2010-05-08,http://hdl.handle.net/1721.1/54787,MIT-CSAIL-TR-2010-024,SIFT Flow: Dense Correspondence across Scenes and its Applications,"While image alignment has been studied in different areas of computer vision for decades, aligning images depicting different scenes remains a challenging problem. Analogous to optical flow where an image is aligned to its temporally adjacent frame, we propose SIFT flow, a method to align an image to its nearest neighbors in a large image corpus containing a variety of scenes. The SIFT flow algorithm consists of matching densely sampled, pixel-wise SIFT features between two images, while preserving spatial discontinuities. The SIFT features allow robust matching across different scene/object appearances, whereas the discontinuity-preserving spatial model allows matching of objects located at different parts of the scene. Experiments show that the proposed approach robustly aligns complex scene pairs containing significant spatial differences. Based on SIFT flow, we propose an alignment-based large database framework for image analysis and synthesis, where image information is transferred from the nearest neighbors to a query image according to the dense scene correspondence. This framework is demonstrated through concrete applications, such as motion field prediction from a single image, motion synthesis via object transfer, satellite image registration and face recognition.",,17 p.,,,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Patrick Winston,"Finlayson, Mark Alan; Hervas, Raquel",2010-08-19T18:15:22Z,2010-08-19T18:15:22Z,2010-05-12,http://hdl.handle.net/1721.1/57507,,"UCM/MIT Indications, Referring Expressions, and Coreference Corpus (UMIREC corpus) v1.1","The corpus comprises 62 files in ""Story Workbench"" annotation format: 30 folktales in English from a variety of sources, and 32 Wall Street Journal articles selected to coincide with articles found in the Penn Treebank. The files are annotated with the location of referring expressions, coreference relations between the referring expressions, and so-called ""indication structures"", which split referring expressions into constituents (nuclei and modifiers) and mark each constituent as either 'distinctive' or 'descriptive', indicating whether or not the constituent contains information required for uniquely identifying the referent. The files distributed in this corpus archive are the gold-standard files, which were constructed by merging annotations done by two trained annotators. The contents of this corpus, the annotation procedure, and the indication structures are described in more detail in a paper titled ""The Prevalence of Descriptive Referring Expressions in News and Narrative"" published in the proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, held in July 2010 in Uppsala, Sweden (ACL-2010). A near-final version of the paper is included in the doc/ directory of the compressed corpus archive file.
This is version 1.1 of the UMIREC corpus, in which the coreference annotations have been fixed relative to version 1.0. UMIREC v1.0 suffered from a bug in the export script that corrupted the coreference data.",,877 ko,,,,Genesis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,"Finlayson, M.A. & Hervás, R. (2010) UCM/MIT Indications, Referring Expressions, and Co-Reference Corpus v1.1 (UMIREC corpus). MIT CSAIL Work Product.",Patrick Winston; Genesis,,,,,http://hdl.handle.net/1721.1/54766,,,,http://hdl.handle.net/1721.1/54765,,,,,,,,,,,,,
Patrick Winston,"Hervas, Raquel; Finlayson, Mark Alan",2010-05-12T15:30:09Z,2010-05-12T15:30:09Z,2010-05-12,http://hdl.handle.net/1721.1/54765,MIT-CSAIL-TR-2010-025,"Annotation Guide for the UCM/MIT Indications, Referential Expressions, and Coreference Corpus (UMIREC Corpus)","This is the annotation guide given to the annotators who created the UCM/MIT Indications, Referring Expressions, and Coreference (UMIREC) Corpus version 1.0. The corpus comprises texts annotated for referring expressions, coreference relations between the referring expressions, and so-called ""indication structures"", which split referring expressions into constituents (nuclei and modifiers) and mark each constituent as either 'distinctive' or 'descriptive', which indicate whether or not the constituent contains information required for uniquely identifying the referent. The contents of this corpus, the annotation procedure, and the indication structures are described in more detail in a paper titled ""The Prevalence of Descriptive Referring Expressions in News and Narrative"" published in the proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, held in July 2010 in Uppsala, Sweden (ACL-2010).",,15 p.,,,,Genesis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,"Finlayson, M.A. & Hervás, R. (2010) Annotation Guide for the UCM/MIT Indications, Referring Expressions, and Co-Reference Corpus (UMIREC corpus). MIT CSAIL Technical Report No. 2010-025.",,,,,,,,,,,,,,,,,,,http://hdl.handle.net/1721.1/57507,,,,
Patrick Winston,"Hervas, Raquel; Finlayson, Mark Alan",2010-05-12T15:30:19Z,2010-05-12T15:30:19Z,2010-05-12,http://hdl.handle.net/1721.1/54766,,"UCM/MIT Indications, Referring Expressions, and Coreference Corpus (UMIREC corpus)","This version of the UMIREC corpus has been superseded by version 1.1, found at http://hdl.handle.net/1721.1/57507.  Please do not use version 1.0, as it contains corrupted coreference information.  The correct, uncorrupted data is found in version 1.1.",,,,,,Genesis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,"Finlayson, M.A. & Hervás, R. (2010) UCM/MIT Indications, Referring Expressions, and Co-Reference Corpus v1.0 (UMIREC corpus). MIT CSAIL Work Product.",,,,http://hdl.handle.net/1721.1/57507,,,,,,http://hdl.handle.net/1721.1/54765,,,,,,,,,,,,,
Martin Rinard,"Misailovic, Sasa; Agarwal, Anant; Carbin, Michael; Sidiroglou, Stelios; Hoffmann, Henry; Rinard, Martin",2010-05-14T22:00:04Z,2010-05-14T22:00:04Z,2010-05-14,http://hdl.handle.net/1721.1/54799,MIT-CSAIL-TR-2010-027,Power-Aware Computing with Dynamic Knobs,"We present PowerDial, a system for dynamically adapting application behavior to execute successfully in the face of load and power fluctuations. PowerDial transforms static configuration parameters into dynamic knobs that the PowerDial control system can manipulate to dynamically trade off the accuracy of the computation in return for reductions in the computational resources that the application requires to produce its results. These reductions translate into power savings. Our experimental results show that PowerDial can enable our benchmark applications to execute responsively in the face of power caps (imposed, for example, in response to cooling system failures) that would otherwise significantly impair the delivered performance. They also show that PowerDial can reduce the number of machines required to meet peak load, in our experiments enabling up to a 75% reduction in direct power and capital costs.",,16 p.,,,Autonomic Computing; Dynamic Analysis; Runtime control; Power Aware computing,Program Analysis,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Katabi, Dina; Gollakota, Shyamnath",2010-06-07T19:15:07Z,2010-06-07T19:15:07Z,2010-06-07,http://hdl.handle.net/1721.1/55650,MIT-CSAIL-TR-2010-028,iJam: Jamming Oneself for Secure Wireless Communication,"Wireless is inherently less secure than wired networks because of its broadcast nature. Attacks that simply snoop on the wireless medium successfully defeat the security of even 802.11 networks using the most recent security standards (WPA2-PSK). In this paper we ask the following question: Can we prevent this kind of eavesdropping from happening? If so, we can potentially defeat the entire class of attacks that rely on snooping. This paper presents iJam, a PHY-layer protocol for OFDM-based wireless systems. iJam ensures that an eavesdropper cannot successfully demodulate a wireless signal not intended for it. To achieve this iJam strategically introduces interference that prevents an eavesdropper from decoding the data, while allowing the intended receiver to decode it. iJam exploits the properties of 802.11â  s OFDM signals to ensure that an eavesdropper cannot even tell which parts of the signal are jammed. We implement iJam and evaluate it in a testbed of GNURadios with an 802.11-like physical layer. We show that iJam makes the data bits at the adversary look random, i.e., the BER becomes close to 50%, whereas the receiver can perfectly decode the data.",,13 p.,,,Physical Layer Security,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Oshman, Rotem; Richa, Andrea; Newport, Calvin; Lynch, Nancy; Kuhn, Fabian",2010-06-08T16:00:03Z,2010-06-08T16:00:03Z,2010-06-08,http://hdl.handle.net/1721.1/55721,MIT-CSAIL-TR-2010-029,Broadcasting in Unreliable Radio Networks,"Practitioners agree that unreliable links, which fluctuate between working and not working, are an important characteristic of wireless networks. In contrast, most theoretical models of radio networks fix a static set of links and assume that these links work reliably throughout an execution. This gap between theory and practice motivates us to investigate how unreliable links affect theoretical bounds on broadcast in radio networks. To that end we consider a model that includes two types of links: reliable links, which always deliver messages, and unreliable links, which sometimes deliver messages and sometimes do not. It is assumed that the graph induced by the reliable links is connected, and unreliable links are controlled by a worst-case adversary. In the new model we show an(n log n) lower bound on deterministic broadcast in undirected graphs, even when all processes are initially awake and have collision detection, and an (n) lower bound on randomized broadcast in undirected networks of constant diameter. This clearly separates the new model from the classical, reliable model. On the positive side, we give two algorithms that tolerate the inherent unreliability: an O(n3=2plog n)-time deterministic algorithm and a randomized algorithm which terminates in O(n log2 n) rounds with high probability.",,25 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"Khan, Omer; Lis, Mieszko; Devadas, Srini",2010-06-18T19:00:10Z,2010-06-18T19:00:10Z,2010-06-12,http://hdl.handle.net/1721.1/55944,MIT-CSAIL-TR-2010-030,EM2: A Scalable Shared-Memory Multicore Architecture,"We introduce the Execution Migration Machine (EM2), a novel, scalable shared-memory architecture for large-scale multicores constrained by off-chip memory bandwidth. EM2 reduces cache miss rates, and consequently off-chip memory usage, by permitting only one copy of data to be stored anywhere in the system: when a thread wishes to access an address not locally cached on the core it is executing on, it migrates to the appropriate core and continues execution. Using detailed simulations of a range of 256-core configurations on the SPLASH-2 benchmark suite, we show that EM2 improves application completion times by 18% on the average while remaining competitive with traditional architectures in silicon area.",,22 p.,,,parallel processing; parallel architecture; distributed memory,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Wang, Jue; Katabi, Dina",2010-07-07T21:15:12Z,2010-07-07T21:15:12Z,2010-07-05,http://hdl.handle.net/1721.1/56252,MIT-CSAIL-TR-2010-031,ChitChat: Making Video Chat Robust to Packet Loss,"Video chat is increasingly popular among Internet users. Often, however, chatting sessions suffer from packet loss, which causes video outage and poor quality. Existing solutions however are unsatisfying. Retransmissions increase the delay and hence can interact negatively with the strict timing requirements of interactive video. FEC codes introduce extra overhead and hence reduce the bandwidth available for video data even in the absence of packet loss. This paper presents ChitChat, a new approach for reliable video chat that neither delays frames nor introduces bandwidth overhead. The key idea is to ensure that the information in each packet describes the whole frame. As a result, even when some packets are lost, the receiver can still use the received packets to decode a smooth version of the original frame. This reduces frame loss and the resulting video freezes and improves the perceived video quality. We have implemented ChitChat and evaluated it over multiple Internet paths. In comparison to Windows Live Messenger 2009, our method reduces the occurrences of video outage events by more than an order of magnitude.",,12 p.,,,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Whitman Richards,"Richards, Whitman; Macindoe, Owen",2010-07-27T20:15:27Z,2010-07-27T20:15:27Z,2010-07-27,http://hdl.handle.net/1721.1/57462,MIT-CSAIL-TR-2010-033,Characteristics of Small Social Networks,"Two dozen networks are analyzed using three parameters that attempt to capture important properties of social networks: leadership L, member bonding B, and diversity of expertise D. The first two of these parameters have antecedents, the third is new. A key part of the analysis is to examine networks at multiple scales by dissecting the entire network into its n subgraphs of a given radius of two edge steps about each of the n nodes. This scale-based analysis reveals constraints on what we have dubbed ""cognitive"" networks, as contrasted with biological or physical networks. Specifically, ""cognitive"" networks appear to maximize bonding and diversity over a range of leadership dominance. Asymptotic relations between the bonding and diversity measures are also found when small, nearly complete subgraphs are aggregated to form larger networks. This aggregation probably underlies changes in a regularity among the LBD parameters; this regularity is a U-shaped function of networks size, n, which is minimal for networks around 80 or so nodes.",,38 p.,,,constraints on social networks; network evolution; small group aggregation; multi-scale analysis,,,,,,,,,,,,,,,,,,,,,Belief Dynamics,,,,,,,,
Saman Amarasinghe,"Ansel, Jason; Wong, Yee Lok; Chan, Cy; Olszewski, Marek; Edelman, Alan; Amarasinghe, Saman",2010-07-27T20:15:11Z,2010-07-27T20:15:11Z,2010-07-27,http://hdl.handle.net/1721.1/57461,MIT-CSAIL-TR-2010-032,Language and Compiler Support for Auto-Tuning Variable-Accuracy Algorithms,"Approximating ideal program outputs is a common technique for solving computationally difficult problems, for adhering to processing or timing constraints, and for performance optimization in situations where perfect precision is not necessary. To this end, programmers often use approximation algorithms, iterative methods, data resampling, and other heuristics. However, programming such variable accuracy algorithms presents difficult challenges since the optimal algorithms and parameters may change with different accuracy requirements and usage environments. This problem is further compounded when multiple variable accuracy algorithms are nested together due to the complex way that accuracy requirements can propagate across algorithms and because of the resulting size of the set of allowable compositions. As a result, programmers often deal with this issue in an ad-hoc manner that can sometimes violate sound programming practices such as maintaining library abstractions. In this paper, we propose language extensions that expose trade-offs between time and accuracy to the compiler. The compiler performs fully automatic compile-time and install-time autotuning and analyses in order to construct optimized algorithms to achieve any given target accuracy. We present novel compiler techniques and a structured genetic tuning algorithm to search the space of candidate algorithms and accuracies in the presence of recursion and sub-calls to other variable accuracy code. These techniques benefit both the library writer, by providing an easy way to describe and search the parameter and algorithmic choice space, and the library user, by allowing high level specification of accuracy requirements which are then met automatically without the need for the user to understand any algorithm-specific parameters. Additionally, we present a new suite of benchmarks, written in our language, to examine the efficacy of our techniques. Our experimental results show that by relaxing accuracy requirements, we can easily obtain performance improvements ranging from 1.1x to orders of magnitude of speedup.",,18 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Meyers, Ethan; Embark, Hamdy; Freiwald, Winrich; Serre, Thomas; Kreiman, Gabriel; Poggio, Tomaso",2010-07-29T18:45:19Z,2010-07-29T18:45:19Z,2010-07-29,http://hdl.handle.net/1721.1/57463,MIT-CSAIL-TR-2010-034; CBCL-289,Examining high level neural representations of cluttered scenes,"Humans and other primates can rapidly categorize objects even when they are embedded in complex visual scenes (Thorpe et al., 1996; Fabre-Thorpe et al., 1998). Studies by Serre et al., 2007 have shown that the ability of humans to detect animals in brief presentations of natural images decreases as the size of the target animal decreases and the amount of clutter increases, and additionally, that a feedforward computational model of the ventral visual system, originally developed to account for physiological properties of neurons, shows a similar pattern of performance. Motivated by these studies, we recorded single- and multi-unit neural spiking activity from macaque superior temporal sulcus (STS) and anterior inferior temporal cortex (AIT), as a monkey passively viewed images of natural scenes. The stimuli consisted of 600 images of animals in natural scenes, and 600 images of natural scenes without animals in them, captured at four different viewing distances, and were the same images used by Serre et al. to allow for a direct comparison between human psychophysics, computational models, and neural data. To analyze the data, we applied population ""readout"" techniques (Hung et al., 2005; Meyers et al., 2008) to decode from the neural activity whether an image contained an animal or not. The decoding results showed a similar pattern of degraded decoding performance with increasing clutter as was seen in the human psychophysics and computational model results. However, overall the decoding accuracies from the neural data lower were than that seen in the computational model, and the latencies of information in IT were long (~125ms) relative to behavioral measures obtained from primates in other studies. Additional tests also showed that the responses of the model units were not capturing several properties of the neural responses, and that detecting animals in cluttered scenes using simple model units based on V1 cells worked almost as well as using more complex model units that were designed to model the responses of IT neurons. While these results suggest AIT might not be the primary brain region involved in this form of rapid categorization, additional studies are needed before drawing strong conclusions.",,50 p.,,,decoding; readout; rapid categorization; inferior temporal cortex; object recognition; scene understanding; neuroscience; visual clutter; electrophysiology,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Wibisono, Andre; Bouvrie, Jake; Rosasco, Lorenzo; Poggio, Tomaso",2010-07-30T17:45:16Z,2010-07-30T17:45:16Z,2010-07-30,http://hdl.handle.net/1721.1/57464,CBCL-290; MIT-CSAIL-TR-2010-035,Learning and Invariance in a Family of Hierarchical Kernels,"Understanding invariance and discrimination properties of hierarchical models is arguably the key to understanding how and why such models, of which the the mammalian visual system is one instance, can lead to good generalization properties and reduce the sample complexity of a given learning task. In this paper we explore invariance to transformation and the role of layer-wise embeddings within an abstract framework of hierarchical kernels motivated by the visual cortex. Here a novel form of invariance is induced by propagating the effect of locally defined, invariant kernels throughout a hierarchy. We study this notion of invariance empirically. We then present an extension of the abstract hierarchical modeling framework to incorporate layer-wise embeddings, which we demonstrate can lead to improved generalization and scalable algorithms. Finally we analyze experimentally sample complexity properties as a function of architectural parameters.",,9 p.,,,artificial intelligence,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Khabbazian, Majid; Kuhn, Fabian; Lynch, Nancy; Medard, Muriel; ParandehGheibi, Ali",2010-08-04T15:15:23Z,2010-08-04T15:15:23Z,2010-08-02,http://hdl.handle.net/1721.1/57473,MIT-CSAIL-TR-2010-036,MAC Design for Analog Network Coding,"Most medium access control mechanisms discard collided packets and consider interference harmful. Recent work on Analog Network Coding (ANC) suggests a different approach, in which multiple interfering transmissions are strategically scheduled. The received collisions are collected and then used in a decoding process, such as the ZigZag decoding process, where the packets involved in the collisions are extracted. In this paper, we present an algebraic representation of collisions and describe a general approach to recovering collisions using ANC. To study the eect of using ANC on the performance of MAC layers, we develop an ANC-based algorithm that implements an abstract MAC layer service, as defined in [1, 2], and analyze its performance. This study proves that ANC can significantly improve the performance of MAC layer services, in terms of probabilistic time guarantees for packet delivery. We illustrate how this improvement at the MAC layer can translate into faster higher-level algorithms, by analyzing the time complexity of a multiple-message network-wide broadcast algorithm that uses our ANC-based MAC service.",,19 p.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joshua Tenenbaum,"Salakhutdinov, Ruslan; Hinton, Geoffrey",2010-08-04T15:15:39Z,2010-08-04T15:15:39Z,2010-08-04,http://hdl.handle.net/1721.1/57474,MIT-CSAIL-TR-2010-037,An Efficient Learning Procedure for Deep Boltzmann Machines,"We present a new learning algorithm for Boltzmann Machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann Machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer ""pre-training"" phase that initializes the weights sensibly. The pre-training also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB datasets showing that Deep Boltzmann Machines learn very good generative models of hand-written digits and 3-D objects. We also show that the features discovered by Deep Boltzmann Machines are a very effective way to initialize the hidden layers of feed-forward neural nets which are then discriminatively fine-tuned.",,32 p.,,,Deep learning; Graphical models; Boltzmann Machines,Computational Cognitive Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Misailovic, Sasa; Kim, Deokhwan; Rinard, Martin",2010-08-05T16:00:10Z,2010-08-05T16:00:10Z,2010-08-05,http://hdl.handle.net/1721.1/57475,MIT-CSAIL-TR-2010-038,Parallelizing Sequential Programs With Statistical Accuracy Tests,"We present QuickStep, a novel system for parallelizing sequential programs. QuickStep deploys a set of parallelization transformations that together induce a search space of candidate parallel programs. Given a sequential program, representative inputs, and an accuracy requirement, QuickStep uses performance measurements, profiling information, and statistical accuracy tests on the outputs of candidate parallel programs to guide its search for a parallelizationthat maximizes performance while preserving acceptable accuracy. When the search completes, QuickStep produces an interactive report that summarizes the applied parallelization transformations, performance, and accuracy results for the automatically generated candidate parallel programs. In our envisioned usage scenarios, the developer examines this report to evaluate the acceptability of the final parallelization and to obtain insight into how the original sequential program responds to different parallelization strategies. Itis also possible for the developer (or even a user of the program who has no software development expertise whatsoever) to simply use the best parallelization out of the box without examining the report or further investigating the parallelization. Results from our benchmark set of applications show that QuickStep can automatically generate accurate and efficient parallel programs---the automatically generated parallel versions of five of our six benchmark applications run between 5.0 and 7.7 times faster on 8 cores than the original sequential versions. Moreover, a comparison with the Intel icc compiler highlights how QuickStep can effectively parallelize applications with features (such as the use of modern object-oriented programming constructs or desirable parallelizations with infrequent but acceptable data races) that place them inherently beyond the reach of standard approaches.",,22 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Benjamin, Michael R.",2010-08-23T21:15:27Z,2010-08-23T21:15:27Z,2010-08-23,http://hdl.handle.net/1721.1/57509,MIT-CSAIL-TR-2010-039,MOOS-IvP Autonomy Tools Users Manual,This document describes fifteen MOOS-IvP autonomy tools. uHelmScope provides a run-time scoping window into the state of an active IvP Helm executing its mission. pMarineViewer is a geo-based GUI tool for rendering marine vehicles and geometric data in their operational area. uXMS is a terminal based tool for scoping on a MOOSDB process. uTermCommand is a terminal based tool for poking a MOOSDB with a set of MOOS file pre-defined variable-value pairs selectable with aliases from the command-line. pEchoVar provides a way of echoing a post to one MOOS variable with a new post having the same value to a different variable. uProcessWatch monitors the presence or absence of a set of MOOS processes and summarizes the collective status in a single MOOS variable. uPokeDB provides a way of poking the MOOSDB from the command line with one or more variable-value pairs without any pre-existing configuration of a MOOS file. uTimerScript will execute a pre-defined timed pausable script of poking variable-value pairs to a MOOSDB. pNodeReporter summarizes a platforms critical information into a single node report string for sharing beyond the vehicle. pBasicContactMgr provides a basic contact management service with the ability to generate range-dependent configurable alerts. The Alog Toolbox is a set of offline tools for analyzing and manipulating log files in the .alog format.,,113 p.,,,MOOS; Contact Manager,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Kuhn, Fabian; Lynch, Nancy; Newport, Calvin",2010-08-26T20:45:18Z,2010-08-26T20:45:18Z,2010-08-26,http://hdl.handle.net/1721.1/57577,MIT-CSAIL-TR-2010-040,The Abstract MAC Layer,"A diversity of possible communication assumptions complicates the study of algorithms and lower bounds for radio networks. We address this problem by defining an abstract MAC layer. This service provides reliable local broadcast communication, with timing guarantees stated in terms of a collection of abstract delay functions applied to the relevant contention. Algorithm designers can analyze their algorithms in terms of these functions, independently of specific channel behavior. Concrete implementations of the abstract MAC Layer over basic radio network models generate concrete definitions for these delay functions, automatically adapting bounds proven for the abstract service to bounds for the specific radio network under consideration. To illustrate this approach, we use the abstract MAC Layer to study the new problem of Multi-Message Broadcast, a generalization of standard single-message broadcast in which multiple messages can originate at different times and locations in the network. We present and analyze two algorithms for Multi-Message Broadcast in static networks: a simple greedy algorithm and one that uses regional leaders. We then indicate how these results can be extended to mobile networks.",,30 p.,,,Wireless networks; Abstraction; Medium access control; Broadcast,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Benjamin, Michael R.; Newman, Paul; Schmidt, Henrik; Leonard, John J.",2010-08-27T16:00:16Z,2010-08-27T16:00:16Z,2010-08-27,http://hdl.handle.net/1721.1/57583,MIT-CSAIL-TR-2010-041,An Overview of MOOS-IvP and a Users Guide to the IvP Helm Autonomy Software,"This document describes the IvP Helm -- an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming -- a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",,255 p.,,,pMarineViewer; uTimerScript; Adaptive Autonomy; USV; UUV; UxV; Unmanned Marine Vehicles; Marine Autonomy; AUV; NURC,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Azar, Pablo; Chen, Jing; Micali, Silvio",2010-09-11T00:00:46Z,2010-09-11T00:00:46Z,2010-09-08,http://hdl.handle.net/1721.1/58486,MIT-CSAIL-TR-2010-042,Conservative-Bayesian Mechanisms,"We put forward a new class of mechanisms. In this extended abstract, we exemplify our approach only for single-good auctions in what we call a conservative-Bayesian setting. (Essentially, no common-knowledge about the underlying distribution of the players' valuations is required.) We prove that our mechanism is optimal in this challenging and realistic setting.",,5 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Cornejo, Alejandro; Lynch, Nancy",2010-09-11T00:00:21Z,2010-09-11T00:00:21Z,2010-09-09,http://hdl.handle.net/1721.1/58484,MIT-CSAIL-TR-2010-043,Reliably Detecting Connectivity using Local Graph Traits,"Local distributed algorithms can only gather sufficient information to identify local graph traits, that is, properties that hold within the local neighborhood of each node. However, it is frequently the case that global graph properties (connectivity, diameter, girth, etc) have a large influence on the execution of a distributed algorithm. This paper studies local graph traits and their relationship with global graph properties. Specifically, we focus on graph k-connectivity. First we prove a negative result that shows there does not exist a local graph trait which perfectly captures graph k-connectivity. We then present three different local graph traits which can be used to reliably predict the k-connectivity of a graph with varying degrees of accuracy. As a simple application of these results, we present upper and lower bounds for a local distributed algorithm which determines if a graph is k-connected. As a more elaborate application of local graph traits, we describe, and prove the correctness of, a local distributed algorithm that preserves k-connectivity in mobile ad hoc networks while allowing nodes to move independently whenever possible.",,16 p,,,Connectivity; Graph Traits; Distributed,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jovan Popovic,"Wang, Robert; Paris, Sylvain; Popovic, Jovan",2010-09-11T00:00:37Z,2010-09-11T00:00:37Z,2010-09-10,http://hdl.handle.net/1721.1/58485,MIT-CSAIL-TR-2010-044,Practical Color-Based Motion Capture,"Motion capture systems have been widely used for high quality content creation and virtual reality but are rarely used in consumer applications due to their price and setup cost. In this paper, we propose a motion capture system built from commodity components that can be deployed in a matter of minutes. Our approach uses one or more webcams and a color shirt to track the upper-body at interactive rates. We describe a robust color calibration system that enables our color-based tracking to work against cluttered backgrounds and under multiple illuminants. We demonstrate our system in several real-world indoor and outdoor settings.",,6 p.,,,augmented reality,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nick Roy,"Banerjee, Ashis Gopal; Roy, Nicholas",2010-09-20T22:45:36Z,2010-09-20T22:45:36Z,2010-09-18,http://hdl.handle.net/1721.1/58609,MIT-CSAIL-TR-2010-045,Learning Solutions of Similar Linear Programming Problems using Boosting Trees,"In many optimization problems, similar linear programming (LP) problems occur in the nodes of the branch and bound trees that are used to solve integer (mixed or pure, deterministic or stochastic) programming problems. Similar LP problems are also found in problem domains where the objective function and constraint coefficients vary due to uncertainties in the operating conditions. In this report, we present a regression technique for learning a set of functions that map the objective function and the constraints to the decision variables of such an LP system by modifying boosting trees, an algorithm we term the Boost-LP algorithm. Matrix transformations and geometric properties of boosting trees are utilized to provide theoretical performance guarantees on the predicted values. The standard form of the loss function is altered to reduce the possibility of generating infeasible LP solutions. Experimental results on three different problems, one each on scheduling, routing, and planning respectively, demonstrate the effectiveness of the Boost-LP algorithm in providing significant computational benefits over regular optimization solvers without generating solutions that deviate appreciably from the optimum values.",,18 p.,,,,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joshua Tenenbaum,"Battaglia, Peter W.",2010-09-22T20:45:11Z,2010-09-22T20:45:11Z,2010-09-21,http://hdl.handle.net/1721.1/58669,MIT-CSAIL-TR-2010-046,Bayesian perceptual inference in linear Gaussian models,"The aim of this paper is to provide perceptual scientists with a quantitative framework for modeling a variety of common perceptual behaviors, and to unify various perceptual inference tasks by exposing their common computational underpinnings. This paper derives a model Bayesian observer for perceptual contexts with linear Gaussian generative processes. I demonstrate the relationship between four fundamental perceptual situations by expressing their corresponding posterior distributions as consequences of the model's predictions under their respective assumptions.",,8 p.,,,cue integration; cue combination; explaining away; discounting,Computational Cognitive Science,,Creative Commons Attribution-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Karen Sollins,"Cheng, Tiffany",2010-09-22T20:15:24Z,2010-09-22T20:15:24Z,2010-09-22,http://hdl.handle.net/1721.1/58668,MIT-CSAIL-TR-2010-047,"A File Location, Replication, and Distribution System for Network Information to Aid Network Management","This thesis demonstrates and evaluates the design, architecture, and implementation of a file location, replication, and distribution system built with the objective of managing information in an Internet network. The system's goal is to enable the availability of information by providing alternative locations for files in case of situations where the original piece of information cannot be found in the network due to failures or other problems. The system provides the mechanism for duplicating files and executes the act of placing them in multiple locations according to predefined rules for distribution. The resulting system is a working model for a file management system that can exist over the Internet and will aid in overall network management by organizing and overseeing the information found within a network.",,105 p.,,,failure; copying,Advanced Network Architecture,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Barbara Liskov,"Popic, Victoria",2010-09-29T21:00:22Z,2010-09-29T21:00:22Z,2010-09-29,http://hdl.handle.net/1721.1/58772,MIT-CSAIL-TR-2010-048,Audit Trails in the Aeolus Distributed Security Platform,"This thesis provides a complete design and implementation of audit trail collection and storage for Aeolus, a distributed security platform based on information flow control. An information flow control system regulates all activities that concern information security. By recording all the operations monitored by Aeolus, our audit trails capture all actions that can affect system security. In our system, event records are collected on each system node and shipped to a centralized location, where they are stored and processed. To correlate audit trail events of different system nodes we store event dependencies directly in the event records. Each audit trail record keeps links to its immediate predecessors. Therefore, our audit trails form dependency graphs that capture the causal relationship among system events. These graphs can be used to reconstruct the chains of events leading to a given system state. Our results show that audit trail collection imposes a small overhead on system performance.",,86 p.,,,,Programming Methodology,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Joshua Tenenbaum,"Salakhutdinov, Ruslan; Tenenbaum, Josh; Torralba, Antonio",2010-11-22T22:15:19Z,2010-11-22T22:15:19Z,2010-10-13,http://hdl.handle.net/1721.1/60025,MIT-CSAIL-TR-2010-052,One-Shot Learning with a Hierarchical Nonparametric Bayesian Model,"We develop a hierarchical Bayesian model that learns to learn categories from single training examples. The model transfers acquired knowledge from previously learned categories to a novel category, in the form of a prior over category means and variances. The model discovers how to group categories into meaningful super-categories that express different priors for new classes. Given a single example of a novel category, we can efficiently infer which super-category the novel category belongs to, and thereby estimate not only the new category's mean but also an appropriate similarity metric based on parameters inherited from the super-category. On MNIST and MSR Cambridge image datasets the model learns useful representations of novel categories based on just a single training example, and performs significantly better than simpler hierarchical Bayesian approaches. It can also discover new categories in a completely unsupervised fashion, given just one or a few examples.",,14 p.,,,hierarchical Bayes; semi-supervised learning; learning to learn,Computational Cognitive Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Hoffmann, Henry; Maggio, Martina; Santambrogio, Marco D.; Leva, Alberto; Agarwal, Anant",2010-10-22T23:15:19Z,2010-10-22T23:15:19Z,2010-10-13,http://hdl.handle.net/1721.1/59519,MIT-CSAIL-TR-2010-049,SEEC: A Framework for Self-aware Computing,"As the complexity of computing systems increases, application programmers must be experts in their application domain and have the systems knowledge required to address the problems that arise from parallelism, power, energy, and reliability concerns. One approach to relieving this burden is to make use of self-aware computing systems, which automatically adjust their behavior to help applications achieve their goals. This paper presents the SEEC framework, a unified computational model designed to enable self-aware computing in both applications and system software. In the SEEC model, applications specify goals, system software specifies possible actions, and the SEEC framework is responsible for deciding how to use the available actions to meet the application-specified goals. The SEEC framework is built around a general and extensible control system which provides predictable behavior and allows SEEC to make decisions that achieve goals while optimizing resource utilization. To demonstrate the applicability of the SEEC framework, this paper presents fivedifferent self-aware systems built using SEEC. Case studies demonstrate how these systems can control the performance of the PARSEC benchmarks, optimize performance per Watt for a video encoder, and respond to unexpected changes in the underlying environment. In general these studies demonstrate that systems built using the SEEC framework are goal-oriented, predictable, adaptive, and extensible.",,13 p.,,,Autonomic Computing; Adaptive Computing; Multicore; Control Theory,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Antonio Torralba,"Choi, Myung Jin; Lim, Joseph J.; Torralba, Antonio; Willsky, Alan S.",2010-10-29T23:00:18Z,2010-10-29T23:00:18Z,2010-10-29,http://hdl.handle.net/1721.1/59799,MIT-CSAIL-TR-2010-050,A Tree-Based Context Model for Object Recognition,"There has been a growing interest in exploiting contextual information in addition to local features to detect and localize multiple object categories in an image. A context model can rule out some unlikely combinations or locations of objects and guide detectors to produce a semantically coherent interpretation of a scene. However, the performance benefit of context models has been limited because most of the previous methods were tested on datasets with only a few object categories, in which most images contain one or two object categories. In this paper, we introduce a new dataset with images that contain many instances of different object categories, and propose an efficient model that captures the contextual information among more than a hundred object categories using a tree structure. Our model incorporates global image features, dependencies between object categories, and outputs of local detectors into one probabilistic framework. We demonstrate that our context model improves object recognition performance and provides a coherent interpretation of a scene, which enables a reliable image querying system by multiple object categories. In addition, our model can be applied to scene understanding tasks that local detectors alone cannot solve, such as detecting objects out of context or querying for the most typical and the least typicalscenes in a dataset.",,14 p.,,,Object recognition; scene analysis; Markov random fields; structural models; image databases,Vision,"This research was partially funded by Shell International Exploration and Production Inc., by Army Research Office under award W911NF-06-1-0076, by NSF Career Award (ISI 0747120), and by the Air Force Office of Scientific Research under Award No.FA9550-06-1-0324. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the Air Force.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Bouvrie, Jake; Poggio, Tomaso; Rosasco, Lorenzo; Smale, Steve; Wibisono, Andre",2010-11-22T22:15:09Z,2010-11-22T22:15:09Z,2010-11-19,http://hdl.handle.net/1721.1/60024,MIT-CSAIL-TR-2010-051; CBCL-292,Generalization and Properties of the Neural Response,"Hierarchical learning algorithms have enjoyed tremendous growth in recent years, with many new algorithms being proposed and applied to a wide range of applications. However, despite the apparent success of hierarchical algorithms in practice, the theory of hierarchical architectures remains at an early stage. In this paper we study the theoretical properties of hierarchical algorithms from a mathematical perspective. Our work is based on the framework of hierarchical architectures introduced by Smale et al. in the paper ""Mathematics of the Neural Response"", Foundations of Computational Mathematics, 2010. We propose a generalized definition of the neural response and derived kernel that allows us to integrate some of the existing hierarchical algorithms in practice into our framework. We then use this generalized definition to analyze the theoretical properties of hierarchical architectures. Our analysis focuses on three particular aspects of the hierarchy. First, we show that a wide class of architectures suffers from range compression; essentially, the derived kernel becomes increasingly saturated at each layer. Second, we show that the complexity of a linear architecture is constrained by the complexity of the first layer, and in some cases the architecture collapses into a single-layer linear computation. Finally, we characterize the discrimination and invariance properties of the derived kernel in the case when the input data are one-dimensional strings. We believe that these theoretical results will provide a useful foundation for guiding future developments within the theory of hierarchical algorithms.",,59 p.,,,hierarchical learning; kernel methods; learning theory,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"Lis, Mieszko; Shim, Keun Sup; Cho, Myong Hyon; Khan, Omer; Devadas, Srinivas",2010-11-23T20:30:14Z,2010-11-23T20:30:14Z,2010-11-22,http://hdl.handle.net/1721.1/60039,MIT-CSAIL-TR-2010-053,Scalable directoryless shared memory coherence using execution migration,"We introduce the concept of deadlock-free migration-based coherent shared memory to the NUCA family of architectures. Migration-based architectures move threads among cores to guarantee sequential semantics in large multicores. Using a execution migration (EM) architecture, we achieve performance comparable to directory-based architectures without using directories: avoiding automatic data replication significantly reduces cache miss rates, while a fast network-level thread migration scheme takes advantage of shared data locality to reduce remote cache accesses that limit traditional NUCA performance. EM area and energy consumption are very competitive, and, on the average, it outperforms a directory-based MOESI baseline by 6.8% and a traditional S-NUCA design by 9.2%. We argue that with EM scaling performance has much lower cost and design complexity than in directory-based coherence and traditional NUCA architectures: by merely scaling network bandwidth from 128 to 256 (512) bit flits, the performance of our architecture improves by an additional 8% (12%), while the baselines show negligible improvement.",,18 p.,,,multicore; memory architecture; cache coherence; nuca,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Arvind,"Adler, Michael; Fleming, Kermin E.; Parashar, Angshuman; Pellauer, Michael; Emer, Joel",2010-11-29T19:15:06Z,2010-11-29T19:15:06Z,2010-11-23,http://hdl.handle.net/1721.1/60045,MIT-CSAIL-TR-2010-054,LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic [Extended Version],"Developers accelerating applications on FPGAs or other reconfigurable logic have nothing but raw memory devices in their standard toolkits. Each project typically includes tedious development of single-use memory management. Software developers expect a programming environment to include automatic memory management. Virtual memory provides the illusion of very large arrays and processor caches reduce access latency without explicit programmer instructions. LEAP scratchpads for reconfigurable logic dynamically allocate and manage multiple, independent, memory arrays in a large backing store. Scratchpad accesses are cached automatically in multiple levels, ranging from shared on-board, RAM-based, set-associative caches to private caches stored in FPGA RAM blocks. In the LEAP framework, scratchpads share the same interface as on-die RAM blocks and are plug-in replacements. Additional libraries support heap management within a storage set. Like software developers, accelerator authors using scratchpads may focus more on core algorithms and less on memory management. Two uses of FPGA scratchpads are analyzed: buffer management in an H.264 decoder and memory management within a processor microarchitecture timing model.",,11 p.,,,FPGA; memory management; caches,Computation Structures,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,"CORRECTION: The authors for entry [4] in the references should have been ""E. S. Chung, 
J. C. Hoe, and K. Mai"".",,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Kim, Deokhwan; Rinard, Martin C.",2010-12-03T21:00:05Z,2010-12-03T21:00:05Z,2010-12-03,http://hdl.handle.net/1721.1/60078,MIT-CSAIL-TR-2010-056,Verification of Semantic Commutativity Conditions and Inverse Operations on Linked Data Structures,"Commuting operations play a critical role in many parallel computing systems. We present a new technique for verifying commutativity conditions, which are logical formulas that characterize when operations commute. Because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not identical) data structure states in different execution orders. We have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (ListSet and HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and ArrayList). This effort involved the specification and verification of 765 commutativity conditions. Many speculative parallel systems need to undo the effects of speculatively executed operations. Inverse operations, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). We present a new technique for verifying such inverse operations. We have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state. Together, the commutativity conditions and inverse operations provide a key resource that language designers and system developers can draw on to build parallel languages and systems with strong correctness guarantees.",,673 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Leibo, Joel Z; Mutch, Jim; Ullman, Shimon; Poggio, Tomaso",2010-12-06T19:00:10Z,2010-12-06T19:00:10Z,2010-12-04,http://hdl.handle.net/1721.1/60216,MIT-CSAIL-TR-2010-057; CBCL-293,From primal templates to invariant recognition,We can immediately recognize novel objects   seen only once before -- in different positions on the retina and at different scales (distances). Is this ability hardwired by our genes or learned during development -- and if so how? We present a computational proof that developmental learning of invariance in recognition is possible and can emerge rapidly. This computational work sets the stage for experiments on the development of object invariance while suggesting a specific mechanism that may be critically tested.,,4 p.,,,vision; object recognition,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Srini Devadas,"Kinsy, Michel; Pellauer, Michael",2010-12-10T19:30:05Z,2010-12-10T19:30:05Z,2010-12-08,http://hdl.handle.net/1721.1/60266,MIT-CSAIL-TR-2010-058,Heracles: Fully Synthesizable Parameterized MIPS-Based Multicore System,"Heracles is an open-source complete multicore system written in Verilog. It is fully parameterized and can be reconfigured and synthesized into different topologies and sizes. Each processing node has a 7-stage pipeline, fully bypassed, microprocessor running the MIPS-III ISA, a 4-stage input-buffer, virtual-channel router, and a local variable-size shared memory. Our design is highly modular with clear interfaces between the core, the memory hierarchy, and the on-chip network. In the baseline design, the microprocessor is attached to two caches, one instruction cache and one data cache, which are oblivious to the global memory organization. The memory system in Heracles can be configured as one single global shared memory (SM), or distributed shared memory (DSM), or any combination thereof. Each core is connected to the rest of the network of processors by a parameterized, realistic, wormhole router. We show different topology configurations of the system, and their synthesis results on the Xilinx Virtex-5 LX330T FPGA board. We also provide a small MIPS cross-compiler toolchain to assist in developing software for Heracles.",,9 p.,,,Multicore Architecture Design; FPGA; Shared-Memory; Distributed Shared Memory; Network-on-Chip; RISC; MIPS; Virtual Channel; Wormhole Router; NoC Routing Algorithm,Computation Structures,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",2010-12-30T09:30:03Z,2010-12-30T09:30:03Z,2010-12-20,http://hdl.handle.net/1721.1/60371,MIT-CSAIL-TR-2010-060,Conservative Rationalizability and The Second-Knowledge Mechanism,"In mechanism design, the traditional way of modeling the players' incomplete information about their opponents is ""assuming a Bayesian."" This assumption, however, is very strong and does not hold in many real applications. Accordingly, we put forward (1) a set-theoretic way to model the knowledge that a player might have about his opponents, and (2) a new class of mechanisms capable of leveraging such more conservative knowledge in a robust way. In auctions of a single good, we show that such a new mechanism can perfectly guarantee a revenue benchmark (always lying in between the second highest and the highest valuation) that no classical mechanism can even approximate in any robust way.",,23 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Azar, Pablo; Chen, Jing; Micali, Silvio",2010-12-30T09:15:11Z,2010-12-30T09:15:11Z,2010-12-20,http://hdl.handle.net/1721.1/60370,MIT-CSAIL-TR-2010-059,Conservative-Bayesian Mechanism Design,"Classical Bayesian mechanism design is ""centralized,"" that is, the designer is assumed to know the distribution D from which the players' type profile has been drawn. We instead investigate a very ""decentralized"" Bayesian model, where the designer has no knowledge at all, and each player only has some probabilistic information about D. For this decentralized model and many contexts of interest, where the goal is to maximize revenue, we show that, for arbitrary type distributions D (in particular, correlated ones), it is possible to design mechanisms matching to a significant extent the performance of the optimal centralized mechanisms. Our results are ""existential"" for a broad class of contexts (including combinatorial auctions) and ""constructive"" for auctions of a single good.",,18 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Leibo, Joel Z; Mutch, Jim; Rosasco, Lorenzo; Ullman, Shimon; Poggio, Tomaso",2011-01-04T22:30:15Z,2011-01-04T22:30:15Z,2010-12-30,http://hdl.handle.net/1721.1/60378,MIT-CSAIL-TR-2010-061; CBCL-294,Learning Generic Invariances in Object Recognition:  Translation and Scale,"Invariance to various transformations is key to object recognition but existing definitions of invariance are somewhat confusing while discussions of invariance are often confused. In this report, we provide an operational definition of invariance by formally defining perceptual tasks as classification problems. The definition should be appropriate for physiology, psychophysics and computational modeling. For any specific object, invariance can be trivially ``learned'' by memorizing a sufficient number of example images of the transformed object. While our formal definition of invariance also covers such cases, this report focuses instead on invariance from very few images and mostly on invariances from one example. Image-plane invariances -- such as translation, rotation and scaling -- can be computed from a single image for any object. They are called generic since in principle they can be hardwired or learned (during development) for any object. In this perspective, we characterize the invariance range of a class of feedforward architectures for visual recognition that mimic the hierarchical organization of the ventral stream. We show that this class of models achieves essentially perfect translation and scaling invariance for novel images. In this architecture a new image is represented in terms of weights of ""templates"" (e.g. ""centers"" or ""basis functions"") at each level in the hierarchy. Such a representation inherits the invariance of each template, which is implemented through replication of the corresponding ""simple"" units across positions or scales and their ""association"" in a ""complex"" unit. We show simulations on real images that characterize the type and number of templates needed to support the invariant recognition of novel objects. We find that 1) the templates need not be visually similar to the target objects and that 2) a very small number of them is sufficient for good recognition. These somewhat surprising empirical results have intriguing implications for the learning of invariant recognition during the development of a biological organism, such as a human baby. In particular, we conjecture that invariance to translation and scale may be learned by the association -- through temporal contiguity -- of a small number of primal templates, that is patches extracted from the images of an object moving on the retina across positions and scales. The number of templates can later be augmented by bootstrapping mechanisms using the correspondence provided by the primal templates -- without the need of temporal contiguity.",,27 p.,,,vision; object recognition; generic transformations; selectivity-invariance trade-off; primal templates,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,CBCL-291,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Mutch, Jim; Leibo, Joel Z; Smale, Steve; Rosasco, Lorenzo; Poggio, Tomaso",2011-01-04T22:30:28Z,2011-01-04T22:30:28Z,2010-12-31,http://hdl.handle.net/1721.1/60379,MIT-CSAIL-TR-2010-062; CBCL-295,Neurons That Confuse Mirror-Symmetric Object Views,"Neurons in inferotemporal cortex that respond similarly to many pairs of mirror-symmetric images -- for example, 45 degree and -45 degree views of the same face -- have often been reported. The phenomenon seemed to be an interesting oddity. However, the same phenomenon has also emerged in simple hierarchical models of the ventral stream. Here we state a theorem characterizing sufficient conditions for this curious invariance to occur in a rather large class of hierarchical networks and demonstrate it with simulations.",,7 p.,,,Mirror Symmetry; Object Recognition,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Patrick Winston,"Finlayson, Mark Alan; Kulkarni, Nidhi",2011-05-09T19:30:20Z,2011-05-09T19:30:20Z,2011,http://hdl.handle.net/1721.1/62793,,jMWE v1.0.0,"jMWE is a Java library for constructing and testing Multi-Word Expression detectors. The library has three main facilities: (1) a detector API, (2) a MWE index facility, and (3) a test harness. This is version 1.0.0 of the library. It contains the source code, compiled binary files, javadocs, a user's manual (pdf), and data for constructing a default MWE index. The freely available version of jMWE is licensed for use for non-commercial purposes only, as long as proper acknowledgment is made. Details can be found in the license, which is included at the end of this document. The copyright on the software is owned by MIT; if you wish to use the software for commercial purposes, please contact the MIT Technology Licensing Office for more information on how to obtain a commercial license.",,1816402 bytes,application/zip,,Java; Multi-word Expressions; MWE; Detection,Genesis,,Creative Commons Attribution-NonCommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,"""June 2011.""","Finlayson, Mark A. and Kulkarni, Nidhi (2011) ""Detecting Multi-Word Expressions improves Word Sense Disambiguation"", in Proceedings of the 2011 Workshop on Multiword Expressions, held at ACL'2011 in Portland, OR",,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Conrad, Patrick R; Williams, Brian C",2011-01-19T23:30:14Z,2011-01-19T23:30:14Z,2011-01-15,http://hdl.handle.net/1721.1/60674,MIT-CSAIL-TR-2011-002,Flexible Execution of Plans with Choice and Uncertainty,"Dynamic plan execution strategies allow an autonomous agent to respond to uncertainties, while improving robustness and reducing the need for an overly conservative plan. Executives have improved robustness by expanding the types of choices made dynamically, such as selecting alternate methods. However, in some approaches to date, these additional choices often induce significant storage requirements to make flexible execution possible. This paper presents a novel system called Drake, which is able to dramatically reduce the storage requirements in exchange for increased execution time for some computations. Drake frames a plan as a collection of related Simple Temporal Problems, and executes the plan with a fast dynamic scheduling algorithm. This scheduling algorithm leverages prior work in Assumption-based Truth Maintenance Systems to compactly record and reason over the family of Simple Temporal Problems. We also allow Drake to reason over temporal uncertainty and choices by using prior work in Simple Temporal Problems with Uncertainty, which can guarantee correct execution, regardless of the uncertain outcomes. On randomly generated structured plans with choice, framed as either Temporal Plan Networks or Disjunctive Temporal Problems, we show a reduction in the size of the solution set of around four orders of magnitude, compared to prior art.",,70 p.,,,dynamic execution; scheduling; Simple Temporal Problems; dispatching; temporal plan,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Misailovic, Sasa; Roy, Daniel M.; Rinard, Martin",2011-01-19T23:45:03Z,2011-01-19T23:45:03Z,2011-01-19,http://hdl.handle.net/1721.1/60675,MIT-CSAIL-TR-2011-003,Probabilistic and Statistical Analysis of Perforated Patterns,"We present a new foundation for the analysis and transformation of computer programs.Standard approaches involve the use of logical reasoning to prove that the applied transformation does not change the observable semantics of the program. Our approach, in contrast, uses probabilistic and statistical reasoning to justify the application of transformations that may change, within probabilistic bounds, the result that the program produces. Loop perforation transforms loops to execute fewer iterations. We show how to use our basic approach to justify the application of loop perforation to a set of computational patterns. Empirical results from computations drawn from the PARSEC benchmark suite demonstrate that these computational patterns occur in practice. We also outline a specification methodology that enables the transformation of subcomputations and discuss how to automate the approach.",,22 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Baldassarre, Luca; Rosasco, Lorenzo; Barla, Annalisa; Verri, Alessandro",2011-02-01T20:00:05Z,2011-02-01T20:00:05Z,2011-01-24,http://hdl.handle.net/1721.1/60875,MIT-CSAIL-TR-2011-004; CBCL-296,Multi-Output Learning via Spectral Filtering,"In this paper we study a class of regularized kernel methods for vector-valued learning which are based on filtering the spectrum of the kernel matrix. The considered methods include Tikhonov regularization as a special case, as well as interesting alternatives such as vector-valued extensions of L2 boosting. Computational properties are discussed for various examples of kernels for vector-valued functions and the benefits of iterative techniques are illustrated. Generalizing previous results for the scalar case, we show finite sample bounds for the excess risk of the obtained estimator and, in turn, these results allow to prove consistency both for regression and multi-category classification. Finally, we present some promising results of the proposed algorithms on artificial and real data.",,37 p.,,,"Computational Learning, Multi-Output Learning, Spectral Methods",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nickolai Zeldovich,"Popa, Raluca Ada; Zeldovich, Nickolai; Balakrishnan, Hari",2011-02-01T20:15:04Z,2011-02-01T20:15:04Z,2011-01-26,http://hdl.handle.net/1721.1/60876,MIT-CSAIL-TR-2011-005,CryptDB: A Practical Encrypted Relational DBMS,"CryptDB is a DBMS that provides provable and practical privacy in the face of a compromised database server or curious database administrators. CryptDB works by executing SQL queries over encrypted data. At its core are three novel ideas: an SQL-aware encryption strategy that maps SQL operations to encryption schemes, adjustable query-based encryption which allows CryptDB to adjust the encryption level of each data item based on user queries, and onion encryption to efficiently change data encryption levels. CryptDB only empowers the server to execute queries that the users requested, and achieves maximum privacy given the mix of queries issued by the users. The database server fully evaluates queries on encrypted data and sends the result back to the client for final decryption; client machines do not perform any query processing and client-side applications run unchanged. Our evaluation shows that CryptDB has modest overhead: on the TPC-C benchmark on Postgres, CryptDB reduces throughput by 27% compared to regular Postgres. Importantly, CryptDB does not change the innards of existing DBMSs: we realized the implementation of CryptDB using client-side query rewriting/encrypting, user-defined functions, and server-side tables for public key information. As such, CryptDB is portable; porting CryptDB to MySQL required changing 86 lines of code, mostly at the connectivity layer.",,13 p.,,,confidentiality; privacy; cloud computing; outsourced databases; queries over encrypted data,Parallel and Distributed Operating Systems,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Nick Roy,"Kollar, Thomas; Dickerson, Steven; Tellex, Stefanie; Banerjee, Ashis Gopal; Walter, Matthew R.; Teller, Seth; Roy, Nicholas",2011-02-02T22:45:06Z,2011-02-02T22:45:06Z,2011-02-01,http://hdl.handle.net/1721.1/60883,MIT-CSAIL-TR-2011-007,Towards Understanding Hierarchical Natural Language Commands for Robotic Navigation and Manipulation,We describe a new model for understanding hierarchical natural language commands for robot navigation and manipulation. The model has three components: a semantic structure that captures the hierarchical structure of language; a cost function that maps the command's semantic structure to the robot's sensorimotor capabilities; and an efficient search method for finding the lowest-cost plan. We present a proof-of-concept system that carries out navigation commands in a simulated setting.,,2 p.,,,graphical models; SDC; ESDC; grounding,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Martin Rinard,"Ganesh, Vijay; Minnes, Mia; Solar-Lezama, Armando; Rinard, Martin",2011-02-01T20:15:11Z,2011-02-01T20:15:11Z,2011-02-01,http://hdl.handle.net/1721.1/60877,MIT-CSAIL-TR-2011-006,What is Decidable about Strings?,"We prove several decidability and undecidability results for the satisfiability/validity problem of formulas over a language of finite-length strings and integers (interpreted as lengths of strings). The atomic formulas over this language are equality over string terms (word equations), linear inequality over length function (length constraints), and membership predicate over regularexpressions (r.e.). These decidability questions are important in logic, program analysis and formal verification. Logicians have been attempting to resolve some of these questions for many decades, while practical satisfiability procedures for these formulas are increasingly important in the analysis of string-manipulating programs such as web applications and scripts. We prove three main theorems. First, we consider Boolean combination of quantifier-free formulas constructed out of word equations and length constraints. We show that if word equations can be converted to a solved form, a form relevant in practice, then the satisfiability problem for Boolean combination of word equations and length constraints is decidable. Second, we show that the satisfiability problem for word equations in solved form that areregular, length constraints and r.e. membership predicate is also decidable. Third, we show that the validity problem for the set of sentences written as a forall-exists quantifier alternation applied to positive word equations is undecidable. A corollary of this undecidability result is that this set is undecidable even with sentences with at most two occurrences of a string variable.",,16 p.,,,Theories of strings; decidability; undecidability; word equations; regular expressions; JavaScript; Formal methods; Program Analysis,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Jakubczak, Szymon; Katabi, Dina",2011-02-22T21:30:19Z,2011-02-22T21:30:19Z,2011-02-15,http://hdl.handle.net/1721.1/61009,MIT-CSAIL-TR-2011-008,SoftCast: Clean-slate Scalable Wireless Video,"Video broadcast and mobile video challenge the conventional wireless design. In broadcast and mobile scenarios the bit rate supported by the channel differs across receivers and varies quickly over time. The conventional design however forces the source to pick a single bit rate and degrades sharply when the channel cannot not support the chosen bit rate. This paper presents SoftCast, a clean-slate design for wireless video where the source transmits one video stream that each receiver decodes to a video quality commensurate with its specific instantaneous channel quality. To do so, SoftCast ensures the samples of the digital video signal transmitted on the channel are linearly related to the pixels' luminance. Thus, when channel noise perturbs the transmitted signal samples, the perturbation naturally translates into approximation in the original video pixels. Hence, a receiver with a good channel (low noise) obtains a high fidelity video, and a receiver with a bad channel (high noise) obtains a low fidelity video. We implement SoftCast using the GNURadio software and the USRP platform. Results from a 20-node testbed show that SoftCast improves the average video quality (i.e., PSNR) across broadcast receivers in our testbed by up to 5.5dB. Even for a single receiver, it eliminates video glitches caused by mobility and increases robustness to packet loss by an order of magnitude.",,14 p.,,,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chiesa, Alessandro; Micali, Silvio; Zhu, Zeyuan Allen",2011-02-22T21:30:08Z,2011-02-22T21:30:08Z,2011-02-16,http://hdl.handle.net/1721.1/61008,MIT-CSAIL-TR-2011-009,Mechanism Design With Approximate Player Types,"We investigate mechanism design when the players do not exactly know their types, but have instead only partial information about them.",,18 p.,,,Type uncertainty; Mechanism Design; Auctions; Social Welfare,Theory of Computation,,,,,,,,,MIT-CSAIL-TR-2011-024,http://hdl.handle.net/1721.1/62296,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chiesa, Alessandro; Micali, Silvio; Zhu, Zeyuan Allen",2011-04-21T18:15:33Z,2011-04-21T18:15:33Z,2011-02-16,http://hdl.handle.net/1721.1/62296,MIT-CSAIL-TR-2011-024,Mechanism Design with Approximate Valuations,"In mechanism design, we replace the strong assumption that each player knows his own payoff type EXACTLY with the more realistic assumption that he knows it only APPROXIMATELY. Specifically, we study the classical problem of maximizing social welfare in single-good auctions when players know their true valuations only within a constant multiplicative factor d in (0,1). Our approach is deliberately non-Bayesian and very conservative: each player i only knows that his true valuation is one among finitely many values in a d-APPROXIMATE SET, Ki, and his true valuation is ADVERSARIALLY and SECRETLY chosen in Ki at the beginning of the auction. We prove tight upper and lower bounds for the fraction of the maximum social welfare achievable in our model, in either dominant or undominated strategies, both via deterministic and probabilistic mechanisms. The landscape emerging is quite unusual and intriguing.",,30 p.,,,,Theory of Computation,,,,,,,,,,http://hdl.handle.net/1721.1/61008,MIT-CSAIL-TR-2011-009,,,,,,,,,,,,,,,,,
Nancy Lynch,"Khabbazian, Majid; Kowalski, Dariusz; Kuhn, Fabian; Lynch, Nancy",2011-03-03T20:15:08Z,2011-03-03T20:15:08Z,2011-02-23,http://hdl.handle.net/1721.1/61391,MIT-CSAIL-TR-2011-010,Decomposing Broadcast Algorithms Using Abstract MAC Layers,"In much of the theoretical literature on global broadcast algorithms for wireless networks, issues of message dissemination are considered together with issues of contention management. This combination leads to complicated algorithms and analysis, and makes it difficult to extend the work to more difficult communication problems. In this paper, we present results aimed at simplifying such algorithms and analysis by decomposing the treatment into two levels, using abstract ""MAC layer"" specifications to encapsulate contention management. We use two different abstract MAC layers: the basic layer of Kuhn, Lynch, and Newport, and a new probabilistic layer. We first present a typical randomized contention-management algorithm for a standard graph-based radio network model and show that it implements both abstract MAC layers. Then we combine this algorithm with greedy algorithms for single-message and multi-message global broadcast and analyze the combinations, using both abstract MAC layers as intermediate layers. Using the basic MAC layer, we prove a bound of O(D log(n / epsilon) log(Delta)) for the time to deliver a single message everywhere with probability 1 - epsilon, where D is the network diameter, n is the number of nodes, and Delta is the maximum node degree. Using the probabilistic layer, we prove a bound of O((D + log(n/epsilon)) log(Delta)), which matches the best previously-known bound for single-message broadcast over the physical network model. For multi-message broadcast, we obtain bounds of O((D + k Delta) log(n/epsilon) log(Delta)) using the basic layer and O((D + k Delta log(n/epsilon)) log(Delta)) using the probabilistic layer, for the time to deliver a message everywhere in the presence of at most k concurrent messages.",,39 p.,,,broadcast protocol; contention management; wireless network algorithms; multi-message broadcast; global broadcast,Theory of Computation,"Author Lynch's research is supported by AFOSR contract FA9550-08-1-0159 and NSF grants CCF-0726514, CNS-0715397, CCF-0937274, and NSF-PURDUE-STC Award 0939370-CCF.  Author Kowalski's research is supported by the Engineering and Physical Sciences Research Council [grant numbers EP/G023018/1,
EP/H018816/1].",,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Poggio, Tomaso",2011-03-04T21:30:04Z,2011-03-04T21:30:04Z,2011-03-04,http://hdl.handle.net/1721.1/61424,MIT-CSAIL-TR-2011-011; CBCL-297,Werner Reichardt: the man and his scientific legacy,"Excerpts from a talk given by Tomaso Poggio in Tübingen on the opening ofthe Werner Reichardt Centrun für Integrative Neurowissenschaften, December 8, 2008.",,16 p.,,,vision; fly vision; theory; figure-ground discrimination; motion detection,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,"Geiger, Gadi",,,,,,,,
Anant Agarwal,"Wentzlaff, David; Gruenwald, Charles, III; Beckmann, Nathan; Belay, Adam; Kasture, Harshad; Modzelewski, Kevin; Youseff, Lamia; Miller, Jason E.; Agarwal, Anant",2011-03-09T18:45:21Z,2011-03-09T18:45:21Z,2011-03-09,http://hdl.handle.net/1721.1/61640,MIT-CSAIL-TR-2011-012,Fleets: Scalable Services in a Factored Operating System,"Current monolithic operating systems are designed for uniprocessor systems, and their architecture reflects this. The rise of multicore and cloud computing is drastically changing the tradeoffs in operating system design. The culture of scarce computational resources is being replaced with one of abundant cores, where spatial layout of processes supplants time multiplexing as the primary scheduling concern. Efforts to parallelize monolithic kernels have been difficult and only marginally successful, and new approaches are needed. This paper presents fleets, a novel way of constructing scalable OS services. With fleets, traditional OS services are factored out of the kernel and moved into user space, where they are further parallelized into a distributed set of concurrent, message-passing servers. We evaluate fleets within fos, a new factored operating system designed from the ground up with scalability as the first-order design constraint. This paper details the main design principles of fleets, and how the system architecture of fos enables their construction. We describe the design and implementation of three critical fleets (network stack, page allocation, and file system) and compare with Linux. These comparisons show that fos achieves superior performance and has better scalability than Linux for large multicores; at 32 cores, fos's page allocator performs 4.5 times better than Linux, and fos's network stack performs 2.5 times better. Additionally, we demonstrate how fleets can adapt to changing resource demand, and the importance of spatial scheduling for good performance in multicores.",,13 p.,,,multicore; scalable operating system,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Li-Shiuan Peh,"Krishna, Tushar; Beckmann, Bradford M.; Peh, Li-Shiuan; Reinhardt, Steven K.",2011-03-14T19:45:24Z,2011-03-14T19:45:24Z,2011-03-14,http://hdl.handle.net/1721.1/61695,MIT-CSAIL-TR-2011-013,BOOM: Broadcast Optimizations for On-chip Meshes,"Future many-core chips will require an on-chip network that can support broadcasts and multicasts at good power-performance. A vanilla on-chip network would send multiple unicast packets for each broadcast packet, resulting in latency, throughput and power overheads. Recent research in on-chip multicast support has proposed forking of broadcast/multicast packets within the network at the router buffers, but these techniques are far from ideal, since they increase buffer occupancy which lowers throughput, and packets incur delay and power penalties at each router. In this work, we analyze an ideal broadcast mesh; show the substantial gaps between state-of-the-art multicast NoCs and the ideal; then propose BOOM, which comprises a WHIRL routing protocol that ideally load balances broadcast traffic, a mXbar multicast crossbar circuit that enables multicast traversal at similar energy-delay as unicasts, and speculative bypassing of buffering for multicast flits. Together, they enable broadcast packets to approach the delay, energy, and throughput of the ideal fabric. Our simulations show BOOM realizing an average network latency that is 5% off ideal, attaining 96% of ideal throughput, with energy consumption that is 9% above ideal. Evaluations using synthetic traffic show BOOM achieving a latency reduction of 61%, throughput improvement of 63%, and buffer power reduction of 80% as compared to a baseline broadcast. Simulations with PARSEC benchmarks show BOOM reducing average request and network latency by 40% and 15% respectively.",,12 p.,,,multicore,Computer Architecture,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Arvind,"Newton, Ryan; Chen, Chih-Ping; Marlow, Simon",2011-03-22T17:00:16Z,2011-03-22T17:00:16Z,2011-03-22,http://hdl.handle.net/1721.1/61759,MIT-CSAIL-TR-2011-015,Intel Concurrent Collections for Haskell,"Intel Concurrent Collections (CnC) is a parallel programming model in which a network of steps (functions) communicate through message-passing as well as a limited form of shared memory. This paper describes a new implementation of CnC for Haskell. Compared to existing parallel programming models for Haskell, CnC occupies a useful point in the design space: pure and deterministic like Evaluation Strategies, but more explicit about granularity and the structure of the parallel computation, which affords the programmer greater control over parallel performance. We present results on 4, 8, and 32-core machines demonstrating parallel speedups over 20x on non-trivial benchmarks.",,21 p.,,,dataflow computation; task graphs; I-structures,Computation Structures,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,Arvind; Computation Structures,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Hoffmann, Henry; Maggio, Martina; Santambrogio, Marco D.; Leva, Alberto; Agarwal, Anant",2011-03-24T21:15:14Z,2011-03-24T21:15:14Z,2011-03-24,http://hdl.handle.net/1721.1/61950,MIT-CSAIL-TR-2011-016,SEEC: A Framework for Self-aware Management of Multicore Resources,"This paper presents SEEC, a self-aware programming model, designed to reduce programming effort in modern multicore systems. In the SEEC model, application programmers specify application goals and progress, while systems programmers separately specify actions system software and hardware can take to affect an application (e.g. resource allocation). The SEEC runtime monitors applications and dynamically selects actions to meet application goals optimally (e.g. meeting performance while minimizing power consumption). The SEEC runtime optimizes system behavior for the application rather than requiring the application programmer to optimize for the system. This paper presents a detailed discussion of the SEEC model and runtime as well as several case studies demonstrating their benefits. SEEC is shown to optimize performance per Watt for a video encoder, find optimal resource allocation for an application with complex resource usage, and maintain the goals of multiple applications in the face of environmental fluctuations.",,14 p.,,,"Self-adaptive, self-optimizing, self-tuning, self-*, power-aware",Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Lau, Eric; Miller, Jason E; Choi, Inseok; Yeung, Donald; Amarasinghe, Saman; Agarwal, Anant",2011-03-25T21:15:08Z,2011-03-25T21:15:08Z,2011-03-25,http://hdl.handle.net/1721.1/61978,MIT-CSAIL-TR-2011-017,Multicore Performance Optimization Using Partner Cores,"As the push for parallelism continues to increase the number of cores on a chip, and add to the complexity of system design, the task of optimizing performance at the application level becomes nearly impossible for the programmer. Much effort has been spent on developing techniques for optimizing performance at runtime, but many techniques for modern processors employ the use of speculative threads or performance counters. These approaches result in stolen cycles, or the use of an extra core, and such expensive penalties put demanding constraints on the gains provided by such methods. While processors have grown in power and complexity, the technology for small, efficient cores has emerged. We introduce the concept of Partner Cores for maximizing hardware power efficiency; these are low-area, low-power cores situated on-die, tightly coupled to each main processor core. We demonstrate that such cores enable performance improvement without incurring expensive penalties, and carry out potential applications that are impossible on a traditional chip multiprocessor.",,7 p.,,,self-aware; adaptive,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nickolai Zeldovich,"Boneh, Dan; Mazieres, David; Popa, Raluca Ada",2011-03-31T20:15:08Z,2011-03-31T20:15:08Z,2011-03-30,http://hdl.handle.net/1721.1/62006,MIT-CSAIL-TR-2011-018,Remote Oblivious Storage: Making Oblivious RAM Practical,"Remote storage of data has become an increasingly attractive and advantageous option, especially due to cloud systems. While encryption protects the data, it does not hide the access pattern to the data. A natural solution is to access remote storage using an Oblivious RAM (ORAM) which provably hides all access patterns. While ORAM is asymptotically efficient, the best existing scheme (Pinkas and Reinman, Crypto'10) still has considerable overhead for a practical implementation: for M stored items, it stores 4 times and sometimes 6 times more items remotely, requires O(log2 M) round trips to storage server per request, and periodically blocks all data requests to shuffle all storage (which is a lengthy process). In this paper, we first define a related notion to ORAM, oblivious storage (OS), which captures more accurately and naturally the security setting of remote storage. Then, we propose a new ORAM/OS construction that solves the practicality issues just outlined: it has a storage constant of ~ 1, achieves O(1) round trips to the storage server per request, and allows requests to happen concurrently with shuffle without jeopardizing security. Our construction consists of a new organization of server memory into a flat main part and a hierarchical shelter, a client-side index for rapidly locating identifiers at the server, a new shelter serving requests concurrent with the shuffle, and a data structure for locating items efficiently in a partially shuffled storage.",,18 p.,,,access patterns; data privacy,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Maggio, Martina; Hoffmann, Henry; Santambrogio, Marco D.; Agarwal, Anant; Leva, Alberto",2011-04-01T19:30:09Z,2011-04-01T19:30:09Z,2011-04-01,http://hdl.handle.net/1721.1/62020,MIT-CSAIL-TR-2011-019,A Comparison of Autonomic Decision Making Techniques,"Autonomic computing systems are capable of adapting their behavior and resources thousands of times a second to automatically decide the best way to accomplish a given goal despite changing environmental conditions and demands. Different decision mechanisms are considered in the literature, but in the vast majority of the cases a single technique is applied to a given instance of the problem. This paper proposes a comparison of some state of the art approaches for decision making, applied to a self-optimizing autonomic system that allocates resources to a software application, which provides direct performance feedback at runtime. The Application Heartbeats framework is used to provide the sensor data (feedback), and a variety of decision mechanisms, from heuristics to control-theory and machine learning, are investigated. The results obtained with these solutions are compared by means of case studies using standard benchmarks.",,10 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Levin, Anat; Weiss, Yair; Durand, Fredo; Freeman, William T.",2011-04-04T15:45:25Z,2011-04-04T15:45:25Z,2011-04-04,http://hdl.handle.net/1721.1/62035,MIT-CSAIL-TR-2011-020,Efficient Marginal Likelihood Optimization in Blind Deconvolution,"In blind deconvolution one aims to estimate from an input blurred image y a sharp image x and an unknown blur kernel k. Recent research shows that a key to success is to consider the overall shape of the posterior distribution p(x, k|y) and not only its mode. This leads to a distinction between MAPx,k strategies which estimate the mode pair x, k and often lead to undesired results, and MAPk strategies which select the best k while marginalizing over all possible x images. The MAPk principle is significantly more robust than the MAPx,k one, yet, it involves a challenging marginalization over latent images. As a result, MAPk techniques are considered complicated, and have not been widely exploited. This paper derives a simple approximated MAPk algorithm which involves only a modest modification of common MAPx,k algorithms. We show that MAPk can, in fact, be optimized easily, with no additional computational complexity.",,12 p.,,,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Radeva, Tsvetomira; Lynch, Nancy",2011-04-21T18:15:25Z,2011-04-21T18:15:25Z,2011-04-14,http://hdl.handle.net/1721.1/62295,MIT-CSAIL-TR-2011-022,Partial Reversal Acyclicity,"Partial Reversal (PR) is a link reversal algorithm which ensures that the underlying graph structure is destination-oriented and acyclic. These properties of PR make it useful in routing protocols and algorithms for solving leader election and mutual exclusion. While proofs exist to establish the acyclicity property of PR, they rely on assigning labels to either the nodes or the edges in the graph. In this work we present simpler direct proof of the acyclicity property of partial reversal without using any external or dynamic labeling mechanism. First, we provide a simple variant of the PR algorithm, and show that it maintains acyclicity. Next, we present a binary relation which maps the original PR algorithm to the new algorithm, and finally, we conclude that the acyclicity proof applies to the original PR algorithm as well.",,20 p.,,,Partial Reversal; Link Reversal; Graph Algorithms,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Chikkerur, Sharat; Poggio, Tomaso",2011-04-21T18:15:06Z,2011-04-21T18:15:06Z,2011-04-14,http://hdl.handle.net/1721.1/62293,MIT-CSAIL-TR-2011-021; CBCL-298,Approximations in the HMAX Model,"The HMAX model is a biologically motivated architecture for computer vision whose components are in close agreement with existing physiological evidence. The model is capable of achieving close to human level performance on several rapid object recognition tasks. However, the model is computationally bound and has limited engineering applications in its current form. In this report, we present several approximations in order to increase the efficiency of the HMAX model. We outline approximations at several levels of the hierarchy and empirically evaluate the trade-offs between efficiency and accuracy. We also explore ways to quantify the representation capacity of the model.",,12 p.,,,"object recognition, approximation",Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Hal Abelson,"Kagal, Lalana; Jacobi, Ian; Khandelwal, Ankesh",2011-04-21T18:15:15Z,2011-04-21T18:15:15Z,2011-04-16,http://hdl.handle.net/1721.1/62294,MIT-CSAIL-TR-2011-023,Gasping for AIR   Why we need Linked Rules and Justifications on the Semantic Web,"The Semantic Web is a distributed model for publishing, utilizing and extending structured information using Web protocols. One of the main goals of this technology is to automate the retrieval and integration of data and to enable the inference of interesting results. This automation requires logics and rule languages that make inferences, choose courses of action, and answer questions. The openness of the Web, however, leads to several issues including the handling of inconsistencies, integration of diverse information, and the determination of the quality and trustworthiness of the data. AIR is a Semantic Web-based rule language that provides this functionality while focusing on generating and tracking explanations for its inferences and actions as well as conforming to Linked Data principles. AIR supports Linked Rules, which allow rules to be combined, re-used and extended in a manner similar to Linked Data. Additionally, AIR explanations themselves are Semantic Web data so they can be used for further reasoning. In this paper we present an overview of AIR, discuss its potential as a Web rule language by providing examples of how its features can be leveraged for different inference requirements, and describe how justifications are represented and generated.",,10 p.,,,,Decentralized Information Group,"This material is based upon work supported by the National Science Foundation under Award No. CNS-0831442, by the Air Force Office of Scientific Research under Award No. FA9550-09-1-0152, and by Intelligence Advanced Research Projects Activity under Award No. FA8750-07-2-0031.",Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jinc; Micali, Silvio",2011-04-22T18:45:10Z,2011-04-22T18:45:10Z,2011-04-22,http://hdl.handle.net/1721.1/62301,MIT-CSAIL-TR-2011-025,Collusive Dominant-Strategy Truthfulness,"Fifty years ago, Vickrey published his famous mechanism for auctioning a single good in limited supply. The main property of Vickrey's mechanism is efficiency in dominant strategies. In absence of collusion, this is a wonderful efficiency guarantee. We note, however, that collusion is far from rare in auctions, and if some colluders exist and have some wrong beliefs, then the Vickrey mechanism dramatically loses its efficiency. Accordingly, we put forward a new mechanism that, despite unconstrained collusion, guarantees efficiency by providing a richer set of strategies and ensuring that it is dominant for every player to reveal truthfully not only his own valuation, but also with whom he is colluding, if he is indeed colluding with someone else. Our approach meaningfully bypasses prior impossibility proofs.",,10 p.,,,Collusion; Auctions; Efficiency,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Jayaraman, Karthick; Ganesh, Vijay; Tripunitara, Mahesh; Rinard, Martin C.; Chapin, Steve J.",2011-04-28T20:30:11Z,2011-04-28T20:30:11Z,2011-04-27,http://hdl.handle.net/1721.1/62562,MIT-CSAIL-TR-2011-026,ARBAC Policy for a Large Multi-National Bank,"Administrative role-based access control (ARBAC) is the first comprehensive administrative model proposed for role-based access control (RBAC). ARBAC has several features for designing highly expressive policies, but current work has not highlighted the utility of these expressive policies. In this report, we present a case study of designing an ARBAC policy for a bank comprising 18 branches. Using this case study we provide an assessment about the features of ARBAC that are likely to be used in realistic policies.",,6 p.,,,ARBAC policy; computer security; access control,Program Analysis,,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Srini Devadas,"Shim, Keun Sup; Cho, Myong Hyon; Lis, Mieszko; Khan, Omer; Devadas, Srinivas",2011-05-03T21:15:13Z,2011-05-03T21:15:13Z,2011-05-02,http://hdl.handle.net/1721.1/62580,MIT-CSAIL-TR-2011-027,Library Cache Coherence,"Directory-based cache coherence is a popular mechanism for chip multiprocessors and multicores. The directory protocol, however, requires multicast for invalidation messages and the collection of acknowledgement messages, which can be expensive in terms of latency and network traffic. Furthermore, the size of the directory increases with the number of cores. We present Library Cache Coherence (LCC), which requires neither broadcast/multicast for invalidations nor waiting for invalidation acknowledgements. A library is a set of timestamps that are used to auto-invalidate shared cache lines, and delay writes on the lines until all shared copies expire. The size of library is independent of the number of cores. By removing the complex invalidation process of directory-based cache coherence protocols, LCC generates fewer network messages. At the same time, LCC also allows reads on a cache block to take place while a write to the block is being delayed, without breaking sequential consistency. As a result, LCC has 1.85X less average memory latency than a MESI directory-based protocol on our set of benchmarks, even with a simple timestamp choosing algorithm; moreover, our experimental results on LCC with an ideal timestamp scheme (though not implementable) show the potential of further improvement for LCC with more sophisticated timestamp schemes.",,6 p.,,,multicore; cache coherence,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
David Clark,"Heikkinen, Mikko V. J.; Berger, Arthur W.",2011-05-03T21:15:05Z,2011-05-03T21:15:05Z,2011-05-03,http://hdl.handle.net/1721.1/62579,MIT-CSAIL-TR-2011-028,Comparison of User Traffic Characteristics on Mobile-Access versus Fixed-Access Networks,"We compare Web traffic characteristics of mobile- versus fixed-access end-hosts, where herein the term ""mobile"" refers to access via cell towers, using for example the 3G/UMTS standard, and the term ""fixed"" includes Wi-Fi access. It is well-known that connection speeds are in general slower over mobile-access networks, and also that often there is higher packet loss. We were curious whether this leads mobile-access users to have smaller connections. We examined the distribution of the number of bytes-per-connection, and packet loss from a sampling of logs from servers of Akamai Technologies. We obtained 149 million connections, across 57 countries. The mean bytes-per-connection was typically larger for fixed-access: for two-thirds of the countries, it was at least one-third larger. Regarding distributions, we found that the difference between the bytes-per-connection for mobile- versus fixed-access, as well as the packet loss, was statistically significant for each of the countries; however the visual difference in plots is typically small. For some countries, mobile-access had the larger connections. As expected, mobile-access often had higher loss than fixed-access, but the reverse pertained for some countries. Typically packet loss increased during the busy period of the day, when mobile-access had a larger increase. Comparing our results from 2010 to those from 2009 of the same time period, we found that connections have become a bit smaller.",,20 p.,,,,Advanced Network Architecture,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Patrick Winston,"Finlayson, Mark Alan; Kulkarni, Nidhi",2011-05-09T19:30:09Z,2011-05-09T19:30:09Z,2011-05-09,http://hdl.handle.net/1721.1/62792,,Source code and data for MWE'2011 papers,"Contains the source code and data necessary to run all computations described in the following two papers: Finlayson, Mark A. and Kulkarni, Nidhi (2011) ""Detecting Multi-Word Expressions improves Word Sense Disambiguation"", in Proceedings of the 2011 Workshop on Multiword Expressions, held at ACL'2011 in Portland, OR; Kulkarni, Nidhi and Finlayson, Mark A. (2011) ""jMWE: A Java Toolkit for Detecting Multi-Word Expressions"" in Proceedings of the 2011 Workshop on Multiword Expressions, held at ACL'2011 in Portland, OR.",,45241266 bytes,application/zip,,Multi-word expressions; MWE; Java; Supplementary material,Genesis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
David Clark,"García, Rubén",2011-05-11T07:15:47Z,2011-05-11T07:15:47Z,2011-05-10,http://hdl.handle.net/1721.1/62812,,Understanding the Performance of Broadband Networks through the Statistical Analysis of Speed Tests - Supplemental materials,"Supplemental materials for the master thesis ""Understanding the Performance of Broadband Networks Through the Statistical Analysis of Speed Tests"", by Rubén García, submitted in May 2011 for the S.M. in Technology and Policy. Supplemental materials include: Source_code: Folder containing the source code for the statistical analysis of NDT speed tests, written for the R statistical package; NDT_data: Folder containing the following datasets (1) ndt4.h5: Initial NDT data that we used for the analysis; (2) ndt3.h5: Reduced version of the ndt4 dataset (same tests but less variables), also contains the 'whois' file that we combine with the NDT data in order to add location information; (3) comcast-ndt.h5: dataset containing the speed tests of a controlled experiment that we ran using different test durations; Aggregated_datasets: Versions of the ndt4.h5 dataset aggregated by IP and by Autonomous System.",,241484228 bytes,,,Web100; NDT,Advanced Network Architecture,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Mroueh, Youssef; Poggio, Tomaso; Rosasco, Lorenzo",2011-06-03T15:15:06Z,2011-06-03T15:15:06Z,2011-06-03,http://hdl.handle.net/1721.1/63175,MIT-CSAIL-TR-2011-029; CBCL-299,Regularization Predicts While Discovering Taxonomy,In this work we discuss a regularization framework to solve multi-category when the classes are described by an underlying class taxonomy. In particular we discuss how to learn the class taxonomy while learning a multi-category classifier.,,5 p.,,,computational learning; classification,Center for Biological and Computational Learning (CBCL),"This research was sponsored by grants from DARPA (IPTO and DSO), National Science Foundation (NSF-0640097, NSF-0827427), AFSOR-THRL (FA8650-05-C-7262) Additional support was provided by: Adobe, Honda Research Institute USA, King Abdullah University Science and Technology grant to B. DeVore, NEC, Sony and especially by the Eugene McDermott Foundation",,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Karen Sollins,"Guo, Nina X.",2011-06-07T21:15:04Z,2011-06-07T21:15:04Z,2011-06-07,http://hdl.handle.net/1721.1/63260,MIT-CSAIL-TR-2011-030,Scalable Information-Sharing Network Management,"This thesis analyzes scalable information-sharing network management. It looks into one of the large problems in network management today: finding information across different network domains. Information-sharing network management is a method to solving the problem, though it is important to make it scalable. The solution proposed uses the Publish-Subscribe Internet Routing Paradigm (PSIRP) inter-domain design as the base structure. The design borrows from Border Gateway Protocol ideas and uses the Chord protocol as one of the key methods of finding information. The conclusion after analyzing the scalability of PSIRP is that its use of Chord gives it an advantage that allows a O(log^2 N) tradeoff between performance and distribution.",,56 p.,,,"network management, scalability, information-centric networking, inter-domain routing",Advanced Network Architecture,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Isik, Leyla; Leibo, Joel Z.; Mutch, Jim; Lee, Sang Wan; Poggio, Tomaso",2011-06-20T19:45:08Z,2011-06-20T19:45:08Z,2011-06-17,http://hdl.handle.net/1721.1/64621,MIT-CSAIL-TR-2011-031; CBCL-300,A hierarchical model of peripheral vision,"We present a peripheral vision model inspired by the cortical architecture discovered by Hubel and Wiesel. As with existing cortical models, this model contains alternating layers of simple cells, which employ tuning functions to increase specificity, and complex cells, which pool over simple cells to increase invariance. To extend the traditional cortical model, we introduce the option of eccentricity-dependent pooling and tuning parameters within a given model layer. This peripheral vision system can be used to model physiological data where receptive field sizes change as a function of eccentricity. This gives the user flexibility to test different theories about filtering and pooling ranges in the periphery. In a specific instantiation of the model, pooling and tuning parameters can increase linearly with eccentricity to model physiological data found in different layers of the visual cortex. Additionally, it can be used to introduce pre-cortical model layers such as retina and LGN. We have tested the model s response with different parameters on several natural images to demonstrate its effectiveness as a research tool. The peripheral vision model presents a useful tool to test theories about crowding, attention, visual search, and other phenomena of peripheral vision.",,13 p.,,,"peripheral vision, computational neuroscience, HMAX",Center for Biological and Computational Learning (CBCL),"This work was supported by the following grants: NSF-0640097, NSF-0827427, NSF-0645960, DARPA-DSO, AFSOR FA8650-50-C-7262, AFSOR FA9550-09-1-0606.",Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Robert Morris,"Boyd-Wickizer, Silas; Kaashoek, M. Frans; Morris, Robert; Zeldovich, Nickolai",2011-06-28T21:45:22Z,2011-06-28T21:45:22Z,2011-06-28,http://hdl.handle.net/1721.1/64698,MIT-CSAIL-TR-2011-032,A Software Approach to Unifying Multicore Caches,"Multicore chips will have large amounts of fast on-chip cache memory, along with relatively slow DRAM interfaces. The on-chip cache memory, however, will be fragmented and spread over the chip; this distributed arrangement is hard for certain kinds of applications to exploit efficiently, and can lead to needless slow DRAM accesses. First, data accessed from many cores may be duplicated in many caches, reducing the amount of distinct data cached. Second, data in a cache distant from the accessing core may be slow to fetch via the cache coherence protocol. Third, software on each core can only allocate space in the small fraction of total cache memory that is local to that core. A new approach called software cache unification (SCU) addresses these challenges for applications that would be better served by a large shared cache. SCU chooses the on-chip cache in which to cache each item of data. As an application thread reads data items, SCU moves the thread to the core whose on-chip cache contains each item. This allows the thread to read the data quickly if it is already on-chip; if it is not, moving the thread causes the data to be loaded into the chosen on-chip cache. A new file cache for Linux, called MFC, uses SCU to improve performance of file-intensive applications, such as Unix file utilities. An evaluation on a 16-core AMD Opteron machine shows that MFC improves the throughput of file utilities by a factor of 1.6. Experiments with a platform that emulates future machines with less DRAM throughput per core shows that MFC will provide benefit to a growing range of applications.",,13 p.,,,,Parallel and Distributed Operating Systems,"This material is based upon work supported by the National Science
Foundation under grant number 0915164.",Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Alvarez, Mauricio A.; Rosasco, Lorenzo; Lawrence, Neil D.",2011-06-30T19:30:08Z,2011-06-30T19:30:08Z,2011-06-30,http://hdl.handle.net/1721.1/64731,MIT-CSAIL-TR-2011-033; CBCL-301,Kernels for Vector-Valued Functions: a Review,"Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.",,38 p.,,,learning theory; kernel methods; multi-output learning,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,"Alvarez, Mauricio A.; Rosasco, Lorenzo; Lawrence, Neil D.",,,,,,,en-US,,,
Anant Agarwal,"Belay, Adam; Wentzlaff, David; Agarwal, Anant",2011-07-28T17:45:15Z,2011-07-28T17:45:15Z,2011-07-27,http://hdl.handle.net/1721.1/64977,MIT-CSAIL-TR-2011-035,Vote the OS off your Core,"Recent trends in OS research have shown evidence that there are performance benefits to running OS services on different cores than the user applications that rely on them. We quantitatively evaluate this claim in terms of one of the most significant architectural constraints: memory performance. To this end, we have created CachEMU, an open-source memory trace generator and cache simulator built as an extension to QEMU for working with system traces. Using CachEMU, we determined that for five common Linux test workloads, it was best to run the OS close, but not too close   on the same package, but not on the same core.",,6 p.,,,,Computer Architecture,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
John Leonard,"Benjamin, Michael R.",2011-08-03T17:15:13Z,2011-08-03T17:15:13Z,2011-07-28,http://hdl.handle.net/1721.1/65074,MIT-CSAIL-TR-2011-036,MOOS-IvP Autonomy Tools Users Manual Release 4.2.1,This document describes 19 MOOS-IvP autonomy tools. uHelmScope provides a run-time scoping window into the state of an active IvP Helm executing its mission. pMarineViewer is a geo-based GUI tool for rendering marine vehicles and geometric data in their operational area. uXMS is a terminal based tool for scoping on a MOOSDB process. uTermCommand is a terminal based tool for poking a MOOSDB with a set of MOOS file pre-defined variable-value pairs selectable with aliases from the command-line. pEchoVar provides a way of echoing a post to one MOOS variable with a new post having the same value to a different variable. uProcessWatch monitors the presence or absence of a set of MOOS processes and summarizes the collective status in a single MOOS variable. uPokeDB provides a way of poking the MOOSDB from the command line with one or more variable-value pairs without any pre-existing configuration of a MOOS file. uTimerScript will execute a pre-defined timed pausable script of poking variable-value pairs to a MOOSDB. pNodeReporter summarizes a platforms critical information into a single node report string for sharing beyond the vehicle. pBasicContactMgr provides a basic contact management service with the ability to generate range-dependent configurable alerts. uSimMarine provides a simple marine vehicle simulator. uSimBeaconRange and uSimContactRange provide further simulation for range-only sensors. The Alog Toolbox is a set of offline tools for analyzing and manipulating log files in the .alog format.,,167 p.,,,Autonomous Marine Vehicles; Unmanned Marine Vehicles; UUVs; AUVs; USVs; IvP Helm; pHelmIvP; LAMSS; MOOS; pLogger; MOOS Scope; MOOSDB,Marine Robotics,,Creative Commons Attribution-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
John Leonard,"Benjamin, Michael R.; Schmidt, Henrik; Newman, Paul; Leonard, John J.",2011-08-03T17:15:06Z,2011-08-03T17:15:06Z,2011-08-03,http://hdl.handle.net/1721.1/65073,MIT-CSAIL-TR-2011-037,An Overview of MOOS-IvP and a Users Guide to the IvP Helm - Release 4.2.1,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moos-ivp.org.",,262 p.,,,Unmanned Underwater Vehicles; UUV; Unmanned Surface Vehicles; USV; Unmanned Vehicles; Autonomy; Marine Vehicles; Marine Autonomy; pHelmIvP; MOOS; Autonomous Vehicles; Autonomous Marine Vehicles; Unmanned Marine Vehicles; Multi-objective Optimization; Behavior-Based Control; Behavior-Based Architecture,Marine Robotics,,Creative Commons Attribution-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Karen Sollins,"Woodrow, Stephen Robert",2011-08-31T23:00:08Z,2011-08-31T23:00:08Z,2011-08-06,http://hdl.handle.net/1721.1/65591,MIT-CSAIL-TR-2011-038,Tragedy of the routing table: An analysis of collective action amongst Internet network operators,"This thesis analyzes and discusses the effectiveness of social efforts to achieve collective action amongst Internet network operators in order to manage the growth of the Internet routing table. The size and rate of growth of the Internet routing table is an acknowledged challenge impeding the scalability of our BGP interdomain routing architecture. While most of the work towards a solution to this problem has focused on architectural improvements, an effort launched in the 1990s called the CIDR Report attempts to incentivize route aggregation using social forces and norms in the Internet operator community. This thesis analyzes the behavior of Internet network operators in response to the CIDR Report from 1997 to 2011 to determine whether the Report was effective in achieving this goal. While it is difficult to causally attribute aggregation behavior to appearance on the CIDR report, there is a trend for networks to improve their prefix aggregation following an appearance on the CIDR Report compared to untreated networks. This suggests that the CIDR Report did affect network aggregation behavior, although the routing table continued to grow. This aggregation improvement is most prevalent early in the study period and becomes less apparent as time goes on. Potential causes of the apparent change in efficacy of the Report are discussed and examined using Ostrom s Common Pool Resource framework. The thesis then concludes with a discussion of options for mitigating routing table growth, including the continued use of community forces to better manage the Internet routing table.",,165 p.,,,,Advanced Network Architecture,,,,S.M. thesis,,,,S.M.,,,,,,,,,,,,,,,,,en-US,,,
Russ Tedrake,"Platt, Robert, Jr.; Kaelbling, Leslie; Lozano-Perez, Tomas; Tedrake, Russ",2011-09-15T18:30:11Z,2011-09-15T18:30:11Z,2011-08-27,http://hdl.handle.net/1721.1/65856,MIT-CSAIL-TR-2011-039,A hypothesis-based algorithm for planning and control in non-Gaussian belief spaces,"We consider the partially observable control problem where it is potentially necessary to perform complex information-gathering operations in order to localize state. One approach to solving these problems is to create plans in belief-space, the space of probability distributions over the underlying state of the system. The belief-space plan encodes a strategy for performing a task while gaining information as necessary. Most approaches to belief-space planning rely upon representing belief state in a particular way (typically as a Gaussian). Unfortunately, this can lead to large errors between the assumed density representation and the true belief state. We propose a new computationally efficient algorithm for planning in non-Gaussian belief spaces. We propose a receding horizon re-planning approach where planning occurs in a low-dimensional sampled representation of belief state while the true belief state of the system is monitored using an arbitrary accurate high-dimensional representation. Our key contribution is a planning problem that, when solved optimally on each re-planning step, is guaranteed, under certain conditions, to enable the system to gain information. We prove that when these conditions are met, the algorithm converges with probability one. We characterize algorithm performance for different parameter settings in simulation and report results from a robot experiment that illustrates the application of the algorithm to robot grasping.",,22 p.,,,,Robot Locomotion Group,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,Russ Tedrake; Robot Locomotion Group,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Isik, Leyla; Leibo, Joel Z; Poggio, Tomaso",2011-09-12T16:00:13Z,2011-09-12T16:00:13Z,2011-09-10,http://hdl.handle.net/1721.1/65646,MIT-CSAIL-TR-2011-040; CBCL-302,Learning and disrupting invariance in visual recognition,"Learning by temporal association rules such as Foldiak's trace rule is an attractive hypothesis that explains the development of invariance in visual recognition. Consistent with these rules, several recent experiments have shown that invariance can be broken by appropriately altering the visual environment but found puzzling differences in the effects at the psychophysical versus single cell level. We show a) that associative learning provides appropriate invariance in models of object recognition inspired by Hubel and Wiesel b) that we can replicate the ""invariance disruption"" experiments using these models with a temporal association learning rule to develop and maintain invariance, and c) that we can thereby explain the apparent discrepancies between psychophysics and singe cells effects. We argue that these models account for the stability of perceptual invariance despite the underlying plasticity of the system, the variability of the visual world and expected noise in the biological mechanisms.",,13 p.,,,"vision, object recognition",Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Daniela Rus,"Julian, Brian J.; Angermann, Michael; Schwager, Mac; Rus, Daniela",2011-07-15T17:15:17Z,2011-07-15T17:15:17Z,2011-09-25,http://hdl.handle.net/1721.1/64821,MIT-CSAIL-TR-2011-034,A Scalable Information Theoretic Approach to Distributed Robot Coordination,"This paper presents a scalable information theoretic approach to infer the state of an environment by distributively controlling robots equipped with sensors. The robots iteratively estimate the environment state using a recursive Bayesian filter, while continuously moving to improve the quality of the estimate by following the gradient of mutual information. Both the filter and the controller use a novel algorithm for approximating the robots' joint measurement probabilities, which combines consensus (for decentralization) and sampling (for scalability). The approximations are shown to approach the true joint measurement probabilities as the size of the consensus rounds grows or as the network becomes complete. The resulting gradient controller runs in constant time with respect to the number of robots, and linear time with respect to the number of sensor measurements and environment discretization cells, while traditional mutual information methods are exponential in all of these quantities. Furthermore, the controller is proven to be convergent between consensus rounds and, under certain conditions, is locally optimal. The complete distributed inference and coordination algorithm is demonstrated in experiments with five quad-rotor flying robots and simulations with 100 robots.",,10 p.,,,,Robotics (Rus),"This work is sponsored by the Department of the Air Force under Air Force contract number FA8721-05-C-0002. The opinions, interpretations, recommendations, and conclusions are those of the authors and are not necessarily endorsed by the United States Government. This work is also supported in part by ARO grant number W911NF-05-1-0219, ONR grant number N00014-09-1-1051, NSF grant number EFRI-0735953, ARL grant number W911NF-08-2-0004, MIT Lincoln Laboratory, the European Commission, and the Boeing Company.",,,,In Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS 11),,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Mosci, Sofia; Rosasco, Lorenzo; Santoro, Matteo; Verri, Alessandro; Villa, Silvia",2011-09-26T20:45:09Z,2011-09-26T20:45:09Z,2011-09-26,http://hdl.handle.net/1721.1/65964,MIT-CSAIL-TR-2011-041; CBCL-303,Nonparametric Sparsity and Regularization,"In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose and study a new regularizer and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art.",,38 p.,,,computational learning; machine learning,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Mroueh, Youssef; Poggio, Tomaso; Rosasco, Lorenzo; Slotine, Jean-Jacques E.",2011-09-27T20:30:07Z,2011-09-27T20:30:07Z,2011-09-27,http://hdl.handle.net/1721.1/66085,MIT-CSAIL-TR-2011-043; CBCL-305,Multi-Class Learning: Simplex Coding And Relaxation Error,"We study multi-category classification in the framework of computational learning theory. We show how a relaxation approach, which is commonly used in binary classification, can be generalized to the multi-class setting. We propose a vector coding, namely the simplex coding, that allows to introduce a new notion of multi-class margin and cast multi-category classification into a vector valued regression problem. The analysis of the relaxation error be quantified and the binary case is recovered as a special case of our theory. From a computational point of view we can show that using the simplex coding we can design regularized learning algorithms for multi-category classification that can be trained at a complexity which is independent to the number of classes.",,3 p.,,,computational learning; machine learning; convex relaxation,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Martin Rinard,"Long, Fan; Ganesh, Vijay; Carbin, Micheal; Sidiroglou, Stelios; Rinard, Martin",2011-10-03T21:00:06Z,2011-10-03T21:00:06Z,2011-10-03,http://hdl.handle.net/1721.1/66170,MIT-CSAIL-TR-2011-044,Automatic Input Rectification,"We present a novel technique, automatic input rectification, and a prototype implementation called SOAP. SOAP learns a set of constraints characterizing typical inputs that an application is highly likely to process correctly. When given an atypical input that does not satisfy these constraints, SOAP automatically rectifies the input (i.e., changes the input so that is satisfies the learned constraints). The goal is to automatically convert potentially dangerous inputs into typical inputs that the program is highly likely to process correctly. Our experimental results show that, for a set of benchmark applications (namely, Google Picasa, ImageMagick, VLC, Swfdec, and Dillo), this approach effectively converts malicious inputs (which successfully exploit vulnerabilities in the application) into benign inputs that the application processes correctly. Moreover, a manual code analysis shows that, if an input does satisfy the learned constraints, it is incapable of exploiting these vulnerabilities. We also present the results of a user study designed to evaluate the subjective perceptual quality of outputs from benign but atypical inputs that have been automatically rectified by SOAP to conform to the learned constraints. Specifically, we obtained benign inputs that violate learned constraints, used our input rectifier to obtain rectified inputs, then paid Amazon Mechanical Turk users to provide their subjective qualitative perception of the difference between the outputs from the original and rectified inputs. The results indicate that rectification can often preserve much, and in many cases all, of the desirable data in the original input.",,13 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,MIT CSAIL,,
Nancy Lynch,"Ghaffari, Mohsen; Lynch, Nancy; Sastry, Srikanth",2011-10-12T18:30:07Z,2011-10-12T18:30:07Z,2011-10-12,http://hdl.handle.net/1721.1/66224,MIT-CSAIL-TR-2011-045,Leader Election Using Loneliness Detection,"We consider the problem of leader election (LE) in single-hop radio networks with synchronized time slots for transmitting and receiving messages. We assume that the actual number n of processes is unknown, while the size u of the ID space is known, but is possibly much larger. We consider two types of collision detection: strong (SCD), whereby all processes detect collisions, and weak (WCD), whereby only non-transmitting processes detect collisions. We introduce loneliness detection (LD) as a key subproblem for solving LE in WCD systems. LD informs all processes whether the system contains exactly one process or more than one. We show that LD captures the difference in power between SCD and WCD, by providing an implementation of SCD over WCD and LD. We present two algorithms that solve deterministic and probabilistic LD in WCD systems with time costs of O(log(u/n)) and O(min(log(u/n), (log(1/epsilon)/n)), respectively, where epsilon is the error probability. We also provide matching lower bounds. We present two algorithms that solve deterministic and probabilistic LE in SCD systems with time costs of O(log u) and O(min(log u, loglog n + log(1/epsilon))), respectively, where epsilon is the error probability. We provide matching lower bounds.",,37 p.,,,wireless networks,Theory of Computation,"This work partially supported by the NSF under award numbers CCF-0937274, and CCF-0726514, and by AFOSR under award number FA9550-08-1-0159. This work is also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Anant Agarwal,"Hoffmann, Henry; Maggio, Martina; Santambrogio, Marco D.; Leva, Alberto; Agarwal, Anant",2011-11-14T21:30:26Z,2011-11-14T21:30:26Z,2011-11-07,http://hdl.handle.net/1721.1/67020,MIT-CSAIL-TR-2011-046,SEEC: A General and Extensible Framework for Self-Aware Computing,"Modern systems require applications to balance competing goals, e.g. achieving high performance and low power. Achieving this balance places an unrealistic burden on application programmers who must understand the power and performance implications of a variety of application and system actions (e.g. changing algorithms or allocating cores). To address this problem, we propose the Self-aware Computing framework, or SEEC. SEEC automatically and dynamically schedules actions to meet application specified goals. While other self-aware implementations have been proposed, SEEC is uniquely distinguished by its decoupled approach, which allows application and systems programmers to separately specify observations and actions, according to their expertise. SEEC s runtime decision engine observes the system and schedules actions automatically, reducing programmer burden. This general and extensible decision engine employs both control theory and machine learning to reason about previously unseen applications and actions while automatically adapting to changes in both application and system models. This paper describes the SEEC framework and evaluates it in several case studies. SEEC is used to build an adaptive system that optimizes performance per Watt for the PARSEC benchmarks on multiple machines, achieving results as least 1.65x better than a classical control system. Additional studies show how SEEC can learn optimal resource allocation online and respond to fluctuations in the underlying hardware while managing multiple applications.",,12 p.,,,Adaptive computing; Self-optimizing systems; Self-adaptive computing; Autonomic Computing; Resource Allocation,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Martin Rinard,"Carbin, Michael; Kim, Deokhwan; Misailovic, Sasa; Rinard, Martin C.",2011-11-15T18:15:04Z,2011-11-15T18:15:04Z,2011-11-15,http://hdl.handle.net/1721.1/67031,MIT-CSAIL-TR-2011-050,Reasoning about Relaxed Programs,"A number of approximate program transformations have recently emerged that enable transformed programs to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control program execution. We call such transformed programs relaxed programs -- they have been extended with additional nondeterminism to relax their semantics and offer greater execution flexibility. We present programming language constructs for developing relaxed programs and proof rules for reasoning about properties of relaxed programs. Our proof rules enable programmers to directly specify and verify acceptability properties that characterize the desired correctness relationships between the values of variables in a program's original semantics (before transformation) and its relaxed semantics. Our proof rules also support the verification of safety properties (which characterize desirable properties involving values in individual executions). The rules are designed to support a reasoning approach in which the majority of the reasoning effort uses the original semantics. This effort is then reused to establish the desired properties of the program under the relaxed semantics. We have formalized the dynamic semantics of our target programming language and the proof rules in Coq, and verified that the proof rules are sound with respect to the dynamic semantics. Our Coq implementation enables developers to obtain fully machine checked verifications of their relaxed programs.",,11 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Frédo Durand,"Aubry, Mathieu; Paris, Sylvain; Hasinoff, Samuel W.; Kautz, Jan; Durand, Frédo",2011-11-15T16:30:09Z,2011-11-15T16:30:09Z,2011-11-15,http://hdl.handle.net/1721.1/67030,MIT-CSAIL-TR-2011-049,Fast and Robust Pyramid-based Image Processing,"Multi-scale manipulations are central to image editing but they are also prone to halos. Achieving artifact-free results requires sophisticated edgeaware techniques and careful parameter tuning. These shortcomings were recently addressed by the local Laplacian filters, which can achieve a broad range of effects using standard Laplacian pyramids. However, these filters are slow to evaluate and their relationship to other approaches is unclear. In this paper, we show that they are closely related to anisotropic diffusion and to bilateral filtering. Our study also leads to a variant of the bilateral filter that produces cleaner edges while retaining its speed. Building upon this result, we describe an acceleration scheme for local Laplacian filters that yields speed-ups on the order of 50x. Finally, we demonstrate how to use local Laplacian filters to alter the distribution of gradients in an image. We illustrate this property with a robust algorithm for photographic style transfer.",,11 p.,,,"Computational photography, photoediting, image processing, Laplacian pyramid, bilateral filter, photographic style transfer",Computer Graphics,,,,,,,,,,,,,,,,,,"Aubry, Mathieu; Paris, Sylvain; Hasinoff, Samuel W.; Kautz, Jan; Durand, Frédo",,,,,,,en-US,,,
Nickolai Zeldovich,"Metreveli, Zviad; Zeldovich, Nickolai; Kaashoek, M. Frans",2011-11-28T18:30:04Z,2011-11-28T18:30:04Z,2011-11-26,http://hdl.handle.net/1721.1/67296,MIT-CSAIL-TR-2011-051,CPHash: A Cache-Partitioned Hash Table,"CPHash is a concurrent hash table for multicore processors. CPHash partitions its table across the caches of cores and uses message passing to transfer lookups/inserts to a partition. CPHash's message passing avoids the need for locks, pipelines batches of asynchronous messages, and packs multiple messages into a single cache line transfer. Experiments on a 80-core machine with 2 hardware threads per core show that CPHash has ~1.6x higher throughput than a hash table implemented using fine-grained locks. An analysis shows that CPHash wins because it experiences fewer cache misses and its cache misses are less expensive, because of less contention for the on-chip interconnect and DRAM. CPServer, a key/value cache server using CPHash, achieves ~5% higher throughput than a key/value cache server that uses a hash table with fine-grained locks, but both achieve better throughput and scalability than memcached. Finally, the throughput of CPHash and CPServer scales near-linearly with the number of cores.",,10 p.,,,,Parallel and Distributed Operating Systems,This work was partially supported by Quanta Computer and by NSF award 915164.,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
,"Durand, Frédo",2011-12-14T19:45:12Z,2011-12-14T19:45:12Z,2011-12-14,http://hdl.handle.net/1721.1/67677,MIT-CSAIL-TR-2011-052,A Frequency Analysis of Monte-Carlo and other Numerical Integration Schemes,"The numerical calculation of integrals is central to many computer graphics algorithms such as Monte-Carlo Ray Tracing. We show that such methods can be studied using Fourier analysis. Numerical error is shown to correspond to aliasing and the link between properties of the sampling pattern and the integrand is studied. The approach also permits the unified study of image aliasing and numerical integration, by considering a multidimensional domain where some dimensions are integrated while others are sampled.",,6 p.,,,Numerical Analysis; Integration; Fourier; Monte-Carlo; Aliasing; Rendering; Ray Tracing,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Nancy Lynch,"Censor-Hillel, Keren; Gilbert, Seth; Kuhn, Fabian; Lynch, Nancy; Newport, Calvin",2011-12-27T20:30:09Z,2011-12-27T20:30:09Z,2011-12-22,http://hdl.handle.net/1721.1/67885,MIT-CSAIL-TR-2011-053,Structuring Unreliable Radio Networks,"In this paper we study the problem of building a connected dominating set with constant degree (CCDS) in the dual graph radio network model.  This model includes two types of links:  reliable links, which
always deliver messages, and unreliable links, which sometimes fail to deliver messages.  Real networks compensate for this differing quality by deploying low-layer detection protocols to filter unreliable from reliable links.  With this in mind, we begin by presenting an algorithm that solves the CCDS problem in the dual graph model under the assumption that every process u is provided with a local ""link detector set"" consisting of every neighbor connected to u by a reliable link.  The algorithm solves the CCDS problem in O((Delta log2(n)/b) + log3(n)) rounds, with high probability, where Delta is the maximum degree in the reliable link graph, n is the network size, and b is an upper bound in bits on the message size.  The algorithm works by first building a Maximal Independent Set (MIS) in log3(n) time, and then leveraging the local topology knowledge to efficiently connect nearby MIS processes.  A natural follow up question is whether the link detector must be perfectly reliable to solve the CCDS problem.  To answer this question, we first describe an algorithm that builds a CCDS in O(Delta polylog(n)) time under the assumption of O(1) unreliable links included in each link detector set.  We then prove this algorithm to be (almost) tight by showing that the possible inclusion of only a single unreliable link in each process's local link detector set is sufficient to require Omega(Delta) rounds to solve the CCDS problem, regardless of message size.  We conclude by discussing how to apply our algorithm in the setting where the topology of reliable and unreliable links can change over time.",,21 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Tan, Cheston; Leibo, Joel Z; Poggio, Tomaso",2012-06-21T19:45:06Z,2012-06-21T19:45:06Z,2012,http://hdl.handle.net/1721.1/71199,MIT-CSAIL-TR-2012-016; CBCL-309,Throwing Down the Visual Intelligence Gauntlet,"In recent years, scientific and technological advances have produced artificial systems that have matched or surpassed human capabilities in narrow domains such as face detection and optical character recognition. However, the problem of producing truly intelligent machines still remains far from being solved. In this chapter, we first describe some of these recent advances, and then review one approach to moving beyond these limited successes---the neuromorphic approach of studying and reverse-engineering the networks of neurons in the human brain (specifically, the visual system). Finally, we discuss several possible future directions in the quest for visual intelligence.",,15 p.,,,Vision; Artificial intelligence,Center for Biological and Computational Learning (CBCL),"This research was sponsored by grants from DARPA (IPTO and DSO), National Science Foundation (NSF-0640097, NSF-0827427), AFSOR-THRL (FA8650-05-C-7262). Additional support was provided by: Adobe, Honda Research Institute USA, King Abdullah University Science and Technology grant to B. DeVore, NEC, Sony and especially by the Eugene McDermott Foundation.",Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,"Machine Learning for Computer Vision (2012); eds: Cipolla R, Battiato S, Giovanni Maria F. Springer: Studies in Computational Intelligence Vol. 411.",,,,,,,,,,,,,,,,,,,,en-US,,,
Frédo Durand,"Judd, Tilke; Durand, Frédo; Torralba, Antonio",2012-01-13T22:30:12Z,2012-01-13T22:30:12Z,2012-01-13,http://hdl.handle.net/1721.1/68590,MIT-CSAIL-TR-2012-001,A Benchmark of Computational Models of Saliency to Predict Human Fixations,"Many computational models of visual attention have been created from a wide variety of different approaches to predict where people look in images. Each model is usually introduced by demonstrating performances on new images, and it is hard to make immediate comparisons between models. To alleviate this problem, we propose a benchmark data set containing 300 natural images with eye tracking data from 39 observers to compare model performances. We calculate the performance of 10 models at predicting ground truth fixations using three different metrics. We provide a way for people to submit new models for evaluation online. We find that the Judd et al. and Graph-based visual saliency models perform best. In general, models with blurrier maps and models that include a center bias perform well. We add and optimize a blur and center bias for each model and show improvements. We compare performances to baseline models of chance, center and human performance. We show that human performance increases with the number of humans to a limit. We analyze the similarity of different models using multidimensional scaling and explore the relationship between model performance and fixation consistency. Finally, we offer observations about how to improve saliency models in the future.",,22 p.,,,"fixation maps, saliency maps, vision",Computer Graphics,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Brian Williams,"Ono, Masahiro",2018-01-31T00:01:17Z,2018-01-31T00:01:17Z,2012-01-20,http://hdl.handle.net/1721.1/113370,MIT-CSAIL-TR-2018-010,Energy-efficient Control of a Smart Grid with Sustainable Homes based on Distributing Risk,"The goal of this thesis is to develop a distributed control system for a smart grid with sustainable homes. A central challenge is how to enhance energy efficiency in the presence of uncertainty. A major source of uncertainty in a smart grid is intermittent energy production by renewable energy sources. In the face of global climate change, it is crucial to reduce dependence on fossil fuels and shift to renewable energy sources, such as wind and solar. However, a large-scale introduction of wind and solar generation to an electrical grid poses a significant risk of blackouts since the energy supplied by the renewables is unpredictable and intermittent. The uncertain behavior of renewable energy sources increases the risk of blackouts. Therefore, an important challenge is to develop an intelligent control mechanism for the electrical grid that is both reliable and efficient. Uncertain weather conditions and human behavior pose challenges for a smart home. For example, autonomous room temperature control of a residential building may occasionally make the room environment uncomfortable for residents. Autonomous controllers must be able to take residents' preferences as an input, and to control the indoor environment in an energy-efficient manner while limiting the risk of failure to meet the residents' requirements in the presence of uncertainties. In order to overcome these challenges, we propose a distributed robust control method for a smart grid that includes smart homes as its building components. The proposed method consists of three algorithms: 1) market-based contingent energy dispatcher for an electrical grid, 2) a risk-sensitive plan executive for temperature control of a residential building, and 3) a chance-constrained model-predictive controller with a probabilistic guarantee of constraint satisfaction, which can control continuously operating systems such as an electrical grid and a building. We build the three algorithms upon the chance-constrained programming framework: minimization of a given cost function with chance constraints, which bound the probability of failure to satisfy given state constraints. Although these technologies provide promising capabilities, they cannot contribute to sustainability unless they are accepted by the society. In this thesis we specify policy challenges for a smart grid and a smart home, and discuss policy options that gives economical and regulatory incentives for the society to introduce these technologies on a large scale.",,145 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-31T00:01:18Z,,,,,,,,,,,,,,,
Nick Roy,"Tellex, Stefanie; Thaker, Pratiksha; Deits, Robin; Simeonov, Dimitar; Kollar, Thomas; Roy, Nicholas",2012-01-24T22:30:02Z,2012-01-24T22:30:02Z,2012-01-23,http://hdl.handle.net/1721.1/68651,MIT-CSAIL-TR-2012-002,Toward a Probabilistic Approach to Acquiring Information from Human Partners Using Language,"Our goal is to build robots that can robustly interact with humans using natural language. This problem is extremely challenging because human language is filled with ambiguity, and furthermore, the robot's model of the environment might be much more limited than the human partner. When humans encounter ambiguity in dialog with each other, a key strategy to resolve it is to ask clarifying questions about whatthey do not understand. This paper describes an approach for enabling robots to take the same approach: asking the human partner clarifying questions about ambiguous commands in order to infer better actions. The robot fuses information from the command, the question, and the answer by creating a joint probabilistic graphical model in the Generalized Grounding Graph framework. We demonstrate that by performing inference using information from the command, question and answer, the robot is able to infer object groundings and follow commands with higher accuracythan by using the command alone.",,2 p.,,,"dialog, robotics, question-asking","Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Tacchetti, Andrea; Mallapragada, Pavan S.; Santoro, Matteo; Rosasco, Lorenzo",2012-02-06T21:15:06Z,2012-02-06T21:15:06Z,2012-01-31,http://hdl.handle.net/1721.1/69034,MIT-CSAIL-TR-2012-003; CBCL-306,GURLS: a Toolbox for Regularized Least Squares Learning,"We present GURLS, a toolbox for supervised learning based on the regularized least squares algorithm. The toolbox takes advantage of all the favorable properties of least squares and is tailored to deal in particular with multi-category/multi-label problems. One of the main advantages of GURLS is that it allows training and tuning a multi-category classifier at essentially the same cost of one single binary classifier. The toolbox provides a set of basic functionalities including different training strategies and routines to handle computations with very large matrices by means of both memory-mapped storage and distributed task execution. The system is modular and can serve as a basis for easily prototyping new algorithms. The toolbox is available for download, easy to set-up and use.",,6 p.,,,"Matlab; Computational Learning; Regularized Least Squares; Large Scale, Multiclass problems; C++",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,MIT CSAIL,,
Brian Williams,"Effinger, Robert",2018-01-30T23:46:14Z,2018-01-30T23:46:14Z,2012-02-02,http://hdl.handle.net/1721.1/113366,MIT-CSAIL-TR-2018-006,Risk-minimizing program execution in robotic domains,"In this thesis, we argue that autonomous robots operating in hostile and uncertain environments can improve robustness by computing and reasoning explicitly about risk. Autonomous robots with a keen sensitivity to risk can be trusted with critical missions, such as exploring deep space and assisting on the battlefield. We introduce a novel, risk-minimizing approach to program execution that utilizes program flexibility and estimation of risk in order to make runtime decisions that minimize the probability of program failure. Our risk-minimizing executive, called Murphy, utilizes two forms of program flexibility, 1) flexible scheduling of activity timing, and 2) redundant choice between subprocedures, in order to minimize two forms of program risk, 1) exceptions arising from activity failures, and 2) exceptions arising from timing constraint violations in a program. Murphy takes two inputs, a program written in a nondeterministic variant of the Reactive Model-based Programming Language (RMPL) and a set of stochastic activity failure models, one for each activity in a program, and computes two outputs, a risk-minimizing decision policy and value function. The decision policy informs Murphy which decisions to make at runtime in order to minimize risk, while the value function quantifies risk. In order to execute with low latency, Murphy computes the decision policy and value function offline, as a compilation step prior to program execution. In this thesis, we develop three approaches to RMPL program execution. First, we develop an approach that is guaranteed to minimize risk. For this approach, we reason probabilistically about risk by framing program execution as a Markov Decision Process (MDP). Next, we develop an approach that avoids risk altogether. For this approach, we frame program execution as a novel form of constraint-based temporal reasoning. Finally, we develop an execution approach that trades optimality in risk avoidance for tractability. For this approach, we leverage prior work in hierarchical decomposition of MDPs in order to mitigate complexity. We benchmark the tractability of each approach on a set of representative RMPL programs, and we demonstrate the applicability of the approach on a humanoid robot simulator.",,161 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,Brian Williams; Model-based Embedded and Robotic Systems,,,,,,,2018-01-30T23:46:14Z,,,,,,,,,,,,,,,
Brian Williams,"Ono, Masahiro",2018-01-31T00:01:14Z,2018-01-31T00:01:14Z,2012-02-02,http://hdl.handle.net/1721.1/113369,MIT-CSAIL-TR-2018-009,"Robust, Goal-directed Plan Execution with Bounded Risk","There is an increasing need for robust optimal plan execution for multi-agent systems in uncertain environments, while guaranteeing an acceptable probability of success. For ex- ample, a fleet of unmanned aerial vehicles (UAVs) and autonomous underwater vehicles (AUVs) are required to operate autonomously for an extensive mission duration in an uncertain environment. Previous work introduced the concept of a model-based executive, which increases the level of autonomy, elevating the level at which systems are commanded. This thesis develops model-based executives that reason explicitly from a stochastic plant model to find the optimal course of action, while ensuring that the probability of failure is within a user-specified risk bound. This thesis presents two robust mode-based executives: probabilisticSulu orp-Sulu, and distributedprobabilisticSulu or dp-Sulu. The objective for p-Sulu and dp-Sulu is to allow users to command continuous, stochastic multi-agent systems in a manner that is both intuitive and safe. The user specifies the desired evolution of the plant state, as well as the acceptable probabilities of failure, as a temporal plan on states called a chance-constrained qualitative state plan (CCQSP). An example of a CCQSP statement is ""go to A through B within 30 minutes, with less than 0.001% probability of failure."" p-Sulu and dp-Sulu take a CCQSP, a continuous plant model with stochastic uncertainty, and an objective function as inputs, and outputs an optimal continuous control sequence, as well as an optimal discrete schedule. The difference between p-Sulu and dp-Sulu is that p-Sulu plans in a centralized manner while dp-Sulu plans in a distributed manner. dp-Sulu enables robust CCQSP execution for multi-agent systems. We solve the problem based on the key concept of risk allocation, which achieves tractability by allocating the specified risk to individual constraints and mapping the result into an equivalent deterministic constrained optimization problem. Risk allocation also enables a distributed plan execution for multi-agent systems by distributing the risk among agents to decompose the optimization problem. Building upon the risk allocation approach, we develop our first CCQSP executive, p-Sulu, in four spirals. First, we develop the Convex Risk Allocation (CRA) algorithm, which can solve a CCQSP planning problem with a convex state space and a fixed schedule, highlighting the capability of optimally allocating risk to individual constraints. Second, we develop the Non-convex Iterative Risk Allocation (NIRA) algorithm, which can handle non-convex state space. Third, we build upon NIRA a full-horizon CCQSP planner, p-Sulu FH, which can optimize not only the control sequence but also the schedule. Fourth, we develop p-Sulu, which enables the real-time execution of CCQSPs by employing the receding horizon approach. Our second CCQSP executive, dp-Sulu, is developed in two spirals. First, we develop the Market-based Iterative Risk Allocation (MIRA) algorithm, which can control a multi-agent system in a distributed manner by optimally distributing risk among agents through the market-based method called tatonnement. Second and finally, we integrate the capability of MIRA into p-Sulu to build the robust model-based executive, dp-Sulu, which can execute CCQSPs on multi-agent systems in a distributed manner. Our simulation results demonstrate that our executives can efficiently execute CCQSP planning problems with significantly reduced suboptimality compared to prior art.",,283 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,Brian Williams; Model-based Embedded and Robotic Systems,,,,,,,2018-01-31T00:01:14Z,,,,,,,,,,,,,,,
Li-Shiuan Peh,"Sun, Chen; Chen, Chia-Hsin Owen; Kurian, George; Wei, Lan; Miller, Jason; Agarwal, Anant; Peh, Li-Shiuan; Stojanovic, Vladimir",2012-02-08T20:15:04Z,2012-02-08T20:15:04Z,2012-02-08,http://hdl.handle.net/1721.1/69050,MIT-CSAIL-TR-2012-004,DSENT - A Tool Connecting Emerging Photonics with Electronics for Opto-Electronic Networks-on-Chip Modeling,"With the advent of many-core chips that place substantial demand on the NoC, photonics has been investigated as a promising alternative to electrical NoCs. While numerous opto-electronic NoCs have been proposed, their evaluations tend to be based on fixed numbers for both photonic and electrical components, making it difficult to co-optimize. Through our own forays into opto-electronic NoC design, we observe that photonics and electronics are very much intertwined, reflecting a strong need for a NoC modeling tool that accurately models parameterized electronic and photonic components within a unified framework, capturing their interactions faithfully. In this paper, we present a tool, DSENT, for design space exploration of electrical and opto-electrical networks. We form a framework that constructs basic NoC building blocks from electrical and photonic technology parameters. To demonstrate potential use cases, we perform a network case study illustrating data-rate tradeoffs, a comparison with scaled electrical technology, and sensitivity to photonics parameters.",,8 p.,,,,Computer Architecture,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
,"Rinard, Martin",2012-02-23T20:30:03Z,2012-02-23T20:30:03Z,2012-02-23,http://hdl.handle.net/1721.1/69177,MIT-CSAIL-TR-2012-005,"A Lossy, Synchronization-Free, Race-Full, But Still Acceptably Accurate Parallel Space-Subdivision Tree Construction Algorithm","We present a new synchronization-free space-subdivision tree construction algorithm. Despite data races, this algorithm produces trees that are consistent enough for the client Barnes-Hut center of mass and force computation phases to use successfully. Our performance results show that eliminating synchronization improves the performance of the parallel algorithm by approximately 20%. End-to-end accuracy results show that the resulting partial data structure corruption has a neglible effect on the overall accuracy of the Barnes-Hut N-body simulation. We note that many data structure manipulation algorithms use many of the same basic operations (linked data structure updates and array insertions) as our tree construction algorithm. We therefore anticipate that the basic principles the we develop in this paper may effectively guide future efforts in this area.",,12 p.,,,Data Structure Repair; Data Races; Acceptability; Parallel Computing; Synchronization,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nickolai Zeldovich,"Popa, Raluca Ada; Zeldovich, Nickolai",2012-03-26T18:00:05Z,2012-03-26T18:00:05Z,2012-03-25,http://hdl.handle.net/1721.1/69859,MIT-CSAIL-TR-2012-006,Cryptographic Treatment of CryptDB's Adjustable Join,"In this document, we provide a cryptographic treatment of the adjustable join protocol from CryptDB. We also discuss how our scheme could be used outside of CryptDB because it provides a simple functionality that may be needed in other settings. Intuitively, it is a pseudorandom permutation where an external party not knowing the secret key can nonetheless adjust a ciphertext under one key to a ciphertext under a different key, given an adjustment token from a party that knows the secret key.",,14 p.,,,SQL queries; encrypted database,Parallel and Distributed Operating Systems,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Ron Weiss,"Beal, Jacob; Weiss, Ron; Yaman, Fusun; Davidsohn, Noah; Adler, Aaron",2012-04-09T17:15:07Z,2012-04-09T17:15:07Z,2012-04-07,http://hdl.handle.net/1721.1/69973,MIT-CSAIL-TR-2012-008,"A Method for Fast, High-Precision Characterization of Synthetic Biology Devices","Engineering biological systems with predictable behavior is a foundational goal of synthetic biology. To accomplish this, it is important to accurately characterize the behavior of biological devices. Prior characterization efforts, however, have generally not yielded enough high-quality information to enable compositional design. In the TASBE (A Tool-Chain to Accelerate Synthetic Biological Engineering) project we have developed a new characterization technique capable of producing such data. This document describes the techniques we have developed, along with examples of their application, so that the techniques can be accurately used by others.",,25 p.,,,Synthetic biology; characterization; TASBE,Synthetic Biology,,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Tomaso Poggio,"Isik, Leyla; Meyers, Ethan M.; Leibo, Joel Z.; Poggio, Tomaso",2012-04-26T18:15:04Z,2012-04-26T18:15:04Z,2012-04-20,http://hdl.handle.net/1721.1/70170,MIT-CSAIL-TR-2012-010; CBCL-307,Preliminary MEG decoding results,"Decoding analysis has been applied to electrophysiology and fMRI data to study the visual system, however, this method has only been applied to MEG visual data in a few instances. Here we use the Neural Decoding Toolbox for Matlab to show that it is possible to decode visual stimuli based on MEG data.",,5 p.,,,Vision; Decoding; MEG,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Silvio Micali,"Azar, Pablo; Micali, Silvio",2012-05-09T22:45:07Z,2012-05-09T22:45:07Z,2012-05-08,http://hdl.handle.net/1721.1/70556,MIT-CSAIL-TR-2012-011,Optimal Parametric Auctions,"We study the problem of profit maximization in auctions of one good where the buyers' valuations are drawn from independent distributions. When these distributions are known to the seller, Myerson's optimal auction is a well-known mechanism for maximizing revenue. In many cases, however, the seller may not know the buyers' distributions. We propose an alternative model where the seller only knows the mean and the variance of each distribution. We call parametric an auction whose mechanism only uses these parameters. We construct parametric auctions both when the seller only has one copy of the good to sell, and when she has an infinite number of identical copies (i.e., when the good is digital). For a very large class of distributions, including (but not limited to) distributions with a monotone hazard rate, our auctions achieve a constant fraction of the revenue of Myerson's auction. When the seller has absolutely no knowledge about the distributions, it is well known that no auction can achieve a constant fraction of the optimal revenue when the players are not identically distributed. Our parametric model gives the seller a small amount of extra information, allowing her to construct auctions for which (1) no two bidders need to be drawn from identical distributions and (2) the revenue obtained is a constant fraction of the revenue in Myerson's optimal auction.",,18 p.,,,,,,,,,,,,,,,,,,,,,,,,Theory of Computation,,,,,,,,
Srini Devadas,"Kurian, George; Khan, Omer; Devadas, Srinivas",2012-05-22T20:15:03Z,2012-05-22T20:15:03Z,2012-05-22,http://hdl.handle.net/1721.1/70909,MIT-CSAIL-TR-2012-012,A Case for Fine-Grain Adaptive Cache Coherence,"As transistor density continues to grow geometrically, processor manufacturers are already able to place a hundred cores on a chip (e.g., Tilera TILE-Gx 100), with massive multicore chips on the horizon. Programmers now need to invest more effort in designing software capable of exploiting multicore parallelism. The shared memory paradigm provides a convenient layer of abstraction to the programmer, but will current memory architectures scale to hundreds of cores? This paper directly addresses the question of how to enable scalable memory systems for future multicores. We develop a scalable, efficient shared memory architecture that enables seamless adaptation between private and logically shared caching at the fine granularity of cache lines. Our data-centric approach relies on in hardware runtime profiling of the locality of each cache line and only allows private caching for data blocks with high spatio-temporal locality. This allows us to better exploit on-chip cache capacity and enable low-latency memory access in large-scale multicores.",,11 p.,,,,Computation Structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Johannsson, Hordur; Kaess, Michael; Fallon, Maurice; Leonard, John J.",2012-05-25T19:15:05Z,2012-05-25T19:15:05Z,2012-05-25,http://hdl.handle.net/1721.1/70952,MIT-CSAIL-TR-2012-013,Temporally Scalable Visual SLAM using a Reduced Pose Graph,"In this paper, we demonstrate a system for temporally scalable visual SLAM using a reduced pose graph representation. Unlike previous visual SLAM approaches that use keyframes, our approach continually uses new measurements to improve the map, yet achieves efficiency by avoiding adding redundant frames and not using marginalization to reduce the graph. To evaluate our approach, we present results using an online binocular visual SLAM system that uses place recognition for both robustness and multi-session operation. To allow large-scale indoor mapping, our system automatically handles elevator rides based on accelerometer data. We demonstrate long-term mapping in a large multi-floor building, using approximately nine hours of data collected over the course of six months. Our results illustrate the capability of our visual SLAM system to scale in size with the area of exploration instead of the time of exploration.",,9 p.,,,robotics navigation,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Poggio, Tomaso",2012-05-31T20:15:04Z,2012-05-31T20:15:04Z,2012-05-31,http://hdl.handle.net/1721.1/70970,MIT-CSAIL-TR-2012-014; CBCL-308,"The Levels of Understanding framework, revised","I discuss the ""levels of understanding"" framework described in Marr's Vision and propose a revised and updated version of it to capture the changes in computation and neuroscience over the last 30 years.",,9 p.,,,"artificial intelligence, computer vision",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Silvio Micali,"Azar, Pablo Daniel; Micali, Silvio",2012-06-18T19:30:03Z,2012-06-18T19:30:03Z,2012-06-14,http://hdl.handle.net/1721.1/71170,MIT-CSAIL-TR-2012-015,Optimal Parametric Auctions,"We study the problem of an auctioneer who wants to maximize her profits. In our model, there are n buyers with private valuations drawn from independent distributions F_1,...,F_n. When these distributions are known to the seller, Myerson's optimal auction is a well known mechanism that maximizes revenue. However, in many cases it is too strong to assume that the seller knows these distributions. We propose an alternative model where the seller only knows the mean mu_i and variance sigma_i^2 of each distribution F_i. We call mechanisms that only use this information parametric auctions. We construct such auctions for all single-dimensional downward closed environments. For a very large class of distributions, including (but not limited to) distributions with a monotone hazard rate, our auctions achieve a constant fraction of the revenue of Myerson's auction. When the seller has absolutely no knowledge about the distributions, it is well known that no auction can achieve a constant fraction of the optimal revenue when the players are not identically distributed. Our parametric model gives the seller a small amount of extra information, allowing her to construct auctions for which (1) she does not know the full distribution of valuations, (2) no two bidders need to be drawn from identical distributions and (3) the revenue obtained is a constant fraction of the revenue in Myerson's optimal auction. For digital goods environments we present a different parametric auction that not only gives a better approximation to the optimal auction, but that is also optimal in a new sense, which we call maximin optimality. Informally, an auction is maximin optimal if it maximizes revenue in the worst case over an adversary's choice of the distribution. We show that our digital parametric is maximin optimal among the class of posted price mechanisms.",,19 p.,,,Auctions; Optimization,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",2012-06-27T20:15:03Z,2012-06-27T20:15:03Z,2012-06-22,http://hdl.handle.net/1721.1/71232,MIT-CSAIL-TR-2012-017,Epistemic Implementation and The Arbitrary-Belief Auction,"In settings of incomplete information we put forward an epistemic framework for designing mechanisms that successfully leverage the players' arbitrary higher-order beliefs, even when such beliefs are totally wrong, and even when the players are rational in a very weak sense. Following Aumann (1995), we consider a player i rational if he uses a pure strategy s_i such that no alternative pure strategy s_i' performs better than s_i in every world i considers possible, and consider him order-k rational if he is rational and believes that all other players are order-(k-1) rational. We then introduce an iterative deletion procedure of dominated strategies and use it to precisely characterize the strategies consistent with the players being order-k rational. We exemplify the power of our framework in single-good auctions by introducing and achieving a new class of revenue benchmarks, defined over the players' arbitrary beliefs, that can be much higher than classical ones, and are unattainable by traditional mechanisms. Namely, we exhibit a mechanism that, for every k greater than or equal to 0 and epsilon>0 and whenever the players are order-(k+1) rational, guarantees revenue greater than or equal to G^k-epsilon, where G^k is the second highest belief about belief about ... (k times) about the highest valuation of some player, even when such a player's identity is not precisely known. Importantly, our mechanism is possibilistic interim individually rational. Essentially this means that, based on his beliefs, a player's utility is non-negative not in expectation, but in each world he believes possible. We finally show that our benchmark G^k is so demanding that it separates the revenue achievable with order-k rational players from that achievable with order-(k+1) rational ones. That is, no possibilistic interim individually rational mechanism can guarantee revenue greater than or equal to G^k-c, for any constant c>0, when the players are only order-k rational.",,31 p.,,,higher-order beliefs; higher-order rationality; revenue,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Leslie Kaelbling; Tomas Lozano-Perez,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2012-07-02T17:15:07Z,2012-07-02T17:15:07Z,2012-06-29,http://hdl.handle.net/1721.1/71521,MIT-CSAIL-TR-2012-018,Integrated Robot Task and Motion Planning in the Now,"This paper provides an approach to integrating geometric motion planning with logical task planning for long-horizon tasks in domains with many objects. We propose a tight integration between the logical and geometric aspects of planning. We use a logical representation which includes entities that refer to poses, grasps, paths and regions, without the need for a priori discretization. Given this representation and some simple mechanisms for geometric inference, we characterize the pre-conditions and effects of robot actions in terms of these logical entities. We then reason about the interaction of the geometric and non-geometric aspects of our domains using the general-purpose mechanism of goal regression (also known as pre-image backchaining). We propose an aggressive mechanism for temporal hierarchical decomposition, which postpones the pre-conditions of actions to create an abstraction hierarchy that both limits the lengths of plans that need to be generated and limits the set of objects relevant to each plan. We describe an implementation of this planning method and demonstrate it in a simulated kitchen environment in which it solves problems that require approximately 100 individual pick or place operations for moving multiple objects in a complex domain.",,68 p.,,,robot manipulation; motion planning,Learning and Intelligent Systems,"This work was supported in part by the NSF under Grant No. 1117325. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We also gratefully acknowledge support from ONR MURI grant N00014-09-1-1051, from AFOSR grant AOARD-104135 and from
the Singapore Ministry of Education under a grant to the Singapore-MIT International Design Center. We thank Willow Garage for the use of the PR2 robot as part of the PR2 Beta Program.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Leslie Kaelbling; Tomas Lozano-Perez,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2012-07-03T18:00:04Z,2012-07-03T18:00:04Z,2012-07-03,http://hdl.handle.net/1721.1/71529,MIT-CSAIL-TR-2012-019,Integrated robot task and motion planning in belief space,"In this paper, we describe an integrated strategy for planning, perception, state-estimation and action in complex mobile manipulation domains. The strategy is based on planning in the belief space of probability distribution over states. Our planning approach is based on hierarchical goal regression (pre-image back-chaining). We develop a vocabulary of fluents that describe sets of belief states, which are goals and subgoals in the planning process. We show that a relatively small set of symbolic operators lead to task-oriented perception in support of the manipulation goals. An implementation of this method is demonstrated in simulation and on a real PR2 robot, showing robust, flexible solution of mobile manipulation problems with multiple objects and substantial uncertainty.",,80 p.,,,,Learning and Intelligent Systems,"This work was supported in part by the NSF under Grant No. 1117325. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We also gratefully acknowledge support from ONR MURI grant N00014-09-1-1051, from AFOSR grant AOARD-104135 and from the Singapore Ministry of Education under a grant to the Singapore-MIT International Design Center. We thank Willow Garage for the use of the PR2 robot as part of the PR2 Beta Program.",Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Whelan, Thomas; Kaess, Michael; Fallon, Maurice; Johannsson, Hordur; Leonard, John; McDonald, John",2012-07-23T17:45:03Z,2012-07-23T17:45:03Z,2012-07-19,http://hdl.handle.net/1721.1/71756,MIT-CSAIL-TR-2012-020,Kintinuous: Spatially Extended KinectFusion,"In this paper we present an extension to the KinectFusion algorithm that permits dense mesh-based mapping of extended scale environments in real-time. This is achieved through (i) altering the original algorithm such that the region of space being mapped by the KinectFusion algorithm can vary dynamically, (ii) extracting a dense point cloud from the regions that leave the KinectFusion volume due to this variation, and, (iii) incrementally adding the resulting points to a triangular mesh representation of the environment. The system is implemented as a set of hierarchical multi-threaded components which are capable of operating in real-time. The architecture facilitates the creation and integration of new modules with minimal impact on the performance on the dense volume tracking and surface reconstruction modules. We provide experimental results demonstrating the system's ability to map areas considerably beyond the scale of the original KinectFusion algorithm including a two story apartment and an extended sequence taken from a car at night. In order to overcome failure of the iterative closest point (ICP) based odometry in areas of low geometric features we have evaluated the Fast Odometry from Vision (FOVIS) system as an alternative. We provide a comparison between the two approaches where we show a trade off between the reduced drift of the visual odometry approach and the higher local mesh quality of the ICP-based approach. Finally we present ongoing work on incorporating full simultaneous localisation and mapping (SLAM) pose-graph optimisation.",,8 p.,,,,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Barbara Liskov,"Liskov, Barbara; Cowling, James",2012-07-23T19:15:02Z,2012-07-23T19:15:02Z,2012-07-23,http://hdl.handle.net/1721.1/71763,MIT-CSAIL-TR-2012-021,Viewstamped Replication Revisited,"This paper presents an updated version of Viewstamped Replication, a replication technique that handles failures in which nodes crash. It describes how client requests are handled, how the group reorganizes when a replica fails, and how a failed replica is able to rejoin the group. The paper also describes a number of important optimizations and presents a protocol for handling reconfigurations that can change both the group membership and the number of failures the group is able to handle.",,14 p.,,,VR; state machine replication; consensus; fault-tolerance; agreement,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",2012-08-01T21:30:09Z,2012-08-01T21:30:09Z,2012-07-31,http://hdl.handle.net/1721.1/71953,MIT-CSAIL-TR-2012-023,"The Order Independence of Iterated Dominance in Extensive Games, with Connections to Mechanism Design and Backward Induction","Shimoji and Watson (1998) prove that a strategy of an extensive game is rationalizable in the sense of Pearce if and only if it survives the maximal elimination of conditionally dominated strategies. Briefly, this process iteratively eliminates conditionally dominated strategies according to a specific order, which is also the start of an order of elimination of weakly dominated strategies. Since the final set of possible payoff profiles, or terminal nodes, surviving iterated elimination of weakly dominated strategies may be order-dependent, one may suspect that the same holds for conditional dominance. We prove that, although the sets of strategy profiles surviving two arbitrary elimination orders of conditional dominance may be very different from each other, they are equivalent in the following sense: for each player i and each pair of elimination orders, there exists a function phi_i mapping each strategy of i surviving the first order to a strategy of i surviving the second order, such that, for every strategy profile s surviving the first order, the profile (phi_i(s_i))_i induces the same terminal node as s does. To prove our results we put forward a new notion of dominance and an elementary characterization of extensive-form rationalizability (EFR) that may be of independent interest. We also establish connections between EFR and other existing iterated dominance procedures, using our notion of dominance and our characterization of EFR.",,35 p.,,,extensive-form rationalizability; distinguishable dominance; order independence,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Armando Solar-Lezama,"Cheung, Alvin; Solar-Lezama, Armando; Madden, Samuel",2012-08-13T21:15:04Z,2012-08-13T21:15:04Z,2012-08-13,http://hdl.handle.net/1721.1/72106,MIT-CSAIL-TR-2012-025,Using Program Synthesis for Social Recommendations,"This paper presents a new approach to select events of interest to a user in a social media setting where events are generated by the activities of the user's friends through their mobile devices. We argue that given the unique requirements of the social media setting, the problem is best viewed as an inductive learning problem, where the goal is to first generalize from the users' expressed ""likes"" and ""dislikes"" of specific events, then to produce a program that can be manipulated by the system and distributed to the collection devices to collect only data of interest. The key contribution of this paper is a new algorithm that combines existing machine learning techniques with new program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application. The approach also improves on standard machine learning techniques in that it produces clear programs that can be manipulated to optimize data collection and filtering.",,10 p.,,,recommender systems; social networking applications; support vector machines,Computer-Aided Programming,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,
Brian Williams,"Dong, Shuonan",2018-01-30T23:46:40Z,2018-01-30T23:46:40Z,2012-08-23,http://hdl.handle.net/1721.1/113367,MIT-CSAIL-TR-2018-007,Learning and recognition of hybrid manipulation tasks in variable environments using probabilistic flow tubes,"Robots can act as proxies for human operators in environments where a human operator is not present or cannot directly perform a task, such as in dangerous or remote situations. Teleoperation is a common interface for controlling robots that are designed to be human proxies. Unfortunately, teleoperation may fail to preserve the natural fluidity of human motions due to interface limitations such as communication delays, non-immersive sensing, and controller uncertainty. I envision a robot that can learn a set of motions that a teleoperator commonly performs, so that it can autonomously execute routine tasks or recognize a user's motion in real time. Tasks can be either primitive activities or compound plans. During online operation, the robot can recognize a user's teleoperated motions on the fly and offer real-time assistance, for example, by autonomously executing the remainder of the task. I realize this vision by addressing three main problems: (1) learning primitive activities by identifying significant features of the example motions and generalizing the behaviors from user demonstration trajectories; (2) recognizing activities in real time by determining the likelihood that a user is currently executing one of several learned activities; and (3) learning complex plans by generalizing a sequence of activities, through auto-segmentation and incremental learning of previously unknown activities. To solve these problems, I first present an approach to learning activities from human demonstration that (1) provides flexibility and robustness when encoding a user's demonstrated motions by using a novel representation called a probabilistic flow tube, and (2) automatically determines the relevant features of a motion so that they can be preserved during autonomous execution in new situations. I next introduce an approach to real-time motion recognition that (1) uses temporal information to successfully model motions that may be non-Markovian, (2) provides fast real-time recognition of motions in progress by using an incremental temporal alignment approach, and (3) leverages the probabilistic flow tube representation to ensure robustness during recognition against varying environment states. Finally, I develop an approach to learn combinations of activities that (1) automatically determines where activities should be segmented in a sequence and (2) learns previously unknown activities on the fly. I demonstrate the results of autonomously executing motions learned by my approach on two different robotic platforms supporting user-teleoperated manipulation tasks in a variety of environments. I also present the results of real-time recognition in different scenarios, including a robotic hardware platform. Systematic testing in a two-dimensional environment shows up to a 27% improvement in activity recognition rates over prior art, while maintaining average computing times for incremental recognition of less than half of human reaction time.",,144 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,,,,,,,,2018-01-30T23:46:40Z,,,,,,,,,,,,,,,
Nancy Lynch,"Musial, Peter M.",2012-09-05T22:00:14Z,2012-09-05T22:00:14Z,2012-08-27,http://hdl.handle.net/1721.1/72537,MIT-CSAIL-TR-2012-027,From Formal Methods to Executable Code,"The objective of this work is the derivation of software that is verifiably correct. Our approach is to abstract system specifications and model these in a formal framework called Timed Input/Output Automata, which provides a notation for expressing distributed systems and mathematical support for reasoning about their properties. Although formal reasoning is easier at an abstract level, it is not clear how to transform these abstractions into executable code. During system implementation, when an abstract system specification is left up to human interpretation, then this opens a possibility of undesirable behaviors being introduced into the final code, thereby nullifying all formal efforts. This manuscript addresses this issue and presents a set of transformation methods for systems described as a network to timed automata into Java code for distributed platforms. We prove that the presented transformation methods preserve guarantees of the source specifications, and therefore, result in code that is correct by construction.",,92 p.,,,,Theory of Computation,,,,Note: the cover page of this report shows an incorrect title.  The title given on the first page of the document itself is correct.,,,,,,,,,,,,,,,,,,,,,,,,
Nancy Lynch,"Censor-Hillel, Keren; Haeupler, Bernhard; Lynch, Nancy; Medard, Muriel",2012-09-05T22:00:07Z,2012-09-05T22:00:07Z,2012-08-27,http://hdl.handle.net/1721.1/72536,MIT-CSAIL-TR-2012-026,Bounded-Contention Coding for Wireless Networks in the High SNR Regime,"Efficient communication in wireless networks is typically challenged by the possibility of interference among several transmitting nodes. Much important research has been invested in decreasing the number of collisions in order to obtain faster algorithms for communication in such networks. This paper proposes a novel approach for wireless communication, which embraces collisions rather than avoiding them, over an additive channel. It introduces a coding technique called Bounded-Contention Coding (BCC) that allows collisions to be successfully decoded by the receiving nodes into the original transmissions and whose complexity depends on a bound on the contention among the transmitters. BCC enables deterministic local broadcast in a network with n nodes and at most a transmitters with information of L bits each within O(a log n + aL) bits of communication with full-duplex radios, and O((a log n + aL)(log n)) bits, with high probability, with half-duplex radios. When combined with random linear network coding, BCC gives global broadcast within O((D + a + log n)(a log n + L)) bits, with high probability. This also holds in dynamic networks that can change arbitrarily over time by a worst-case adversary. When no bound on the contention is given, it is shown how to probabilistically estimate it and obtain global broadcast that is adaptive to the true contention in the network.",,17 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silvio Micali,"Chiesa, Alessandro; Micali, Silvio; Zhu, Zeyuan Allen",2012-09-07T22:15:03Z,2012-09-07T22:15:03Z,2012-09-07,http://hdl.handle.net/1721.1/72584,MIT-CSAIL-TR-2012-028,A Social-Welfare Optimal Probabilistic Mechanism for Knightian Single-Good Auctions,"We provide an optimal probabilistic mechanism for maximizing social welfare in single-good auctions when each player does not know his true valuation for the good, but only a set of valuations that is guaranteed to include his true one.",,19 p.,,,Knightian Auctions; Probabilistic Mechanisms; Social Welfare,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Little, Anna V.; Maggioni, Mauro; Rosasco, Lorenzo",2012-09-10T18:00:08Z,2012-09-10T18:00:08Z,2012-09-08,http://hdl.handle.net/1721.1/72597,MIT-CSAIL-TR-2012-029; CBCL-310,"Multiscale Geometric Methods for Data Sets I: Multiscale SVD, Noise and Curvature","Large data sets are often modeled as being noisy samples from probability distributions in R^D, with D large. It has been noticed that oftentimes the support M of these probability distributions seems to be well-approximated by low-dimensional sets, perhaps even by manifolds. We shall consider sets that are locally well approximated by k-dimensional planes, with k << D, with k-dimensional manifolds isometrically embedded in R^D being a special case. Samples from this distribution; are furthermore corrupted by D-dimensional noise. Certain tools from multiscale geometric measure theory and harmonic analysis seem well-suited to be adapted to the study of samples from such probability distributions, in order to yield quantitative geometric information about them. In this paper we introduce and study multiscale covariance matrices, i.e. covariances corresponding to the distribution restricted to a ball of radius r, with a fixed center and varying r, and under rather general geometric assumptions we study how their empirical, noisy counterparts behave. We prove that in the range of scales where these covariance matrices are most informative, the empirical, noisy covariances are close to their expected, noiseless counterparts. In fact, this is true as soon as the number of samples in the balls where the covariance matrices are computed is linear in the intrinsic dimension of M. As an application, we present an algorithm for estimating the intrinsic dimension of M.",,59 p.,,,machine learning; high dimensional data,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Barbara Liskov,"Liskov, Barbara",2012-09-17T18:30:04Z,2012-09-17T18:30:04Z,2012-09-14,http://hdl.handle.net/1721.1/73017,MIT-CSAIL-TR-2012-030,Aeolus Reference Manual,This document describes the interface that the Aeolus information flow platform provides for users who are implementing applications using Java. The document explains how the Aeolus features are made available by means of a Java library.,,37 p.,,,information flow control; DIFC; data privacy,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Leonard,"Whelan, Thomas; Johannsson, Hordur; Kaess, Michael; Leonard, John J.; McDonald, John",2012-09-25T16:00:05Z,2012-09-25T16:00:05Z,2012-09-17,http://hdl.handle.net/1721.1/73167,MIT-CSAIL-TR-2012-031,Robust Tracking for Real-Time Dense RGB-D Mapping with Kintinuous,"This paper describes extensions to the Kintinuous algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused real-time surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.",,8 p.,,,,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Gharbi, Michael; Malisiewicz, Tomasz; Paris, Sylvain; Durand, Frédo",2012-10-09T16:45:04Z,2012-10-09T16:45:04Z,2012-10-01,http://hdl.handle.net/1721.1/73685,MIT-CSAIL-TR-2012-032,A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although exemplar-based SVM is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of- the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency.",,11 p.,,,"Image retrieval, object detection, computer vision, parametric model",Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Levine, Steven J.",2012-10-09T16:45:14Z,2012-10-09T16:45:14Z,2012-10-04,http://hdl.handle.net/1721.1/73686,MIT-CSAIL-TR-2012-033,Monitoring the Execution of Temporal Plans for Robotic Systems,"To achieve robustness in dynamic and uncertain environments, robotic systems must monitor the progress of their plans during execution. This thesis develops a plan executive called Pike that is capable of executing and monitoring plans. The execution monitor at its core quickly and efficiently detects relevant disturbances that threaten future actions in the plan. We present a set of novel offline algorithms that extract sets of candidate causal links from temporally-flexible plans. A second set of algorithms uses these causal links to monitor the execution online and detect problems with low latency. We additionally introduce the TBurton executive, a system capable of robustly meeting a user s high-level goals through the combined use of Pike and a temporal generative planner. An innovative voice-commanded robot is demonstrated in hardware and simulation that robustly meets high level goals and verbalizes any causes of failure using the execution monitor",,125 p.,,,temporal plan; execution monitoring; causal link; disturbance; executive,Model-based Embedded and Robotic Systems,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Levin, Anat; Nadler, Boaz; Durand, Fredo; Freeman, William T.",2012-07-31T17:45:08Z,2012-07-31T17:45:08Z,2012-10-07,http://hdl.handle.net/1721.1/71919,MIT-CSAIL-TR-2012-022,"Patch complexity, finite pixel correlations and optimal denoising","Image restoration tasks are ill-posed problems, typically solved withpriors. Since the optimal prior is the exact unknown density of natural images,actual priors are only approximate and typically restricted to small patches. Thisraises several questions: How much may we hope to improve current restorationresults with future sophisticated algorithms? And more fundamentally, even withperfect knowledge of natural image statistics, what is the inherent ambiguity ofthe problem? In addition, since most current methods are limited to finite supportpatches or kernels, what is the relation between the patch complexity of naturalimages, patch size, and restoration errors? Focusing on image denoising, we makeseveral contributions. First, in light of computational constraints, we study the relation between denoising gain and sample size requirements in a non parametricapproach. We present a law of diminishing return, namely that with increasingpatch size, rare patches not only require a much larger dataset, but also gain littlefrom it. This result suggests novel adaptive variable-sized patch schemes for denoising. Second, we study absolute denoising limits, regardless of the algorithmused, and the converge rate to them as a function of patch size. Scale invarianceof natural images plays a key role here and implies both a strictly positive lowerbound on denoising and a power law convergence. Extrapolating this parametriclaw gives a ballpark estimate of the best achievable denoising, suggesting thatsome improvement, although modest, is still possible.",,23 p.,,,,Computer Graphics,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,supplemental material for conference paper at ECCV 2012 (European Conf. on Computer Vision),,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Belcour, Laurent; Soler, Cyril; Subr, Kartic; Holzschuch, Nicolas; Durand, Fredo",2012-11-16T18:00:09Z,2012-11-16T18:00:09Z,2012-11-16,http://hdl.handle.net/1721.1/74662,MIT-CSAIL-TR-2012-034,5D Covariance Tracing for Efficient Defocus and Motion Blur,"The rendering of effects such as motion blur and depth-of-field requires costly 5D integrals. We dramatically accelerate their computation through adaptive sampling and reconstruction based on the prediction of the anisotropy and bandwidth of the integrand. For this, we develop a new frequency analysis of the 5D temporal light-field, and show that first-order motion can be handled through simple changes of coordinates in 5D. We further introduce a compact representation of the spectrum using the co- variance matrix and Gaussian approximations. We derive update equations for the 5 × 5 covariance matrices for each atomic light transport event, such as transport, occlusion, BRDF, texture, lens, and motion. The focus on atomic operations makes our work general, and removes the need for special-case formulas. We present a new rendering algorithm that computes 5D covariance matrices on the image plane by tracing paths through the scene, focusing on the single-bounce case. This allows us to reduce sampling rates when appropriate and perform reconstruction of images with complex depth-of-field and motion blur effects.",,19 p.,,,Rendering; Computer Graphics; Fourier,Computer Graphics,,,,,,Fredo Durand,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Poggio, Tomaso; Mutch, Jim; Leibo, Joel; Rosasco, Lorenzo; Tacchetti, Andrea",2013-01-10T21:15:06Z,2013-01-10T21:15:06Z,2012-12-29,http://hdl.handle.net/1721.1/76248,MIT-CSAIL-TR-2012-035,The computational magic of the ventral stream: sketch of a theory (and why some deep architectures work).,"This paper explores the theoretical consequences of a simple assumption: the computational goal of the feedforward path in the ventral stream -- from V1, V2, V4 and to IT -- is to discount image transformations, after learning them during development.",,120 p.,,,visual cortex; ventral stream; symmetries principles in sensory perception; visual recognition; invariances of sensory perception; learning to learn invariants in the visual world,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Wang, Jue; Hassanieh, Haitham; Katabi, Dina; Kohno, Tadayoshi",2013-01-14T17:00:03Z,2013-01-14T17:00:03Z,2013-01-12,http://hdl.handle.net/1721.1/76260,MIT-CSAIL-TR-2013-001,Securing Deployed RFIDs by Randomizing the Modulation and the Channel,"RFID cards are widely used today in sensitive applications such as access control, payment systems, and asset tracking. Past work shows that an eavesdropper snooping on the communication between a card and its legitimate reader can break their cryptographic protocol and obtain their secret keys. One solution for this problem is to install stronger cryptographic protocols on the cards. However, RFIDs' size, power, and cost limitations do not allow for conventional cryptographic protocols. Further, installing new protocols requires revoking billions of cards in consumers  hands and facilities worldwide, which is costly and impractical. In this paper, we ask whether one can secure RFIDs from such attacks without revoking or changing the insecure cards. We propose LocRF, a solution that changes the signal used to read the RFID cards but does not require any changes to the cards themselves. LocRF introduces a new approach that randomizes the modulation of the RFID signal as well as the wireless channel. This design protects RFIDs from eavesdroppers even if they use multi-antenna MIMO receivers. We built a prototype of LocRF on software-defined radios and used it to secure the communication of off-the-shelf cards. Both our analysis and empirical evaluation demonstrate theeffectiveness of LocRF.",,15 p.,,,,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brian Williams,"Yu, Peng",2018-01-31T00:01:21Z,2018-01-31T00:01:21Z,2013-01-25,http://hdl.handle.net/1721.1/113372,MIT-CSAIL-TR-2018-012,continuous Relaxation to Over-constrained Temporal Plans,"When humans fail to understand the capabilities of an autonomous system or its environmental limitations, they can jeopardize their objectives and the system by asking for unrealistic goals. The objective of this thesis is to enable consensus between human and autonomous system, by giving autonomous systems the ability to communicate to the user the reasons for goal failure and the relaxations to goals that archive feasibility. We represent our problem in the context of over-constrained temporal plans. They are commonly encountered while operating autonomous and decision support systems, when user objectives are in conflict with the environment. Over constrained plans are addressed by relaxing goals and or constraints, such as delaying the arrival time of a trip, with some candidate relaxations being preferable to others. In this thesis we present Uhura, a temporal plan diagnosis and relaxation algorithm that is designed to take over-constrained input plans with temporal flexibility and contingencies, and generate temporal relaxations that make the input plan executable. We introduce two innovative approaches within Uhura: collaborative plan diagnosis and continuous relaxation. Uhura focuses on novel ways of satisfying three goals to make the plan relaxation process more convenient for the users: small perturbation, quick response and simple interaction. We have incorporated Uhura within an autonomous executive that collaborates with human operators to resolve over-constrained temporal plans. Its effectiveness has been demonstrated both in simulation and in hardware on a Personal Transportation System concept. We believe that Uhura's collaborative temporal plan diagnosis capability can benefit a wide range of applications, both within industrial applications and in our daily lives.",,168 p.,,,Over-constrained temporal plan; Plan diagnosis; Mixed initiative planning,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-31T00:01:21Z,,,,,,,,,,,,,,,
Nancy Lynch,"Cornejo, Alejandro; Lynch, Nancy; Sastry, Srikanth",2013-02-01T20:00:04Z,2013-02-01T20:00:04Z,2013-01-30,http://hdl.handle.net/1721.1/76716,MIT-CSAIL-TR-2013-002,Asynchronous Failure Detectors,"Failure detectors -- oracles that provide information about process crashes -- are an important abstraction for crash tolerance in distributed systems. The generality of failure-detector theory, while providing great expressiveness, poses significant challenges in developing a robust hierarchy of failure detectors. We address some of these challenges by proposing (1) a variant of failure detectors called asynchronous failure detectors and (2) an associated modeling framework. Unlike the traditional failure-detector framework, our framework eschews real-time completely. We show that asynchronous failure detectors are sufficiently expressive to include several popular failure detectors including, but not limited to, the canonical Chandra-Toueg failure detectors, Sigma and other quorum failure detectors, Omega, anti-Omega, Omega^k, and Psi_k. Additionally, asynchronous failure detectors satisfy many desirable properties: they are self-implementable, guarantee that stronger asynchronous failure-detectors solve harder problems, and ensure that their outputs encode no information other than the set of crashed processes. We introduce the notion of a failure detector being representative for a problem to capture the idea that some problems encode the same information about process crashes as their weakest failure detectors do. We show that a large class of problems, called bounded problems, do not have representative failure detectors. Finally, we use the asynchronous failure-detector framework to show how sufficiently strong AFDs circumvent the impossibility of consensus in asynchronous systems.",,46 p.,,,"Asynchronous System, Fault-Tolerance, I/O Automata",Theory of Computation,,,,This report is superseded by MIT-CSAIL-TR-2013-025.,,,,,http://hdl.handle.net/1721.1/81371,,,,,,,,,,,,,,,,,,,
Brian Williams,"Wang, Andrew J.",2018-01-31T00:01:19Z,2018-01-31T00:01:19Z,2013-01-31,http://hdl.handle.net/1721.1/113371,MIT-CSAIL-TR-2018-011,Risk Allocation for Temporal Risk Assessment,"Temporal uncertainty arises when performing any activity in the natural world. When activities are composed into temporal plans, then, there is a risk of not meeting the plan requirements. Currently, we do not have quantitatively precise methods for assessing temporal risk of a plan. Existing methods that deal with temporal uncertainty either forgo probabilistic models or try to optimize a single objective, rather than satisfy multiple objectives. This thesis offers a method for evaluating whether a schedule exists that meets a set of temporal constraints, with acceptable risk of failure. Our key insight is to assume a form of risk allocation to each source of temporal uncertainty in our plan, such that we may reformulate the probabilistic plan into an STNU parameterized on the risk allocation. We show that the problem becomes a deterministic one of finding a risk allocation which implies a schedulable STNU within acceptable risk. By leveraging the principles behind STNU analysis, we derive conditions which encode this problem as a convex feasibility program over risk allocations. Furthermore, these conditions may be learned incrementally as temporal conflicts. Thus, to boost computational efficiency, we employ a generate-and-test approach to determine whether a schedule may be found.",,64 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,MEng thesis,,,,,,,,,2018-01-31T00:01:19Z,,,,,,,,,,,,,,,
Hari Balakrishnan,"LaCurts, Katrina; Deng, Shuo; Balakrishnan, Hari",2013-02-28T17:15:10Z,2013-02-28T17:15:10Z,2013-02-12,http://hdl.handle.net/1721.1/77238,MIT-CSAIL-TR-2013-003,A Plan for Optimizing Network-Intensive Cloud Applications,"A significant and growing number of applications deployed on cloud infrastructures are network-intensive. These applications are frequently bottlenecked by the speed of network connections between the machines on which they are deployed. Due to the complexity and size of cloud networks, such applications often run slowly or have unpredictable completion times and/or throughput, both of which can result in increased cost to the customer. In this paper, we argue that cloud customers should be able to express the demands and objectives of their applications. We outline an architecture that allows for this type of expression, and distributes applications within the cloud network such that the application's objectives are met. We discuss some of the key questions that need to be addressed to implement the architecture, as well as the interactions between optimizations done by clients and by cloud providers. We also present preliminary results that indicate that these types of systems are feasible and improve performance.",,7 p.,,,,Networks & Mobile Systems,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Tomaso Poggio,"Tan, Cheston; Poggio, Tomaso",2013-03-18T22:45:05Z,2013-03-18T22:45:05Z,2013-03-18,http://hdl.handle.net/1721.1/77936,MIT-CSAIL-TR-2013-004; CBCL-311,"Faces as a ""Model Category"" for Visual Object Recognition","Visual recognition is an important ability that is central to many everyday tasks such as reading, navigation and social interaction, and is therefore actively studied in neuroscience, cognitive psychology and artificial intelligence. There exist thousands of object categories, all of which pose similar challenges to biological and artificial visual systems: accurate recognition under varying location, scale, view angle, illumination and clutter. In many areas of science, important discoveries have been made using ""model organisms"" such as fruit flies, mice and macaques. For the thousands of object categories, the important and well-studied category of faces could potentially serve as a ""model category"" upon which efforts are focused, and from which fundamental insights are drawn. However, it has been hotly debated whether faces are processed by the brain in a manner fundamentally different from other categories. Here we show that ""neural tuning size"" -- a single parameter in a computational model of object processing -- is able to account for important face-specific phenomena. Thus, surprisingly, ""face-like"" processing is explainable by physiological mechanisms that differ only quantitatively from ""object-like"" processing. Our computational proof-of-principle provides specific neural tuning properties that correspond to the so-far qualitative and controversial notion of ""holistic"" face processing. Overall, faces may be a viable model category. Since faces are highly amenable to complementary experimental techniques like functional MRI, electrophysiology, electroencephalography and transcranial magnetic stimulation, this further raises the odds that the algorithms and neural circuits underlying visual recognition may first be solved for faces. With faces serving as a model category, the great scientific challenge of understanding and reverse-engineering general visual recognition can be greatly accelerated.",,19 p.,,,face recognition; object recognition; holistic processing; composite effect; inversion effect,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leslie Kaelbling,"Glover, Jared; Kaelbling, Leslie Pack",2013-03-29T21:30:06Z,2013-03-29T21:30:06Z,2013-03-27,http://hdl.handle.net/1721.1/78248,MIT-CSAIL-TR-2013-005,Tracking 3-D Rotations with the Quaternion Bingham Filter,"A deterministic method for sequential estimation of 3-D rotations is presented. The Bingham distribution is used to represent uncertainty directly on the unit quaternion hypersphere. Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions. Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic. We present two versions of the QBF, suitable for tracking the state of first- and second-order rotating dynamical systems.",,13 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,
Joshua Tenenbaum,"Wingate, David; Diuk, Carlos; O'Donnell, Timothy; Tenenbaum, Joshua; Gershman, Samuel",2013-04-18T00:45:04Z,2013-04-18T00:45:04Z,2013-04-12,http://hdl.handle.net/1721.1/78573,MIT-CSAIL-TR-2013-007,Compositional Policy Priors,"This paper describes a probabilistic framework for incorporating structured inductive biases into reinforcement learning. These inductive biases arise from policy priors, probability distributions over optimal policies. Borrowing recent ideas from computational linguistics and Bayesian nonparametrics, we define several families of policy priors that express compositional, abstract structure in a domain. Compositionality is expressed using probabilistic context-free grammars, enabling a compact representation of hierarchically organized sub-tasks. Useful sequences of sub-tasks can be cached and reused by extending the grammars nonparametrically using Fragment Grammars. We present Monte Carlo methods for performing inference, and show how structured policy priors lead to substantially faster learning in complex domains compared to methods without inductive biases.",,17 p.,,,,Computational Cognitive Science,"This work was supported by AFOSR FA9550-07-1-0075 and ONR
N00014-07-1-0937. SJG was supported by a Graduate Research Fellowship from the NSF.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fredo Durand,"Levin, Anat; Glasner, Daniel; Xiong, Ying; Durand, Fredo; Freeman, William; Matusik, Wojciech; Zickler, Todd",2013-04-24T18:00:04Z,2013-04-24T18:00:04Z,2013-04-24,http://hdl.handle.net/1721.1/78590,MIT-CSAIL-TR-2013-008,High Spatial Resolution BRDFs with Metallic powders Using Wave Optics Analysis,"This manuscript completes the analysis of our SIGGRAPH 2013 paper ""Fabricating BRDFs at High Spatial Resolution Using Wave Optics"" in which photolithography fabrication was used for manipulating reflectance effects. While photolithography allows for precise reflectance control, it is costly to fabricate. Here we explore an inexpensive alternative to micro-fabrication, in the form of metallic powders. Such powders are readily available at a variety of particle sizes and morphologies. Using an analysis similar to the micro-fabrication paper, we provide guidelines for the relation between the particles' shape and size and the reflectance functions they can produce.",,4 p.,,,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dina Katabi,"Hassanieh, Haitham; Shi, Lixin; Abari, Omid; Hamed, Ezzeldine; Katabi, Dina",2013-06-03T23:45:02Z,2013-06-03T23:45:02Z,2013-05-22,http://hdl.handle.net/1721.1/79058,MIT-CSAIL-TR-2013-009,BigBand: GHz-Wide Sensing and Decoding on Commodity Radios,"The goal of this paper is to make sensing and decoding GHz of spectrum simple, cheap, and low power. Our thesis is simple: if we can build a technology that captures GHz of spectrum using commodity Wi-Fi radios, it will have the right cost and power budget to enable a variety of new applications such as GHz-widedynamic access and concurrent decoding of diverse technologies. This vision will change today s situation where only expensive power-hungry spectrum analyzers can capture GHz-wide spectrum. Towards this goal, the paper harnesses the sparse Fourier transform to compute the frequency representation of a sparse signal without sampling it at full bandwidth. The paper makes the following contributions. First, it presents BigBand, a receiver that can sense and decode a sparse spectrum wider than its own digital bandwidth. Second, it builds a prototype of its design using 3 USRPs that each samples the spectrum at 50 MHz, producing a device that captures 0.9 GHz -- i.e., 6x larger bandwidth than the three USRPs combined. Finally, it extends its algorithm to enable spectrum sensing in scenarios where the spectrum is not sparse.",,13 p.,,,Spectrum Sensing; Sparse Fourier Transform; Wireless; ADC; Software Radios,Networks & Mobile Systems,,,,,,,,,,,,,2013-06-03T23:45:03Z,,,,,,,,,,,,,,,
Gerald Sussman,"Evans, Isaac; Lynch, Joseph",2013-06-03T23:30:05Z,2013-06-03T23:30:05Z,2013-05-24,http://hdl.handle.net/1721.1/79057,MIT-CSAIL-TR-2013-010,Organon: A Symbolic Constraint Framework & Solver,"Organon is an open source system for expressing and solving complex symbolic constraints between generic entities. Our design avoids restricting the programmer s ability to phrase constraints; Organon acts purely as a framework that defines and holds together the key concepts of forms, constraints, and solvers. It has three main components: (1) Forms: Abstract representations of the entities to be constrained. (2) Constraints: Functions that symbolically express requirements on the relationships between forms as well as provide information a solver can use to improve the constraint s satisfaction. (3) Solvers: Functions which inspect instantiations of forms and manipulate them in an attempt to satisfy a set of objective constraints.",,33 p.,,,scheme; propagator; exponential solver; annealing solver,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,2013-06-03T23:30:06Z,,,,,,,,,,,,,,,
Karen Sollins,"Simosa, Jorge D.",2013-06-04T20:15:04Z,2013-06-04T20:15:04Z,2013-06-04,http://hdl.handle.net/1721.1/79060,MIT-CSAIL-TR-2013-011,A Publish-Subscribe Implementation of Network Management,"As modern networks become highly integrated, heterogeneous, and experience exponential growth, the task of network management becomes increasingly unmanageable for network administrators and designers. The Knowledge Plane (KP) is designed to support a self-managing network, given the organizational constraints of network management, as well as to create synergy and exploit commonality among network applications. In this thesis, to build an Information Plane that is suitable to the requirements of the KP, we propose a publish/subscribe system that provides a clear and systematic framework for resolving tussles in the network. To evaluate the effectiveness of this design, we configured a network of PlanetLab nodes and conducted experiments involving a variety of file sizes and source-destination pairs. The results suggest that the system's performance is not only comparable to existing file transfer services, but that the system also introduces several performance gains that are unattainable with current network architectures.",,77 p.,,,,Advanced Network Architecture,,Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,,,,,,2013-06-04T20:15:04Z,,,,,,,,,,,,,,,
Brian Williams,"Shroff, Ameya",2013-06-06T22:30:03Z,2013-06-06T22:30:03Z,2013-06-06,http://hdl.handle.net/1721.1/79078,MIT-CSAIL-TR-2013-012,Reactive Integrated Motion Planning and Execution Using Chekhov,"We envision a world in which robots and humans can collaborate to perform complex tasks in real-world environments. Current motion planners successfully generate trajectories for a robot with multiple degrees of freedom, in a cluttered environment, and ensure that the robot can achieve its goal while avoiding all the obstacles in the environment. However, these planners are not practical in real world scenarios that involve unstructured, dynamic environments for a three primary reasons. First, these motion planners assume that the environment the robot is functioning in, is well-known and static, both during plan generation and plan execution. Second, these planners do not support temporal constraints, which are crucial for planning in a rapidly-changing environment and for allowing task synchronisation between the robot and other agents, like a human or even another robot. Third, the current planners do not adequately represent the requirements of the task. They often over-constrain the task description and are hence unable to take advantage of task flexibility which may aid in optimising energy efficiency or robustness. In this thesis we present Chekhov, a reactive, integrated motion planning and execution executive that addresses these shortcomings using four key innovations. First, unlike traditional planners, the planning and execution components of Chekhov are very closely integrated. This close coupling blurs the traditional, sharp boundary between the two components and allows for optimal collaboration. Second, Chekhov represents temporal constraints, which allows it to perform operations that are temporally synchronised with external events. Third, Chekhov uses an incremental search algorithm which allows it to rapidly generate a new plan if a disturbance is encountered that threatens the execution of the existing plan. Finally, unlike standard planners which generate a single reference trajectory from the start pose to the goal pose, Chekhov generates a Qualitative Control Plan using Flow Tubes that represent families of feasible trajectories and associated control policies. These flow tubes provide Chekhov with a flexibility that is extremely valuable and serve as Chekhov's first line of defence.",,100 p,,,Integrated Planning and Control; Manipulation Planning and Control,Model-based Embedded and Robotic Systems,,,,MEng thesis,,,,,,,,,2013-06-06T22:30:03Z,,,,,,,,,,,,en-GB,,,
Tomaso Poggio,"Kim, Heejung; Wohlwend, Jeremy; Leibo, Joel Z.; Poggio, Tomaso",2013-06-20T17:00:04Z,2013-06-20T17:00:04Z,2013-06-18,http://hdl.handle.net/1721.1/79354,MIT-CSAIL-TR-2013-013; CBCL-312,Body-form and body-pose recognition with a hierarchical model of the ventral stream,"When learning to recognize a novel body shape, e.g., a panda bear, we are not misled by changes in its pose. A ""jumping panda bear"" is readily recognized, despite having no prior visual experience with the conjunction of these concepts. Likewise, a novel pose can be estimated in an invariant way, with respect to the actor's body shape. These body and pose recognition tasks require invariance to non-generic transformations that previous models of the ventral stream do not have. We show that the addition of biologically plausible, class-specific mechanisms associating previously-viewed actors in a range of poses enables a hierarchical model of object recognition to account for this human capability. These associations could be acquired in an unsupervised manner from past experience.",,10 p.,,,Ventral stream; Modularity; Computational neuroscience; HMAX,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,2013-06-20T17:00:05Z,,,,,,,,,,,,,,,
Martin Rinard,"Carbin, Michael; Misailovic, Sasa; Rinard, Martin",2013-06-20T17:00:08Z,2013-06-20T17:00:08Z,2013-06-19,http://hdl.handle.net/1721.1/79355,MIT-CSAIL-TR-2013-014,Verifying Quantitative Reliability of Programs That Execute on Unreliable Hardware,"Emerging high-performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. Full detection and recovery from soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors. In this paper we present Rely, a programming language that enables developers to reason about the quantitative reliability of an application -- namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces. We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification, that characterizes the reliability of the underlying hardware components, and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely.",,22 p.,,,"unreliable hardware, probabilistic semantics, quantitative reliability",Computer Architecture,"This research was supported in part by the National Science Foundation (Grants CCF-0905244, CCF-1036241, CCF-1138967, CCF-1138967, and IIS-0835652), the United States Department of Energy (Grant DE-SC0008923), and DARPA (Grants FA8650-11-C-7192, FA8750-12-2-0110).",,,,,,,,,,,,2013-06-20T17:00:08Z,,,,,,,,,,,,en,,,
Nancy Lynch,"Attie, Paul C.; Lynch, Nancy A.",2013-07-08T18:15:04Z,2013-07-08T18:15:04Z,2013-07-08,http://hdl.handle.net/1721.1/79420,MIT-CSAIL-TR-2013-015,Dynamic Input/Output Automata: a Formal and Compositional Model for Dynamic Systems,"We present dynamic I/O automata (DIOA), a compositional model of dynamic systems, based on I/O automata. In our model, automata can be created and destroyed dynamically, as computation proceeds. In addition, an automaton can dynamically change its signature, that is, the set of actions in which it can participate. This allows us to model mobility, by enforcing the constraint that only automata at the same location may synchronize on common actions. Our model features operators for parallel composition, action hiding, and action renaming. It also features a notion of automaton creation, and a notion of trace inclusion from one dynamic system to another, which can be used to prove that one system implements the other. Our model is hierarchical: a dynamically changing system of interacting automata is itself modeled as a single automaton that is ""one level higher."" This can be repeated, so that an automaton that represents such a dynamic system can itself be created and destroyed. We can thus model the addition and removal of entire subsystems with a single action. We establish fundamental compositionality results for DIOA: if one component is replaced by another whose traces are a subset of the former, then the set of traces of the system as a whole can only be reduced, and not increased, i.e., no new behaviors are added. That is, parallel composition, action hiding, and action renaming, are all monotonic with respect to trace inclusion. We also show that, under certain technical conditions, automaton creation is monotonic with respect to trace inclusion: if a system creates automaton Ai instead of (previously) creating automaton A'i, and the traces of Ai are a subset of the traces of A'i,then the set of traces of the overall system is possibly reduced, but not increased. Our trace inclusion results imply that trace equivalence is a congruence relation with respect to parallel composition, action hiding, and action renaming. Our trace inclusion results enable a design and refinement methodology based solely on the notion of externally visible behavior, and which is therefore independent of specific methods of establishing trace inclusion. It permits the refinement of components and subsystems in isolation from the entire system, and provides more flexibility in refinement than a methodology which is, for example, based on the monotonicity of forward simulation with respect to parallel composition. In the latter, every automaton must be refined using forward simulation, whereas in our framework different automata can be refined using different methods. The DIOA model was defined to support the analysis of mobile agent systems, in a joint project with researchers at Nippon Telegraph and Telephone. It can also be used for other forms of dynamic systems, such as systems described by means of object-oriented programs, and systems containing services with changing access permissions.",,63 p.,,,,Theory of Computation,,,,,,,,,,,,,2013-07-08T18:15:05Z,,,,,,,,,,,,,,,
Nancy Lynch,"Cadambe, Viveck R.; Lynch, Nancy; Medard, Muriel; Musial, Peter",2013-07-17T18:45:04Z,2013-07-17T18:45:04Z,2013-07-17,http://hdl.handle.net/1721.1/79606,MIT-CSAIL-TR-2013-016,Coded Emulation of Shared Atomic Memory for Message Passing Architectures,"This paper considers the communication and storage costs of emulating atomic (linearizable) read/write shared memory in distributed message-passing systems. We analyze the costs of previously-proposed algorithms by Attiya, Bar-Noy, and Dolev (the ABD algorithm) and by Fan and Lynch (the LDR algorithm), and develop new coding-based algorithms that significantly reduce these costs. The paper contains three main contributions: (1) We present a new shared-memory algorithm that we call CAS, for Coded Atomic Storage. This algorithm uses erasure coding methods. (2) In a storage system with N servers that is resilient to f server failures, we show that the communication costs for the ABD and LDR algorithms, measured in terms of number of object values, are both at least f + 1, whereas the communication cost for CAS is N/(N-2f). (3) We also explicitly quantify the storage costs of the ABD, LDR, and CAS algorithms. The storage cost of the ABD algorithm, measured in terms of number of object values, is N; whereas the storage costs of the LDR and CAS algorithms are both unbounded. We present a modification of the CAS algorithm based on the idea of garbage collection. The modified version of CAS has a storage cost of (d + 1) N/(N-2f), where d in an upper bound on the number of operations that are concurrent with a read operation. Thus, if d is sufficiently small, the storage cost of CAS is lower than those of both the ABD and LDR algorithms.",,24 p.,,,,Theory of Computation,,,,,,,,,,,,,2013-07-17T18:45:05Z,,,,,,,,,,,,,,,
Tomaso Poggio,"Poggio, Tomaso; Mutch, Jim; Anselmi, Fabio; Tacchetti, Andrea; Rosasco, Lorenzo; Leibo, Joel Z.",2013-08-12T02:30:12Z,2013-08-12T02:30:12Z,2013-08-06,http://hdl.handle.net/1721.1/79828,MIT-CSAIL-TR-2013-019; CBCL-313,Does invariant recognition predict tuning of neurons in sensory cortex?,"Tuning properties of simple cells in cortical V1 can be described in terms of a ""universal shape"" characterized by parameter values which hold across different species. This puzzling set of findings begs for a general explanation grounded on an evolutionarily important computational function of the visual cortex. We ask here whether these properties are predicted by the hypothesis that the goal of the ventral stream is to compute for each image a ""signature"" vector which is invariant to geometric transformations, with the the additional assumption that the mechanism for continuously learning and maintaining invariance consists of the memory storage of a sequence of neural images of a few objects undergoing transformations (such as translation, scale changes and rotation) via Hebbian synapses. For V1 simple cells the simplest version of this hypothesis is the online Oja rule which implies that the tuning of neurons converges to the eigenvectors of the covariance of their input. Starting with a set of dendritic fields spanning a range of sizes, simulations supported by a direct mathematical analysis show that the solution of the associated ""cortical equation"" provides a set of Gabor-like wavelets with parameter values that are in broad agreement with the physiology data. We show however that the simple version of the Hebbian assumption does not predict all the physiological properties. The same theoretical framework also provides predictions about the tuning of cells in V4 and in the face patch AL which are in qualitative agreement with physiology data.",,10 p.,,,,,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,2013-08-12T02:30:12Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Sidiroglou-Douskos, Stelios; Kim, Deokhwan; Rinard, Martin",2013-08-12T02:30:08Z,2013-08-12T02:30:08Z,2013-08-06,http://hdl.handle.net/1721.1/79827,MIT-CSAIL-TR-2013-018,Sound Input Filter Generation for Integer Overflow Errors,"We present a system, SIFT, for generating input filters that nullify integer overflow errors associated with critical program sites such as memory allocation or block copy sites. SIFT uses a static program analysis to generate filters that discard inputs that may trigger integer overflow errors in the computations of the sizes of allocated memory blocks or the number of copied bytes in block copy operations. The generated filters are sound   if an input passes the filter, it will not trigger an integer overflow error for any analyzed site. Our results show that SIFT successfully analyzes (and therefore generates sound input filters for) 52 out of 58 memory allocation and block memory copy sites in analyzed input processing modules from five applications (VLC, Dillo, Swfdec, Swftools, and GIMP). These nullified errors include six known integer overflow vulnerabilities. Our results also show that applying these filters to 62895 real-world inputs produces no false positives. The analysis and filter generation times are all less than a second.",,20 p.,,,Static Analysis; Integer Overflow; Filter Generation,Computer Architecture,,,,,,,,,,,,,2013-08-12T02:30:08Z,,,,,,,,,,,,,,,
,"Jackson, Daniel",2013-08-12T02:30:04Z,2013-08-12T02:30:04Z,2013-08-08,http://hdl.handle.net/1721.1/79826,MIT-CSAIL-TR-2013-020,Conceptual Design of Software: A Research Agenda,"A research agenda in software design is outlined, focusing on the role of concepts. The notions of concepts as ""abstract affordances"" and of conceptual integrity are discussed, and a series of small examples of conceptual models is given.",,17 p.,,,Software design; Usability; Conceptual integrity; Conceptual models,Software Design,,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,2013-08-12T02:30:05Z,,,,,,,,,,,,,,,
Leslie Kaelbling,"Jordan, Matthew; Perez, Alejandro",2013-08-16T18:30:03Z,2013-08-16T18:30:03Z,2013-08-15,http://hdl.handle.net/1721.1/79884,MIT-CSAIL-TR-2013-021,Optimal Bidirectional Rapidly-Exploring Random Trees,"In this paper we present a simple, computationally-efficient, two-tree variant of the RRT* algorithm along with several heuristics.",,12 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution-NonCommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,,,,,,,,,,2013-08-16T18:30:03Z,,,,,,,,,,,,,,,
Brian Williams,"Bush, Lawrence A. M.",2018-01-30T23:45:58Z,2018-01-30T23:45:58Z,2013-08-22,http://hdl.handle.net/1721.1/113363,MIT-CSAIL-TR-2018-003,Decision Uncertainty Minimization and Autonomous Information Gathering,"Over the past several decades, technologies for remote sensing and exploration have be- come increasingly powerful but continue to face limitations in the areas of information gathering and analysis. These limitations affect technologies that use autonomous agents, which are devices that can make routine decisions independent of operator instructions. Bandwidth and other communications limitation require that autonomous differentiate between relevant and irrelevant information in a computationally efficient manner.This thesis presents a novel approach to this problem by framing it as an adaptive sensing problem. Adaptive sensing allows agents to modify their information collection strategies in response to the information gathered in real time. We developed and tested optimization algorithms that apply information guides to Monte Carlo planners. Information guides provide a mechanism by which the algorithms may blend online (realtime) and offline (previously simulated) planning in order to incorporate uncertainty into the decision- making process. This greatly reduces computational operations as well as decisional and communications overhead. We begin by introducing a 3-level hierarchy that visualizes adaptive sensing at synoptic (global), mesoscale (intermediate) and microscale (close-up) levels (a spatial hierarchy). We then introduce new algorithms for decision uncertainty minimization (DUM) and representational uncertainty minimization (RUM). Finally, we demonstrate the utility of this approach to real-world sensing problems, including bathymetric mapping and disaster relief. We also examine its potential in space exploration tasks by describing its use in a hypothetical aerial exploration of Mars. Our ultimate goal is to facilitate future large-scale missions to extraterrestrial objects for the purposes of scientific advancement and human exploration.",,310 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,Brian Williams; Model-based Embedded and Robotic Systems,,,,,,,2018-01-30T23:45:58Z,,,,,,,,,,,,,,,
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2013-07-31T18:30:05Z,2013-07-31T18:30:05Z,2013-09-01,http://hdl.handle.net/1721.1/79746,MIT-CSAIL-TR-2013-017,Jigsaw: Scalable Software-Defined Caches (Extended Version),"Shared last-level caches, widely used in chip-multiprocessors (CMPs), face two fundamental limitations. First, the latency and energy of shared caches degrade as the system scales up. Second, when multiple workloads share the CMP, they suffer from interference in shared cache accesses. Unfortunately, prior research addressing one issue either ignores or worsens the other: NUCA techniques reduce access latency but are prone to hotspots and interference, and cache partitioning techniques only provide isolation but do not reduce access latency. We present Jigsaw, a technique that jointly addresses the scalability and interference problems of shared caches. Hardware lets software define shares, collections of cache bank partitions that act as virtual caches, and map data to shares. Shares give software full control over both data placement and capacity allocation. Jigsaw implements efficient hardware support for share management, monitoring, and adaptation. We propose novel resource-management algorithms and use them to develop a system-level runtime that leverages Jigsaw to both maximize cache utilization and place data close to where it is used. We evaluate Jigsaw using extensive simulations of 16- and 64-core tiled CMPs. Jigsaw improves performance by up to 2.2x (18% avg) over a conventional shared cache, and significantly outperforms state-of-the-art NUCA and partitioning techniques.",,16 p.,,,"cache, memory, NUCA, partitioning, isolation, multicore, virtual memory",Computation Structures,This work was supported in part by DARPA PERFECT contract HR0011-13-2-0005 and Quanta Computer.,Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,2013-07-31T18:30:05Z,,,,,,,,,,,,,,,
Regina Barzilay,"Kushman, Nate; Adib, Fadel; Katabi, Dina; Barzilay, Regina",2013-09-10T21:30:03Z,2013-09-10T21:30:03Z,2013-09-10,http://hdl.handle.net/1721.1/80380,MIT-CSAIL-TR-2013-022,Harvesting Application Information for Industry-Scale Relational Schema Matching,"Consider the problem of migrating a company's CRM or ERP database from one application to another, or integrating two such databases as a result of a merger. This problem requires matching two large relational schemas with hundreds and sometimes thousands of fields. Further, the correct match is likely complex: rather than a simple one-to-one alignment, some fields in the source database may map to multiple fields in the target database, and others may have no equivalent fields in the target database. Despite major advances in schema matching, fully automated solutions to large relational schema matching problems are still elusive. This paper focuses on improving the accuracy of automated large relational schema matching. Our key insight is the observation that modern database applications have a rich user interface that typically exhibits more consistency across applications than the underlying schemas. We associate UI widgets in the application with the underlying database fields on which they operate and demonstrate that this association delivers new information useful for matching large and complex relational schemas. Additionally, we show how to formalize the schema matching problem as a quadratic program, and solve it efficiently using standard optimization and machine learning techniques. We evaluate our approach on real-world CRM applications with hundreds of fields and show that it improves the accuracy by a factor of 2-4x.",,12 p.,,,,Natural Language Processing,,,,,,,,,,,,,2013-09-10T21:30:03Z,,,,,,,,,,,,,,,
Tomaso Poggio,"Ni, Yuzhao; Frogner, Charles A.; Poggio, Tomaso A.",2013-09-19T22:30:06Z,2013-09-19T22:30:06Z,2013-09-19,http://hdl.handle.net/1721.1/80815,MIT-CSAIL-TR-2013-023; CBCL-314,Mouse Behavior Recognition with The Wisdom of Crowd,"In this thesis, we designed and implemented a crowdsourcing system to annotatemouse behaviors in videos; this involves the development of a novel clip-based video labeling tools, that is more efficient than traditional labeling tools in crowdsourcing platform, as well as the design of probabilistic inference algorithms that predict the true labels and the workers' expertise from multiple workers' responses. Our algorithms are shown to perform better than majority vote heuristic. We also carried out extensive experiments to determine the effectiveness of our labeling tool, inference algorithms and the overall system.",,69 p.,,,crowdsourcing; video labeling; human computation; mouse phenotyping; action recognition,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,2013-09-19T22:30:06Z,,,,,,,,,,,,,,,
Gerald Sussman,"Panchekha, Pavel; Brodsky, Micah Z. (Micah Zev)",2013-10-09T17:30:04Z,2013-10-09T17:30:04Z,2013-10-08,http://hdl.handle.net/1721.1/81365,MIT-CSAIL-TR-2013-024,Distributed Shared State with History Maintenance,"Shared mutable state is challenging to maintain in a distributed environment. We develop a technique, based on the Operational Transform, that guides independent agents into producing consistent states through inconsistent but equivalent histories of operations. Our technique, history maintenance, extends and streamlines the Operational Transform for general distributed systems. We describe how to use history maintenance to create eventually-consistent, strongly-consistent, and hybrid systems whose correctness is easy to reason about.",,21 p.,,,eventual consistency; distributed systems; operational transform,Mathematics and Computation,,,,,,,,,,,,,2013-10-09T17:30:04Z,,,,,,,,,,,,,,,
Nancy Lynch,"Cornejo, Alejandro; Lynch, Nancy; Sastry, Srikanth",2013-10-10T18:15:04Z,2013-10-10T18:15:04Z,2013-10-10,http://hdl.handle.net/1721.1/81371,MIT-CSAIL-TR-2013-025,Asynchronous Failure Detectors,"Failure detectors -- oracles that provide information about process crashes -- are an important abstraction for crash tolerance in distributed systems. The generality of failure-detector theory, while providing great expressiveness, poses significant challenges in developing a robust hierarchy of failure detectors. We address some of these challenges by proposing (1) a variant of failure detectors called asynchronous failure detectors and (2) an associated modeling framework. Unlike the traditional failure-detector framework, our framework eschews real-time completely. We show that asynchronous failure detectors are sufficiently expressive to include several popular failure detectors including, but not limited to, the canonical Chandra-Toueg failure detectors, Sigma and other quorum failure detectors, Omega, anti-Omega, Omega^k, and Psi_k. Additionally, asynchronous failure detectors satisfy many desirable properties: they are self-implementable, guarantee that stronger asynchronous failure-detectors solve harder problems, and ensure that their outputs encode no information other than the set of crashed processes. We introduce the notion of a failure detector being representative for a problem to capture the idea that some problems encode the same information about process crashes as their weakest failure detectors do. We show that a large class of problems, called bounded problems, do not have representative failure detectors. Finally, we use the asynchronous failure-detector framework to show how sufficiently strong AFDs circumvent the impossibility of consensus in asynchronous systems.",,47 p.,,,,Theory of Computation,,,,This report supersedes MIT-CSAIL-TR-2013-002.,,,,,,,http://hdl.handle.net/1721.1/76716,,2013-10-10T18:15:05Z,,,,,,,,,,,,,,,
Saman Amarasinghe,"Ansel, Jason; Kamil, Shoaib; Veeramachaneni, Kalyan; O'Reilly, Una-May; Amarasinghe, Saman",2013-11-01T20:30:03Z,2013-11-01T20:30:03Z,2013-11-01,http://hdl.handle.net/1721.1/81958,MIT-CSAIL-TR-2013-026,OpenTuner: An Extensible Framework for Program Autotuning,"Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 6 distinct projects and 14 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.",,13 p.,,,,Computer Architecture,"This work is partially supported by DOE award DE-SC0005288 and DOD DARPA award HR0011-10-9-0009.  This research used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.",,,,,,,,,,,,2013-11-01T20:30:03Z,,,,,,,,,,,,,,,
Patrick Winston,"Finlayson, Mark Alan",2013-11-01T17:18:16Z,2013-11-01T17:18:16Z,2013-11-01,http://hdl.handle.net/1721.1/81949,,Code for Java Libraries for Accessing the Princeton Wordnet: Comparison and Evaluation,"This archive contains the code and data for running the evaluations described in: Finlayson, Mark Alan (2014) ""Java Libraries for Accessing the Princeton Wordnet: comparison and Evaluation"" in Proceedings of the 7th Global Wordnet Conference (GWC 2014). Tartu, Estonia, 25-29 January 2014. The archive contains five Eclipse projects (compatible with Eclipse 3.8.0) that may be imported directly into an Eclipse workspace. You will need a Java 1.4, 1.5, and 1.6 JRE to run all the code in the archive. Paper abstract: Java is a popular programming language for natural language processing. I compare and evaluate 12 Java libraries designed to access the information in the original Princeton Wordnet databases. From this comparison emerges a set of decision criteria that will enable a user to pick the library most suited to their purposes. I identify five deciding features: (1) availability of similarity metrics; (2) support for editing; (3) availability via Maven; (4) compatibility with retired Java versions; and (5) support for Enterprise Java. I also provide a comparison of other features of each library, the information exposed by each API, and the versions of Wordnet each library supports, and I evaluate each library for the speed of various retrieval operations. In the case that the user's application does not require one of the deciding features, I show that my library, JWI, the MIT Java Wordnet Interface, is the highest-performance, widest-coverage, easiest-to-use library available.",,,,,,Genesis,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,2013-11-01T17:18:17Z,,,,,,,,,,,,,,,
Tomas Lozano-Perez,"Perez, Alejandro",2013-11-18T20:00:08Z,2013-11-18T20:00:08Z,2013-11-18,http://hdl.handle.net/1721.1/82462,MIT-CSAIL-TR-2013-027,On Randomized Path Coverage of Configuration Spaces,We present a sampling-based algorithm that generates a set of locally-optimal paths that differ in visibility.,,13 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,2013-11-18T20:00:09Z,,,,,,,,,,,,,,,
Sam Madden,"Taft, Rebecca; Vartak, Manasi; Satish, Nadathur Rajagopalan; Sundaram, Narayanan; Madden, Samuel; Stonebraker, Michael",2013-11-20T17:00:05Z,2013-11-20T17:00:05Z,2013-11-19,http://hdl.handle.net/1721.1/82517,MIT-CSAIL-TR-2013-028,GenBase: A Complex Analytics Genomics Benchmark,"This paper introduces a new benchmark, designed to test database management system (DBMS) performance on a mix of data management tasks (joins, filters, etc.) and complex analytics (regression, singular value decomposition, etc.) Such mixed workloads are prevalent in a number of application areas, including most science workloads and web analytics. As a specific use case, we have chosen genomics data for our benchmark, and have constructed a collection of typical tasks in this area. In addition to being representative of a mixed data management and analytics workload, this benchmark is also meant to scale to large dataset sizes and multiple nodes across a cluster. Besides presenting this benchmark, we have run it on a variety of storage systems including traditional row stores, newer column stores, Hadoop, and an array DBMS. We present performance numbers on all systems on single and multiple nodes, and show that performance differs by orders of magnitude between the various solutions. In addition, we demonstrate that most platforms have scalability issues. We also test offloading the analytics onto a coprocessor. The intent of this benchmark is to focus research interest in this area; to this end, all of our data, data generators, and scripts are available on our web site.",,12 p.,,,,Database,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,2013-11-20T17:00:05Z,,,,,,,,,,,,,,,
Silvio Micali,"Chiesa, Alessandro; Micali, Silvio; Zhu, Zeyuan Allen",2013-12-03T20:45:05Z,2013-12-03T20:45:05Z,2013-12-03,http://hdl.handle.net/1721.1/82632,MIT-CSAIL-TR-2013-029,Bridging Utility Maximization and Regret Minimization,"We relate the strategies obtained by (1) utility maximizers who use regret to refine their set of undominated strategies, and (2) regret minimizers who use weak domination to refine their sets of regret-minimizing strategies.",,9 p.,,,,Theory of Computation,,,,,,,,,,,,,2013-12-03T20:45:05Z,,,,,,,,,,,,,,,
Dina Katabi,"Adib, Fadel; Kabelac, Zach; Katabi, Dina; Miller, Robert C.",2013-12-11T18:30:03Z,2013-12-11T18:30:03Z,2013-12-11,http://hdl.handle.net/1721.1/82913,MIT-CSAIL-TR-2013-030,3D Tracking via Body Radio Reflections,"This paper introduces WiTrack, a system that tracks the 3D motion of a user from the radio signals reflected off her body. It works even if the person is occluded from the WiTrack device or in a different room. WiTrack does not require the user to carry any wireless device, yet its accuracy exceeds current RF localization systems, which require the user to hold a transceiver. Empirical measurements with a WiTrack prototype show that, on average, it localizes the center of a human body to within 10 to 13 cm in the x and y dimensions, and 21 cm in the z dimension. It also provides coarse tracking of body parts, identifying the direction of a pointing hand with a median of 11.2 degrees. WiTrack bridges a gap between RF-based localization systems which locate a user through walls and occlusions, and human-computer interaction systems like WiTrack, which can track a user without instrumenting her body, but require the user to stay within the direct line of sight of the device.",,13 p.,,,Seeing Through Walls; 3D Motion Tracking; Wireless Signals; Body Reflections,Networks & Mobile Systems,,,,,,,,,,,,,2013-12-11T18:30:03Z,,,,,,,,,,,,,,,
Brian Williams,"Timmons, Eric",2018-01-30T23:46:03Z,2018-01-30T23:46:03Z,2013-12-11,http://hdl.handle.net/1721.1/113364,MIT-CSAIL-TR-2018-004,"Fast, Approximate State Estimation of Concurrent Probabilistic Hybrid Automata","It is an undeniable fact that autonomous systems are simultaneously becoming more common place, more complex, and deployed in more inhospitable environments. Examples include smart homes, smart cars, Mars rovers, unmanned aerial vehicles, and autonomous underwater vehicles. A common theme that all of these autonomous systems share is that in order to appropriately control them and prevent mission failure, they must be able to quickly estimate their internal state and the state of the world. A natural representation of many real world systems is to describe them in terms of a mixture of continuous and discrete variables. Unfortunately, hybrid estimation is typically intractable due to the large space of possible assignments to the discrete variables. In this thesis, we investigate how to incorporate conflict directed techniques from the consistency-based, model-based diagnosis community into a hybrid framework that is no longer purely consistency based. We introduce a novel search algorithm, A&#8727; with Bounding Conflicts, that uses conflicts to not only record infeasiblilities, but also learn where in the search space the heuristic function provided to the A&#8727; search is weak (possibly due to heavy to moderate sensor or process noise). Additionally, we describe a hybrid state estimation algorithm that uses this new search to perform estimation on hybrid discrete/continuous systems.",,73 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-30T23:46:03Z,,,,,,,,,,,,,,,
Martin Rinard,"Misailovic, Sasa; Rinard, Martin",2013-12-30T20:00:02Z,2013-12-30T20:00:02Z,2013-12-29,http://hdl.handle.net/1721.1/83397,MIT-CSAIL-TR-2013-031,Synthesis of Randomized Accuracy-Aware Map-Fold Programs,"We present Syndy, a technique for automatically synthesizing randomized map/fold computations that trade accuracy for performance. Given a specification of a fully accurate computation, Syndy automatically synthesizes approximate implementations of map and fold tasks, explores the approximate computation space that these approximations induce, and derives an accuracy versus performance tradeoff curve that characterizes the explored space. Each point on the curve corresponds to an approximate randomized program configuration that realizes the probabilistic error and time bounds associated with that point.",,22 p.,,,,Computer Architecture,,,,,,,,,,,,,2013-12-30T20:00:02Z,,,,,,,,,,,,,,,
Martin Rinard,"Misailovic, Sasa; Carbin, Michael; Achour, Sara; Qi, Zichao; Rinard, Martin",2014-01-09T23:45:05Z,2014-01-09T23:45:05Z,2014-01-09,http://hdl.handle.net/1721.1/83843,MIT-CSAIL-TR-2014-001,Reliability-Aware Optimization of Approximate Computational Kernels with Rely,"Emerging high-performance architectures are anticipated to contain unreliable components (e.g., ALUs) that offer low power consumption at the expense of soft errors. Some applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors and can therefore trade accuracy of their results for reduced energy consumption by utilizing these unreliable hardware components. We present and evaluate a technique for reliability-aware optimization of approximate computational kernel implementations. Our technique takes a standard implementation of a computation and automatically replaces some of its arithmetic operations with unreliable versions that consume less power, but may produce incorrect results with some probability. Our technique works with a developer-provided specification of the required reliability of a computation -- the probability that it returns the correct result -- and produces an unreliable implementation that satisfies that specification. We evaluate our approach on five applications from the image processing, numerical analysis, and financial analysis domains and demonstrate how our technique enables automatic exploration of the trade-off between the reliability of a computation and its performance.",,11 p.,,,,Computer Architecture,,,,,,,,,,,,,2014-01-09T23:45:06Z,,,,,,,,,,,,,,,
Frans Kaashoek,"Beckmann, Nathan Z.; Gruenwald III, Charles; Johnson, Christopher R.; Kasture, Harshad; Sironi, Filippo; Agarwal, Anant; Kaashoek, M. Frans; Zeldovich, Nickolai",2014-01-29T19:30:05Z,2014-01-29T19:30:05Z,2014-01-28,http://hdl.handle.net/1721.1/84608,MIT-CSAIL-TR-2014-002,PIKA: A Network Service for Multikernel Operating Systems,"PIKA is a network stack designed for multikernel operating systems that target potential future architectures lacking cache-coherent shared memory but supporting message passing. PIKA splits the network stack into several servers that communicate using a low-overhead message passing layer. A key challenge faced by PIKA is the maintenance of shared state, such as a single accept queue and load balance information. PIKA addresses this challenge using a speculative 3-way handshake for connection acceptance, and a new distributed load balancing scheme for spreading connections. A PIKA prototype achieves competitive performance, excellent scalability, and low service times under load imbalance on commodity hardware. Finally, we demonstrate that splitting network stack processing by function across separate cores is a net loss on commodity hardware, and we describe conditions under which it may be advantageous.",,14  p.,,,Operating Systems; Multi-kernel; Micro-kernel; Scalability; Networking,Parallel and Distributed Operating Systems,,,,,,,,,,,,,2014-01-29T19:30:05Z,,,,,,,,,,,,,,,
Seth Teller,"Fallon, Maurice; Kuindersma, Scott; Karumanchi, Sisir; Antone, Matthew; Schneider, Toby; Dai, Hongkai; Perez D'Arpino, Claudia; Deits, Robin; DiCicco, Matt; Fourie, Dehann; Koolen, Twan; Marion, Pat; Posa, Michael; Valenzuela, Andres; Yu, Kuan-Ting; Shah, Julie; Iagnemma, Karl; Tedrake, Russ; Teller, Seth",2014-03-17T21:30:06Z,2014-03-17T21:30:06Z,2014-03-16,http://hdl.handle.net/1721.1/85690,MIT-CSAIL-TR-2014-003,An Architecture for Online Affordance-based Perception and Whole-body Planning,"The DARPA Robotics Challenge Trials held in December 2013 provided a landmark demonstration of dexterous mobile robots executing a variety of tasks aided by a remote human operator using only data from the robot's sensor suite transmitted over a constrained, field-realistic communications link. We describe the design considerations, architecture, implementation and performance of the software that Team MIT developed to command and control an Atlas humanoid robot. Our design emphasized human interaction with an efficient motion planner, where operators expressed desired robot actions in terms of affordances fit using perception and manipulated in a custom user interface. We highlight several important lessons we learned while developing our system on a highly compressed schedule.",,29 p.,,,,"Robotics, Vision & Sensor Networks",,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-03-17T21:30:06Z,,,,,,,,,,,,,,,
Patrick Winston,"Finlayson, Mark A.; Halverson, Jeffry R.; Corman, Steven R.",2014-03-22T16:45:08Z,2014-03-22T16:45:08Z,2014-03-22,http://hdl.handle.net/1721.1/85893,,The N2 Corpus v1.0,"The N2 Corpus (Narrative Networks Corpus) comprises 100 story texts (42,480 words) relevant to Islamist Extremism, drawn from religious stories, online material, and promotional magazines. The corpus has been annotated for 14 different layers of syntax and semantics. This v1.0 version is missing 33 texts that will be added in later versions. The corpus is described in: Mark A. Finlayson, Jeffry R. Halverson, and Steven R. Corman (2014) ""The N2 Corpus: A semantically annotated collection of Islamist extremist stories"", Proceedings of the 9th Language Resources and Evaluation Conference (LREC), Reykjavik, Iceland.",,,,,,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-03-22T16:45:08Z,,,,,,,,,,,,,MIT CSAIL,,
Hari Balakrishnan,"LaCurts, Katrina; Mogul, Jeffrey C.; Balakrishnan, Hari; Turner, Yoshio",2014-03-31T20:15:06Z,2014-03-31T20:15:06Z,2014-03-24,http://hdl.handle.net/1721.1/85975,MIT-CSAIL-TR-2014-004,Cicada: Predictive Guarantees for Cloud Network Bandwidth,"In cloud-computing systems, network-bandwidth guarantees have been shown to improve predictability of application performance and cost. Most previous work on cloud-bandwidth guarantees has assumed that cloud tenants know what bandwidth guarantees they want. However, application bandwidth demands can be complex and time-varying, and many tenants might lack sufficient information to request a bandwidth guarantee that is well-matched to their needs. A tenant's lack of accurate knowledge about its future bandwidth demands can lead to over-provisioning (and thus reduced cost-efficiency) or under-provisioning (and thus poor user experience in latency-sensitive user-facing applications). We analyze traffic traces gathered over six months from an HP Cloud Services datacenter, finding that application bandwidth consumption is both time-varying and spatially inhomogeneous. This variability makes it hard to predict requirements. To solve this problem, we develop a prediction algorithm usable by a cloud provider to suggest an appropriate bandwidth guarantee to a tenant. The key idea in the prediction algorithm is to treat a set of previously observed traffic matrices as ""experts"" and learn online the best weighted linear combination of these experts to make its prediction. With tenant VM placement using these predictive guarantees, we find that the inter-rack network utilization in certain datacenter topologies can be more than doubled.",,13 p.,,,networking; machine learning; traffic prediction,Networks & Mobile Systems,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2014-03-31T20:15:06Z,,,,,,,,,,,,,,,
Boris Katz,"Borchardt, Gary C.",2014-04-14T23:00:06Z,2014-04-14T23:00:06Z,2014-04-09,http://hdl.handle.net/1721.1/86174,MIT-CSAIL-TR-2014-005,"Moebius Language Reference, Version 1.2","Moebius is a representation and interface language based on a subset of English. It is designed for use as a means of encoding information and as a means of conveying information between software components and other software components, between software components and humans, and between data repositories and their users -- human or machine. This report describes the structure and use of the Moebius language and presents three applications of the language to date.",,31 p.,,,,Infolab,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2014-04-14T23:00:07Z,,,,,,,,,,,,,,,
Sam Madden,"Cheung, Alvin; Madden, Samuel; Solar-Lezama, Armando",2014-04-14T23:00:03Z,2014-04-14T23:00:03Z,2014-04-14,http://hdl.handle.net/1721.1/86173,MIT-CSAIL-TR-2014-006,Sloth: Being Lazy is a Virtue (When Issuing Database Queries),"Many web applications store persistent data in databases. During execution, such applications spend a significant amount of time communicating with the database for retrieval and storing of persistent data over the network. These network round trips represent a significant fraction of the overall execution time for many applications and as a result increase their latency. While there has been prior work that aims to eliminate round trips by batching queries, they are limited by 1) a requirement that developers manually identify batching opportunities, or 2) the fact that they employ static program analysis techniques that cannot exploit many opportunities for batching. In this paper, we present Sloth, a new system that extends traditional lazy evaluation to expose query batching opportunities during application execution, even across loops, branches, and method boundaries. We evaluated Sloth using over 100 benchmarks from two large-scale open-source applications, and achieved up to a 3x reduction in page load time by delaying computation.",,21 p.,,,network round trips; application optimization; database applications; compilers; lazy evaluation,Database,,,,,,,,,,,,,2014-04-14T23:00:03Z,,,,,,,,,,,,,,,
Daniel Jackson,"Near, Joseph P.; Jackson, Daniel",2014-04-24T19:30:05Z,2014-04-24T19:30:05Z,2014-04-22,http://hdl.handle.net/1721.1/86235,MIT-CSAIL-TR-2014-007,Symbolic Execution for (Almost) Free: Hijacking an Existing Implementation to Perform Symbolic Execution,"Symbolic execution of a language is traditionally achieved by replacing the language s interpreter with an entirely new interpreter. This may be an unnecessary burden, and it is tempting instead to try to use as much of the existing interpret infrastructure as possible, both for handling aspects of the computation that are not symbolic, and for propagating symbolic ones. This approach was used to implement Rubicon, a bounded verification system for Ruby on Rails web applications, in less than 1000 lines of Ruby code. Rubicon uses symbolic execution to derive verification conditions from Rails applications and an off-the-shelf solver to check them. Despite its small size, Rubicon has been used to find previously unknown bugs in open-source Rails applications. The key idea is to encode symbolic values and operations in a library written in the target language itself, overriding only a small part of the standard interpreter. We formalize this approach, showing that replacing a few key operators with symbolic versions in a standard interpreter gives the same effect as replacing the entire interpreter with a symbolic one.",,12 p.,,,symbolic execution; web applications; security; verification,Software Design,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2014-04-24T19:30:05Z,,,,,,,,,,,,,,,
William Freeman,"Wadhwa, Neal; Rubinstein, Michael; Durand, Fredo; Freeman, William T.",2014-04-28T18:30:10Z,2014-04-28T18:30:10Z,2014-04-26,http://hdl.handle.net/1721.1/86300,MIT-CSAIL-TR-2014-009,Quaternionic Representation of the Riesz Pyramid for Video Magnification,"Recently, we presented a new image pyramid, called the Riesz pyramid, that uses the Riesz transform to manipulate the phase in non-oriented sub-bands of an image sequence to produce real-time motion-magnified videos. In this report we give a quaternionic formulation of the Riesz pyramid, and show how several seemingly heuristic choices in how to use the Riesz transform for phase-based video magnification fall out of this formulation in a natural and principled way. We intend this report to accompany the original paper on the Riesz pyramid for video magnification.",,5 p.,,,,Vision,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2014-04-28T18:30:10Z,,,,,,,,,,,,,,,
Dina Katabi,"Adib, Fadel; Kabelac, Zachary; Katabi, Dina",2014-04-28T18:30:06Z,2014-04-28T18:30:06Z,2014-04-26,http://hdl.handle.net/1721.1/86299,MIT-CSAIL-TR-2014-008,Multi-Person Motion Tracking via RF Body Reflections,"Recently, we have witnessed the emergence of technologies that can localize a user and track her gestures based purely on radio reflections off the person's body. These technologies work even if the user is behind a wall or obstruction. However, for these technologies to be fully practical, they need to address major challenges such as scaling to multiple people, accurately localizing them and tracking their gestures, and localizing static users as opposed to requiring the user to move to be detectable. This paper presents WiZ, the first multi-person centimeter-scale motion tracking system that pinpoints people's locations based purely on RF reflections off their bodies. WiZ can also locate static users by sensing minute changes in their RF reflections due to breathing. Further, it can track concurrent gestures made by different individuals, even when they carry no wireless device on them. We implement a prototype of WiZ and show that it can localize up to five users each with a median accuracy of 8-18 cm and 7-11 cm in the x and y dimensions respectively. WiZ can also detect 3D pointing gestures of multiple users with a median orientation error of 8 -16 degrees for each of them. Finally, WiZ can track breathing motion and output the breath count of multiple people with high accuracy.",,14 p.,,,,Networks & Mobile Systems,,,,,,,,,,,,,2014-04-28T18:30:06Z,,,,,,,,,,,,,,,
Dina Katabi,"Abari, Omid; Rahul, Hariharan; Katabi, Dina",2014-04-28T18:30:03Z,2014-04-28T18:30:03Z,2014-04-27,http://hdl.handle.net/1721.1/86298,MIT-CSAIL-TR-2014-010,One Clock to Rule Them All: A Primitive for Distributed Wireless Protocols at the Physical Layer,"Implementing distributed wireless protocols at the physical layer today is challenging because different nodes have different clocks, each of which has slightly different frequencies. This causes the nodes to have frequency offset relative to each other, as a result of which transmitted signals from these nodes do not combine in a predictable manner over time. Past work tackles this challenge and builds distributed PHY layer systems by attempting to address the effects of the frequency offset and compensating for it in the transmitted signals. In this paper, we address this challenge by addressing the root cause - the different clocks with different frequencies on the different nodes. We present AirClock, a new wireless coordination primitive that enables multiple nodes to act as if they are driven by a single clock that they receive wirelessly over the air. AirClock presents a synchronized abstraction to the physical layer, and hence enables direct implementation of diverse kinds of distributed PHY protocols. We illustrate AirClock's versatility by using it to build three different systems: distributed MIMO, distributed rate adaptation for wireless sensors, and pilotless OFDM, and show that they can provide significant performance benefits over today's systems.",,14 p.,,,Clock Synchronization; Wireless; Distributed MIMO; Distributed Rate Adaptation; Frequency Synchronization; Wireless Sensor Networks,Networks & Mobile Systems,,,,,,,,,,,,,2014-04-28T18:30:03Z,,,,,,,,,,,,,,,
Julie A Shah,"Kim, Been; Rudin, Cynthia; Shah, Julie",2014-05-27T18:15:05Z,2014-05-27T18:15:05Z,2014-05-26,http://hdl.handle.net/1721.1/87548,MIT-CSAIL-TR-2014-011,Latent Case Model: A Generative Approach for  Case-Based Reasoning and Prototype Classification,"We present a general framework for Bayesian case-based reasoning and prototype classification and clustering -- Latent Case Model (LCM). LCM learns the most representative prototype observations of a dataset by performing joint inference on cluster prototypes and features. Simultaneously, LCM pursues sparsity by learning subspaces, the sets of few features that play important roles in characterizing the prototypes. The prototype and subspace representation preserves interpretability in high dimensional data. We validate the approach preserves classification accuracy on standard data sets, and verify through human subject experiments that the output of LCM produces statistically significant improvements in participants' performance on a task requiring an understanding of clusters within a dataset.",,10 p.,,,,Interactive Robotics Group,,,,,,,,,,,,,2014-05-27T18:15:05Z,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",2014-06-10T21:00:02Z,2014-06-10T21:00:02Z,2014-06-09,http://hdl.handle.net/1721.1/87727,MIT-CSAIL-TR-2014-013,Possibilistic Beliefs and Higher-Level Rationality,"We consider rationality and rationalizability for normal-form games of incomplete information in which the players have possibilistic beliefs about their opponents. In this setting, we prove that the strategies compatible with the players being level-k rational coincide with the strategies surviving a natural k-step iterated elimination procedure. We view the latter strategies as the (level-k) rationalizable ones in our possibilistic setting.",,10 p.,,,,Theory of Computation,,,,,,,,,,,,,2014-06-10T21:00:03Z,,,,,,,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",2014-06-09T20:15:07Z,2014-06-09T20:15:07Z,2014-06-09,http://hdl.handle.net/1721.1/87710,MIT-CSAIL-TR-2014-012,Possibilistic Beliefs and Higher-Level Rationality,"We consider rationality and rationalizability for normal-form games of incomplete information in which the players have possibilistic beliefs about their opponents. In this setting, we prove that the strategies compatible with the players being level-k rational coincide with the strategies surviving a natural k-step iterated elimination procedure. We view the latter strategies as the (level-k) rationalizable ones in our possibilistic setting.",,10 p.,,,,Theory of Computation,,,,,,,,,,,,,2014-06-09T20:15:07Z,,,,,,,,,,,,,,,
Saman Amarasinghe,"Ding, Yufei; Ansel, Jason; Veeramachaneni, Kalyan; Shen, Xipeng; O'Reilly, Una-May; Amarasinghe, Saman",2014-06-23T21:45:03Z,2014-06-23T21:45:03Z,2014-06-23,http://hdl.handle.net/1721.1/88083,MIT-CSAIL-TR-2014-014,Autotuning Algorithmic Choice for Input Sensitivity,"Empirical autotuning is increasingly being used in many domains to achieve optimized performance in a variety of different execution environments. A daunting challenge faced by such autotuners is input sensitivity, where the best autotuned configuration may vary with different input sets. In this paper, we propose a two level solution that: first, clusters to find input sets that are similar in input feature space; then, uses an evolutionary autotuner to build an optimized program for each of these clusters; and, finally, builds an adaptive overhead aware classifier which assigns each input to a specific input optimized program. Our approach addresses the complex trade-off between using expensive features, to accurately characterize an input, and cheaper features, which can be computed with less overhead. Experimental results show that by adapting to different inputs one can obtain up to a 3x speedup over using a single configuration for all inputs.",,14 p.,,,,Computer Architecture,,,,,,,,,,,,,2014-06-23T21:45:03Z,,,,,,,,,,,,,,,
Nancy Lynch,"Cadambe, Viveck R.; Lynch, Nancy; Medard, Muriel; Musial, Peter",2014-08-06T18:00:06Z,2014-08-06T18:00:06Z,2014-08-01,http://hdl.handle.net/1721.1/88551,MIT-CSAIL-TR-2014-015,A Coded Shared Atomic Memory Algorithm for Message Passing Architectures,"This paper considers the communication and storage costs of emulating atomic (linearizable) multi-writer multi-reader shared memory in distributed message-passing systems. The paper contains three main contributions: (1) We present a atomic shared-memory emulation algorithm that we call Coded Atomic Storage (CAS). This algorithm uses erasure coding methods. In a storage system with 'N' servers that is resilient to 'f' server failures, we show that the communication cost of CAS is N/(N-2f) . The storage cost of CAS is unbounded. (2) We present a modification of the CAS algorithm known as CAS with Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer 'd' and has a bounded storage cost. We show that in every execution where the number of write operations that are concurrent with a read operation is no bigger than 'd', the CASGC algorithm with parameter 'd' satisfies atomicity and liveness. We explicitly characterize the storage cost of CASGC, and show that it has the same communication cost as CASGC. (3) We describe an algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS) algorithm that achieves a smaller communication cost than CAS and CASGC. In particular, CCOAS incurs read and write communication costs of N/(N-f) measured in terms of number of object values. We also discuss drawbacks of CCOAS as compared with CAS and CASGC.",,28 p.,,,,Theory of Computation,,,,,,,,,,,,,2014-08-06T18:00:06Z,,,,,,,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Piselli, Paolo; Rinard, Martin",2014-10-22T21:30:07Z,2014-10-22T21:30:07Z,2014-08-11,http://hdl.handle.net/1721.1/91148,MIT-CSAIL-TR-2014-024,Automatic Error Elimination by Multi-Application Code Transfer,"We present pDNA, a system for automatically transferring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using three donor applications to eliminate seven errors in four recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer overflow errors at critical memory allocation sites. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",,14 p.,,,Automatic Program Repair,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,MIT-CSAIL-TR-2014-026,http://hdl.handle.net/1721.1/91150,,,2014-10-22T21:30:07Z,,,,,,,,,,,,,,,
Martin Rinard,"Achour, Sara; Rinard, Martin",2014-08-19T21:00:06Z,2014-08-19T21:00:06Z,2014-08-19,http://hdl.handle.net/1721.1/88926,MIT-CSAIL-TR-2014-016,Energy-Efficient Approximate Computation in Topaz,"We present Topaz, a new task-based language for computations that execute on approximate computing platforms that may occasionally produce arbitrarily inaccurate results. The Topaz implementation maps approximate tasks onto the approximate machine and integrates the approximate results into the main computation, deploying a novel outlier detection and reliable reexecution mechanism to prevent unacceptably inaccurate results from corrupting the overall computation. Topaz therefore provides the developers of approximate hardware with substantial freedom in producing designs with little or no precision or accuracy guarantees. Experimental results from our set of benchmark applications demonstrate the effectiveness of Topaz and the Topaz implementation in enabling developers to productively exploit emerging approximate hardware platforms.",,41 p.,,,,Computer Architecture,,,,,,,,,,,,,2014-08-19T21:00:07Z,,,,,,,,,,,,,,,
Seth Teller,"Park, Jun-geun; Teller, Seth",2014-08-26T20:30:04Z,2014-08-26T20:30:04Z,2014-08-26,http://hdl.handle.net/1721.1/89075,MIT-CSAIL-TR-2014-017,Motion Compatibility for Indoor Localization,"Indoor localization -- a device's ability to determine its location within an extended indoor environment -- is a fundamental enabling capability for mobile context-aware applications. Many proposed applications assume localization information from GPS, or from WiFi access points. However, GPS fails indoors and in urban canyons, and current WiFi-based methods require an expensive, and manually intensive, mapping, calibration, and configuration process performed by skilled technicians to bring the system online for end users. We describe a method that estimates indoor location with respect to a prior map consisting of a set of 2D floorplans linked through horizontal and vertical adjacencies. Our main contribution is the notion of ""path compatibility,"" in which the sequential output of a classifier of inertial data producing low-level motion estimates (standing still, walking straight, going upstairs, turning left etc.) is examined for agreement with the prior map. Path compatibility is encoded in an HMM-based matching model, from which the method recovers the user s location trajectory from the low-level motion estimates. To recognize user motions, we present a motion labeling algorithm, extracting fine-grained user motions from sensor data of handheld mobile devices. We propose ""feature templates,"" which allows the motion classifier to learn the optimal window size for a specific combination of a motion and a sensor feature function. We show that, using only proprioceptive data of the quality typically available on a modern smartphone, our motion labeling algorithm classifies user motions with 94.5% accuracy, and our trajectory matching algorithm can recover the user's location to within 5 meters on average after one minute of movements from an unknown starting location. Prior information, such as a known starting floor, further decreases the time required to obtain precise location estimate.",,14 p.,,,Indoor localization; Inertial sensing; Motion classification; Trajectory matching; Sensor fusion; Route networks; Conditional random fields; Hidden Markov models,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,2014-08-26T20:30:04Z,,,,,,,,,,,,,,,
Daniel Jackson,"Milicevic, Aleksandar; Near, Joseph P.; Kang, Eunsuk; Jackson, Daniel",2014-09-03T18:15:05Z,2014-09-03T18:15:05Z,2014-09-02,http://hdl.handle.net/1721.1/89157,MIT-CSAIL-TR-2014-018,Alloy*: A Higher-Order Relational Constraint Solver,"The last decade has seen a dramatic growth in the use of constraint solvers as a computational mechanism, not only for analysis and synthesis of software, but also at runtime. Solvers are available for a variety of logics but are generally restricted to first-order formulas. Some tasks, however, most notably those involving synthesis, are inherently higher order; these are typically handled by embedding a first-order solver (such as a SAT or SMT solver) in a domain-specific algorithm. Using strategies similar to those used in such algorithms, we show how to extend a first-order solver (in this case Kodkod, a model finder for relational logic used as the engine of the Alloy Analyzer) so that it can handle quantifications over higher-order structures. The resulting solver is sufficiently general that it can be applied to a range of problems; it is higher order, so that it can be applied directly, without embedding in another algorithm; and it performs well enough to be competitive with specialized tools on standard benchmarks. Although the approach is demonstrated for a particular relational logic, the principles behind it could be applied to other first-order solvers. Just as the identification of first-order solvers as reusable backends advanced the performance of specialized tools and simplified their architecture, factoring out higher-ordersolvers may bring similar benefits to a new class of tools.",,15 p.,,,,Software Design,,,,,,,,,,,,,2014-09-03T18:15:05Z,,,,,,,,,,,,,,,
Nickolai Zeldovich,"Boyd-Wickizer, Silas; Kaashoek, M. Frans; Morris, Robert; Zeldovich, Nickolai",2014-09-16T19:30:05Z,2014-09-16T19:30:05Z,2014-09-16,http://hdl.handle.net/1721.1/89653,MIT-CSAIL-TR-2014-019,OpLog: a library for scaling update-heavy data structures,"Existing techniques (e.g., RCU) can achieve good multi-core scaling for read-mostly data, but for update-heavy data structures only special-purpose techniques exist. This paper presents OpLog, a general-purpose library supporting good scalability for update-heavy data structures. OpLog achieves scalability by logging each update in a low-contention per-core log; it combines logs only when required by a read to the data structure. OpLog achieves generality by logging operations without having to understand them, to ease application to existing data structures. OpLog can further increase performance if the programmer indicates which operations can be combined in the logs. An evaluation shows how to apply OpLog to three update-heavy Linux kernel data structures. Measurements on a 48-core AMD server show that the result significantly improves the performance of the Apache web server and the Exim mail server under certain workloads.",,13 p.,,,,Parallel and Distributed Operating Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-09-16T19:30:05Z,,,,,,,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Piselli, Paolo; Rinard, Martin",2014-10-22T21:30:11Z,2014-10-22T21:30:11Z,2014-09-30,http://hdl.handle.net/1721.1/91149,MIT-CSAIL-TR-2014-025,Automatic Error Elimination by Multi-Application Code Transfer,"We present pDNA, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",,15 p.,,,automatic patching; software self-healing,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,MIT-CSAIL-TR-2014-026,http://hdl.handle.net/1721.1/91150,,,2014-10-22T21:30:12Z,,,,,,,,,,,,,,,
Armando Solar-Lezama,"Rose, Eva",2014-10-02T21:45:04Z,2014-10-02T21:45:04Z,2014-10-01,http://hdl.handle.net/1721.1/90560,MIT-CSAIL-TR-2014-020,Constraint Generation for the Jeeves Privacy Language,"Our goal is to present a completed, semantic formalization of the Jeeves privacy language evaluation engine, based on the original Jeeves constraint semantics defined by Yang et al at POPL12, but sufficiently strong to support a first complete implementation thereof. Specifically, we present and implement a syntactically and semantically completed concrete syntax for Jeeves that meets the example criteria given in the paper. We also present and implement the associated translation to J, but here formulated by a completed and decompositional operational semantic formulation. Finally, we present an enhanced and decompositional, non-substitutional operational semantic formulation and implementation of the J evaluation engine (the dynamic semantics) with privacy constraints. In particular, we show how implementing the constraints can be defined as a monad, and evaluation can be defined as monadic operation on the constraint environment. The implementations are all completed in Haskell, utilizing its almost one-to-one capability to transparently reflect the underlying semantic reasoning when formalized this way. In practice, we have applied the ""literate"" program facility of Haskell to this report, a feature that enables the source LATEX to also serve as the source code for the implementation (skipping the report-parts as comment regions). The implementation is published as a github project.",,56 p.,,,,Computer-Aided Programming,,,,,,,,,,,,,2014-10-02T21:45:04Z,,,,,,,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",2014-10-22T21:30:14Z,2014-10-22T21:30:14Z,2014-10-02,http://hdl.handle.net/1721.1/91150,MIT-CSAIL-TR-2014-026,Automatic Error Elimination by Multi-Application Code Transfer,"We present pDNA, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",,16 p.,,,automatic program repair,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,MIT-CSAIL-TR-2014-025; MIT-CSAIL-TR-2014-024,,2014-10-22T21:30:14Z,,,,,,,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",2014-10-02T21:45:09Z,2014-10-02T21:45:09Z,2014-10-02,http://hdl.handle.net/1721.1/90561,MIT-CSAIL-TR-2014-021,Automatic Error Elimination by Multi-Application Code Transfer,"We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of CP to transfer code across applications to eliminate otherwise fatal integer and buffer over- flow errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to eliminate software errors via the successful transfer of correct code across applications.",,16 p.,,,automatic program repair,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2014-10-02T21:45:09Z,,,,,,,,,,,,,,,
Boris Katz,"Borchardt, Gary; Katz, Boris; Nguyen, Hong-Linh; Felshin, Sue; Senne, Ken; Wang, Andy",2014-10-08T20:45:02Z,2014-10-08T20:45:02Z,2014-10-08,http://hdl.handle.net/1721.1/90812,MIT-CSAIL-TR-2014-022,An Analyst's Assistant for the Interpretation of Vehicle Track Data,"This report describes the Analyst's Assistant, a software system for language-interactive, collaborative user-system interpretation of events, specifically targeting vehicle events that can be recognized on the basis of vehicle track data. The Analyst's Assistant utilizes language not only as a means of interaction, but also as a basis for internal representation of scene information, background knowledge, and results of interpretation. Building on this basis, the system demonstrates emerging intelligent systems techniques related to event recognition, summarization of events, partitioning of subtasks between user and system, and handling of language and graphical references to scene entities during interactive analysis.",,73 p.,,,,Infolab,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2014-10-08T20:45:03Z,,,,,,,,,,,,,,,
Brian Williams,"Wang, David; Williams, Brian C.",2014-10-24T18:15:04Z,2014-10-24T18:15:04Z,2014-10-24,http://hdl.handle.net/1721.1/91170,MIT-CSAIL-TR-2014-027,tBurton: A Divide and Conquer Temporal Planner,"Planning for and controlling a network of interacting devices requires a planner that accounts for the automatic timed transitions of devices while meeting deadlines and achieving durative goals. For example, a planner for an imaging satellite with a camera intolerant of exhaust would need to determine that opening a valve causes a chain reaction that ignites the engine, and thus needs to shield its camera. While planners exist that support deadlines and durative goals, currently, no planners can handle automatic timed transitions. We present tBurton, a temporal planner that supports these features while additionally producing a temporally least-commitment plan. tBurton uses a divide and conquer approach: dividing the problem using causal-graph decomposition and conquering each factor with heuristic forward search. The `sub-plans' from each factor are unified in a conflict directed search, guided by the causal graph structure. We describe why tBurton is fast and efficient and present its efficacy on benchmarks from the International Planning Competition.",,7 p.,,,timed automata; timed concurrent automata; temporal planning; simple temporal network,Model-based Embedded and Robotic Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-10-24T18:15:04Z,,,,,,,,,,,,,,,
,"Feizi, Soheil; Duffy, Ken; Kellis, Manolis; Medard, Muriel",2014-12-04T18:30:07Z,2014-12-04T18:30:07Z,2014-12-02,http://hdl.handle.net/1721.1/92031,,Network Infusion to Infer Information Sources in Networks,"Several models exist for diffusion of signals across biological, social, or engineered networks. However, the inverse problem of identifying the source of such propagated information appears more difficult even in the presence of multiple network snapshots, and especially for the single-snapshot case, given the many alternative, often similar, progression of diffusion that may lead to the same observed snapshots. Mathematically, this problem can be undertaken using a diffusion kernel that represents diffusion processes in a given network, but computing this kernel is computationally challenging in general. Here, we propose a path-based network diffusion kernel which considers edge-disjoint shortest paths among pairs of nodes in the network and can be computed efficiently for both homogeneous and heterogeneous continuous-time diffusion models. We use this network diffusion kernel to solve the inverse diffusion problem, which we term Network Infusion (NI), using both likelihood maximization and error minimization. The minimum error NI algorithm is based on an asymmetric Hamming premetric function and can balance between false positive and false negative error types. We apply this framework for both single-source and multi-source diffusion, for both single-snapshot and multi-snapshot observations, and using both uninformative and informative prior probabilities for candidate source nodes. We also provide proofs that under a standard susceptible-infected diffusion model, (1) the maximum-likelihood NI is mean-field optimal for tree structures or sufficiently sparse Erdos-Renyi graphs, (2) the minimum-error algorithm is mean-field optimal for regular tree structures, and (3) for sufficiently-distant sources, the multi-source solution is mean-field optimal in the regular tree structure. Moreover, we provide techniques to learn diffusion model parameters such as observation times. We apply NI to several synthetic networks and compare its performance to centrality-based and distance-based methods for Erdos-Renyi graphs, power-law networks, symmetric and asymmetric grids. Moreover, we use NI in two real-world applications. First, we identify the news sources for 3,553 stories in the Digg social news network, and validate our results based on annotated information, that was not provided to our algorithm. Second, we use NI to identify infusion hubs of human diseases, defined as gene candidates that can explain the connectivity pattern of disease-related genes in the human regulatory network. NI identifies infusion hubs of several human diseases including T1D, Parkinson, MS, SLE, Psoriasis and Schizophrenia. We show that, the inferred infusion hubs are biologically relevant and often not identifiable using the raw p-values.",,45 p.,,,,,,Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,Manolis Kellis; Computational Biology (Kellis),,,,,,,2014-12-04T18:30:07Z,,,,,,,,,,,,,MIT CSAIL,,
,"Gombolay, Matthew; Golen, Toni; Shah, Neel; Shah, Julie",2014-12-16T22:00:06Z,2014-12-16T22:00:06Z,2014-12-16,http://hdl.handle.net/1721.1/92354,,Queueing Theory Analysis of Labor & Delivery at a Tertiary Care Center,"Labor and Delivery is a complex clinical service requiring the support of highly trained healthcare professionals from Obstetrics, Anesthesiology, and Neonatology and the access to a finite set of valuable resources. In the United States, the rate of cesarean sections on labor floors is approximately twice as high as considered appropriate for patient care. We analyze one month of data from a Boston-area hospital to assess how well the labor and delivery process can be modelled with tools from queueing theory. We find that the labor and delivery process is highly amenable to analysis under queueing theory models. We also investigate the problem of high cesarean section rates and the potential effects of resource utilization of lowering the rate of cesarean section.",,10 p.,,,Obstetrics; Healthcare; Queueing Theory; Operations Research,,,Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,Julie A Shah; Interactive Robotics Group,,,,,,,2014-12-16T22:00:07Z,,,,,,,,,,,,,MIT CSAIL,,
Patrick Winston,"Finlayson, Mark Alan",2014-12-30T21:45:10Z,2014-12-30T21:45:10Z,2014-12-30,http://hdl.handle.net/1721.1/92563,,"Supplementary Materials for ""A Survey of Corpora in Computational and Cognitive Narrative Science""","This archive contains supplementary materials for the article titled ""A Survey of Corpora in Computational and Cognitive Narrative Science"" by Mark A. Finlayson, published in the journal *Sprache und Datenverarbeitung*. The archive contains two files. The first file is the raw bibliographic data of the survey, containing 2600+ citations. The second file is a spreadsheet with the coded features of each corpus, plus the analyses that underlie sections 3 & 4 of the paper.",,1172839 bytes,,,,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-12-30T21:45:10Z,,,,,,,,,,,,,,,
Nick Roy,"Banerjee, Ashis Gopal; Roy, Nicholas",2015-01-21T19:45:08Z,2015-01-21T19:45:08Z,2015-01-21,http://hdl.handle.net/1721.1/93099,MIT-CSAIL-TR-2015-001,Efficiently Solving Repeated Integer Linear Programming Problems by Learning Solutions of Similar Linear Programming Problems using Boosting Trees,"It is challenging to obtain online solutions of large-scale integer linear programming (ILP) problems that occur frequently in slightly different forms during planning for autonomous systems. We refer to such ILP problems as repeated ILP problems. The branch-and-bound (BAB) algorithm is commonly used to solve ILP problems, and a significant amount of computation time is expended in solving numerous relaxed linear programming (LP) problems at the nodes of the BAB trees. We observe that the relaxed LP problems, both within a particular BAB tree and across multiple trees for repeated ILP problems, are similar to each other in the sense that they contain almost the same number of constraints, similar objective function and constraint coefficients, and an identical number of decision variables. We present a boosting tree-based regression technique for learning a set of functions that map the objective function and the constraints to the decision variables of such a system of similar LP problems; this enables us to efficiently infer approximately optimal solutions of the repeated ILP problems. We provide theoretical performance guarantees on the predicted values and demonstrate the effectiveness of the algorithm in four representative domains involving a library of benchmark ILP problems, aircraft carrier deck scheduling, vehicle routing, and vehicle control.",,48 p.,,,"Combinatorial optimization, linear programming, regression, boosting trees, planning","Robotics, Vision & Sensor Networks",,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2015-01-21T19:45:09Z,,,,,,,,,,,,,,,
Karen Sollins,"Beckler, Kendra K.",2015-02-02T16:30:05Z,2015-02-02T16:30:05Z,2015-01-31,http://hdl.handle.net/1721.1/93253,MIT-CSAIL-TR-2015-002,Improved Caching Strategies for Publish/Subscribe Internet Networking,"The systemic structure of TCP/IP is outdated; a new scheme for data transportation is needed in order to make the internet more adaptive to modern demands of mobility, information-driven demand, ever-increasing quantity of users and data, and performance requirements. While an information centric networking system addresses these issues, one required component for publish subscribe or content-addressed internet networking systems to work properly is an improved caching system. This allows the publish subscribe internet networking to dynamically route packets to mobile users, as an improvement over pure hierarchical or pure distributed caching systems. To this end, I proposed, implemented, and analyzed the workings of a superdomain caching system. The superdomain caching system is a hybrid of hierarchical and dynamic caching systems designed to continue reaping the benefits of the caching system for mobile users (who may move between neighboring domains in the midst of a network transaction) while minimizing the latency inherent in any distributed caching system to improve upon the content-addressed system.",,82 p.,,,Network caching; information centric networking,Advanced Network Architecture,,,,MEng thesis,,,,,,,,,2015-02-02T16:30:05Z,,,,,,,,,,,,,,,
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-02-02T22:00:04Z,2015-02-02T22:00:04Z,2015-02-02,http://hdl.handle.net/1721.1/93255,,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems (Supplementary Material),"We analyze reported patches for three prior generate-and-validate patch generation systems (GenProg, RSRepair, and AE). Because of experimental error, the majority of the reported patches violate the basic principle behind the design of these systems -- they do not produce correct outputs even for the inputs in the test suite used to validate the patches. We also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. We also present Kali, a generate-and-validate patch generation system that simply deletes functionality. Working with a simpler and more effectively focused search space, Kali produces more correct patches and at least as many patches that produce correct outputs for the inputs in the validation test suite as prior GenProg, RSRepair, and AE systems.",,,,,,Computer Architecture,,,,,,,,,,http://hdl.handle.net/1721.1/94337,,,2015-02-02T22:00:04Z,,Main paper: http://hdl.handle.net/1721.1/94337,,,,,,,,,,,,,
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-02-11T19:45:04Z,2015-02-11T19:45:04Z,2015-02-10,http://hdl.handle.net/1721.1/94337,MIT-CSAIL-TR-2015-003,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,"We analyze reported patches for three prior generate-and-validate patch generation systems (GenProg, RSRepair, and AE). Because of experimental error, the majority of the reported patches violate the basic principle behind the design of these systems -- they do not produce correct outputs even for the inputs in the test suite used to validate the patches. We also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. We also present Kali, a generate-and-validate patch generation system that simply deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many plausible patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks.",,13 p.,,,Automatic Program Repair; Patch Analysis; Functionality Elimination,Computer Architecture,,,,,,,,,,http://hdl.handle.net/1721.1/93255,,See supplementary material at http://hdl.handle.net/1721.1/93255,2015-02-11T19:45:04Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Qi, Zichao; Achour, Sara; Rinard, Martin",2015-02-12T21:00:03Z,2015-02-12T21:00:03Z,2015-02-12,http://hdl.handle.net/1721.1/94520,MIT-CSAIL-TR-2015-004,Automatic Program Repair with Condition Synthesis and Compound Mutations,"We present PCR, a new automatic patch generation system. PCR uses a new condition synthesis technique to efficiently discover logical expressions that generate desired control- flow transfer patterns. Presented with a set of test cases, PCR deploys condition synthesis to find and repair incorrect if conditions that cause the application to produce the wrong result for one or more of the test cases. PCR also leverages condition synthesis to obtain a set of compound modifications that generate a rich, productive, and tractable search space of candidate patches. We evaluate PCR on a set of 105 defects from the GenProg benchmark set. For 40 of these defects, PCR generates plausible patches (patches that generate correct outputs for all inputs in the test suite used to validate the patch). For 12 of these defects, PCR generates correct patches that are functionally equivalent to developer patches that appear in subsequent versions. For comparison purposes, GenProg generates plausible patches for only 18 defects and correct patches for only 2 defects. AE generates plausible patches for only 27 defects and correct patches for only 3 defects.",,14 p.,,,,Computer Architecture,,,,,,,,,,,,,2015-02-12T21:00:03Z,,,,,,,,,,,,,,,
Manolis Kellis,"Feizi, Soheil; Quon, Gerald; Medard, Muriel; Kellis, Manolis; Jadbabaie, Ali",2015-02-18T18:45:06Z,2015-02-18T18:45:06Z,2015-02-18,http://hdl.handle.net/1721.1/94606,MIT-CSAIL-TR-2015-005,Spectral Alignment of Networks,"Network alignment refers to the problem of finding a bijective mapping across vertices of two or more graphs to maximize the number of overlapping edges and/or to minimize the number of mismatched interactions across networks. This paper introduces a network alignment algorithm inspired by eigenvector analysis which creates a simple relaxation for the underlying quadratic assignment problem. Our method relaxes binary assignment constraints along the leading eigenvector of an alignment matrix which captures the structure of matched and mismatched interactions across networks. Our proposed algorithm denoted by EigeAlign has two steps. First, it computes the Perron-Frobenius eigenvector of the alignment matrix. Second, it uses this eigenvector in a linear optimization framework of maximum weight bipartite matching to infer bijective mappings across vertices of two graphs. Unlike existing network alignment methods, EigenAlign considers both matched and mismatched interactions in its optimization and therefore, it is effective in aligning networks even with low similarity. We show that, when certain technical conditions hold, the relaxation given by EigenAlign is asymptotically exact over Erdos-Renyi graphs with high probability. Moreover, for modular network structures, we show that EigenAlign can be used to split the large quadratic assignment optimization into small subproblems, enabling the use of computationally expensive, but tight semidefinite relaxations over each subproblem. Through simulations, we show the effectiveness of the EigenAlign algorithm in aligning various network structures including Erdos-Renyi, power law, and stochastic block models, under different noise models. Finally, we apply EigenAlign to compare gene regulatory networks across human, fly and worm species which we infer by integrating genome-wide functional and physical genomics datasets from ENCODE and modENCODE consortia. EigenAlign infers conserved regulatory interactions across these species despite large evolutionary distances spanned. We find strong conservation of centrally-connected genes and some biological pathways, especially for human-fly comparisons.",,61 p.,,,Network alignment; Graph isomorphism; Gene regulatory networks,Computational Biology (Kellis),,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-02-18T18:45:06Z,,,,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Sastry, Srikanth",2015-03-03T21:00:09Z,2015-03-03T21:00:09Z,2015-03-02,http://hdl.handle.net/1721.1/95775,MIT-CSAIL-TR-2015-006,Consensus using Asynchronous Failure Detectors,"The FLP result shows that crash-tolerant consensus is impossible to solve in asynchronous systems, and several solutions have been proposed for crash-tolerant consensus under alternative (stronger) models. One popular approach is to augment the asynchronous system with appropriate failure detectors, which provide (potentially unreliable) information about process crashes in the system, to circumvent the FLP impossibility. In this paper, we demonstrate the exact mechanism by which (sufficiently powerful) asynchronous failure detectors enable solving crash-tolerant consensus. Our approach, which borrows arguments from the FLP impossibility proof and the famous result from CHT, which shows that Omega is a weakest failure detector to solve consensus, also yields a natural proof to Omega as a weakest asynchronous failure detector to solve consensus. The use of I/O automata theory in our approach enables us to model execution in a more detailed fashion than CHT and also addresses the latent assumptions and assertions in the original result in CHT.",,66 p.,,,,Theory of Computation,,,,,,,,,,,,,2015-03-03T21:00:09Z,,,,,,,,,,,,,,,
Howard Shrobe,"Khan, Muhammad Taimoor; Serpanos, Dimitrios; Shrobe, Howard",2015-03-03T21:00:05Z,2015-03-03T21:00:05Z,2015-03-03,http://hdl.handle.net/1721.1/95774,MIT-CSAIL-TR-2015-007,On the Formal Semantics of the Cognitive Middleware AWDRAT,"The purpose of this work is two fold: on one hand we want to formalize the behavior of critical components of the self generating and adapting cognitive middleware AWDRAT such that the formalism not only helps to understand the semantics and technical details of the middleware but also opens an opportunity to extend the middleware to support other complex application domains of cybersecurity; on the other hand, the formalism serves as a prerequisite for our proof of the behavioral correctness of the critical components to ensure the safety of the middleware itself. However, here we focus only on the core and critical component of the middleware, i.e. Execution Monitor which is a part of the module ""Architectural Differencer"" of AWDRAT. The role of the execution monitor is to identify inconsistencies between run-time observations of the target system and predictions of the System Architectural Model. Therefore, to achieve this goal, we first define the formal (denotational) semantics of the observations (run-time events) and predictions (executable specifications as of System Architectural Model); then based on the aforementioned formal semantics, we formalize the behavior of the ""Execution Monitor"" of the middleware.",,60 p.,,,cyber-security; reference-monitor; wrappers; executable specification,Cybersecurity,,,,,,AIRE,,,,,,,2015-03-03T21:00:05Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Rinard, Martin",2015-03-11T19:45:06Z,2015-03-11T19:45:06Z,2015-03-05,http://hdl.handle.net/1721.1/95963,,Staged Program Repair in SPR (Supplementary Material),"We present SPR, a new program repair system that uses condition synthesis to instantiate transformation schemas to repair program defects. SPR's staged repair strategy combines a rich space of potential repairs with a targeted search algorithm that makes this space viably searchable in practice. This strategy enables SPR to successfully find correct program repairs within a space that contains many correct patches. The majority of these correct patches are not within the search spaces of previous automatic program repair systems.",,1213290 bytes,,,Program Repair; Transformation Schema; Condition Synthesis,Computer Architecture,,,,,,,,,,,,,2015-03-11T19:45:06Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Rinard, Martin",2015-03-11T21:15:02Z,2015-03-11T21:15:02Z,2015-03-11,http://hdl.handle.net/1721.1/95970,MIT-CSAIL-TR-2015-008,Staged Program Repair in SPR,"We present SPR, a new program repair system that uses condition synthesis to instantiate transformation schemas to repair program defects. SPR s staged repair strategy combines a rich space of potential repairs with a targeted search algorithm that makes this space viably searchable in practice. This strategy enables SPR to successfully find correct program repairs within a space that contains many meaningful and useful patches. The majority of these correct repairs are not within the search spaces of previous automatic program repair systems.",,15 p.,,,Program Repair; Transformation Schema; Condition Synthesis,Computer Architecture,,,,,,,,,,,,,2015-03-11T21:15:02Z,,,,,,,,,,,,,,,
Boris Katz,"Borchardt, Gary C.",2015-03-31T22:15:06Z,2015-03-31T22:15:06Z,2015-03-30,http://hdl.handle.net/1721.1/96300,MIT-CSAIL-TR-2015-009,A Suite of Techniques for Describing Activity in Terms of Events,"This report presents a set of software techniques that support the tasks of event recognition, summarization of event sequences, explanation of recognized events, explanation of non-recognized events, prediction of event completions, and question answering by leveraging language-encoded human knowledge of what typically happens during various types of events. The techniques operate on sequences of timestamped, three-dimensional positions and contacts for humans, body parts, and objects, provided by a Microsoft Kinect sensor plus associated software. Appendices describe 64 activity sequences used for development and testing of the techniques and 102 event models created as part of the effort.",,89 p.,,,,Infolab,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-03-31T22:15:06Z,,,,,,,,,,,,,,,
Julie A Shah,"Kim, Been; Glassman, Elena; Johnson, Brittney; Shah, Julie",2015-04-01T17:30:03Z,2015-04-01T17:30:03Z,2015-04-01,http://hdl.handle.net/1721.1/96315,MIT-CSAIL-TR-2015-010,iBCM: Interactive Bayesian Case Model Empowering Humans via Intuitive Interaction,"Clustering methods optimize the partitioning of data points with respect to an internal metric, such as likelihood, in order to approximate the goodness of clustering. However, this internal metric does not necessarily translate into effective clustering from the user's perspective. This work presents the interactive Bayesian Case Model (iBCM), a model that opens a communication channel between the clustering model and the user. Users can provide direct input to iBCM in order to achieve effective clustering results, and iBCM optimizes the clustering by creating a balance between what the data indicate and what makes the most sense to the user. This model provides feedback for users and does not assume any prior knowledge of machine learning on their part. We provide quantitative evidence that users are able to obtain more satisfactory clustering results through iBCM than without an interactive model. We also demonstrate the use of this method in a real-world setting where computer language class teachers utilize iBCM to cluster students' coding assignments for grading.",,10 p.,,,interactive machine learning; machine learning; user interaction,Interactive Robotics Group,,,,,,,,,,,,,2015-04-01T17:30:03Z,,,,,,,,,,,,,,,
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2015-04-10T18:45:07Z,2015-04-10T18:45:07Z,2015-04-09,http://hdl.handle.net/1721.1/96525,MIT-CSAIL-TR-2015-011,A Cache Model for Modern Processors,"Modern processors use high-performance cache replacement policies that outperform traditional alternatives like least-recently used (LRU). Unfortunately, current cache models use stack distances to predict LRU or its variants, and cannot capture these high-performance policies. Accurate predictions of cache performance enable many optimizations in multicore systems. For example, cache partitioning uses these predictions to divide capacity among applications in order to maximize performance, guarantee quality of service, or achieve other system objectives. Without an accurate model for high-performance replacement policies, these optimizations are unavailable to modern processors. We present a new probabilistic cache model designed for high-performance replacement policies. This model uses absolute reuse distances instead of stack distances, which makes it applicable to arbitrary age-based replacement policies. We thoroughly validate our model on several high-performance policies on synthetic and real benchmarks, where its median error is less than 1%. Finally, we present two case studies showing how to use the model to improve shared and single-stream cache performance.",,15 p.,,,,Computation Structures,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-04-10T18:45:07Z,,,,,,,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Davis, Eli; Rinard, Martin",2015-04-14T20:45:09Z,2015-04-14T20:45:09Z,2015-04-14,http://hdl.handle.net/1721.1/96585,MIT-CSAIL-TR-2015-012,Horizontal Code Transfer via Program Fracture and Recombination,"We present a new horizontal code transfer technique, program fracture and recombination, for automatically replacing, deleting, and/or combining code from multiple applications. Benefits include automatic generation of new applications incorporating the best or most desirable functionality developed anywhere, the automatic elimination of security vulnerabilities, effective software rejuvenation, the automatic elimination of obsolete or undesirable functionality, and improved performance, simplicity, analyzability, and clarity.",,12 p.,,,program fracture and recombination; horizontal code transfer,Program Analysis,,,,,,,,,,,,,2015-04-14T20:45:10Z,,,,,,,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Rinard, Martin",2015-04-15T21:30:04Z,2015-04-15T21:30:04Z,2015-04-15,http://hdl.handle.net/1721.1/96625,MIT-CSAIL-TR-2015-013,Automatic Error Elimination by Horizontal Code Transfer Across Multiple Applications,"We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications.",,15 p.,,,,Program Analysis,,,,,,,,,,,,,2015-04-15T21:30:04Z,,,,,,,,,,,,,,,
Nick Roy,"Richter, Charles; Vega-Brown, William; Roy, Nicholas",2015-05-01T21:15:04Z,2015-05-01T21:15:04Z,2015-04-27,http://hdl.handle.net/1721.1/96879,MIT-CSAIL-TR-2015-014,Markov Chain Hallway and Poisson Forest Environment Generating Distributions,We document two environment-generating distributions used for sampling random 2D maps. The first generates random hallway environments based on a Markov chain and the second generates random forest environments based on the Poisson distribution.,,2 p.,,,,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,2015-05-01T21:15:04Z,,,,,,,,,,,,,,,
Martin Rinard,"Rubin, Julia; Gordon, Michael I.; Nguyen, Nguyen; Rinard, Martin",2015-05-04T20:30:03Z,2015-05-04T20:30:03Z,2015-05-04,http://hdl.handle.net/1721.1/96909,MIT-CSAIL-TR-2015-015,Non-Essential Communication in Mobile Applications,"This paper studies communication patterns in mobile applications. Our analysis shows that 65% of the HTTP, socket, and RPC communication in top-popular Android applications from Google Play have no effect on the user-observable application functionality. We present a static analysis that is able to detect non-essential communication with 84%-90% precision and 63%-64% recall, depending on whether advertisement content is interpreted as essential or not. We use our technique to analyze the 500 top-popular Android applications from Google Play and determine that more than 80% of the connection statements in these applications are non-essential.",,11 p.,,,,Program Analysis,,,,,,,,,,,,,2015-05-04T20:30:03Z,,,,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Newport, Calvin",2015-05-18T20:00:06Z,2015-05-18T20:00:06Z,2015-05-18,http://hdl.handle.net/1721.1/97014,MIT-CSAIL-TR-2015-016,A (Truly) Local Broadcast Layer for Unreliable Radio Networks,"In this paper, we implement an efficient local broadcast service for the dual graph model, which describes communication in a radio network with both reliable and unreliable links. Our local broadcast service offers probabilistic latency guarantees for: (1) message delivery to all reliable neighbors (i.e., neighbors connected by reliable links), and (2) receiving some message when one or more reliable neighbors are broadcasting. This service significantly simplifies the design and analysis of algorithms for the otherwise challenging dual graph model. To this end, we also note that our solution can be interpreted as an implementation of the abstract MAC layer specification---therefore translating the growing corpus of algorithmic results studied on top of this layer to the dual graph model. At the core of our service is a seed agreement routine which enables nodes in the network to achieve ""good enough"" coordination to overcome the difficulties of unpredictable link behavior. Because this routine has potential application to other problems in this setting, we capture it with a formal specification---simplifying its reuse in other algorithms. Finally, we note that in a break from much work on distributed radio network algorithms, our problem definitions (including error bounds), implementation, and analysis do not depend on global network parameters such as the network size, a goal which required new analysis techniques. We argue that breaking the dependence of these algorithms on global parameters makes more sense and aligns better with the rise of ubiquitous computing, where devices will be increasingly working locally in an otherwise massive network. Our push for locality, in other words, is a contribution independent of the specific radio network model and problem studied here.",,27 p.,,,,Theory of Computation,,,,,,,,,,,,,2015-05-18T20:00:07Z,,,,,,,,,,,,,,,
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-05-21T21:00:09Z,2015-05-21T21:00:09Z,2015-05-21,http://hdl.handle.net/1721.1/97051,,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems (Supplementary Material),"We analyze reported patches for three prior generate-and-validate patch generation systems (GenProg, RSRepair, and AE). Because of errors in the patch evaluation infrastructure, the majority of the reported patches violate the basic principle behind the design of these systems   they do not produce correct outputs even for the inputs in the test suite used to validate the patches. We also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",,13152246 bytes,,,Automatic Repair; Patch Analysis; Function Elimination,Computer Architecture,,,,,,,,,,,,,2015-05-21T21:00:09Z,,,,,,,,,,,,,,,
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-05-26T23:00:02Z,2015-05-26T23:00:02Z,2015-05-26,http://hdl.handle.net/1721.1/97089,MIT-CSAIL-TR-2015-020,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,"We analyze reported patches for three existing generate-and-validate patch generation systems (GenProg, RSRepair, and AE). The basic principle behind generate-and-validate systems is to accept only plausible patches that produce correct outputs for all inputs in the test suite used to validate the patches. Because of errors in the patch evaluation infrastructure, the majority of the reported patches are not plausible --- they do not produce correct outputs even for the inputs in the validation test suite. The overwhelming majority of the reported patches are not correct and are equivalent to a single modification that simply deletes functionality. Observed negative effects include the introduction of security vulnerabilities and the elimination of desirable standard functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",,24 p.,,,,Program Analysis,,,,,,,,,,,,,2015-05-26T23:00:02Z,,,,,,,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",2015-05-26T21:30:02Z,2015-05-26T21:30:02Z,2015-05-26,http://hdl.handle.net/1721.1/97087,MIT-CSAIL-TR-2015-018,Automatic Discovery and Patching of Buffer and Integer Overflow Errors,"We present Targeted Automatic Patching (TAP), an automatic buffer and integer overflow discovery and patching system. Starting with an application and a seed input that the application processes correctly, TAP dynamically analyzes the execution of the application to locate target memory allocation sites and statements that access dynamically or statically allocated blocks of memory. It then uses targeted error-discovery techniques to automatically generate inputs that trigger integer and/or buffer overflows at the target sites. When it discovers a buffer or integer overflow error, TAP automatically matches and applies patch templates to generate patches that eliminate the error. Our experimental results show that TAP successfully discovers and patches two buffer and six integer overflow errors in six real-world applications.",,10 p.,,,,Program Analysis,,,,,,,,,,,,,2015-05-26T21:30:03Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Rinard, Martin",2015-05-26T22:00:02Z,2015-05-26T22:00:02Z,2015-05-26,http://hdl.handle.net/1721.1/97088,MIT-CSAIL-TR-2015-019,Prophet: Automatic Patch Generation via Learning from Successful Human Patches,"We present Prophet, a novel patch generation system that learns a probabilistic model over candidate patches from a large code database that contains many past successful human patches. It defines the probabilistic model as the combination of a distribution over program points based on error localization algorithms and a parameterized log-linear distribution over modification operations. It then learns the model parameters via maximum log-likelihood, which identifies important characteristics of the successful human patches. For a new defect, Prophet generates a search space that contains many candidate patches, applies the learned model to prioritize those potentially correct patches that are consistent with the identified successful patch characteristics, and then validates the candidate patches with a user supplied test suite.",,7 p.,,,,Program Analysis,,,,,,,,,,,,,2015-05-26T22:00:02Z,,,,,,,,,,,,,,,
Saman Amarasinghe,"Kjolstad, Fredrik; Kamil, Shoaib; Ragan-Kelley, Jonathan; Levin, David I.W.; Sueda, Shinjiro; Chen, Desai; Vouga, Etienne; Kaufman, Danny M.; Kanwar, Gurtej; Matusik, Wojciech; Amarasinghe, Saman",2015-05-26T19:15:03Z,2015-05-26T19:15:03Z,2015-05-26,http://hdl.handle.net/1721.1/97075,MIT-CSAIL-TR-2015-017,Simit: A Language for Physical Simulation,"Using existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. The alternative is to prototype simulations in a high-level language like Matlab, thereby sacrificing performance. The Matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. However, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. Translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and the machine. High-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude. In this paper, we present Simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph, and as a set of global vectors, matrices and tensors depending on what is convenient at any given time. Simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. Using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. We demonstrate that Simit is easy to use: a Simit program is typically shorter than a Matlab program; that it is high-performance: a Simit program running sequentially on a CPU performs comparably to hand-optimized simulations; and that it is portable: Simit programs can be compiled for GPUs with no change to the program, delivering 5-25x speedups over our optimized CPU code.",,17 p.,,,"Graphs, Matrices, Tensors, Simulation",Computer Architecture,,,,,,,,,,,,,2015-05-26T19:15:04Z,,,,,,,,,,,,,,,
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-05-29T20:45:03Z,2015-05-29T20:45:03Z,2015-05-29,http://hdl.handle.net/1721.1/97130,MIT-CSAIL-TR-2015-021,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,"We analyze reported patches for three existing generate-and-validate patch generation systems (GenProg, RSRepair, and AE). The basic principle behind generate-and-validate systems is to accept only plausible patches that produce correct outputs for all inputs in the test suite used to validate the patches. Because of errors in the patch evaluation infrastructure, the majority of the reported patches are not plausible -- they do not produce correct outputs even for the inputs in the validation test suite. The overwhelming majority of the reported patches are not correct and are equivalent to a single modification that simply deletes functionality. Observed negative effects include the introduction of security vulnerabilities and the elimination of desirable standard functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",,24 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-05-29T20:45:03Z,,,,,,,,,,,,,,,
Martin Rinard,"Stanley-Marbell, Phillip; Rinard, Martin",2015-06-04T17:00:11Z,2015-06-04T17:00:11Z,2015-06-04,http://hdl.handle.net/1721.1/97180,MIT-CSAIL-TR-2015-022,Value-Deviation-Bounded Serial Data Encoding for Energy-Efficient Approximate Communication,"Transferring data between ICs accounts for a growing proportion of system power in wearable and mobile systems. Reducing signal transitions reduces the dynamic power dissipated in this data transfer, but traditional approaches cannot be applied when the transfer interfaces are serial buses. To address this challenge, we present a family of optimal value-deviation-bounded approximate serial encoders (VDBS encoders) that significantly reduce signal transitions (and hence, dynamic power) for bit-serial communication interfaces. When the data in transfer are from sensors, VDBS encoding enables a tradeoff between power efficiency and application fidelity, by exploiting the tolerance of many of the typical algorithms consuming sensor data to deviations in values. We derive analytic formulations for the family of VDBS encoders and introduce an efficient algorithm that performs close to the Pareto-optimal encoders. We evaluate the algorithm in two applications: Encoding data between a camera and processor in a text-recognition system, and between an accelerometer and processor in a pedometer system. For the text recognizer, the algorithm reduces signal transitions by 55% on average, while maintaining OCR accuracy at over 90% for previously-correctly-recognized text. For the pedometer, the algorithm reduces signal transitions by an average of 54% in exchange for step count errors of under 5%.",,20 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-06-04T17:00:11Z,,,,,,,,,,,,,,,
Julie A Shah,"Gombolay, Matthew C.",2015-07-06T22:15:05Z,2015-07-06T22:15:05Z,2015-07-02,http://hdl.handle.net/1721.1/97689,MIT-CSAIL-TR-2015-025,PhD Thesis Proposal: Human-Machine Collaborative Optimization via Apprenticeship Scheduling,"Resource optimization in health care, manufacturing, and military operations requires the careful choreography of people and equipment to effectively fulfill the responsibilities of the profession. However, resource optimization is a computationally challenging problem, and poorly utilizing resources can have drastic consequences. Within these professions, there are human domain experts who are able to learn from experience to develop strategies, heuristics, and rules-of-thumb to effectively utilize the resources at their disposal. Manually codifying these heuristics within a computational tool is a laborious process and leaves much to be desired. Even with a codified set of heuristics, it is not clear how to best insert an autonomous decision-support system into the human decision-making process. The aim of this thesis is to develop an autonomous computational method for learning domain-expert heuristics from demonstration that can support the human decision-making process. We propose a new framework, called apprenticeship scheduling, which learns and embeds these heuristics within a scalable resource optimization algorithm for real-time decision-support. Our initial investigation, comprised of developing scalable methods for scheduling and studying shared control in human-machine collaborative resource optimization, inspires the development of our apprenticeship scheduling approach. We present a promising, initial prototype for learning heuristics from demonstration and outline a plan for our continuing work.",,24 pages,,,,Interactive Robotics Group,,,,,,,,,,,,,2015-07-06T22:15:05Z,,,,,,,,,,,,,,,
Daniel Weitzner,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",2015-07-07T02:15:02Z,2015-07-07T02:15:02Z,2015-07-06,http://hdl.handle.net/1721.1/97690,MIT-CSAIL-TR-2015-026,Keys Under Doormats: Mandating insecurity by requiring government access to all data and communications,"Twenty years ago, law enforcement organizations lobbied to require data and communication services to engineer their products to guarantee law enforcement access to all data. After lengthy debate and vigorous predictions of enforcement channels going dark, these attempts to regulate the emerging Internet were abandoned. In the intervening years, innovation on the Internet flourished, and law enforcement agencies found new and more effective means of accessing vastly larger quantities of data. Today we are again hearing calls for regulation to mandate the provision of exceptional access mechanisms. In this report, a group of computer scientists and security experts, many of whom participated in a 1997 study of these same topics, has convened to explore the likely effects of imposing extraordinary access mandates. We have found that the damage that could be caused by law enforcement exceptional access requirements would be even greater today than it would have been 20 years ago. In the wake of the growing economic and social cost of the fundamental insecurity of today's Internet environment, any proposals that alter the security dynamics online should be approached with caution. Exceptional access would force Internet system developers to reverse forward secrecy design practices that seek to minimize the impact on user privacy when systems are breached. The complexity of today's Internet environment, with millions of apps and globally connected services, means that new law enforcement requirements are likely to introduce unanticipated, hard to detect security flaws. Beyond these and other technical vulnerabilities, the prospect of globally deployed exceptional access systems raises difficult problems about how such an environment would be governed and how to ensure that such systems would respect human rights and the rule of law.",,34 p.,,,,Decentralized Information Group,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-07-07T16:15:15Z,,,,,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",,,,,,,,,,
Martin Rinard,"Long, Fan; Rinard, Martin",2015-07-14T15:45:04Z,2015-07-14T15:45:04Z,2015-07-13,http://hdl.handle.net/1721.1/97735,MIT-CSAIL-TR-2015-027,Prophet: Automatic Patch Generation via Learning from Successful Patches,"We present Prophet, a novel patch generation system that learns a probabilistic model over candidate patches from a database of past successful patches. Prophet defines the probabilistic model as the combination of a distribution over program points based on defect localization algorithms and a parametrized log-linear distribution over modification operations. It then learns the model parameters via maximum log-likelihood, which identifies important characteristics of the previous successful patches in the database. For a new defect, Prophet generates a search space that contains many candidate patches, applies the learned model to prioritize those potentially correct patches that are consistent with the identified successful patch characteristics, and then validates the candidate patches with a user supplied test suite. The experimental results indicate that these techniques enable Prophet to generate correct patches for 15 out of 69 real-world defects in eight open source projects. The previous state of the art generate and validate system, which uses a set of hand-code heuristics to prioritize the search, generates correct patches for 11 of these same 69 defects.",,13 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-07-14T15:45:05Z,,,,,,,,,,,,,,,
Manolis Kellis,"Feizi, Soheil; Makhdoumi, Ali; Duffy, Ken; Kellis, Manolis; Medard, Muriel",2015-09-23T19:00:07Z,2015-09-23T19:00:07Z,2015-09-21,http://hdl.handle.net/1721.1/98878,MIT-CSAIL-TR-2015-028,Network Maximal Correlation,"Identifying nonlinear relationships in large datasets is a daunting task particularly when the form of the nonlinearity is unknown. Here, we introduce Network Maximal Correlation (NMC) as a fundamental measure to capture nonlinear associations in networks without the knowledge of underlying nonlinearity shapes. NMC infers, possibly nonlinear, transformations of variables with zero means and unit variances by maximizing total nonlinear correlation over the underlying network. For the case of having two variables, NMC is equivalent to the standard Maximal Correlation. We characterize a solution of the NMC optimization using geometric properties of Hilbert spaces for both discrete and jointly Gaussian variables. For discrete random variables, we show that the NMC optimization is an instance of the Maximum Correlation Problem and provide necessary conditions for its global optimal solution. Moreover, we propose an efficient algorithm based on Alternating Conditional Expectation (ACE) which converges to a local NMC optimum. For this algorithm, we provide guidelines for choosing appropriate starting points to jump out of local maximizers. We also propose a distributed algorithm to compute a 1-$\epsilon$ approximation of the NMC value for large and dense graphs using graph partitioning. For jointly Gaussian variables, under some conditions, we show that the NMC optimization can be simplified to a Max-Cut problem, where we provide conditions under which an NMC solution can be computed exactly. Under some general conditions, we show that NMC can infer the underlying graphical model for functions of latent jointly Gaussian variables. These functions are unknown, bijective, and can be nonlinear. This result broadens the family of continuous distributions whose graphical models can be characterized efficiently. We illustrate the robustness of NMC in real world applications by showing its continuity with respect to small perturbations of joint distributions. We also show that sample NMC (NMC computed using empirical distributions) converges exponentially fast to the true NMC value. Finally, we apply NMC to different cancer datasets including breast, kidney and liver cancers, and show that NMC infers gene modules that are significantly associated with survival times of individuals while they are not detected using linear association measures.",,48 p.,,,,Computational Biology (Kellis),,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-09-23T19:00:07Z,,,,,,,,,,,,,,,
Hari Balakrishnan,"Chen, Tiffany Yu-Han; Sivaraman, Anirudh; Das, Somak; Ravindranath, Lenin; Balakrishnan, Hari",2015-09-25T15:45:09Z,2015-09-25T15:45:09Z,2015-09-24,http://hdl.handle.net/1721.1/98905,MIT-CSAIL-TR-2015-029,Designing a Context-Sensitive Context Detection Service for Mobile Devices,"This paper describes the design, implementation, and evaluation of Amoeba, a context-sensitive context detection service for mobile devices. Amoeba exports an API that allows a client to express interest in one or more context types (activity, indoor/outdoor, and entry/exit to/from named regions), subscribe to specific modes within each context (e.g., ""walking"" or ""running"", but no other activity), and specify a response latency (i.e., how often the client is notified). Each context has a detector that returns its estimate of the mode. The detectors take both the desired subscriptions and the current context detection into account, adjusting both the types of sensors and the sampling rates to achieve high accuracy and low energy consumption. We have implemented Amoeba on Android. Experiments with Amoeba on 45+ hours of data show that our activity detector achieves an accuracy between 92% and 99%, outperforming previous proposals like UCLA* (59%), EEMSS (82%) and SociableSense (72%), while consuming 4 to 6× less energy.",,12 p.,,,context detection; context sensing; activity recognition; indoor detection; geofence; sensors; mobile sensing; energy,Networks & Mobile Systems,,,,,,,,,,,,,2015-09-25T15:45:09Z,,,,,,,,,,,,,,,
Karen Sollins; Danny Weitzner,"Bruce, Elizabeth; Sollins, Karen; Vernon, Mona; Weitzner, Danny",2015-10-02T15:45:04Z,2015-10-02T15:45:04Z,2015-10-01,http://hdl.handle.net/1721.1/99127,MIT-CSAIL-TR-2015-030,Big Data Privacy Scenarios,"This paper is the first in a series on privacy in Big Data. As an outgrowth of a series of workshops on the topic, the Big Data Privacy Working Group undertook a study of a series of use scenarios to highlight the challenges to privacy that arise in the Big Data arena. This is a report on those scenarios. The deeper question explored by this exercise is what is distinctive about privacy in the context of Big Data. In addition, we discuss an initial list of issues for privacy that derive specifically from the nature of Big Data. These derive from observations across the real world scenarios and use cases explored in this project as well as wider reading and discussions:* Scale: The sheer size of the datasets leads to challenges in creating, managing and applying privacy policies.* Diversity: The increased likelihood of more and more diverse participants in Big Data collection, management, and use, leads to differing agendas and objectives. By nature, this is likely to lead to contradictory agendas and objectives.* Integration: With increased data management technologies (e.g. cloud services, data lakes, and so forth), integration across datasets, with new and often surprising opportunities for cross-product inferences, will also come new  information  about individuals and their behaviors.* Impact on secondary participants: Because many pieces of information are reflective of not only the targeted subject, but secondary, often unattended, participants, the inferences and resulting information will increasingly be reflective of other people, not originally considered as the subject of privacy concerns and approaches.* Need for emergent policies for emergent information: As inferences over merged data sets occur, emergent information or understanding will occur. Although each unique data set may have existing privacy policies and enforcement mechanisms, it is not clear that it is possible to develop the requisite and appropriate emerged privacy policies and appropriate enforcement of them automatically.",,53 p.,,,Big Data; Use scenarios; Privacy,Advanced Network Architecture; Decentralized Information Group,,Creative Commons Attribution-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nd/4.0/,,,,,,,,,,2015-10-02T15:45:04Z,,,,,,,,,,,,,,,
Michael Stonebraker,"Battle, Leilani; Chang, Remco; Stonebraker, Michael",2015-10-19T20:15:05Z,2015-10-19T20:15:05Z,2015-10-19,http://hdl.handle.net/1721.1/99361,MIT-CSAIL-TR-2015-031,Dynamic Prefetching of Data Tiles for Interactive Visualization,"In this paper, we present ForeCache, a general-purpose tool for exploratory browsing of large datasets. ForeCache utilizes a client-server architecture, where the user interacts with a lightweight client-side interface to browse datasets, and the data to be browsed is retrieved from a DBMS running on a back-end server. We assume a detail-on-demand browsing paradigm, and optimize the back-end support for this paradigm by inserting a separate middleware layer in front of the DBMS. To improve response times, the middleware layer fetches data ahead of the user as she explores a dataset. We consider two different mechanisms for prefetching: (a) learning what to fetch from the user's recent movements, and (b) using data characteristics (e.g., histograms) to find data similar to what the user has viewed in the past. We incorporate these mechanisms into a single prediction engine that adjusts its prediction strategies over time, based on changes in the user's behavior. We evaluated our prediction engine with a user study, and found that our dynamic prefetching strategy provides: (1) significant improvements in overall latency when compared with non-prefetching systems (430% improvement); and (2) substantial improvements in both prediction accuracy (25% improvement) and latency (88% improvement) relative to existing prefetching techniques.",,13 p.,,,visualization; interactive exploration; databases,Database,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2015-10-19T20:15:05Z,,,,,,,,,,,,,,,
Leslie Kaelbling,"Zewdie, Dawit H.; Konidaris, George",2015-11-30T19:30:04Z,2015-11-30T19:30:04Z,2015-11-24,http://hdl.handle.net/1721.1/100053,MIT-CSAIL-TR-2015-032,Representation Discovery for Kernel-Based Reinforcement Learning,"Recent years have seen increased interest in non-parametric reinforcement learning. There are now practical kernel-based algorithms for approximating value functions; however, kernel regression requires that the underlying function being approximated be smooth on its domain. Few problems of interest satisfy this requirement in their natural representation. In this paper we define Value-Consistent Pseudometric (VCPM), the distance function corresponding to a transformation of the domain into a space where the target function is maximally smooth and thus well-approximated by kernel regression. We then present DKBRL, an iterative batch RL algorithm interleaving steps of Kernel-Based Reinforcement Learning and distance metric adjustment. We evaluate its performance on Acrobot and PinBall, continuous-space reinforcement learning domains with discontinuous value functions.",,16 p.,,,Metric learning,Learning and Intelligent Systems,,Creative Commons Attribution-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-sa/4.0/,,,,,,,,,,2015-11-30T19:30:04Z,,,,,,,,,,,,,,,
Patrick Winston,"Finlayson, Mark Alan",2015-12-03T16:30:05Z,2015-12-03T16:30:05Z,2015-12-02,http://hdl.handle.net/1721.1/100054,,"Supplementary materials for ""ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory""","This archive contains the supplementary material for the journal article ""ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory"", published in the Journal of Digital Scholarship in the Humanities (DSH), ca. 2016.The archive contains several different types of files. First, it contains the annotation guides that were used to train the annotators. The guides are numbered to match the team numbers in Table 6. Included here are not only detailed guides for some layers, as produced by the original developers of the specification, but also our synopsis guides for each layer, which were used as a reference and further training material for the annotators. Also of interest are the general annotator and adjudicator training guides, which outline the general procedures followed by the teams when conducting annotation. Those who are organizing their own annotation projects may find this material useful.Second, the archive contains a comprehensive manifest, in Excel spreadsheet format, listing the word counts, sources, types, and titles (in both Russian and English) of all the texts that are part of the corpus. Finally, the archive contains the actual corpus data files, in Story Workbench format, an XML-encoded stand-off annotation scheme. The scheme is described in the file format specification file, also included in the archive. These files can be parsed with the aid of any normal XML reading software, or can be loaded and edited easily with the Story Workbench annotation tool, also freely available.",,8341 KiB,,,,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,Patrick Winston; Genesis,,,,,,,2015-12-03T16:30:05Z,,,,,,,,,,,,,,,
Daniel Sanchez,"Beckmann, Nathan; Tsai, Po-An; Sanchez, Daniel",2015-12-21T19:00:18Z,2015-12-21T19:00:18Z,2015-12-19,http://hdl.handle.net/1721.1/100466,MIT-CSAIL-TR-2015-035,Jenga: Harnessing Heterogeneous Memories through Reconfigurable Cache Hierarchies,"Conventional memory systems are organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit a wide range of applications, because working sets settle at the smallest (and fastest) level they fit in. However, rigid hierarchies also cause significant overheads, because each level adds latency and energy even when it does not capture the working set. In emerging systems with heterogeneous memory technologies such as stacked DRAM, these overheads often limit performance and efficiency. We propose Jenga, a reconfigurable cache hierarchy that avoids these pathologies and approaches the performance of a hierarchy optimized for each application. Jenga monitors application behavior and dynamically builds virtual cache hierarchies out of heterogeneous, distributed cache banks. Jenga uses simple hardware support and a novel software runtime to configure virtual cache hierarchies. On a 36-core CMP with a 1 GB stacked-DRAM cache, Jenga outperforms a combination of state-of-the-art techniques by 10% on average and by up to 36%, and does so while saving energy, improving system-wide energy-delay product by 29% on average and by up to 96%.",,12 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:18Z,,,,,,,,,,,,,,,
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2015-12-21T19:00:09Z,2015-12-21T19:00:09Z,2015-12-19,http://hdl.handle.net/1721.1/100464,MIT-CSAIL-TR-2015-033,Cache Calculus: Modeling Caches through Differential Equations,"Caches are critical to performance, yet their behavior is hard to understand and model. In particular, prior work does not provide closed-form solutions of cache performance, i.e. simple expressions for the miss rate of a specific access pattern. Existing cache models instead use numerical methods that, unlike closed-form solutions, are computationally expensive and yield limited insight. We present cache calculus, a technique that models cache behavior as a system of ordinary differential equations, letting standard calculus techniques find simple and accurate solutions of cache performance for common access patterns.",,4 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:10Z,,,,,,,,,,,,,,,
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2015-12-21T19:00:15Z,2015-12-21T19:00:15Z,2015-12-19,http://hdl.handle.net/1721.1/100465,MIT-CSAIL-TR-2015-034,Bridging Theory and Practice in Cache Replacement,"Much prior work has studied processor cache replacement policies, but a large gap remains between theory and practice. The optimal policy (MIN) requires unobtainable knowledge of the future, and prior theoretically-grounded policies use reference models that do not match real programs. Meanwhile, practical policies are designed empirically. Lacking a strong theoretical foundation, they do not make the best use of the information available to them. This paper bridges theory and practice. We propose that practical policies should replace lines based on their economic value added (EVA), the difference of their expected hits from the average. We use Markov decision processes to show that EVA is optimal under some reasonable simplifications. We present an inexpensive, practical implementation of EVA and evaluate it exhaustively over many cache sizes. EVA outperforms prior practical policies and saves area at iso-performance. These results show that formalizing cache replacement yields practical benefits.",,14 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:15Z,,,,,,,,,,,,,,,
Martin Rinard,"Shen, Jiasi; Rinard, Martin",2015-12-28T22:15:05Z,2015-12-28T22:15:05Z,2015-12-27,http://hdl.handle.net/1721.1/100542,MIT-CSAIL-TR-2015-036,Filtered Iterators For Safe and Robust Programs in RIFL,"We present a new language construct, filtered iterators, for safe and robust input processing. Filtered iterators are designed to eliminate many common input-processing errors while enabling robust continued execution. The design is inspired by (a) observed common input-processing errors and (b) continued execution strategies that are implemented by developers fixing input validation errors. Filtered iterators decompose inputs into input units, atomically and automatically discarding units that trigger errors. Statistically significant results from a developer study highlight the difficulties that developers encounter when developing input-processing code using standard language constructs. These results also demonstrate the effectiveness of filtered iterators in eliminating many of these difficulties and enabling developers to produce safe and robust input-processing code.",,111 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-12-28T22:15:05Z,,,,,,,,,,,,,,,
Daniel Jackson,"McCutchen, Richard Matthew; Itzhaky, Shachar; Jackson, Daniel",2016-01-12T21:15:03Z,2016-01-12T21:15:03Z,2016-01-12,http://hdl.handle.net/1721.1/100803,MIT-CSAIL-TR-2016-001,Initial report on Object Spreadsheets,"There is a growing demand for data-driven web applications that help automate organizational and business processes of low to medium complexity by letting users view and update structured data in controlled ways. We present Object Spreadsheets, an end-user development tool that combines a spreadsheet interface with a rich data model to help the process administrators build the logic for such applications themselves. Its all-in-one interface with immediate feedback has the potential to bring more complex tasks within reach of end-user developers, compared to existing approaches. Our data model is based on the structure of entity-relationship models and directly supports nested variable-size collections and object references, which are common in web applications but poorly accommodated by traditional spreadsheets. Object Spreadsheets has a formula language suited to the data model and supports stored procedures to specify the forms of updates that application users may make. Formulas can be used to assemble data in the exact structure in which it is to be shown in the application UI, simplifying the task of UI building; we intend for Object Spreadsheets to be integrated with a UI builder to provide a complete solution for application development. We describe our prototype implementation and several example applications we built to demonstrate the applicability of the tool.",,27 p.,,,,Software Design,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2016-01-12T21:15:03Z,,,,,,,,,,,,,,,
Adam Chlipala,"Pit-Claudel, Clément; Mariet, Zelda; Harding, Rachael; Madden, Sam",2016-02-10T18:45:06Z,2016-02-10T18:45:06Z,2016-02-08,http://hdl.handle.net/1721.1/101150,MIT-CSAIL-TR-2016-002,Outlier Detection in Heterogeneous Datasets using Automatic Tuple Expansion,"Rapidly developing areas of information technology are generating massive amounts of data. Human errors, sensor failures, and other unforeseen circumstances unfortunately tend to undermine the quality and consistency of these datasets by introducing outliers -- data points that exhibit surprising behavior when compared to the rest of the data. Characterizing, locating, and in some cases eliminating these outliers offers interesting insight about the data under scrutiny and reinforces the confidence that one may have in conclusions drawn from otherwise noisy datasets. In this paper, we describe a tuple expansion procedure which reconstructs rich information from semantically poor SQL data types such as strings, integers, and floating point numbers. We then use this procedure as the foundation of a new user-guided outlier detection framework, dBoost, which relies on inference and statistical modeling of heterogeneous data to flag suspicious fields in database tuples. We show that this novel approach achieves good classification performance, both in traditional numerical datasets and in highly non-numerical contexts such as mostly textual datasets. Our implementation is publicly available, under version 3 of the GNU General Public License.",,12 p.,,,,Programming Languages and Verification,,,,,,,,,,,,,2016-02-10T18:45:07Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Rinard, Martin",2016-02-18T20:45:03Z,2016-02-18T20:45:03Z,2016-02-18,http://hdl.handle.net/1721.1/101211,MIT-CSAIL-TR-2016-003,An Analysis of the Search Spaces for Generate and Validate Patch Generation Systems,"We present the first systematic analysis of the characteristics of patch search spaces for automatic patch generation systems. We analyze the search spaces of two current state-of- the-art systems, SPR and Prophet, with 16 different search space configurations. Our results are derived from an analysis of 1104 different search spaces and 768 patch generation executions. Together these experiments consumed over 9000 hours of CPU time on Amazon EC2.The analysis shows that 1) correct patches are sparse in the search spaces (typically at most one correct patch per search space per defect), 2) incorrect patches that nevertheless pass all of the test cases in the validation test suite are typically orders of magnitude more abundant, and 3) leveraging information other than the test suite is therefore critical for enabling the system to successfully isolate correct patches.We also characterize a key tradeoff in the structure of the search spaces. Larger and richer search spaces that contain correct patches for more defects can actually cause systems to find fewer, not more, correct patches. We identify two reasons for this phenomenon: 1) increased validation times because of the presence of more candidate patches and 2) more incorrect patches that pass the test suite and block the discovery of correct patches. These fundamental properties, which are all characterized for the first time in this paper, help explain why past systems often fail to generate correct patches and help identify challenges, opportunities, and productive future directions for the field.",,45 p.,,,Program repair,Program Analysis and Compilation,,,,,,,,,,,,,2016-02-18T20:45:03Z,,,,,,,,,,,,,,,
Hari Balakrishnan,"Deng, Shuo; Sivaraman, Anirudh; Balakrishnan, Hari",2016-03-09T00:00:06Z,2016-03-09T00:00:06Z,2016-02-25,http://hdl.handle.net/1721.1/101636,MIT-CSAIL-TR-2016-004,Delphi: A Software Controller for Mobile Network Selection,"This paper presents Delphi, a mobile software controller that helps applications select the best network among available choices for their data transfers. Delphi optimizes a specified objective such as transfer completion time, or energy per byte transferred, or the monetary cost of a transfer. It has four components: a performance predictor that uses features gathered by a network monitor, and a traffic profiler to estimate transfer sizes near the start of a transfer, all fed into a network selector that uses the prediction and transfer size estimate to optimize an objective.For each transfer, Delphi either recommends the  best  single network to use, or recommends Multi-Path TCP (MPTCP), but crucially selects the network for MPTCP s  primary subflow . The choice of primary subflow has a strong impact onthe transfer completion time, especially for short transfers.We designed and implemented Delphi in Linux. It requires no application modifications. Our evaluation shows that Delphi reduces application network transfer time by 46% for Web browsing and by 49% for video streaming, comparedwith Android s default policy of always using Wi-Fi when it is available. Delphi can also be configured to achieve high throughput while being battery-efficient: in this configuration, it achieves 1.9x the throughput of Android s default policy while only consuming 6% more energy.",,14 p.,,,MPTCP; Network Selection; Wi-Fi; LTE,Networks & Mobile Systems,,,,,,,,,,,,,2016-03-09T00:00:06Z,,,,,,,,,,,,,,,
Leslie Kaelbling,"Kawaguchi, Kenji",2016-05-24T20:45:08Z,2016-05-24T20:45:08Z,2016-05-23,http://hdl.handle.net/1721.1/102665,MIT-CSAIL-TR-2016-005,Deep Learning without Poor Local Minima,"In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.",,26 p.,,,Optimization; Neural Network; Machine Learning; High Dimension; Convex; Non-convex; Local minimum; Global minimum; Saddle point; Critical point,Learning and Intelligent Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2016-05-24T20:45:08Z,,,,,,,,,,,,,,,
Leslie Kaelbling,"Kawaguchi, Kenji",2016-06-01T22:00:14Z,2016-06-01T22:00:14Z,2016-05-26,http://hdl.handle.net/1721.1/102796,MIT-CSAIL-TR-2016-006,Towards Practical Theory: Bayesian Optimization and Optimal Exploration,"This thesis discusses novel principles to improve the theoretical analyses of a class of methods, aiming to provide theoretically driven yet practically useful methods. The thesis focuses on a class of methods, called bound-based search, which includes several planning algorithms (e.g., the A* algorithm and the UCT algorithm), several optimization methods (e.g., Bayesian optimization and Lipschitz optimization), and some learning algorithms (e.g., PAC-MDP algorithms). For Bayesian optimization, this work solves an open problem and achieves an exponential convergence rate. For learning algorithms, this thesis proposes a new analysis framework, called PAC-RMDP, and improves the previous theoretical bounds. The PAC-RMDP framework also provides a unifying view of some previous near-Bayes optimal and PAC-MDP algorithms. All proposed algorithms derived on the basis of the new principles produced competitive results in our numerical experiments with standard benchmark tests.",,87 p.,,,PAC-MDP; AI planning; Global optimization,Learning and Intelligent Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,SM thesis,,,,,,,,,2016-06-01T22:00:15Z,,,,,,,,,,,,,,,
Karen Sollins,"Jing, Yuxin",2016-06-28T22:15:02Z,2016-06-28T22:15:02Z,2016-06-28,http://hdl.handle.net/1721.1/103381,MIT-CSAIL-TR-2016-009,Evaluating Caching Mechanisms In Future Internet Architectures,"This thesis seeks to test and evaluate the effects of in-­network storage in novel proposed Internet architectures in terms of their performance. In a world where more and more people are mobile and connected to the Internet, we look at how the added variable of user mobility can affect how these architectures perform under different loads. Evaluating the effects of in­-network storage and caching in these novel architectures will provide another facet to understanding how viable of an alternative they would be to the current TCP/IP paradigm of today's Internet. In Named Data Networking, where the storage is used to directly cache content, we see its use of storage impact the locality of where things are, while in MobilityFirst, where storage is used to cache chunks to provide robust delivery, we look at how its different layers work together in a mobility event.",,58 p.,,,in-network storage; mobility; scalability,Advanced Network Architecture,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,MEng thesis,,,,,,,,,2016-06-28T22:15:02Z,,,,,,,,,,,,,,,
Karen Sollins,"Xu, Shidan",2016-06-28T21:30:08Z,2016-06-28T21:30:08Z,2016-06-28,http://hdl.handle.net/1721.1/103379,MIT-CSAIL-TR-2016-007,Modeling Network User Behavior: Various Approaches,"This project involves learning to predict users' mobility within the network topology. Topological mobility, as opposed to physical mobility, can be substantial as a user switches from LTE to wifi network, while moving minimally physically. Our dataset consists of email IMAP logs as they document associated client IP addresses, as well as the clients' identifiers. Prediction for online mobility is of particular interest to the networks community. If we can predict online mobility with high probability, then new network architecture can be designed to optimize the caching system by minimizing resending packets. We used various approaches and techniques to model the user's behavior, including probabilistic programming, regression, neural nets, and clustering algorithms. We compare and contrast how models differ in their prediction accuracy, speed of convergence, and algorithmic complexity.",,60 p.,,,,Advanced Network Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,MEng thesis,,,,,,,,,2016-06-28T21:30:09Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Amidon, Peter; Rinard, Martin",2017-05-02T21:45:03Z,2017-05-02T21:45:03Z,2016-07-08,http://hdl.handle.net/1721.1/108619,MIT-CSAIL-TR-2017-008,Automatic Inference of Code Transforms and Search Spaces for Automatic Patch Generation Systems,"We present a new system, Genesis, that processes sets of human patches to automatically infer code transforms and search spaces for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the resulting complete Genesis patch generation system working with real-world patches and errors collected from top 1000 github Java software development projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from successful patches.",,26 p.,,,,Program Analysis and Compilation,,,,,,,,,,http://hdl.handle.net/1721.1/103556,MIT-CSAIL-TR-2016-010,,2017-05-02T21:45:04Z,,,,,,,,,,,,,,,
Martin Rinard,"Long, Fan; Amidon, Peter; Rinard, Martin",2016-07-08T20:15:07Z,2016-07-08T20:15:07Z,2016-07-08,http://hdl.handle.net/1721.1/103556,MIT-CSAIL-TR-2016-010,Automatic Inference of Code Transforms and Search Spaces for Automatic Patch Generation Systems,"We present a new system, Genesis, that processes sets of human patches to automatically infer code transforms and search spaces for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the resulting complete Genesis patch generation system working with real-world patches and errors collected from top 1000 github Java software development projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from successful patches.",,14 p.,,,,Program Analysis and Compilation,,,,,,,,,MIT-CSAIL-TR-2017-008,http://hdl.handle.net/1721.1/108619,,,2016-07-08T20:15:08Z,,,,,,,,,,,,,,,
Hari Balakrishnan,"Perry, Jonathan; Balakrishnan, Hari; Shah, Devavrat",2016-08-15T20:00:07Z,2016-08-15T20:00:07Z,2016-08-15,http://hdl.handle.net/1721.1/103920,MIT-CSAIL-TR-2016-011,Flowtune: Flowlet Control for Datacenter Networks,"Rapid convergence to a desired allocation of network resources to endpoint traffic has been a long-standing challenge for packet-switched networks. The reason for this is that congestion control decisions are distributed across the endpoints, which vary their offered load in response to changes in application demand and network feedback on a packet-by-packet basis. We propose a different approach for datacenter networks, flowlet control, in which congestion control decisions are made at the granularity of a flowlet, not a packet. With flowlet control, allocations have to change only when flowlets arrive or leave. We have implemented this idea in a system called Flowtune using a centralized allocator that receives flowlet start and end notifications from endpoints. The allocator computes optimal rates using a new, fast method for network utility maximization, and updates endpoint congestion-control parameters. Experiments show that Flowtune outperforms DCTCP, pFabric, sfqCoDel, and XCP on tail packet delays in various settings, converging to optimal rates within a few packets rather than over several RTTs. Our implementation of Flowtune handles 10.4x more throughput per core and scales to 8x more cores than Fastpass, for an 83-fold throughput gain.",,15 p.,,,,Networks & Mobile Systems,,,,,,,,,,,,,2016-08-15T20:00:08Z,,,,,,,,,,,,,,,
Karen Sollins,"Rock, Colleen T.",2016-09-26T22:15:03Z,2016-09-26T22:15:03Z,2016-09-26,http://hdl.handle.net/1721.1/104385,MIT-CSAIL-TR-2016-012,Examining Key Mobility Resources through Denial of Service Attacks on proposed Global Name Resolution Services,"The problem we address in this thesis is to uncover the design elements in a network architecture design that may open it up to denial of service (DoS) attacks and to expose the tradeoffs in mitigating those DoS opportunities. We take as our candidate network architecture design the Future Internet Architecture project MobilityFirst. MobilityFirst's overarching goal, driven by increasingly available wireless communication, is the support of mobility in an Internet architecture. At its core, MobilityFirst separates identification from location, as distinct from the current Internet architecture, and postulates the existence of globally unique, flat identifiers. In order to support mobility in this context, it also postulates a global name resolution service (GNRS). In this thesis we examine three alternative designs for the GNRS and the opportunities they expose for DoS attacks. We consider each one in depth analytically. As an example, we then study one particular attack in depth and are forced to conclude that approaches to mitigating this attack would have significant negative impact on the support of mobility thus exposing the dilemma in such system design tradeoffs.",,68 p.,,,DMap; GMap; Auspice,Advanced Network Architecture,,,,MEng thesis,,,,,,,,,2016-09-26T22:15:03Z,,,,,,,,,,,,,,,
Nickolai Zeldovich,"Lazar, David; Zeldovich, Nickolai",2016-10-26T16:00:07Z,2016-10-26T16:00:07Z,2016-10-05,http://hdl.handle.net/1721.1/105093,MIT-CSAIL-TR-2016-013,Alpenhorn: Bootstrapping Secure Communication without Leaking Metadata,"Alpenhorn is the first system for initiating an encrypted connection between two users that provides strong privacy and forward secrecy guarantees for metadata (i.e., information about which users connected to each other) and that does not require out-of-band communication other than knowing the other user's Alpenhorn username (email address). This resolves a significant shortcoming in all prior works on private messaging, which assume an out-of-band key distribution mechanism. Alpenhorn's design builds on three ideas. First, Alpenhorn provides each user with an address book of friends that the user can call to establish a connection. Second, when a user adds a friend for the first time, Alpenhorn ensures the adversary does not learn the friend's identity, by using identity-based encryption in a novel wayto privately determine the friend's public key. Finally, when calling a friend, Alpenhorn ensures forward secrecy of metadata by storing pairwise shared secrets in friends' address books, and evolving them over time, using a new keywheel construction. Alpenhorn relies on a number of servers, but operates in an anytrust model, requiring just one of the servers to be honest. We implemented a prototype of Alpenhorn, and integrated it into the Vuvuzela private messaging system (which did not previously provide privacy or forward secrecy of metadata when initiating conversations). Experimental results show that Alpenhorn can scale to many users, supporting 10 million users on three Alpenhorn servers with an average call latency of 150 seconds and a client bandwidth overhead of 3.7 KB/sec.",,17 p.,,,,Parallel and Distributed Operating Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,"David Lazar and Nickolai Zeldovich. Alpenhorn: Bootstrapping Secure Communication without Leaking Metadata. In Proceedings of the 12th Symposium on Operating Systems Design and Implementation (OSDI), Savannah, GA, Nov. 2016.",,,,,,,,2016-10-26T16:00:07Z,,,,,,,,,,,,,,,
Brian Williams,"Yu, Peng",2017-02-07T23:00:15Z,2017-02-07T23:00:15Z,2016-10-14,http://hdl.handle.net/1721.1/106886,MIT-CSAIL-TR-2017-001,Collaborative Diagnosis of Over-Subscribed Temporal Plans,"Over-subscription, that is, being assigned too many tasks or requirements that are too demanding, is commonly encountered in temporal planning problems. As human beings, we often want to do more than we can, ask for things that may not be available, while underestimating how long it takes to perform each task. It is often difficult for us to detect the causes of failure in such situations and then find resolutions that are effective. We can greatly benefit from tools that assist us by looking out for these plan failures, by identifying their root causes, and by proposing preferred resolutions to these failures that lead to feasible plans. In recent literature, several approaches have been developed to resolve such over-subscribed problems, which are often framed as over-constrained scheduling, configuration design or optimal planning problems. Most of them take an all-or-nothing approach, in which over-subscription is resolved through suspending constraints or dropping goals. While helpful, in real-world scenarios, we often want to preserve our plan goals as much possible. As human beings, we know that slightly weakening the requirements of a travel plan, or replacing one of its destinations with an alternative one is often sufficient to resolve an over-subscription problem, no matter if the requirement being weakened is the duration of a deep-sea survey being planned for, or the restaurant cuisine for a dinner date. The goal of this thesis is to develop domain independent relaxation algorithms that perform this type of slight weakening of constraints, which we will formalize as continuous relaxation, and to embody them in a computational aid, Uhura, that performs tasks akin to an experienced travel agent or ocean scientists. In over-subscribed situations, Uhura helps us diagnose the causes of failure, suggests alternative plans, and collaborates with us in order to resolve conflicting requirements in the most preferred way. Most importantly, the algorithms underlying Uhura supports the weakening, instead of suspending, of constraints and variable domains in a temporally flexible plan. The contribution of this thesis is two-fold. First, we developed an algorithmic framework, called Best-first Conflict-Directed Relaxation (BCDR), for performing plan relaxation. Second, we use the BCDR framework to perform relaxation for several different families of plan representations involving different types of constraints. These include temporal constraints, chance constraints and variable domain constraints, and we incorporate several specialized conflict detection and resolution algorithms in support of the continuous weakening of them. The key idea behind BCDR's approach to continuous relaxation is to generalize the concepts of discrete conflicts and relaxations, first introduced by the model-based diagnosis community, to hybrid conflicts and relaxations, which denote minimal inconsistencies and minimal relaxations to both discrete and continuous relaxable constraints.",,197 p.,,,planning; scheduling; constraint relaxation; conflict-directed search,Model-based Embedded and Robotic Systems,,,,PhD thesis,"Collaborative Diagnosis of Over-Subscribed Temporal Plans, Peng Yu, PhD Thesis, Massachusetts Institute of Technology, 2016",,,,,,,,2017-02-07T23:00:15Z,,,,,,,,,,,,,,,
Patrick Winston,"Finlayson, Mark Alan",2016-11-08T23:00:04Z,2016-11-08T23:00:04Z,2016-11-08,http://hdl.handle.net/1721.1/105270,MIT-CSAIL-TR-2016-014,Report on the 2015 NSF Workshop on Unified Annotation Tooling,"On March 30 & 31, 2015, an international group of twenty-three researchers with expertise in linguistic annotation convened in Sunny Isles Beach, Florida to discuss problems with and potential solutions for the state of linguistic annotation tooling. The participants comprised 14 researchers from the U.S. and 9 from outside the U.S., with 7 countries and 4 continents represented, and hailed from fields and specialties including computational linguistics, artificial intelligence, speech processing, multi-modal data processing, clinical & medical natural language processing, linguistics, documentary linguistics, sign-language linguistics, corpus linguistics, and the digital humanities. The motivating problem of the workshop was the balkanization of annotation tooling, namely, that even though linguistic annotation requires sophisticated tool support to efficiently generate high-quality data, the landscape of tools for the field is fractured, incompatible, inconsistent, and lacks key capabilities. The overall goal of the workshop was to chart the way forward, centering on five key questions: (1) What are the problems with current tool landscape? (2) What are the possible benefits of solving some or all of these problems? (3) What capabilities are most needed? (4) How should we go about implementing these capabilities? And, (5) How should we ensure longevity and sustainability of the solution? I surveyed the participants before their arrival, which provided significant raw material for ideas, and the workshop discussion itself resulted in identification of ten specific classes of problems, five sets of most-needed capabilities. Importantly, we identified annotation project managers in computational linguistics as the key recipients and users of any solution, thereby succinctly addressing questions about the scope and audience of potential solutions. We discussed management and sustainability of potential solutions at length. The participants agreed on sixteen recommendations for future work. This technical report contains a detailed discussion of all these topics, a point-by-point review of the discussion in the workshop as it unfolded, detailed information on the participants and their expertise, and the summarized data from the surveys.",,61 p.,,,Linguistic Annotation; Annotation Tools & Resources; Natural Language Processing,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2016-11-08T23:00:04Z,,,,,,,,,,,,,,,
Patrick Winston,"Eisenberg, Joshua D.; Finlayson, Mark A.",2016-11-10T20:45:06Z,2016-11-10T20:45:06Z,2016-11-09,http://hdl.handle.net/1721.1/105279,,"Data and Code for ""Automatic Identification of Narrative Diegesis and Point of View""","This archive contains the code and data for the workshop article ""Automatic Identification of Narrative Diegesis and Point of View,"" published in 2016 in the 2nd Workshop for Computing News Storylines (CNewsStory 2016), co-located with EMNLP 2016 in Austin, TX. The root of the archive contains a README file which explains the archive contents. Furthermore, the archive can be imported directly into the Eclipse IDE as a project encapsulating the executable code required to reproduce the results of the paper; the code compiles with Java 1.8. The archive also contains a copy of the final version of the paper for reference.",,224 MiB,,,,Genesis,,,,,,,,,,,,,2016-11-10T20:45:06Z,,,,,,,,,,,,,,,
Frans Kaashoek,"Chajed, Tej; Gjengset, Jon; Kaashoek, M. Frans; Mickens, James; Morris, Robert; Zeldovich, Nickolai",2016-12-12T23:30:04Z,2016-12-12T23:30:04Z,2016-12-08,http://hdl.handle.net/1721.1/105802,MIT-CSAIL-TR-2016-015,Oort: User-Centric Cloud Storage with Global Queries,"In principle, the web should provide the perfect stage for user-generated content, allowing users to share their data seamlessly with other users across services and applications. In practice, the web fragments a user's data over many sites, each exposing only limited APIs for sharing. This paper describes Oort, a new cloud storage system that organizes data primarily by user rather than by application or web site. Oort allows users to choose which web software to use with their data and which other users to share it with, while giving applications powerful tools to query that data. Users rent space from providers that cooperate to provide a global, federated, general-purpose storage system. To support large-scale, multi-user applications such as Twitter and e-mail, Oort provides global queries that find and combine data from relevant users across all providers. Oort makes global query execution efficient by recognizing and merging similar queries issued by many users' application instances, largely eliminating the per-user factor in the global complexity of queries. Our evaluation predicts that an Oort implementation could handle traffic similar to that seen by Twitter using a hundred cooperating Oort servers, and that applications with other sharing patterns, like e-mail, can also be executed efficiently.",,14 p.,,,cloud storage; global queries; user-centric storage; sharing; web development,Parallel and Distributed Operating Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2016-12-12T23:30:04Z,,,,,,,,,,,,,,,
Brian Williams,"Lane, Spencer Dale",2016-12-15T23:15:10Z,2016-12-15T23:15:10Z,2016-12-14,http://hdl.handle.net/1721.1/105848,MIT-CSAIL-TR-2016-016,Propositional and Activity Monitoring Using Qualitative Spatial Reasoning,"Communication is the key to effective teamwork regardless of whether the team members are humans or machines. Much of the communication that makes human teams so effective is non-verbal; they are able to recognize the actions that the other team members are performing and take their own actions in order to assist. A robotic team member should be able to make the same inferences, observing the state of the environment and inferring what actions are being taken. In this thesis I introduce a novel approach to the combined problem of activity recognition and propositional monitoring. This approach breaks down the problem into smaller sub-tasks. First, the raw sensor input is parsed into simple, easy to understand primitive semantic relationships known as qualitative spatial relations (QSRs). These primitives are then combined to estimate the state of the world in the same language used by most planners, planning domain definition language (PDDL) propositions. Both the primitives and propositions are combined to infer the status of the actions that the human is taking. I describe an algorithm for solving each of these smaller problems and describe the modeling process for a variety of tasks from an abstracted electronic component assembly (ECA) scenario. I implemented this scenario on a robotic testbed and collected data of a human performing the example actions.",,80 p.,,,"hybrid systems, filtering, activity recognition, qualitative spatial relations",Model-based Embedded and Robotic Systems,,,,SM thesis,,,,,,,,,2016-12-15T23:15:10Z,,,,,,,,,,,,,,,
Howard Shrobe,"Khan, M. Taimoor; Serpanos, Dimitrios; Shrobe, Howard",2016-12-15T23:15:03Z,2016-12-15T23:15:03Z,2016-12-15,http://hdl.handle.net/1721.1/105847,MIT-CSAIL-TR-2016-017,Sound and Complete Runtime Security Monitor for Application Software,"We present a run-time security monitor that detects both known and unknown cyber attacks by checking that the run-time behavior of the application is consistent with the expected behavior modeled by an application specification. This is crucial because, even if the implementation is consistent with its specification, the application may still be vulnerable due to flaws in the supporting infrastructure. This run-time security monitor is sound and complete, eliminating false alarms, as well as efficient, so that it does not limit run-time application performance and so that it supports real-time systems. Importantly, this monitor is readily applicable to both legacy and new system platforms.The security monitor takes as input the application specification and the application implementation, which may be expressed in different languages. The security monitor detects attacks by systematically comparing the application execution and specification behaviors at run-time, even though they operate at two different levels of abstraction. We define the denotational semantics of the specification language and prove that the monitor is sound and complete, i.e. if the application is consistent with its specification, the security monitor will produce no false alarms (soundness) and that it will detect any deviation of the application from the behavior sanctioned by the specification language (completeness). Importantly, the application specification language enables the description of known or potential attack plans, enabling not only attack detection but attack characterization as well.",,42 p.,,,,Cybersecurity,,Creative Commons Attribution-NonCommercial 4.0 International,http://creativecommons.org/licenses/by-nc/4.0/,,,,,,,,,,2016-12-15T23:15:03Z,,,,,,,,,,,,,,,
Tomas Lozano-Perez,"Anders, Ariel; Kaelbling, Leslie; Lozano-Perez, Tomas",2017-04-28T19:45:06Z,2017-04-28T19:45:06Z,2017-01-30,http://hdl.handle.net/1721.1/108510,MIT-CSAIL-TR-2017-007,Planning Robust Strategies for Constructing Multi-object Arrangements,"A crucial challenge in robotics is achieving reliable results in spite of sensing and control uncertainty. A prominent strategy for dealing with uncertainty is to construct a feedback policy, where actions are chosen as a function of the current state estimate. However, constructing such policies is computationally very difficult. An alternative strategy is conformant planning which finds open-loop action sequences that achieve the goal for all input states and action outcomes. In this work, we investigate the conformant planning approach to robot manipulation. In particular, we tackle the problem of pushing multiple objects simultaneously to achieve a specified arrangement. Conformant planning is a belief-state planning problem. A belief state is the set of all possible states of the world, and the goal is to find a sequence of actions that will bring an initial belief state to a goal belief state To do forward belief-state planning, we created a deterministic belief-state transition model from supervised learning based on physics simulations. A key pitfall in conformant planning is that the complexity of the belief state tends to increase with each operation, making it increasingly harder to compute the effect of actions. This work explores the idea that we can construct conformant plans for robot manipulation by only using actions resulting in compact belief states.",,8 pp.,,,manipulation; robotics; machine learning; belief space; planning; uncertainty,Learning and Intelligent Systems,,,,,,,,,,,,,2017-04-28T19:45:06Z,,,,,,,,,,,,,,,
John Leonard,"Rosen, David M.; Carlone, Luca; Bandeira, Afonso S.; Leonard, John J.",2017-02-07T23:00:06Z,2017-02-07T23:00:06Z,2017-02-05,http://hdl.handle.net/1721.1/106885,MIT-CSAIL-TR-2017-002,SE-Sync: A Certifiably Correct Algorithm for Synchronization over the Special Euclidean Group,"Many important geometric estimation problems naturally take the form of synchronization over the special Euclidean group: estimate the values of a set of unknown poses given noisy measurements of a subset of their pairwise relative transforms. Examples of this class include the foundational problems of pose-graph simultaneous localization and mapping (SLAM) (in robotics), camera motion estimation (in computer vision), and sensor network localization (in distributed sensing), among others. This inference problem is typically formulated as a nonconvex maximum-likelihood estimation that is computationally hard to solve in general. Nevertheless, in this paper we present an algorithm that is able to efficiently recover certifiably globally optimal solutions of the special Euclidean synchronization problem in a non-adversarial noise regime. The crux of our approach is the development of a semidefinite relaxation of the maximum-likelihood estimation whose minimizer provides an exact MLE so long as the magnitude of the noise corrupting the available measurements falls below a certain critical threshold; furthermore, whenever exactness obtains, it is possible to verify this fact a posteriori, thereby certifying the optimality of the recovered estimate. We develop a specialized optimization scheme for solving large-scale instances of this semidefinite relaxation by exploiting its low-rank, geometric, and graph-theoretic structure to reduce it to an equivalent optimization problem defined on a low-dimensional Riemannian manifold, and then design a Riemannian truncated-Newton trust-region method to solve this reduction efficiently. Finally, we combine this fast optimization approach with a simple rounding procedure to produce our algorithm, SE-Sync. Experimental evaluation on a variety of simulated and real-world pose-graph SLAM datasets shows that SE-Sync is capable of recovering certifiably globally optimal solutions when the available measurements are corrupted by noise up to an order of magnitude greater than that typically encountered in robotics and computer vision applications, and does so more than an order of magnitude faster than the Gauss-Newton-based approach that forms the basis of current state-of-the-art techniques.",,"49 pages, 20 figures",,,Simultaneous localization and mapping (SLAM); Maximum-likelihood estimation; Convex relaxation; Low-rank semidefinite programming; Riemannian optimization,Marine Robotics,,,,,,,,,,,,,2017-02-07T23:00:06Z,,,,,,,,,,,,,,,
Saman Amarasinghe,"Kjolstad, Fredrik; Kamil, Shoaib; Chou, Stephen; Lugato, David; Amarasinghe, Saman",2017-02-21T22:00:07Z,2017-02-21T22:00:07Z,2017-02-17,http://hdl.handle.net/1721.1/107013,MIT-CSAIL-TR-2017-003,The Tensor Algebra Compiler,"Tensor and linear algebra is pervasive in data analytics and the physical sciences. Often the tensors, matrices or even vectors are sparse. Computing expressions involving a mix of sparse and dense tensors, matrices and vectors requires writing kernels for every operation and combination of formats of interest. The number of possibilities is infinite, which makes it impossible to write library code for all. This problem cries out for a compiler approach. This paper presents a new technique that compiles compound tensor algebra expressions combined with descriptions of tensor formats into efficient loops. The technique is evaluated in a prototype compiler called taco, demonstrating competitive performance to best-in-class hand-written codes for tensor and matrix operations.",,14 p.,,,Tensor Algebra; Linear Algebra; Compiler; C++ Library,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2017-02-21T22:00:07Z,,,,,,,,,,,,,,,
Vinod Vaikuntanathan,"Micali, Silvio; Vaikuntanathan, Vinod",2017-04-06T23:00:04Z,2017-04-06T23:00:04Z,2017-03-31,http://hdl.handle.net/1721.1/107927,MIT-CSAIL-TR-2017-004,Optimal and Player-Replaceable Consensus with an Honest Majority,"We construct a Byzantine Agreement protocol that tolerates t < n/2 corruptions, is very efficient in terms of the number of rounds and the number of bits of communication, and satisfies a strong notion of robustness called player replaceability (defined in [Mic16]). We provide an analysis of our protocol when executed on real-world networks such as the ones employed in the bitcoin protocol.",,8 p.,,,Byzantine Agreement; Cryptography; Honest Majority,Theory of Computation,,,,,,,,,,,,,2017-04-06T23:00:04Z,,,,,,,,,,,,,,,
Vinod Vaikuntanathan,"Lombardi, Alex; Vaikuntanathan, Vinod",2017-04-06T23:00:10Z,2017-04-06T23:00:10Z,2017-04-06,http://hdl.handle.net/1721.1/107928,MIT-CSAIL-TR-2017-005,On the Non-Existence of Blockwise 2-Local PRGs with Applications to Indistinguishability Obfuscation,"Lin and Tessaro (Eprint 2017/250) recently proposed indistinguishability obfuscation and functional encryption candidates and proved their security based on a standard assumption on bilinear maps and a non-standard assumption on ``Goldreich-like'' pseudorandom generators (PRG). In a nutshell, they require the existence of pseudo-random generators $G:\Sigma^n \to \{0,1\}^m$ for some $\mathsf{poly}(n)$-size alphabet $\Sigma$ where each output bit depends on at most two input alphabet symbols, and which achieve sufficiently large stretch. We show a polynomial-time attack against such generators. Our attack uses tools from the literature on two-source extractors (Chor and Goldreich, SICOMP 1988) and efficient refutation of 2-CSPs over large alphabets (Allen, O'Donnell and Witmer, FOCS 2015). Finally, we propose new ways to instantiate the Lin-Tessaro construction that do not immediately fall to our attacks. While we cannot say with any confidence that these modifications are secure, they certainly deserve further cryptanalysis.",,12 p.,,,Indistinguishability Obfuscation,Theory of Computation,,,,,,,,,,,,,2017-04-06T23:00:10Z,,,,,,,,,,,,,,,
Martin Rinard,"Rinard, Martin; Shen, Jiasi",2017-04-24T19:45:07Z,2017-04-24T19:45:07Z,2017-04-24,http://hdl.handle.net/1721.1/108383,MIT-CSAIL-TR-2017-006,Inference and Regeneration of Programs that Store and Retrieve Data,"As modern computation platforms become increasingly complex, their programming interfaces are increasingly difficult to use. This complexity is especially inappropriate given the relatively simple core functionality that many of the computations implement. We present a new approach for obtaining so ware that executes on modern computing platforms with complex programming interfaces. Our approach starts with a simple seed program, written in the language of the developer's choice, that implements the desired core functionality. It then systematically generates inputs and observes the resulting outputs to learn the core functionality. It finally automatically regenerates new code that implements the learned core functionality on the target computing platform. This regenerated code contains both (a) boilerplate code for the complex programming interfaces that the target computing platform presents and (b) systematic error and vulnerability checking code that makes the new implementations robust and secure. By providing a productive new mechanism for capturing and encapsulating knowledge about how to use modern complex interfaces, this new approach promises to greatly reduce the developer effort required to obtain secure, robust so ware that executes on modern computing platforms.",,22 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2017-04-24T19:45:07Z,,,,,,,,,,,,,,,
John Leonard,"Benjamin, Michael R.",2017-05-17T16:00:05Z,2017-05-17T16:00:05Z,2017-05-16,http://hdl.handle.net/1721.1/109146,MIT-CSAIL-TR-2017-009,Autonomous COLREGS Modes and Velocity Functions,"This paper concerns an implementation of an autonomy system for unmanned surface vessels operating in accordance with the Coast Guard Collision Regulations (COLREGS). The autonomy system is implemented by associating a dedicated ownship behavior module for each contact for collision avoidance. For each behavior, a mode determination is made based on the COLREGS rules, ownship position and trajectory, and the contact position and trajectory. Based on the mode, an appropriate objective function is generated, over the set of possible ownship maneuvers, to bias the vehicle in accordance with the COLREGS. The focus on this paper is solely on (a) the mode determination algorithms, (b) the requisite ownship and contact terms regarding position, trajectory and relative position utilized in the mode determination algorithms, and (c) the form and equations used in making the objective functions associated with each mode.",,47 p.,,,Autonomy; Marine Autonomy; Unmanned Surface Vehicles; Collision Avoidance; Multi-Objective Optimization; Interval Programming; IvP; MOOS-IvP; pHelmIvP; Collision Regulations; Velocity Functions; Objective Functions; Autonomous Surface Vehicle; Autonomous Marine Vehicle; Rules of the Road,Marine Robotics,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2017-05-17T16:00:05Z,,,,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Vlachos, Georgios",2017-06-07T22:00:04Z,2017-06-07T22:00:04Z,2017-06-05,http://hdl.handle.net/1721.1/109726,MIT-CSAIL-TR-2017-010,Multi-Unit Auction Revenue with Possibilistic Beliefs,"The revenue of traditional auction mechanisms is benchmarked solely against the players' own valuations, despite the fact that they may also have valuable beliefs about each other's valuations. Not much is known about generating revenue in auctions of multiple identical copies of a same good. (In particular the celebrated Vickrey mechanism has no revenue guarantees.) For such auctions, we (1) put forward an attractive revenue benchmark, based on the players' possibilistic about each other, and (2) construct a mechanism that achieves such benchmark, assuming that the players are two-level rational (where the rationality is in the sense of Aumann).",,20 p.,,,Multi-unit auctions; Possibilistic beliefs,Theory of Computation,,,,,,,,,,,,,2017-06-07T22:00:05Z,,,,,,,,,,,,,,,
Alan Edelman,"Ahrens, Willow; Schiefer, Nicholas; Xu, Helen",2017-06-12T15:45:16Z,2017-06-12T15:45:16Z,2017-06-09,http://hdl.handle.net/1721.1/109792,MIT-CSAIL-TR-2017-011,An Efficient Fill Estimation Algorithm for Sparse Matrices and Tensors in Blocked Formats,"Tensors, linear-algebraic extensions of matrices in arbitrary dimensions, have numerous applications in computer science and computational science. Many tensors are sparse, containing more than 90% zero entries. Efficient algorithms can leverage sparsity to do less work, but the irregular locations of the nonzero entries pose challenges to performance engineers. Many tensor operations such as tensor-vector multiplications can be sped up substantially by breaking the tensor into equally sized blocks (only storing blocks which contain nonzeros) and performing operations in each block using carefully tuned code. However, selecting the best block size is computationally challenging. Previously, Vuduc et al. defined the fill of a sparse tensor to be the number of stored entries in the blocked format divided by the number of nonzero entries, and showed that the fill can be used as an effective heuristic to choose a good block size. However, they gave no accuracy bounds for their method for estimating the fill, and it is vulnerable to adversarial examples. In this paper, we present a sampling-based method for finding a (1 + epsilon)-approximation to the fill of an order N tensor for all block sizes less than B, with probability at least 1 - delta, using O(B^(2N) log(B^N / delta) / epsilon^2) samples for each block size. We introduce an efficient routine to sample for all B^N block sizes at once in O(N B^N) time. We extend our concentration bounds to a more efficient bound based on sampling without replacement, using the recent Hoeffding-Serfling inequality. We then implement our algorithm and compare our scheme to that of Vuduc, as implemented in the Optimized Sparse Kernel Interface (OSKI) library. We find that our algorithm provides faster estimates of the fill at all accuracy levels, providing evidence that this is both a theoretical and practical improvement. Our code is available under the BSD 3-clause license at https://github.com/peterahrens/FillEstimation.",,19 p.,,,Randomized Algorithm; Sampling Algorithm; Autotuning; Performance Engineering; Numerical Linear Algebra,Theory of Computation,,,,,,,,,,,,,2017-06-12T15:45:16Z,,,,,,,,,,,,,,https://github.com/peterahrens/FillEstimation,
Martin Rinard,"Shen, Jiasi; Rinard, Martin",2017-08-29T22:00:05Z,2017-08-29T22:00:05Z,2017-08-29,http://hdl.handle.net/1721.1/111067,MIT-CSAIL-TR-2017-012,Inference and Regeneration of Programs that Manipulate Relational Databases,"We present a new technique that infers models of programs that manipulate relational databases. This technique generates test databases and input commands, runs the program, then observes the resulting outputs and updated databases to infer the model. Because the technique works only with the externally observable inputs, outputs, and databases, it can infer the behavior of programs written in arbitrary languages using arbitrary coding styles and patterns. We also present a technique for automatically regenerating an implementation of the program based on the inferred model. The regenerator can produce a translated implementation in a different language and systematically include relevant security and error checks. We present results that illustrate the use of the technique to eliminate SQL injection vulnerabilities and the translation of applications from Java and Ruby on Rails to Python.",,14 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2017-08-29T22:00:05Z,,,,,,,,,,,,,,,
,"Fourie, Dehann",2022-09-01T18:03:04Z,2022-09-01T18:03:04Z,2017-08-31,https://hdl.handle.net/1721.1/145253,,Multi-modal and Inertial sensor Solutions for Navigation-type Factor Graphs,"This thesis presents a sum-product inference algorithm for platform navigation called Multi-modal iSAM (incremental smoothing and mapping). Common Gaussian only likelihoods are restrictive and require a complex front-end processes to deal with non-Gaussian measurements. Instead, our approach allows the front-end to defer ambiguities with non-Gaussian measurement models. We retain the acyclic Bayes tree (and incremental update strategy) from the predecessor iSAM2 max-product algorithm [Kaess et al., IJRR 2012]. The approach propagates continuous beliefs on the Bayes (Junction) tree, which is an efficient symbolic refactorization
of the nonparametric factor graph, and asymptotically approximates the underlying Chapman-Kolmogorov equations. Our method tracks dominant modes in the marginal posteriors of all variables with minimal approximation error, while suppressing almost all low likelihood modes (in a non-permanent manner). Keeping with existing inertial navigation, we present a novel, continuous-time, retroactively calibrating inertial odometry residual function, using preintegration to seamlessly incorporate pure inertial sensor measurements into a factor graph. We centralize around a factor graph (with starved graph databases) to separate elements of the navigation into an ecosystem of processes. Practical examples are included, such as how to infer multi-modal marginal posterior belief estimates for ambiguous loop closures; raw beam-formed acoustic measurements; or conventional parametric likelihoods, and others.",,,,en_US,"SLAM, robotics, navigation, mapping, localization, factor graph, Bayes tree, non-Gaussian, multi-modal, junction tree, inertial odometry, preintegration, IMU, radar, acoustic, Chapman-Kolmogorov transit integral",,"NSF, ONR, DARPA",CC0 1.0 Universal,http://creativecommons.org/publicdomain/zero/1.0/,,"Fourie, D., 2017. Multi-modal and inertial sensor solutions for navigation-type factor graphs (Doctoral dissertation, Massachusetts Institute of Technology and Woods Hole Oceanographic Institution).",,,,,,,,,,,,Thesis,,,,,,,,,MIT,,
John Leonard,"Benjamin, Michael R.",2017-09-01T20:15:02Z,2017-09-01T20:15:02Z,2017-09-01,http://hdl.handle.net/1721.1/111117,MIT-CSAIL-TR-2017-013,The Interval Programming Model Solution Algorithm Experimentation Tools and Results,"Interval programming (IvP) is model for representing multi-objective optimization problems along with a set of solution algorithms. This paper describes a set of IvP solution experiments run over randomly generated problem instances, using five different versions of the Recursive Interval Programming ALgorithm (RIPAL). The final version is the algorithm used most extensively in practice, with the first four provided mostly for comparison as the final version is built up in complexity. The full details of the algorithms are outside the scope of this paper, with the focus here being the experimental results, and the software tools and technique used in generating the problem instances. Additional tools are described for facilitating the experiments, including visualization tools, and tools for generating the plots and tables shown in this document. All software tools are available under an open source license, and all problem instances reported here are also available online. This document is meant to supplement other discussions on the IvP model, algorithm, and IvP applications to provide the detail of reporting that would not be possible due to length restrictions of other papers.",,68 p.,,,Multi-objective Optimization; piecewise linear functions; decision making,Marine Robotics,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,,,,,,,,,2017-09-01T20:15:03Z,,,,,,,,,,,,,,,
Michael Carbin,"Atkinson, Eric; Carbin, Michael",2017-11-13T18:30:11Z,2017-11-13T18:30:11Z,2017-11-09,http://hdl.handle.net/1721.1/112172,MIT-CSAIL-TR-2017-014,Typesafety for Explicitly-Coded Probabilistic Inference Procedures,"Researchers have recently proposed several systems that ease the process of developing Bayesian probabilistic inference algorithms. These include systems for automatic inference algorithm synthesis as well as stronger abstractions for manual algorithm development. However, existing systems whose performance relies on the developer manually constructing a part of the inference algorithm have limited support for reasoning about the correctness of the resulting algorithm. In this paper, we present Shuffle, a programming language for developing manual inference algorithms that enforces 1) the basic rules of probability theory and 2) statistical dependencies of the algorithm's corresponding probabilistic model. We have used Shuffle to develop inference algorithms for several standard probabilistic models. Our results demonstrate that Shuffle enables a developer to deliver performant implementations of these algorithms with the added benefit of Shuffle's correctness guarantees.",,41 p.,,,type safety,Programming Systems,,,,,,,,,,,,,2017-11-13T18:30:11Z,,,,,,,,,,,,,,,
Martin Rinard,"Cambronero, Jose; Rinard, Martin",2017-12-22T21:45:58Z,2017-12-22T21:45:58Z,2017-12-21,http://hdl.handle.net/1721.1/112949,MIT-CSAIL-TR-2017-015,Generating Component-based Supervised Learning Programs From Crowdsourced Examples,"We present CrowdLearn, a new system that processes an existing corpus of crowdsourced machine learning programs to learn how to generate effective pipelines for solving supervised machine learning problems. CrowdLearn uses a probabilistic model of program likelihood, conditioned on the current sequence of pipeline components and on the characteristics of the input data to the next component in the pipeline, to predict candidate pipelines. Our results highlight the effectiveness of this technique in leveraging existing crowdsourced programs to generate pipelines that work well on a range of supervised learning problems.",,14 p.,,,program synthesis; automated machine learning; code mining,Program Analysis and Compilation,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2017-12-22T21:45:58Z,,,,,,,,,,,,,,,
Karen Sollins,"Alawaji, Ahmed; Sollins, Karen",2018-01-24T20:15:10Z,2018-01-24T20:15:10Z,2018-01-24,http://hdl.handle.net/1721.1/113292,MIT-CSAIL-TR-2018-001,Privacy and Security Risks for National Health Records Systems,"A review of national health records (NEHR) systems shows that privacy and security risks have a profound impact on the success of such projects. Countries have different approaches when dealing with privacy and security considerations. The aims of this study were to explore how governments can design secure national health records systems. To do that systematically, we developed a framework to analyze NEHR systems. We then applied the framework to investigate the privacy and security risks in these systems. The studied systems demonstrate that getting privacy and security right have a considerable impact on the success of NEHR projects. Also, our study reveals that the healthcare system structure has a substantial impact on the adoption and usage rates of the system. The studied cases uncover many opportunities for improving privacy and security measures in future projects. The framework demonstrates the utility of applying it to the three cases.",,104 p.,,,Privacy; Security; Healthcare,Advanced Network Architecture,,,,SM thesis,,,,,,,,,2018-01-24T20:15:10Z,,,,,,,,,,,,,,,
Brian Williams,"Bhargava, Nikhil; Muise, Christian; Vaquero, Tiago; Williams, Brian",2018-01-29T23:15:05Z,2018-01-29T23:15:05Z,2018-01-29,http://hdl.handle.net/1721.1/113340,MIT-CSAIL-TR-2018-002,Delay Controllability: Multi-Agent Coordination under Communication Delay,"Simple Temporal Networks with Uncertainty provide a useful framework for modeling temporal constraints and, importantly, for modeling actions with uncertain durations. To determine whether we can construct a schedule for a given network, we typically consider one of two types of controllability: dynamic or strong. These controllability checks have strict conditions on how uncertainty is resolved; uncertain outcomes are either recognized immediately or not at all. In this paper, we introduce delay controllability, a novel generalization of both strong and dynamic controllability that additionally exposes a large range of controllability classes in between. To do so, we use a delay function to parameterize our controllability checking. This delay function represents the difference between when an event happens and the time that it is observed. We also provide a single unified algorithm for checking delay controllability that runs in O(n^3) time, matching the best known runtime for dynamic controllability, which we use to motivate the decision to generalize dynamic and strong controllability. We conclude by providing an empirical evaluation of delay controllability, demonstrating its superior accuracy and practical efficiency as compared to other existing approximations.",,34 p.,,,Temporal Controllability; Scheduling; Temporal Uncertainty,Model-based Embedded and Robotic Systems,,,,"New version posted April 19, 2019 with slight tweaks to the algorithm and added clarity based on reviewer feedback.",,,,,,,,,2018-01-29T23:15:05Z,,,,,,,,,,,,,,,
Boris Katz,"Katz, Boris; Borchardt, Gary; Felshin, Sue; Mora, Federico",2018-03-02T16:47:20Z,2018-03-02T16:47:20Z,2018-03-01,http://hdl.handle.net/1721.1/113912,MIT-CSAIL-TR-2018-013,A Natural Language Interface for Mobile Devices,"Creating a robust, automated capability to respond to natural language requests has been a longstanding goal in the development of intelligent systems. This article describes the StartMobile system, originally developed in 2005-2007, which has served as an important precursor to Apple's Siri system and other commercial natural language interfaces to mobile devices and computational resources. The article begins with a discussion of goals in creating natural language interfaces, continues with a description of the general-purpose START information access system, describes the StartMobile system and its capabilities, and concludes with a discussion of current commercial systems and future directions.",,21 p.,,,,Infolab,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2018-03-02T16:47:21Z,,,,,,,,,,,,,,,
Leslie Kaelbling,"Kawaguchi, Kenji; Kaelbling, Leslie Pack; Bengio, Yoshua",2018-05-09T19:55:51Z,2018-05-09T19:55:51Z,2018-05-01,http://hdl.handle.net/1721.1/115274,MIT-CSAIL-TR-2018-014,Generalization in Deep Learning,"With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.",,31 pages,,,neural network; learning theory,Learning and Intelligent Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2018-05-09T19:55:51Z,,,,,,,,,,,,,,,
Julie A Shah,"Unhelkar, Vaibhav V.; Shah, Julie A.",2018-05-17T19:19:11Z,2018-05-17T19:19:11Z,2018-05-17,http://hdl.handle.net/1721.1/115482,MIT-CSAIL-TR-2018-015,Learning Models of Sequential Decision-Making without Complete State Specification using Bayesian Nonparametric Inference and Active Querying,"Learning models of decision-making behavior during sequential tasks is useful across a variety of applications, including human-machine interaction. In this paper, we present an approach to learning such models within Markovian domains based on observing and querying a decision-making agent. In contrast to classical approaches to behavior learning, we do not assume complete knowledge of the state features that impact an agent's decisions. Using tools from Bayesian nonparametric inference and time series of agents  decisions, we first provide an inference algorithm to identify the presence of any unmodeled state features that impact decision making, as well as likely candidate models. In order to identify the best model among these candidates, we next provide an active querying approach that resolves model ambiguity by querying the decision maker. Results from our evaluations demonstrate that, using the proposed algorithms, an observer can identify the presence of latent state features, recover their dynamics, and estimate their impact on decisions during sequential tasks.",,11 p.,,,"Decision Making, Graphical Models, Human-AI Collaboration",Interactive Robotics Group,,,,,,,,,,,,,2018-05-17T19:19:11Z,,,,,,,,,,,,,,,
Brian Williams,"Timmons, Eric; Williams, Brian C.",2018-05-24T21:15:06Z,2018-05-24T21:15:06Z,2018-05-24,http://hdl.handle.net/1721.1/115882,MIT-CSAIL-TR-2018-016,"Best-first Enumeration Based on Bounding Conflicts, and its Application to Large-scale Hybrid Estimation","With the rise of autonomous systems, there is a need for them to have high levels of robustness and safety. This robustness can be achieved through systems that are self-repairing. Underlying this is the ability to diagnose subtle failures. Likewise, online planners can generate novel responses to exceptional situations. These planners require an accurate estimate of state. Estimation methods based on hybrid discrete/continuous state models have emerged as a method of computing precise state estimates, which can be employed for either diagnosis or planning in hybrid domains. However, existing methods have difficulty scaling to systems with more than a handful of components. Discrete state estimation capabilities can scale to this level by combining best-first enumeration and conflict-directed search. Best-first methods have been developed for hybrid estimation, but the creation of conflict-directed methods has previously been elusive. While conflicts are used to learn from constraint violation, probabilistic hybrid estimation is relatively unconstrained. In this paper we present an approach to hybrid estimation that unifies best-first enumeration and conflict-directed search through the concept of ""bounding"" conflicts, an extension of conflicts that represent tighter bounds on the cost of regions of the search space. This paper presents a general best-first search and enumeration algorithm based on bounding conflicts (A*BC) and a hybrid estimation method based on this enumeration algorithm. Experiments show that an A*BC powered state estimator produces estimates faster than the current state of the art, particularly on large systems.",,27 p.,,,belief revision and update; diagnosis; heuristics; search,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,2018-05-24T21:15:06Z,,,,,,,,,,,,,,,
Patrick Winston,"Labiba, Jahan,; Geeticka, Chauhan,; A., Finlayson, Mark",2018-06-07T18:30:15Z,2018-06-07T18:30:15Z,2018-06-07,http://hdl.handle.net/1721.1/116172,,"Data and Code for ""A New Approach to Animacy Detection""","This archive contains the code and data for the workshop article ""A New Approach to Animacy Detection,"" published in 2018 in the the 27th International Conference on Computational Linguistics (COLING 2018), in Santa Fe, NM. The root of the archive contains a readme file which explains the archive contents. Furthermore, the archive can be imported directly into the Eclipse IDE as a project encapsulating the executable code and data required to reproduce the results of the paper; the code compiles with Java 1.8. The archive also contains a copy of the near-final version of the paper for reference.",,373820733 bytes,,,referring expressions; coreference chains,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2018-06-07T18:30:15Z,,,,,,,,,,,,,,,
,"Shen, Jiasi; Rinard, Martin",2018-08-28T17:22:15Z,2018-08-28T17:22:15Z,2018-08-28,http://hdl.handle.net/1721.1/117593,,Using Active Learning to Synthesize Models of Applications That  Access Databases,"We present a new technique that uses active learning to infer models of 
applications that manipulate relational databases. This technique 
comprises a domain-specific language for modeling applications that 
access databases (each model is a program in this language) and an 
associated inference algorithm that infers models of applications whose 
behavior can be expressed in this language. The inference algorithm 
generates test inputs and database configurations, runs the application, 
then observes the resulting database traffic and outputs to 
progressively refine its current model hypothesis.  The end result is a 
model that completely captures the behavior of the application.  Because 
the technique works only with the externally observable inputs, outputs, 
and databases, it can infer the behavior of applications written in 
arbitrary languages using arbitrary coding styles (as long as the 
behavior of the application is expressible in the domain-specific language).

We also present a technique for automatically regenerating an 
implementation from the inferred model. The regenerator can produce a 
translated implementation in a different language and systematically 
include relevant security and error checks.",,,,,,,,,,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Shen, Jiasi; Rinard, MArtin",2018-09-27T13:22:38Z,2018-09-27T13:22:38Z,2018-09-27,http://hdl.handle.net/1721.1/118184,,Using Dynamic Monitoring to Synthesize Models of Applications That  Access Databases,"We previously developed Konure, a tool that uses active learning to 
infer the functionality of database applications. An alternative 
approach is to observe the inputs, outputs, and database traffic from a 
running system in normal use and then synthesize a model of the 
application from this information.  To evaluate these two approaches, we 
present Etch, which uses information from typical usage scenarios to 
synthesize a model of the functionality of database applications whose 
computation can be expressed in the Konure DSL.",,,,en_US,,,,,,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Kawaguchi, Kenji; Benigo, Yoshua; Verma, Vikas; Kaelbling, Leslie Pack",2018-10-01T15:44:44Z,2018-10-01T15:44:44Z,2018-10-01,http://hdl.handle.net/1721.1/118307,;MIT-CSAIL-TR-2018-019,Towards Understanding Generalization via Analytical Learning Theory,"This paper introduces a novel measure-theoretic theory for machine learning
that does not require statistical assumptions. Based on this theory, a new
regularization method in deep learning is derived and shown to outperform
previous methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed
theory provides a theoretical basis for a family of practically successful
regularization methods in deep learning. We discuss several consequences of
our results on one-shot learning, representation learning, deep learning,
and curriculum learning. Unlike statistical learning theory, the proposed
learning theory analyzes each problem instance individually via measure
theory, rather than a set of problem instances via statistics. As a result,
it provides different types of results and insights when compared to
statistical learning theory.",,,,en_US,Machine Learning; Measure Theory; Regularization method; Neural Network,,,,,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Cusumano-Towner, Marco F.; Saad, Feras A.; Lew, Alexander; Mansinghka, Vikash K.",2018-11-26T17:33:26Z,2018-11-26T17:33:26Z,2018-11-26,http://hdl.handle.net/1721.1/119255,;MIT-CSAIL-TR-2018-020,Gen: A General-Purpose Probabilistic Programming System with Programmable Inference,"Probabilistic modeling and inference are central to many fields. A key challenge for wider adoption of probabilistic programming languages is designing systems that are both flexible and performant. This paper introduces Gen, a new probabilistic programming system with novel language con- structs for modeling and for end-user customization and optimization of inference. Gen makes it practical to write probabilistic programs that solve problems from multiple fields. Gen programs can combine generative models written in Julia, neural networks written in TensorFlow, and custom inference algorithms based on an extensible library of Monte Carlo and numerical optimization techniques. This paper also presents techniques that enable Gen’s combination of flexibility and performance: (i) the generative function inter- face, an abstraction for encapsulating probabilistic and/or differentiable computations; (ii) domain-specific languages with custom compilers that strike different flexibility/per- formance tradeoffs; (iii) combinators that encode common patterns of conditional independence and repeated compu- tation, enabling speedups from caching; and (iv) a standard inference library that supports custom proposal distributions also written as programs in Gen. This paper shows that Gen outperforms state-of-the-art probabilistic programming systems, sometimes by multiple orders of magnitude, on problems such as nonlinear state-space modeling, structure learning for real-world time series data, robust regression, and 3D body pose estimation from depth images.",,,,en_US,,,"This research was supported in part by the US Department of the Air Force contract FA8750-17-C-0239, grants from the MIT Media Lab/Harvard Berkman Center Ethics & Gover- nance of AI Fund and the MIT CSAIL Systems that Learn Consortium, a gift from the Aphorism Foundation, and the US Department of Defense through the the National De- fense Science & Engineering Graduate Fellowship (NDSEG) Program.",,,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Gadient, Austin; Ortiz, Baltazar; Barrato, Ricardo; Davis, Eli; Perkins, Jeff; Rinard, Martin",2019-06-11T13:45:02Z,2019-06-11T13:45:02Z,2019-06-11,https://hdl.handle.net/1721.1/121246,,Automatic Exploitation of Fully Randomized Executables,"We present Marten, a new end to end system for automatically discovering, exploiting, and combining information leakage and buffer overflow vulnerabilities to derandomize and exploit remote, fully randomized processes. Results from two case studies high- light Marten’s ability to generate short, robust ROP chain exploits that bypass address space layout randomization and other modern defenses to download and execute injected code selected by an attacker.",,,,en_US,exploit; Symbolic Execution; taint analysis; information leakage,,DARPA Grant HR001118C0059,Attribution-NonCommercial-NoDerivs 3.0 United States,http://creativecommons.org/licenses/by-nc-nd/3.0/us/,"We present an automated system, Marten, that automatically generates control flow hijacking exploits against fully randomized executables by combining information leakage and buffer overflow exploits.",,,,,,,,,,,,,Article,,,,,,,,,,,
,"Bhargava, Nikhil; Williams, Brian C.",2019-08-15T17:30:01Z,2019-08-15T17:30:01Z,2019-08,https://hdl.handle.net/1721.1/121993,,Faster Dynamic Controllability Checking in Temporal Networks with Integer Bounds,"Simple Temporal Networks with Uncertainty (STNUs) provide a useful formalism with which to reason about events and the temporal constraints that apply to them. STNUs are in particular notable because they facilitate reasoning over stochastic, or uncontrollable, actions and their corresponding durations. To evaluate the feasibility of a set of constraints associated with an STNU, one checks the network's \textit{dynamic controllability}, which determines whether an adaptive schedule can be constructed on-the-fly. Our work provides a dynamic controllability checker that is able to quickly refute the controllability of an STNU with integer bounds, such as those found in planning problems. Our work is faster than the existing best runtime for networks with integer bounds and executes in O(min(mn, m\sqrt{n}\log{N}) + km + k^2n + kn\log{n}). Our approach pre-processes the STNU using an existing O(n^3) dynamic controllability checking algorithm and provides tighter bounds on its runtime. This makes our work easily adaptable to other algorithms that rely on checking variants of dynamic controllability.",,,,en_US,temporal reasoning; scheduling,,,,,,,,,,,,,,,,,,Article,,,,,,,,,International Joint Conference in Artificial Intelligence,,
,"Perkins, Jeff; Eikenberry, Jordan; Coglio, Alessandro; Rinard, Martin",2019-11-19T19:24:11Z,2019-11-19T19:24:11Z,2019-11-19,https://hdl.handle.net/1721.1/122969,,Comprehensive Java Metadata Tracking for Attack Detection and Repair,"We present ClearTrack, a system that tracks 32 bits of metadata for each primitive value in Java programs to detect and nullify a range of vulnerabilities such as integer overflow and underflow vulnerabilities, SQL injection vulnerabilities, and command injection vulnerabilities. Contributions include new techniques for eliminating false positives associated with benign integer overflows and underflows, new metadata-aware techniques for detecting and nullifying SQL and command injection attacks, and results from an evaluation of ClearTrack performed by a Test and Evaluation team hired by the sponsor of this research (an anonymous agency of the United States government). These results show that 1) ClearTrack operates successfully on Java programs comprising hundreds of thousands of lines of code (including instrumented jar files and Java system libraries, the majority of the applications comprise over 3 million lines of code), 2) because of computations such as cryptography and hash table calculations, these applications perform millions of benign integer overflows and underflows, and 3) ClearTrack successfully detects and nullifies all tested integer overflow and underflow, SQL injection, and command injection vulnerabilities in the benchmark applications.",,,,en_US,security; runtime instrumentation; numeric errrors,,,Attribution-NonCommercial-NoDerivs 3.0 United States,http://creativecommons.org/licenses/by-nc-nd/3.0/us/,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Gordon, Michael; Eikenberry, Jordan; Eden, Anthony; Perkins, Jeff; Rinard, Martin",2019-11-19T19:23:10Z,2019-11-19T19:23:10Z,2019-11-19,https://hdl.handle.net/1721.1/122968,,Precise and Comprehensive Provenance Tracking for Android Devices,"Detailed information about the paths that data take through a system is invaluable for understanding sources and behaviors of complex exfiltration malware. We present a new system, ClearScope, that tracks, at the level of individual bytes, the complete paths that data follow through Android systems. These paths include the original source where data entered the device (such as sensors or network connections), files in which the data was temporarily stored, applications that the data traversed during its time in the device, and sinks through which the data left the device.

The ClearScope system design enables this unprecedented level of provenance tracking detail by 1) structuring the provenance representation as references, via provenance tags, to provenance events that record the movement of data between system components and into or out of the device and 2) adopting a split design in which provenance events are streamed to a remote server for storage, with only the minimal information required to generate the tagged stream of events retained on the device. ClearScope also includes compiler optimizations that enable efficient provenance tracking within applications by eliminating unnecessary provenance tracking computations and adopting and efficient aggregate provenance representation for arrays when all array elements have the same provenance.

Experience using ClearScope to analyze the notorious Adups FOTA malware highlights the significant benefits that this level of comprehensive detail can bring. Performance experiments with the Caffeine Mark benchmarks show that the overall ClearScope provenance tracking overhead on this benchmark suite is 14%.",,,,en_US,security; runtime instrumentation; provenance,,DARPA (Grant FA8650-15-C-7564),Attribution-NonCommercial-NoDerivs 3.0 United States,http://creativecommons.org/licenses/by-nc-nd/3.0/us/,,,,,,,,,,,,,,Technical Report,,,,,,,,,,,
,"Nachin, Mergen",2021-03-02T23:12:23Z,2021-03-02T23:12:23Z,2021-03-02,https://hdl.handle.net/1721.1/130056,6.UAP: Undergraduate Advanced Project;Spring 2010,Lower Bounds on the Column Sparsity of Compressed Sensing Matrices,,,,,en,compressed sensing; sketching,,,Attribution 3.0 United States,http://creativecommons.org/licenses/by/3.0/us/,,,,,,,,,,,,,,Technical Report,,,,,,,,,,,
,"Zhang, Yuening",2021-03-02T23:13:24Z,2021-03-02T23:13:24Z,2021-03-02,https://hdl.handle.net/1721.1/130057,,Bucket Elimination Algorithm for Dynamic Controllability Checking of Simple Temporal Networks with Uncertainty,"Simple Temporal Networks with Uncertainty (STNU) can represent temporal problems where duration between events may be uncontrollable, e.g. when the event is caused by nature. An STNU is dynamically controllable (DC) if it can be successfully scheduled online. In this paper, we introduce a novel usage of bucket elimination algorithms for DC checking that matches the state of the art in achieving O(n^3) performance. Bucket elimination algorithms exist for STNs (path consistency and Fourier algorithms), but adapting it to STNUs is non-trivial. As a result, consistency checking becomes a special case of our algorithm. Due to the familiarity to bucket elimination algorithms, the final algorithm is easier to understand and implement. Additionally, conflict extraction is also easily supported in this framework.",,,,en,temporal networks; dynamic controllability; bucket elimination; scheduling with uncertainty,,,CC0 1.0 Universal,http://creativecommons.org/publicdomain/zero/1.0/,,,,,,,,,,,,,,Technical Report,,,,,,,,,,,
,"Shen, Jiasi; Rinard, Martin",2021-09-09T14:44:49Z,2021-09-09T14:44:49Z,2021-09-09,https://hdl.handle.net/1721.1/131244,,Active Loop Detection for Applications that Access Databases,"We present Shear, a new system that observes and manipulates the interaction between an application and its surrounding environment to learn a model of the behavior of the application. Shear implements active loop detection to infer the looping structure in the application. This technique repeatedly presents the application with the same input, altering the program's interaction with the environment at precisely chosen execution points to elicit different program behaviors depending on the loop structure in the application. The ability to alter interactions between the application and the environment enables Shear to infer a broader range of looping structures otherwise undetectable given only the ability to observe application behavior. Active loop detection therefore enables Shear to infer a broader range of looping structures than previous approaches.",,,,en,,,,,,,,,,,,,,,,,,,Technical Report,,,,,,,,,,,
,"Shen, Jiasi; Rinard, Martin",2021-11-15T22:40:41Z,2021-11-15T22:40:41Z,2021-11-15,https://hdl.handle.net/1721.1/138144,,Active Loop Detection for Applications that Access Databases,"We present Shear, a new system that observes and manipulates the interaction between an application and its surrounding environment to learn a model of the behavior of the application. Shear implements active loop detection to infer the loop structures in the application. This technique repeatedly presents the application with the same input, altering the program's interaction with the environment at precisely chosen execution points to elicit different program behaviors depending on the loop structure in the application. The ability to alter interactions between the application and the environment enables Shear to infer a broader range of loop structures otherwise undetectable given only the ability to observe application behavior. Active loop detection therefore enables Shear to infer a broader range of loop structures than previous approaches.",,,,en_US,,,,,,,,,,,,,,,,,,,Technical Report,,,,,,,,,,,
,"Wang, Yanwei; Shah, Julie",2022-06-15T14:42:32Z,2022-06-15T14:42:32Z,2022-06-15,https://hdl.handle.net/1721.1/143430,,Universal Motion Generator: Trajectory Autocompletion by Motion Prompts,"Foundation models, which are large neural networks trained on massive datasets, have shown
impressive generalization in both the language and the vision domain. While fine-tuning foundation
models for new tasks at test-time is impractical due to billions of parameters in those models, prompts
have been employed to re-purpose models for test-time tasks on the fly. In this report, we ideate the equivalent foundation model for motion generation and the corresponding formats of prompt that can condition such a model. The central goal is to learn a behavior prior for motion generation that can be re-used in a novel scene.",,,,en_US,"Robot Learning, Large Language Models, Motion Generation",,CSAIL NSF MI project – 6939398,Attribution-NonCommercial-NoDerivs 3.0 United States,http://creativecommons.org/licenses/by-nc-nd/3.0/us/,,,,,,,,,,,,,,Working Paper,,,,,,,,,,,
,"Sun, Jennifer J; Tjandrasuwita, Megan; Sehgal, Atharva; Solar-Lezama, Armando; Chaudhuri, Swarat; Yue, Yisong; Costilla Reyes, Omar",2022-10-12T01:58:33Z,2022-10-12T01:58:33Z,2022-10-12,https://hdl.handle.net/1721.1/145783,,Neurosymbolic Programming for Science,"Neurosymbolic Programming (NP) techniques have the potential to accelerate scientific discovery across fields. These models combine neural and symbolic components to learn complex patterns and representations from data, using high-level concepts or known constraints. As a result, NP techniques can interface with symbolic domain knowledge from scientists, such as prior knowledge and experimental context, to produce interpretable outputs. Here, we identify opportunities and challenges between current NP models and scientific workflows, with real-world examples from behavior analysis in science. We define concrete next steps to move the NP for science field forward, to enable its use broadly for workflows across the natural and social sciences.",,,,en_US,programming languages; deep learning; science; domain knowledge,,"This project was supported by the the National Science Foundation under Grant No. 1918839 ""Understanding the World Through Code"" http://www.neurosymbolic.org",,,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Rivest, Ronald; Schiefelbein, M. Curran; Zissman, Marc A.; Bay, Jason; Bugnion, Edouard; Finnerty, Jill; Liccardi, Ilaria; Nelson, Brad; Norige, Adam S.; Shen, Emily H.; Wanger, Jenny; Yahalom, Raphael; Alekseyev, Jesslyn D.; Brubaker, Chad; Ferretti, Luca; Ishikawa, Charlie; Raykova, Mariana; Schlaman, Brendan; Schwartz, Robert X.; Sudduth, Emma; Tessaro, Stefano",2023-02-22T17:36:33Z,2023-02-22T17:36:33Z,2023-02-22,https://hdl.handle.net/1721.1/148149,Lincoln Laboratory Technical Report;TR-1288,Automated Exposure Notification for COVID-19,"Private Automated Contact Tracing (PACT) was a collaborative team and effort formed during the beginning of the Coronavirus Disease 2019 (COVID-19) pandemic. PACT’s mission was to enhance contact tracing in pandemic response by designing exposure-detection functions in personal digital communication devices that have maximal public health utility while preserving privacy. PACT had four major lines of effort: proximity detection efficacy, privacy, public health integration, and public health efficacy. In support of these lines of effort, PACT executed several cross-layer activities that helped demonstrate public health efficacy. These included prototype development and  demonstrations; system analysis; data collection and experimentation; and large-scale deployment support. PACT convened two scientific workshops relating to privacy-preserving AEN: one virtual workshop in April 2020 and a second hybrid workshop in October 2021. This report is an outcome of the second workshop and serves as PACT’s final report. It seeks to explain and discuss the use of automated exposure notification during the COVID-19 pandemic and to provide some recommendations for those who may try to design and deploy similar technologies in future pandemics.",,,,en_US,COVID-19; exposure notification; contact tracing; digital health; public health; GAEN; privacy; secure multiparty computation; differential privacy; Bluetooth; BLE; SARS-CoV-2; pandemic; governance; adoption; trust; PACT; proximity; TCFTL; smartphone; AEN; impact,,"IBM Research, the U.S. Defense Advanced Research Projects Agency (DARPA), and the U.S. Centers for Disease Control and Prevention (CDC).",,,The authors were among the 70+ in-person and virtual participants in the October 2021 ImPACT 2021 workshop. This final report has been heavily influenced by the discussion at that workshop.,,,,,,,,,,,,,Technical Report,,,,,,,,,,,
,"Canas, Juan Sebastian; Gomez, Francisco; Costilla Reyes, Omar",2023-06-15T01:48:43Z,2023-06-15T01:48:43Z,2023-06-15,https://hdl.handle.net/1721.1/150908,,Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping,"Clinical practice in psychiatry is burdened with the increased demand for healthcare services and the scarce resources available. New paradigms of health data powered with machine learning techniques could open the possibility to improve clinical workflow in critical stages of clinical assessment and treatment in psychiatry. 
In this work, we propose a machine learning system capable of predicting, detecting, and explaining individual changes in symptoms of patients with Schizophrenia by using behavioral digital phenotyping data. We forecast symptoms of patients with an error rate below 10%. 
The system detects decreases in symptoms using changepoint algorithms and uses counterfactual explanations as a recourse in a simulated continuous monitoring scenario in healthcare.  Overall, this study offers valuable insights into the performance and potential of counterfactual explanations, predictive models, and change-point detection within a simulated clinical workflow. These findings lay the foundation for further research to explore additional facets of the workflow, aiming to enhance its effectiveness and applicability in real-world healthcare settings. By leveraging these components, the goal is to develop an actionable, interpretable, and trustworthy integrative decision support system that combines real-time clinical assessments with sensor-based inputs.",,,,en_US,conterfactual; machine learning; digital phenotyping; interpretability; mental health,,National Science Foundation (NSF) under Grant No. 1918839,,,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Makatura, Liane; Foshey, Michael; Wang, Bohan; Hähnlein, Felix; Ma, Pingchuan; Deng, Bolei; Tjandrasuwita, Megan; Spielberg, Andrew; Owens, Crystal Elaine; Chen, Peter Yichen; Zhao, Allan; Zhu, Amy; Norton, Wil J; Gu, Edward; Jacob, Joshua; Li, Yifei; Schulz, Adriana; Matusik, Wojciech",2023-07-27T17:28:58Z,2023-07-27T17:28:58Z,2023-07-27,https://hdl.handle.net/1721.1/151174,,How Can Large Language Models Help Humans in Design And Manufacturing?,,,,,en_US,"Large Language Models, GPT-4, computational design, computational fabrication, CAD, CAM, design for manufacturing, simulation, inverse design",,,Attribution-NonCommercial-ShareAlike 3.0 United States,http://creativecommons.org/licenses/by-nc-sa/3.0/us/,,,,,,,,,,,,,,Article,,,,,,,,,,,
,"Merrill, Kelsey; Newman, Zachary; Torres-Arias, Santiago; Sollins, Karen",2023-09-19T19:03:28Z,2023-09-19T19:03:28Z,2023-09-19,https://hdl.handle.net/1721.1/152179,,"Speranza: Usable, privacy-friendly software signing","Software repositories, used for wide-scale open software distribu- tion, are a significant vector for security attacks. Software signing provides authenticity, mitigating many such attacks. Developer- managed signing keys pose usability challenges, but certificate- based systems introduce privacy problems. This work, Speranza, uses certificates to verify software authenticity but still provides anonymity to signers using zero-knowledge identity co-commitments.
In Speranza, a signer uses an automated certificate authority (CA) to create a private identity-bound signature and proof of authoriza- tion. Verifiers check that a signer was authorized to publish a pack- age without learning the signer’s identity. The package repository privately records each package’s authorized signers, but publishes only commitments to identities in a public map. Then, when issuing certificates, the CA issues the certificate to a distinct commitment to the same identity. The signer then creates a zero-knowledge proof that these are identity co-commitments.
We implemented a proof-of-concept for Speranza. We find that costs to maintainers (signing) and end users (verifying) are small (sub-millisecond), even for a repository with millions of packages. Techniques inspired by recent key transparency systems reduce the bandwidth for serving authorization policies to 2 KiB. Server costs in this system are negligible. Our evaluation finds that Speranza is practical on the scale of the largest software repositories.
We also emphasize practicality and deployability in this project. By building on existing technology and employing relatively sim- ple and well-established cryptographic techniques, Speranza can be deployed for wide-scale use with only a few hundred lines of code and minimal changes to existing infrastructure. Speranza is a practical way to bring privacy and authenticity together for more trustworthy open-source software.",,,,en_US,"Key management, digital signature, pseudonymity, anonymity, untraceability, usability",,,Attribution 3.0 United States,http://creativecommons.org/licenses/by/3.0/us/,"This is an extended version of the shorter paper by the same name, published in ACM CCS 2023.",,,,,,,,,,,,,Article,,,,,,,,,,,
,"Atkinson, Eric",2023-11-27T17:01:50Z,2023-11-27T17:01:50Z,2023-11-27,https://hdl.handle.net/1721.1/153053,,Belief Programming Implementation,,,,,,,,,,,,,,,,,,,,,,,,Software,,,,,,,,,,,
Daniela Rus,"Gil, Stephanie; Kumar, Swarun; Mazumder, Mark; Katabi, Dina; Rus, Daniela",2015-06-16T22:00:03Z,2015-06-16T22:00:03Z,July 2015,http://hdl.handle.net/1721.1/97442,MIT-CSAIL-TR-2015-024,Guaranteeing Spoof-Resilient Multi-Robot Networks,"Multi-robot networks use wireless communication to provide wide-ranging services such as aerial surveillance and unmanned delivery. However, effective coordination between multiple robots requires trust, making them particularly vulnerable to cyber-attacks. Specifically, such networks can be gravely disrupted by the Sybil attack, where even a single malicious robot can spoof a large number of fake clients. This paper proposes a new solution to defend against the Sybil attack, without requiring expensive cryptographic key-distribution. Our core contribution is a novel algorithm implemented on commercial Wi-Fi radios that can ""sense"" spoofers using the physics of wireless signals. We derive theoretical guarantees on how this algorithm bounds the impact of the Sybil Attack on a broad class of robotic coverage problems. We experimentally validate our claims using a team of AscTec quadrotor servers and iRobot Create ground clients, and demonstrate spoofer detection rates over 96%.",,13 p.,,,cyber-physcial systems; cybersecurity,Robotics (Rus),,Creative Commons Attribution-Non Commercial-No Derivative Works 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,Robotics Science and Systems,,,,,,,,2015-06-16T22:00:03Z,,,,,,,,,,,,,,,
,"Agarwal, Anant",2023-03-29T14:36:53Z,2023-03-29T14:36:53Z,,https://hdl.handle.net/1721.1/149209,MIT-LCS-TM-488,Modeling Multiprogrammed Caches,"This paper presents a simple, yet accurate, model for multiprogrammed caches and validates it against trace-driven simulation. The model takes into account nonstationary behavior of processes and process sharing. By making judicious approximations, the paper shows that a very simple expression of the form u^2(p - 1)/tS accurately models the multiprogramming component of the miss rate of large direct-mapped caches. In the above expression, t is the context-switching interval, S is the cache size in blocks, p is the number of processes, and u is the number of unique blocks accesses by a process during the interval t.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Benczúr, András A.",2023-03-29T15:23:41Z,2023-03-29T15:23:41Z,,https://hdl.handle.net/1721.1/149784,MIT-LCS-TR-639,The Structure of Near-minimum Edge Cuts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Lynch, Nancy A.; Segala, Roberto; Vaandrager, Frits",2023-03-29T15:37:31Z,2023-03-29T15:37:31Z,,https://hdl.handle.net/1721.1/149992,MIT-LCS-TR-907,Compositionality for Probabilistic Automata,"We establish that on the dfomain of probabilistic automata, the trace distribution preorder coincides with the simulation preorder.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Chong, Frederic T.; Barua, Rajeev; Dahlgren, Fredrik; Kubiatowicz, John D.; Agarwal, Anant",2023-03-29T14:40:24Z,2023-03-29T14:40:24Z,,https://hdl.handle.net/1721.1/149269,MIT-LCS-TM-562,The Sensitivity of Communication Mechanisms to Bandwidth and Latency,"The goal of this paper is to gain insight into the relative performance of communication mechanisms as bisection bandwidth and network latency vary. We compare shared memory with and without prefetching, message passing with interrupts and with polling, and bulk transfer via DMA. We present two sets of experiments involing four irregular applications on the MIT Alewife multiprocessor. First, we introduce I/O cross-traffic to vary bisection bandwidth. Second, we change processor clock speeds to vary relative network latency. We establish a framework from which to understand a range of results. On Alewife, shared memory provides good performance, even on producer-consumer applications with little data-reuse. On machines with lower bisection bandwidth and higher network latency, however, message-passing mechanisms become important. In particular, the high communication volume of shared memory threatens to become difficult to support on future machines without expensive, high-dimensional networks. Furthermore, the round-trip nature of shared memory may not be able to tolerate the latencies of future networks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Keidar, Idit; Khazan, Roger",2023-03-29T15:32:27Z,2023-03-29T15:32:27Z,,https://hdl.handle.net/1721.1/149904,MIT-LCS-TR-794,"A Client-Server Approach to Virtually Synchronous  Group Multicast: Specifications, Algorithms, and Proofs",This paper presents a formal design for a novel group multicast service that provides virtually synchronous semantics in asynchronous fault-prone environments.  The design employs a client-server architecture in which group membership is maintained not by,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Isaksen, Aaron; McMillan, Leonard; Gortler, Steven J.",2023-03-29T15:31:48Z,2023-03-29T15:31:48Z,,https://hdl.handle.net/1721.1/149891,MIT-LCS-TR-778,Dynamically Reparameterized Light Fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Tennenhouse, David L.; Smith, Jonathan M.; Sincoskie, W. David; Wetherall, David J.; Minden, Gary J.",2023-03-29T14:40:13Z,2023-03-29T14:40:13Z,,https://hdl.handle.net/1721.1/149265,MIT-LCS-TM-557,A Survey of Active Network Research,"Active networks are a novel approach to network architecture in which the switches of the network perform customized computations on the messages flowing through them. This approach is motivated by both lead user applications, which perform user-driven computation at nodes within the network today, and the emergence of mobile code technologies that make dynamic network service innovation attainable. In this paper, we discuss two approaches to the realization of active networks and provide a snapshot of the current research issues and activities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Shatterjee, Sandeep; Devadas, Srinivas",2023-03-29T14:41:31Z,2023-03-29T14:41:31Z,,https://hdl.handle.net/1721.1/149289,MIT-LCS-TM-596,The MASC Composable Computing Infrastructure for Intelligent Environments,"We present a system architecture and framework for creating rapidly deployable intelligent environments. The rapid pace of innovation of computer hardware and intelligent systems software leads to uncertainty that deters manufacturers from adopting a single processor, network, or software environment for placement into their products. The MASC Composable Computing infrastructure addresses these issues by providing an upgradable hardware and software infrastructure that supports rapid development and deployment, as well as simple and economical maintenance of intelligent environmental systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Engler, Dawson R.; Hsieh, Wilson C.; Kaashoek, M. Frans",2023-03-29T14:38:55Z,2023-03-29T14:38:55Z,,https://hdl.handle.net/1721.1/149242,MIT-LCS-TM-526,"'C: A Language for High-Level, Efficient, and Machine-independant Dynamic Code Generation","Dynamic code generation allows specialized code sequences to be crafted using runtime information. Since this information is by definition not available statically, the use of dynamic code generation can achieve performance inherently beyond that of static code generation. Previous attempts to support dynamic code generation have been low-leveled, expensive, or machine-dependent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Moses, Joel","Charniak, Eugene",2023-03-29T14:51:00Z,2023-03-29T14:51:00Z,,https://hdl.handle.net/1721.1/149330,MIT-LCS-TR-005,"CARPS, A Program Which Solves Calculus Word Problems",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Vazirir, Mandana; Lynch, Nancy A.; Wing, Jeannette",2023-03-29T14:40:44Z,2023-03-29T14:40:44Z,,https://hdl.handle.net/1721.1/149276,MIT-LCS-TM-576,Proving Correctness of a Controller Algorithm for the RAID Level 5 System,"Mos RAID controllers implemented in industry are complicated and difficult to reason about. This complexity has led to software and hardware systems that are difficult to debug and hard to modify. To overcome this problem Courtright and Gibson have developed a rapidf prototyping framework for RAID architectures which relies on a generic controller algorithm [1]. The designer of a new architecture needs to specify parts of the generic controller algorithm and must justify the validity of the controller algorithm obtained. However the latter task may be difficult due to the concurrency of operations on the disks. This is the reason why it would be useful to provide designers with an automated verification tool tailored specificially for the RAID prototyping system. As a first step towards building such a tool, our approach consists of studying several controller algorithms manually, to determine the key properties that need to be verified. This paper presents the modeling and verification of a controller algorithm for the RAID Level 5 System [5]. We model the system using I/O automata [6], give an external requirements specification, and prove that the model implements its specification. We use a key invariant to find an error in a controller algorithm for the RAID Level 6 System [5].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Nettels, Scott; O'Toole James",2023-03-29T15:20:03Z,2023-03-29T15:20:03Z,,https://hdl.handle.net/1721.1/149742,MIT-LCS-TR-570,Implementing Orthogonal Persistence: A Simple Optimization Based on Replicating Collection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Nimmer, Jeremy W.; Ernst, Michael D.",2023-03-29T15:34:25Z,2023-03-29T15:34:25Z,,https://hdl.handle.net/1721.1/149926,MIT-LCS-TR-823,Automatic Generation and Checking of Program Specifications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Dornbrook, Michael; Blank, Marc",2023-03-29T15:07:34Z,2023-03-29T15:07:34Z,,https://hdl.handle.net/1721.1/149570,MIT-LCS-TR-292,The MDL Programming Language Primer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Galley, S.W.; Pfister, Greg",2023-03-29T15:07:58Z,2023-03-29T15:07:58Z,,https://hdl.handle.net/1721.1/149571,MIT-LCS-TR-293,The MDL Programming Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19337379.0
,"Micali, Silvio",2023-03-29T14:39:42Z,2023-03-29T14:39:42Z,,https://hdl.handle.net/1721.1/149254,MIT-LCS-TM-542,Enhanced Certificate Revocation System,"We apply off-linne digital signatures to provide a novel approach to certificate revocation. Our approach dismisses with traditional CRLs and yields pubilc-key infrastructures that are several-hundred times cheaper to run than traditional ones. More generally, our technology also yields effective methods to lengthen the validity of a digital signature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Koksal, Can Emre; Kassab, Hisham; Balakrishnan, Hari",2023-03-29T15:33:08Z,2023-03-29T15:33:08Z,,https://hdl.handle.net/1721.1/149911,MIT-LCS-TR-807,An Analysis of Short-Term Fairness in Wireless Media Access Protocols,"We investigate the problem of unfairness over short time scales in decentralized wireless media access (MAC) protocols.  Motivated by experimental results over a CSMA/CA-based WaveLAN wireless LAN that shows starvation and degraded TCP performance, we see",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Mohtashemi, Mojdeh",2023-03-29T15:32:14Z,2023-03-29T15:32:14Z,,https://hdl.handle.net/1721.1/149900,MIT-LCS-TR-787,Natural Selection and Loop Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Harchol-Balter, Mor; Crovella, Mark E.; Murta, Cristina D.",2023-03-29T15:31:09Z,2023-03-29T15:31:09Z,,https://hdl.handle.net/1721.1/149878,MIT-LCS-TR-757,On Choosing a Task Assignment Policy for a Distributed Server System,We consider a distributed server system model and ask which policy should be used for assigning tasks to hosts.  In our model each host processes tasks in First-Come-First-Serve order and the task's service demand is known in advance.  We consider four ta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,"Teichmann, Marek; Teller, Seth",2023-03-29T15:31:30Z,2023-03-29T15:31:30Z,,https://hdl.handle.net/1721.1/149884,MIT-LCS-TR-766,Polygonal Approximation of Voronoi Diagrams of Set of Triangles in Three Dimensions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
