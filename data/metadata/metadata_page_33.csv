dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.other,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation,dc.title,dc.subject,dc.relation.isreplacedby,dc.relation.uri
Peter Szolovits,"Rudin, Robert",Clinical Decision-Making,2008-02-19T13:45:08Z,2008-02-19T13:45:08Z,2008-02-17,MIT-CSAIL-TR-2008-010,http://hdl.handle.net/1721.1/40285,"Hurricane Katrina showed that the current methods for handling medicalrecords are minimally resilient to large scale disasters. This research presents a preliminary model for measuring the resilience of medical records systemsagainst public policy goals and uses the model to illuminate the current state of medical record resilience. From this analysis, three recommendations for how to make medical records more resilient are presented.The recommendations are: 1) Federal and state governments should use the preliminary resiliencemodel introduced here as the basis for compliance requirements for electronicmedical record technical architectures. 2) Regional Health Information Organizations (RHIOs) should consideroffering services in disaster management to healthcare organizations. This willhelp RHIOs create sustainable business models. 3) Storage companies should consider developing distributed storagesolutions based on Distributed Hash Table (DHT) technology for medical recordstorage. Distributed storage would alleviate public concerns over privacy withcentralized storage of medical records. Empirical evidence is presenteddemonstrating the performance of DHT technology using a prototype medicalrecord system.",92 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Making Medical Records More Resilient,,,
Charles Leiserson,"Agrawal, Kunal; Lee, I-Ting Angelina; Sukha, Jim",Theory of Computation,2008-06-30T13:00:16Z,2008-06-30T13:00:16Z,2008-02-20 and 2008-06-14,MIT-CSAIL-TR-2008-038,http://hdl.handle.net/1721.1/41871,"Researchers in transactional memory (TM) have proposed open nesting asa methodology for increasing the concurrency of a program. The ideais to ignore certain ""low-level"" memory operations of anopen-nested transaction when detecting conflicts for its parenttransaction, and instead perform abstract concurrency control for the""high-level"" operation that nested transaction represents. Tosupport this methodology, TM systems use an open-nested commitmechanism that commits all changes performed by an open-nestedtransaction directly to memory, thereby avoiding low-levelconflicts. Unfortunately, because the TM runtime is unaware of thedifferent levels of memory, an unconstrained use of open-nestedcommits can lead to anomalous program behavior.In this paper, we describe a framework of ownership-awaretransactional memory which incorporates the notion of modules into theTM system and requires that transactions and data be associated withspecific transactional modules or Xmodules. We propose a newownership-aware commit mechanism, a hybrid between anopen-nested and closed-nested commit which commits a piece of datadifferently depending on whether the current Xmodule owns the data ornot. Moreover, we give a set of precise constraints on interactionsand sharing of data among the Xmodules based on familiar notions ofabstraction. We prove that ownership-aware TM has has cleanmemory-level semantics and can guarantee serializability bymodules, which is an adaptation of multilevel serializability fromdatabases to TM. In addition, we describe how a programmer canspecify Xmodules and ownership in a Java-like language. Our typesystem can enforce most of the constraints required by ownership-awareTM statically, and can enforce the remaining constraints dynamically.Finally, we prove that if transactions in the process of aborting obeyrestrictions on their memory footprint, the OAT model is free fromsemantic deadlock.",36 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Safe Open-Nested Transactions Through Ownership,"abstract serializability, open-nested transactions, ownership types, ownership-aware transactions, serializability by levels, serializability by modules, transactional memory, Xmodules",,
Leslie Kaelbling,"Aycinena, Meg; Kaelbling, Leslie Pack; Lozano-Perez, Tomas",Learning and Intelligent Systems,2008-02-25T19:46:04Z,2008-02-25T19:46:04Z,2008-02-25,MIT-CSAIL-TR-2008-011,http://hdl.handle.net/1721.1/40288,"Many object recognition systems are limited by their inability to share common parts or structure among related object classes. This capability is desirable because it allows information about parts and relationships in one object class to be generalized to other classes for which it is relevant. With this goal in mind, we have designed a representation and recognition framework that captures structural variability and shared part structure within and among object classes. The framework uses probabilistic geometric grammars (PGGs) to represent object classes recursively in terms of their parts, thereby exploiting the hierarchical and substitutive structure inherent to many types of objects. To incorporate geometric and appearance information, we extend traditional probabilistic context-free grammars to represent distributions over the relative geometric characteristics of object parts as well as the appearance of primitive parts. We describe an efficient dynamic programming algorithm for object categorization and localization in images given a PGG model. We also develop an EM algorithm to estimate the parameters of a grammar structure from training data, and a search-based structure learning approach that finds a compact grammar to explain the image data while sharing substructure among classes. Finally, we describe a set of experiments that demonstrate empirically that the system provides a performance benefit.",28 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Learning Grammatical Models for Object Recognition,,,
Gerald Sussman,"Beal, Jacob; Bachrach, Jonathan; Vickery, Dan; Tobenkin, Mark",Mathematics and Computation,2007-11-02T18:45:13Z,2007-11-02T18:45:13Z,2008-03,MIT-CSAIL-TR-2007-050,http://hdl.handle.net/1721.1/39418,"We present CRF-Gradient, a self-healing gradient algorithm that provably reconfigures in O(diameter) time. Self-healing gradients are a frequently used building block for distributed self-healing systems, but previous algorithms either have a healing rate limited by the shortest link in the network or must rebuild invalid regions from scratch. We have verified CRF-Gradient in simulation and on a network of Mica2 motes. Our approach can also be generalized and applied to create other self-healing calculations, such as cumulative probability fields.",7 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Fast Self-Healing Gradients,amorphous computing; spatial computing; spatial computer,,
Trevor Darrell,"Quattoni, Ariadna; Collins, Michael; Darrell, Trevor",Vision,2008-03-03T14:45:13Z,2008-03-03T14:45:13Z,2008-03-03,MIT-CSAIL-TR-2008-012,http://hdl.handle.net/1721.1/40797,"To learn a new visual category from few examples, prior knowledge from unlabeled data as well as previous related categories may be useful.  We develop a new method for transfer learning which exploits available unlabeled data and an arbitrary kernel function; we form a representation based on kernel distances to a large set of unlabeled data points. To transfer knowledge from previous related problems we observe that a category might be learnable using only a small subset of reference prototypes. Related problems may share a significant number of relevant prototypes; we find such a reduced representation by performing a joint loss minimization over the training sets of related problems with a shared regularization penalty that minimizes the total number of prototypes involved in the approximation.This optimization problem can be formulated as a linear program thatcan be solved efficiently. We conduct experiments on a news-topic prediction task where the goal is to predict whether an image belongs to a particularnews topic. Our results show that when only few examples are available for training a target topic, leveraging knowledge learnt from other topics can significantly improve performance.",8 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Transfer learning for image classification with sparse prototype representations,transfer learning; image classification,,
Brian Williams,"Ono, Masahiro; Williams, Brian C.",Model-based Embedded and Robotic Systems,2008-03-06T14:30:20Z,2008-03-06T14:30:20Z,2008-03-06,MIT-CSAIL-TR-2008-014,http://hdl.handle.net/1721.1/40804,"When controlling dynamic systems such as mobile robots in uncertain environments, there is a trade off between risk and reward. For example, a race car can turn a corner faster by taking a more challenging path. This paper proposes a new approach to planning a control sequence with guaranteed risk bound. Given a stochastic dynamic model, the problem is to find a control sequence that optimizes a performance metric, while satisfying chance constraints i.e. constraints on the upper bound of the probability of failure. We propose a two-stage optimization approach, with the upper stage optimizing the risk allocation and the lower stage calculating the optimal control sequence that maximizes the reward. In general, upper-stage is a non-convex optimization problem, which is hard to solve. We develop a new iterative algorithm for this stage that efficiently computes the risk allocation with a small penalty to optimality. The algorithm is implemented and tested on the autonomous underwater vehicle (AUV) depth planning problem, which demonstrates the substantial improvement in computation cost and suboptimality compared to the prior arts.",8 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Two-stage Optimization Approach to Robust Model Predictive Control with a Joint Chance Constraint,,,
Brian Williams,"Ono, Masahiro; Williams, Brian C.",Model-based Embedded and Robotic Systems,2008-03-06T14:30:11Z,2008-03-06T14:30:11Z,2008-03-06,MIT-CSAIL-TR-2008-013,http://hdl.handle.net/1721.1/40803,"When controlling dynamic systems such as mobile robots in uncertain environments, there is a trade off between risk and reward. For example, a race car can turn a corner faster by taking a more challenging path. This paper proposes a new approach to planning a control sequence with guaranteed risk bound. Given a stochastic dynamic model, the problem is to find a control sequence that optimizes a performance metric, while satisfying chance constraints i.e. constraints on the upper bound of the probability of failure. We propose a two-stage optimization approach, with the upper stage optimizing the risk allocation and the lower stage calculating the optimal control sequence that maximizes the reward. In general, upper-stage is a non-convex optimization problem, which is hard to solve. We develop a new iterative algorithm for this stage that efficiently computes the risk allocation with a small penalty to optimality. The algorithm is implemented and tested on the autonomous underwater vehicle (AUV) depth planning problem, which demonstrates the substantial improvement in computation cost and suboptimality compared to the prior arts.",7 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Efficient Motion Planning Algorithm for Stochastic Dynamic Systems with Constraints on Probability of Failure,Joint chance constraint; Model Predictive Control; MPC; Robust Model Predictive Control; RMPC,,
Gerald Sussman,"Greenstadt, Rachel; Beal, Jacob",Mathematics and Computation,2008-03-24T18:15:12Z,2008-03-24T18:15:12Z,2008-03-17,MIT-CSAIL-TR-2008-016,http://hdl.handle.net/1721.1/40810,"Humans should be able to think of computers as extensions of their body, as craftsmen do with their tools. Current security models, however, are too unlike those used in human minds---for example, computers authenticate users by challenging them to repeat a secret rather than by continually observing the many subtle cues offered by their appearance and behavior. We propose three lines of research that can be combined to produce cognitive security on computers and other personal devices: imprinting and continuously deployed multi-modal biometrics, self-protection through virtualization and trusted computing, and adjustably autonomous security.",6 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Cognitive Security for Personal Devices,artificial intelligence; trusted computing,,
Tomaso Poggio,"Caponnetto, Andrea; Poggio, Tomaso; Smale, Steve",Center for Biological and Computational Learning (CBCL),2008-06-05T18:00:40Z,2008-06-05T18:00:40Z,2008-04-04,MIT-CSAIL-TR-2008-030; CBCL-272,http://hdl.handle.net/1721.1/41858,"In this paper we present a class of algorithms for similarity learning on spaces of images. The general framework that we introduce is motivated by some well-known hierarchical pre-processing architectures for object recognition which have been developed during the last decade, and which have been in some cases inspired by functional models of the ventral stream of the visual cortex. These architectures are characterized by the construction of a hierarchy of â€œlocalâ€ feature representations of the visual stimulus. We show that our framework includes some well-known techniques, and that it is suitable for the analysis of dynamic visual stimuli, presenting a quantitative error analysis in this setting.",20 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,On a model of visual cortex: learning invariance and selectivity,Learning Theory; Hierarchical Architecture Theory; Unsupervised Learning; Theory of the Visual Cortex,,
Trevor Darrell,"Lee, John J.",Vision,2008-04-07T20:45:20Z,2008-04-07T20:45:20Z,2008-04-07,MIT-CSAIL-TR-2008-017,http://hdl.handle.net/1721.1/41070,"LIBPMK is a C++ implementation of Grauman and Darrell's pyramid match algorithm. This toolkit provides a flexible framework with which developers can quickly match sets of image features and run experiments. LIBPMK provides functionality for $k$-means and hierarchical clustering, dealing with data sets too large to fit in memory, building multi-resolution histograms, quickly performing pyramid matches, and training and testing support vector machines (SVMs). This report provides a tutorial on how to use the LIBPMK code, and gives the specifications of the LIBPMK API.",217 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,LIBPMK: A Pyramid Match Toolkit,pmk; vgpmk; pyramid match kernel,,
Dina Katabi,"Katabi, Dina; Gollakota, Shyamnath",Networks & Mobile Systems,2008-04-08T19:15:18Z,2008-04-08T19:15:18Z,2008-04-08,MIT-CSAIL-TR-2008-018,http://hdl.handle.net/1721.1/41084,"This paper presents ZigZag, an 802.11 receiver that combats hidden terminals. ZigZag exploits 802.11 retransmissions which, in the case of hidden terminals, cause successive collisions. Due to asynchrony, these collisions have different interference-free stretches at their start, which ZigZag uses to bootstrap its decoding.  ZigZag makes no changes to the 802.11 MAC and introduces no overhead when there are no collisions. But, when senders collide, ZigZag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. We build a prototype of ZigZag in GNU Radio. In a testbed of 14 USRP nodes, ZigZag reduces the average packet loss rate at hidden terminals from 82.3% to about 0.7%.",14 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,ZigZag Decoding: Combating Hidden Terminals in Wireless Networks,Hidden Terminals; Software Radios; Wireless Networks,http://hdl.handle.net/1721.1/42842,http://hdl.handle.net/1721.1/42842
Tomaso Poggio,"Bileschi, Stanley M",Center for Biological and Computational Learning (CBCL),2008-04-09T20:15:10Z,2008-04-09T20:15:10Z,2008-04-09,MIT-CSAIL-TR-2008-019; CBCL-271,http://hdl.handle.net/1721.1/41093,"Recently, several powerful image features have been proposed whichcan be described as spatial histograms of oriented energy. Forinstance, the HoG, HMAX C1, SIFT, and Shape Context feature allrepresent an input image using with a discrete set of bins whichaccumulate evidence for oriented structures over a spatial regionand a range of orientations. In this work, we generalize thesetechniques to allow for a foveated input image, rather than arectilinear raster. It will be shown that improved object detectionaccuracy can be achieved via inputting a spectrum of imagemeasurements, from sharp, fine-scale image sampling within a smallspatial region within the target to coarse-scale sampling of a widefield of view around the target. Several alternative featuregeneration algorithms are proposed and tested which suitably makeuse of foveated image inputs. In the experiments we show thatfeatures generated from the foveated input format produce detectorsof greater accuracy, as measured for four object types from commonlyavailable data-sets. Finally, a flexible algorithm for generatingfeatures is described and tested which is independent of inputtopology and uses ICA to learn appropriate filters.",8 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,A Multi-Scale Generalization of the HoG and HMAX Image Descriptors for Object Detection,"Object Detection, ICA, Multi-Scale, Image Features",,
Trevor Darrell,"Urtasun, Raquel; Quattoni, Ariadna; Lawrence, Neil; Darrell, Trevor",Vision,2008-05-05T15:46:05Z,2008-05-05T15:46:05Z,2008-04-11,MIT-CSAIL-TR-2008-020,http://hdl.handle.net/1721.1/41517,"When a series of problems are related, representations derived from learning earlier tasks may be useful in solving later problems. In this paper we propose a novel approach to transfer learning with low-dimensional, non-linear latent spaces. We show how such representations can be jointly learned across multiple tasks in a Gaussian Process framework. When transferred to new tasks with relatively few training examples, learning can be faster and/or more accurate. Experiments on digit recognition and newsgroup classification tasks show significantly improved performance when compared to baseline performance with a representation derived from a semi-supervised learning approach or with a discriminative approach that uses only the target data.",10 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Transferring Nonlinear Representations using Gaussian Processes with a Shared Latent Space,,,
William Freeman,"Levin, Anat; Freeman, William T.; Durand, Fredo",Vision,2008-05-05T15:45:16Z,2008-05-05T15:45:16Z,2008-04-16,MIT-CSAIL-TR-2008-021,http://hdl.handle.net/1721.1/41513,"Computer vision has traditionally focused on extracting structure,such as depth, from images acquired using thin-lens or pinhole optics. The development of computational imaging is broadening this scope; a variety of unconventional cameras do not directly capture a traditional image anymore, but instead require the joint reconstruction of structure and image information. For example, recent coded aperture designs have been optimized to facilitate the joint reconstruction of depth and intensity. The breadth of imaging designs requires new tools to understand the tradeoffs implied by different strategies.This paper introduces a unified framework for analyzing computational imagingapproaches. Each sensor element is modeled as an inner product over the 4D light field. The imaging task is then posed as Bayesian inference: given the observed noisy light field projections and a new prior on light field signals, estimatethe original light field. Under common imaging conditions, we compare the performance of various camera designs using 2D light field simulations. This framework allows us to better understand the tradeoffs of each camera type andanalyze their limitations.",21 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Understanding camera trade-offs through a Bayesian analysis of light field projections,,,
Shafi Goldwasser,"Pass, Rafael; Vaikuntanathan, Vinod",Theory of Computation,2008-05-05T15:46:14Z,2008-05-05T15:46:14Z,2008-04-16,MIT-CSAIL-TR-2008-022,http://hdl.handle.net/1721.1/41518,"We introduce new and general complexity theoretic hardness assumptions. These assumptions abstract out concrete properties of a random oracle and are significantly stronger than traditional cryptographic hardness assumptions; however, assuming their validity we can resolve a number of longstandingopen problems in cryptography.",28 pp.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,New-Age Cryptography,"Cryptographic Assumptions, Non-malleable Commitment, Non-malleable Zero-knowledge",,
Silvio Micali,"Chen, Jing",Theory of Computation,2008-05-05T15:45:41Z,2008-05-05T15:45:41Z,2008-05-01,MIT-CSAIL-TR-2008-023,http://hdl.handle.net/1721.1/41515,"Micali and Valiant proposed a mechanism for combinatorial auctions that is dominant-strategy truthful, guarantees reasonably high revenue, and is very resilient against collusions. Their mechanism, however, uses as a subroutine the VCG mechanism, that is not polynomial time.We propose a modification of their mechanism that is efficient, while retaining their collusion resilience and a good fraction of their revenue, if given as a subroutine an efficient approximation of the VCG mechanism.",7 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Generalization of the MV Mechanism,,,
Piotr Indyk,"Andoni, Alexandr; Ba, Khanh Do; Indyk, Piotr",Theory of Computation,2008-05-05T15:45:27Z,2008-05-05T15:45:27Z,2008-05-02,MIT-CSAIL-TR-2008-024,http://hdl.handle.net/1721.1/41514,"e study a natural generalization of the heavy hitters problem in thestreaming context. We term this generalization *block heavy hitters* and define it as follows. We are to stream over a matrix$A$, and report all *rows* that are heavy, where a row is heavy ifits ell_1-norm is at least phi fraction of the ell_1 norm ofthe entire matrix $A$. In comparison, in the standard heavy hittersproblem, we are required to report the matrix *entries* that areheavy. As is common in streaming, we solve the problem approximately:we return all rows with weight at least phi, but also possibly someother rows that have weight no less than (1-eps)phi. To solve theblock heavy hitters problem, we show how to construct a linear sketchof A from which we can recover the heavy rows of A.The block heavy hitters problem has already found applications forother streaming problems. In particular, it is a crucial buildingblock in a streaming algorithm that constructs asmall-size sketch for the Ulam metric, a metric on non-repetitivestrings under the edit (Levenshtein) distance.",3 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Block Heavy Hitters,,,
Leslie Kaelbling,"McAllester, David; Milch, Brian; Goodman, Noah D.",Learning and Intelligent Systems,2008-05-05T15:45:52Z,2008-05-05T15:45:52Z,2008-05-03,MIT-CSAIL-TR-2008-025,http://hdl.handle.net/1721.1/41516,"We consider three desiderata for a language combining logic and probability: logical expressivity, random-world semantics, and the existence of a useful syntactic condition for probabilistic independence. Achieving these three desiderata simultaneously is nontrivial. Expressivity can be achieved by using a formalism similar to a programming language, but standard approaches to combining programming languages with probabilities sacrifice random-world semantics. Naive approaches to restoring random-world semantics undermine syntactic independence criteria. Our main result is a syntactic independence criterion that holds for a broad class of highly expressive logics under random-world semantics. We explore various examples including Bayesian networks, probabilistic context-free grammars, and an example from Mendelian genetics. Our independence criterion supports a case-factor inference technique that reproduces both variable elimination for BNs and the inside algorithm for PCFGs.",6 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Random-World Semantics and Syntactic Independence for Expressive Languages,,,
Trevor Darrell,"Lee, John J.",Vision,2008-05-06T23:00:12Z,2008-05-06T23:00:12Z,2008-05-06,MIT-CSAIL-TR-2008-026,http://hdl.handle.net/1721.1/41519,"Algorithms for recognition and retrieval tasks generally call for both speed and accuracy. When scaling up to very large applications, however, we encounter additional significant requirements: adaptability and scalability. In many real-world systems, large numbers of images are constantly added to the database, requiring the algorithm to quickly tune itself to recent trends so it can serve queries more effectively. Moreover, the systems need to be able to meet the demands of simultaneous queries from many users. In this thesis, I describe two new algorithms intended to meet these requirements and give an extensive experimental evaluation for both. The first algorithm constructs an adaptive vocabulary forest, which is an efficient image-database model that grows and shrinks as needed while adapting its structure to tune itself to recent trends. The second algorithm is a method for efficiently performing classification tasks by comparing query images to only afixed number of training examples, regardless of the size of the image database. These two methods can be combined to create a fast, adaptable, and scalable vision system suitable for large-scale applications. I also introduce LIBPMK, a fast implementation of common computer vision processing pipelines such as that of the pyramid match kernel. This implementation was used to build several successful interactive applications as well as batch experiments for research settings. This implementation, in addition to the two new algorithms introduced by this thesis, are a step toward meeting the speed, adaptability, and scalability requirements of practical large-scale vision systems.",93 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Efficient Object Recognition and Image Retrieval for Large-Scale Applications,,,
Randall Davis,"Eisenstein, Jacob",Natural Language Processing,2008-05-08T16:30:14Z,2008-05-08T16:30:14Z,2008-05-07,MIT-CSAIL-TR-2008-027,http://hdl.handle.net/1721.1/41526,"Computers cannot fully understand spoken language without access to the wide range of modalities that accompany speech. This thesis addresses the particularly expressive modality of hand gesture, and focuses on building structured statistical models at the intersection of speech, vision, and meaning.My approach is distinguished in two key respects. First, gestural patterns are leveraged to discover parallel structures in the meaning of the associated speech. This differs from prior work that attempted to interpret individual gestures directly, an approach that was prone to a lack of generality across speakers. Second, I present novel, structured statistical models for multimodal language processing, which enable learning about gesture in its linguistic context, rather than in the abstract.These ideas find successful application in a variety of language processing tasks: resolving ambiguous noun phrases, segmenting speech into topics, and producing keyframe summaries of spoken language. In all three cases, the addition of gestural features -- extracted automatically from video -- yields significantly improved performance over a state-of-the-art text-only alternative. This marks the first demonstration that hand gesture improves automatic discourse processing.",153 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Gesture in Automatic Discourse Processing,,,
