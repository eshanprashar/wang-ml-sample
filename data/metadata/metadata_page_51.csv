dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.title,dc.date.updated,dc.subject,dc.rights,dc.rights.uri,dspace.orderedauthors,dc.contributor
Martin Rinard,"Long, Fan; Rinard, Martin",Program Analysis,2015-05-26T22:00:02Z,2015-05-26T22:00:02Z,2015-05-26,http://hdl.handle.net/1721.1/97088,"We present Prophet, a novel patch generation system that learns a probabilistic model over candidate patches from a large code database that contains many past successful human patches. It defines the probabilistic model as the combination of a distribution over program points based on error localization algorithms and a parameterized log-linear distribution over modification operations. It then learns the model parameters via maximum log-likelihood, which identifies important characteristics of the successful human patches. For a new defect, Prophet generates a search space that contains many candidate patches, applies the learned model to prioritize those potentially correct patches that are consistent with the identified successful patch characteristics, and then validates the candidate patches with a user supplied test suite.",7 p.,MIT-CSAIL-TR-2015-019,Prophet: Automatic Patch Generation via Learning from Successful Human Patches,2015-05-26T22:00:02Z,,,,,
Saman Amarasinghe,"Kjolstad, Fredrik; Kamil, Shoaib; Ragan-Kelley, Jonathan; Levin, David I.W.; Sueda, Shinjiro; Chen, Desai; Vouga, Etienne; Kaufman, Danny M.; Kanwar, Gurtej; Matusik, Wojciech; Amarasinghe, Saman",Computer Architecture,2015-05-26T19:15:03Z,2015-05-26T19:15:03Z,2015-05-26,http://hdl.handle.net/1721.1/97075,"Using existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. The alternative is to prototype simulations in a high-level language like Matlab, thereby sacrificing performance. The Matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. However, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. Translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and the machine. High-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude. In this paper, we present Simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph, and as a set of global vectors, matrices and tensors depending on what is convenient at any given time. Simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. Using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. We demonstrate that Simit is easy to use: a Simit program is typically shorter than a Matlab program; that it is high-performance: a Simit program running sequentially on a CPU performs comparably to hand-optimized simulations; and that it is portable: Simit programs can be compiled for GPUs with no change to the program, delivering 5-25x speedups over our optimized CPU code.",17 p.,MIT-CSAIL-TR-2015-017,Simit: A Language for Physical Simulation,2015-05-26T19:15:04Z,"Graphs, Matrices, Tensors, Simulation",,,,
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",Program Analysis and Compilation,2015-05-29T20:45:03Z,2015-05-29T20:45:03Z,2015-05-29,http://hdl.handle.net/1721.1/97130,"We analyze reported patches for three existing generate-and-validate patch generation systems (GenProg, RSRepair, and AE). The basic principle behind generate-and-validate systems is to accept only plausible patches that produce correct outputs for all inputs in the test suite used to validate the patches. Because of errors in the patch evaluation infrastructure, the majority of the reported patches are not plausible -- they do not produce correct outputs even for the inputs in the validation test suite. The overwhelming majority of the reported patches are not correct and are equivalent to a single modification that simply deletes functionality. Observed negative effects include the introduction of security vulnerabilities and the elimination of desirable standard functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",24 p.,MIT-CSAIL-TR-2015-021,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,2015-05-29T20:45:03Z,,,,,
Martin Rinard,"Stanley-Marbell, Phillip; Rinard, Martin",Program Analysis and Compilation,2015-06-04T17:00:11Z,2015-06-04T17:00:11Z,2015-06-04,http://hdl.handle.net/1721.1/97180,"Transferring data between ICs accounts for a growing proportion of system power in wearable and mobile systems. Reducing signal transitions reduces the dynamic power dissipated in this data transfer, but traditional approaches cannot be applied when the transfer interfaces are serial buses. To address this challenge, we present a family of optimal value-deviation-bounded approximate serial encoders (VDBS encoders) that significantly reduce signal transitions (and hence, dynamic power) for bit-serial communication interfaces. When the data in transfer are from sensors, VDBS encoding enables a tradeoff between power efficiency and application fidelity, by exploiting the tolerance of many of the typical algorithms consuming sensor data to deviations in values. We derive analytic formulations for the family of VDBS encoders and introduce an efficient algorithm that performs close to the Pareto-optimal encoders. We evaluate the algorithm in two applications: Encoding data between a camera and processor in a text-recognition system, and between an accelerometer and processor in a pedometer system. For the text recognizer, the algorithm reduces signal transitions by 55% on average, while maintaining OCR accuracy at over 90% for previously-correctly-recognized text. For the pedometer, the algorithm reduces signal transitions by an average of 54% in exchange for step count errors of under 5%.",20 p.,MIT-CSAIL-TR-2015-022,Value-Deviation-Bounded Serial Data Encoding for Energy-Efficient Approximate Communication,2015-06-04T17:00:11Z,,,,,
Julie A Shah,"Gombolay, Matthew C.",Interactive Robotics Group,2015-07-06T22:15:05Z,2015-07-06T22:15:05Z,2015-07-02,http://hdl.handle.net/1721.1/97689,"Resource optimization in health care, manufacturing, and military operations requires the careful choreography of people and equipment to effectively fulfill the responsibilities of the profession. However, resource optimization is a computationally challenging problem, and poorly utilizing resources can have drastic consequences. Within these professions, there are human domain experts who are able to learn from experience to develop strategies, heuristics, and rules-of-thumb to effectively utilize the resources at their disposal. Manually codifying these heuristics within a computational tool is a laborious process and leaves much to be desired. Even with a codified set of heuristics, it is not clear how to best insert an autonomous decision-support system into the human decision-making process. The aim of this thesis is to develop an autonomous computational method for learning domain-expert heuristics from demonstration that can support the human decision-making process. We propose a new framework, called apprenticeship scheduling, which learns and embeds these heuristics within a scalable resource optimization algorithm for real-time decision-support. Our initial investigation, comprised of developing scalable methods for scheduling and studying shared control in human-machine collaborative resource optimization, inspires the development of our apprenticeship scheduling approach. We present a promising, initial prototype for learning heuristics from demonstration and outline a plan for our continuing work.",24 pages,MIT-CSAIL-TR-2015-025,PhD Thesis Proposal: Human-Machine Collaborative Optimization via Apprenticeship Scheduling,2015-07-06T22:15:05Z,,,,,
Daniel Weitzner,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",Decentralized Information Group,2015-07-07T02:15:02Z,2015-07-07T02:15:02Z,2015-07-06,http://hdl.handle.net/1721.1/97690,"Twenty years ago, law enforcement organizations lobbied to require data and communication services to engineer their products to guarantee law enforcement access to all data. After lengthy debate and vigorous predictions of enforcement channels going dark, these attempts to regulate the emerging Internet were abandoned. In the intervening years, innovation on the Internet flourished, and law enforcement agencies found new and more effective means of accessing vastly larger quantities of data. Today we are again hearing calls for regulation to mandate the provision of exceptional access mechanisms. In this report, a group of computer scientists and security experts, many of whom participated in a 1997 study of these same topics, has convened to explore the likely effects of imposing extraordinary access mandates. We have found that the damage that could be caused by law enforcement exceptional access requirements would be even greater today than it would have been 20 years ago. In the wake of the growing economic and social cost of the fundamental insecurity of today's Internet environment, any proposals that alter the security dynamics online should be approached with caution. Exceptional access would force Internet system developers to reverse forward secrecy design practices that seek to minimize the impact on user privacy when systems are breached. The complexity of today's Internet environment, with millions of apps and globally connected services, means that new law enforcement requirements are likely to introduce unanticipated, hard to detect security flaws. Beyond these and other technical vulnerabilities, the prospect of globally deployed exceptional access systems raises difficult problems about how such an environment would be governed and how to ensure that such systems would respect human rights and the rule of law.",34 p.,MIT-CSAIL-TR-2015-026,Keys Under Doormats: Mandating insecurity by requiring government access to all data and communications,2015-07-07T16:15:15Z,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",
Martin Rinard,"Long, Fan; Rinard, Martin",Program Analysis and Compilation,2015-07-14T15:45:04Z,2015-07-14T15:45:04Z,2015-07-13,http://hdl.handle.net/1721.1/97735,"We present Prophet, a novel patch generation system that learns a probabilistic model over candidate patches from a database of past successful patches. Prophet defines the probabilistic model as the combination of a distribution over program points based on defect localization algorithms and a parametrized log-linear distribution over modification operations. It then learns the model parameters via maximum log-likelihood, which identifies important characteristics of the previous successful patches in the database. For a new defect, Prophet generates a search space that contains many candidate patches, applies the learned model to prioritize those potentially correct patches that are consistent with the identified successful patch characteristics, and then validates the candidate patches with a user supplied test suite. The experimental results indicate that these techniques enable Prophet to generate correct patches for 15 out of 69 real-world defects in eight open source projects. The previous state of the art generate and validate system, which uses a set of hand-code heuristics to prioritize the search, generates correct patches for 11 of these same 69 defects.",13 p.,MIT-CSAIL-TR-2015-027,Prophet: Automatic Patch Generation via Learning from Successful Patches,2015-07-14T15:45:05Z,,,,,
Manolis Kellis,"Feizi, Soheil; Makhdoumi, Ali; Duffy, Ken; Kellis, Manolis; Medard, Muriel",Computational Biology (Kellis),2015-09-23T19:00:07Z,2015-09-23T19:00:07Z,2015-09-21,http://hdl.handle.net/1721.1/98878,"Identifying nonlinear relationships in large datasets is a daunting task particularly when the form of the nonlinearity is unknown. Here, we introduce Network Maximal Correlation (NMC) as a fundamental measure to capture nonlinear associations in networks without the knowledge of underlying nonlinearity shapes. NMC infers, possibly nonlinear, transformations of variables with zero means and unit variances by maximizing total nonlinear correlation over the underlying network. For the case of having two variables, NMC is equivalent to the standard Maximal Correlation. We characterize a solution of the NMC optimization using geometric properties of Hilbert spaces for both discrete and jointly Gaussian variables. For discrete random variables, we show that the NMC optimization is an instance of the Maximum Correlation Problem and provide necessary conditions for its global optimal solution. Moreover, we propose an efficient algorithm based on Alternating Conditional Expectation (ACE) which converges to a local NMC optimum. For this algorithm, we provide guidelines for choosing appropriate starting points to jump out of local maximizers. We also propose a distributed algorithm to compute a 1-$\epsilon$ approximation of the NMC value for large and dense graphs using graph partitioning. For jointly Gaussian variables, under some conditions, we show that the NMC optimization can be simplified to a Max-Cut problem, where we provide conditions under which an NMC solution can be computed exactly. Under some general conditions, we show that NMC can infer the underlying graphical model for functions of latent jointly Gaussian variables. These functions are unknown, bijective, and can be nonlinear. This result broadens the family of continuous distributions whose graphical models can be characterized efficiently. We illustrate the robustness of NMC in real world applications by showing its continuity with respect to small perturbations of joint distributions. We also show that sample NMC (NMC computed using empirical distributions) converges exponentially fast to the true NMC value. Finally, we apply NMC to different cancer datasets including breast, kidney and liver cancers, and show that NMC infers gene modules that are significantly associated with survival times of individuals while they are not detected using linear association measures.",48 p.,MIT-CSAIL-TR-2015-028,Network Maximal Correlation,2015-09-23T19:00:07Z,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,
Hari Balakrishnan,"Chen, Tiffany Yu-Han; Sivaraman, Anirudh; Das, Somak; Ravindranath, Lenin; Balakrishnan, Hari",Networks & Mobile Systems,2015-09-25T15:45:09Z,2015-09-25T15:45:09Z,2015-09-24,http://hdl.handle.net/1721.1/98905,"This paper describes the design, implementation, and evaluation of Amoeba, a context-sensitive context detection service for mobile devices. Amoeba exports an API that allows a client to express interest in one or more context types (activity, indoor/outdoor, and entry/exit to/from named regions), subscribe to specific modes within each context (e.g., ""walking"" or ""running"", but no other activity), and specify a response latency (i.e., how often the client is notified). Each context has a detector that returns its estimate of the mode. The detectors take both the desired subscriptions and the current context detection into account, adjusting both the types of sensors and the sampling rates to achieve high accuracy and low energy consumption. We have implemented Amoeba on Android. Experiments with Amoeba on 45+ hours of data show that our activity detector achieves an accuracy between 92% and 99%, outperforming previous proposals like UCLA* (59%), EEMSS (82%) and SociableSense (72%), while consuming 4 to 6× less energy.",12 p.,MIT-CSAIL-TR-2015-029,Designing a Context-Sensitive Context Detection Service for Mobile Devices,2015-09-25T15:45:09Z,context detection; context sensing; activity recognition; indoor detection; geofence; sensors; mobile sensing; energy,,,,
Karen Sollins; Danny Weitzner,"Bruce, Elizabeth; Sollins, Karen; Vernon, Mona; Weitzner, Danny",Advanced Network Architecture; Decentralized Information Group,2015-10-02T15:45:04Z,2015-10-02T15:45:04Z,2015-10-01,http://hdl.handle.net/1721.1/99127,"This paper is the first in a series on privacy in Big Data. As an outgrowth of a series of workshops on the topic, the Big Data Privacy Working Group undertook a study of a series of use scenarios to highlight the challenges to privacy that arise in the Big Data arena. This is a report on those scenarios. The deeper question explored by this exercise is what is distinctive about privacy in the context of Big Data. In addition, we discuss an initial list of issues for privacy that derive specifically from the nature of Big Data. These derive from observations across the real world scenarios and use cases explored in this project as well as wider reading and discussions:* Scale: The sheer size of the datasets leads to challenges in creating, managing and applying privacy policies.* Diversity: The increased likelihood of more and more diverse participants in Big Data collection, management, and use, leads to differing agendas and objectives. By nature, this is likely to lead to contradictory agendas and objectives.* Integration: With increased data management technologies (e.g. cloud services, data lakes, and so forth), integration across datasets, with new and often surprising opportunities for cross-product inferences, will also come new  information  about individuals and their behaviors.* Impact on secondary participants: Because many pieces of information are reflective of not only the targeted subject, but secondary, often unattended, participants, the inferences and resulting information will increasingly be reflective of other people, not originally considered as the subject of privacy concerns and approaches.* Need for emergent policies for emergent information: As inferences over merged data sets occur, emergent information or understanding will occur. Although each unique data set may have existing privacy policies and enforcement mechanisms, it is not clear that it is possible to develop the requisite and appropriate emerged privacy policies and appropriate enforcement of them automatically.",53 p.,MIT-CSAIL-TR-2015-030,Big Data Privacy Scenarios,2015-10-02T15:45:04Z,Big Data; Use scenarios; Privacy,Creative Commons Attribution-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nd/4.0/,,
Michael Stonebraker,"Battle, Leilani; Chang, Remco; Stonebraker, Michael",Database,2015-10-19T20:15:05Z,2015-10-19T20:15:05Z,2015-10-19,http://hdl.handle.net/1721.1/99361,"In this paper, we present ForeCache, a general-purpose tool for exploratory browsing of large datasets. ForeCache utilizes a client-server architecture, where the user interacts with a lightweight client-side interface to browse datasets, and the data to be browsed is retrieved from a DBMS running on a back-end server. We assume a detail-on-demand browsing paradigm, and optimize the back-end support for this paradigm by inserting a separate middleware layer in front of the DBMS. To improve response times, the middleware layer fetches data ahead of the user as she explores a dataset. We consider two different mechanisms for prefetching: (a) learning what to fetch from the user's recent movements, and (b) using data characteristics (e.g., histograms) to find data similar to what the user has viewed in the past. We incorporate these mechanisms into a single prediction engine that adjusts its prediction strategies over time, based on changes in the user's behavior. We evaluated our prediction engine with a user study, and found that our dynamic prefetching strategy provides: (1) significant improvements in overall latency when compared with non-prefetching systems (430% improvement); and (2) substantial improvements in both prediction accuracy (25% improvement) and latency (88% improvement) relative to existing prefetching techniques.",13 p.,MIT-CSAIL-TR-2015-031,Dynamic Prefetching of Data Tiles for Interactive Visualization,2015-10-19T20:15:05Z,visualization; interactive exploration; databases,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,,
Leslie Kaelbling,"Zewdie, Dawit H.; Konidaris, George",Learning and Intelligent Systems,2015-11-30T19:30:04Z,2015-11-30T19:30:04Z,2015-11-24,http://hdl.handle.net/1721.1/100053,"Recent years have seen increased interest in non-parametric reinforcement learning. There are now practical kernel-based algorithms for approximating value functions; however, kernel regression requires that the underlying function being approximated be smooth on its domain. Few problems of interest satisfy this requirement in their natural representation. In this paper we define Value-Consistent Pseudometric (VCPM), the distance function corresponding to a transformation of the domain into a space where the target function is maximally smooth and thus well-approximated by kernel regression. We then present DKBRL, an iterative batch RL algorithm interleaving steps of Kernel-Based Reinforcement Learning and distance metric adjustment. We evaluate its performance on Acrobot and PinBall, continuous-space reinforcement learning domains with discontinuous value functions.",16 p.,MIT-CSAIL-TR-2015-032,Representation Discovery for Kernel-Based Reinforcement Learning,2015-11-30T19:30:04Z,Metric learning,Creative Commons Attribution-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-sa/4.0/,,
Patrick Winston,"Finlayson, Mark Alan",Genesis,2015-12-03T16:30:05Z,2015-12-03T16:30:05Z,2015-12-02,http://hdl.handle.net/1721.1/100054,"This archive contains the supplementary material for the journal article ""ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory"", published in the Journal of Digital Scholarship in the Humanities (DSH), ca. 2016.The archive contains several different types of files. First, it contains the annotation guides that were used to train the annotators. The guides are numbered to match the team numbers in Table 6. Included here are not only detailed guides for some layers, as produced by the original developers of the specification, but also our synopsis guides for each layer, which were used as a reference and further training material for the annotators. Also of interest are the general annotator and adjudicator training guides, which outline the general procedures followed by the teams when conducting annotation. Those who are organizing their own annotation projects may find this material useful.Second, the archive contains a comprehensive manifest, in Excel spreadsheet format, listing the word counts, sources, types, and titles (in both Russian and English) of all the texts that are part of the corpus. Finally, the archive contains the actual corpus data files, in Story Workbench format, an XML-encoded stand-off annotation scheme. The scheme is described in the file format specification file, also included in the archive. These files can be parsed with the aid of any normal XML reading software, or can be loaded and edited easily with the Story Workbench annotation tool, also freely available.",8341 KiB,,"Supplementary materials for ""ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory""",2015-12-03T16:30:05Z,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,Patrick Winston; Genesis
Daniel Sanchez,"Beckmann, Nathan; Tsai, Po-An; Sanchez, Daniel",Computer Architecture,2015-12-21T19:00:18Z,2015-12-21T19:00:18Z,2015-12-19,http://hdl.handle.net/1721.1/100466,"Conventional memory systems are organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit a wide range of applications, because working sets settle at the smallest (and fastest) level they fit in. However, rigid hierarchies also cause significant overheads, because each level adds latency and energy even when it does not capture the working set. In emerging systems with heterogeneous memory technologies such as stacked DRAM, these overheads often limit performance and efficiency. We propose Jenga, a reconfigurable cache hierarchy that avoids these pathologies and approaches the performance of a hierarchy optimized for each application. Jenga monitors application behavior and dynamically builds virtual cache hierarchies out of heterogeneous, distributed cache banks. Jenga uses simple hardware support and a novel software runtime to configure virtual cache hierarchies. On a 36-core CMP with a 1 GB stacked-DRAM cache, Jenga outperforms a combination of state-of-the-art techniques by 10% on average and by up to 36%, and does so while saving energy, improving system-wide energy-delay product by 29% on average and by up to 96%.",12 p.,MIT-CSAIL-TR-2015-035,Jenga: Harnessing Heterogeneous Memories through Reconfigurable Cache Hierarchies,2015-12-21T19:00:18Z,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",Computer Architecture,2015-12-21T19:00:09Z,2015-12-21T19:00:09Z,2015-12-19,http://hdl.handle.net/1721.1/100464,"Caches are critical to performance, yet their behavior is hard to understand and model. In particular, prior work does not provide closed-form solutions of cache performance, i.e. simple expressions for the miss rate of a specific access pattern. Existing cache models instead use numerical methods that, unlike closed-form solutions, are computationally expensive and yield limited insight. We present cache calculus, a technique that models cache behavior as a system of ordinary differential equations, letting standard calculus techniques find simple and accurate solutions of cache performance for common access patterns.",4 p.,MIT-CSAIL-TR-2015-033,Cache Calculus: Modeling Caches through Differential Equations,2015-12-21T19:00:10Z,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",Computer Architecture,2015-12-21T19:00:15Z,2015-12-21T19:00:15Z,2015-12-19,http://hdl.handle.net/1721.1/100465,"Much prior work has studied processor cache replacement policies, but a large gap remains between theory and practice. The optimal policy (MIN) requires unobtainable knowledge of the future, and prior theoretically-grounded policies use reference models that do not match real programs. Meanwhile, practical policies are designed empirically. Lacking a strong theoretical foundation, they do not make the best use of the information available to them. This paper bridges theory and practice. We propose that practical policies should replace lines based on their economic value added (EVA), the difference of their expected hits from the average. We use Markov decision processes to show that EVA is optimal under some reasonable simplifications. We present an inexpensive, practical implementation of EVA and evaluate it exhaustively over many cache sizes. EVA outperforms prior practical policies and saves area at iso-performance. These results show that formalizing cache replacement yields practical benefits.",14 p.,MIT-CSAIL-TR-2015-034,Bridging Theory and Practice in Cache Replacement,2015-12-21T19:00:15Z,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,
Martin Rinard,"Shen, Jiasi; Rinard, Martin",Program Analysis and Compilation,2015-12-28T22:15:05Z,2015-12-28T22:15:05Z,2015-12-27,http://hdl.handle.net/1721.1/100542,"We present a new language construct, filtered iterators, for safe and robust input processing. Filtered iterators are designed to eliminate many common input-processing errors while enabling robust continued execution. The design is inspired by (a) observed common input-processing errors and (b) continued execution strategies that are implemented by developers fixing input validation errors. Filtered iterators decompose inputs into input units, atomically and automatically discarding units that trigger errors. Statistically significant results from a developer study highlight the difficulties that developers encounter when developing input-processing code using standard language constructs. These results also demonstrate the effectiveness of filtered iterators in eliminating many of these difficulties and enabling developers to produce safe and robust input-processing code.",111 p.,MIT-CSAIL-TR-2015-036,Filtered Iterators For Safe and Robust Programs in RIFL,2015-12-28T22:15:05Z,,,,,
Daniel Jackson,"McCutchen, Richard Matthew; Itzhaky, Shachar; Jackson, Daniel",Software Design,2016-01-12T21:15:03Z,2016-01-12T21:15:03Z,2016-01-12,http://hdl.handle.net/1721.1/100803,"There is a growing demand for data-driven web applications that help automate organizational and business processes of low to medium complexity by letting users view and update structured data in controlled ways. We present Object Spreadsheets, an end-user development tool that combines a spreadsheet interface with a rich data model to help the process administrators build the logic for such applications themselves. Its all-in-one interface with immediate feedback has the potential to bring more complex tasks within reach of end-user developers, compared to existing approaches. Our data model is based on the structure of entity-relationship models and directly supports nested variable-size collections and object references, which are common in web applications but poorly accommodated by traditional spreadsheets. Object Spreadsheets has a formula language suited to the data model and supports stored procedures to specify the forms of updates that application users may make. Formulas can be used to assemble data in the exact structure in which it is to be shown in the application UI, simplifying the task of UI building; we intend for Object Spreadsheets to be integrated with a UI builder to provide a complete solution for application development. We describe our prototype implementation and several example applications we built to demonstrate the applicability of the tool.",27 p.,MIT-CSAIL-TR-2016-001,Initial report on Object Spreadsheets,2016-01-12T21:15:03Z,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,
Adam Chlipala,"Pit-Claudel, Clément; Mariet, Zelda; Harding, Rachael; Madden, Sam",Programming Languages and Verification,2016-02-10T18:45:06Z,2016-02-10T18:45:06Z,2016-02-08,http://hdl.handle.net/1721.1/101150,"Rapidly developing areas of information technology are generating massive amounts of data. Human errors, sensor failures, and other unforeseen circumstances unfortunately tend to undermine the quality and consistency of these datasets by introducing outliers -- data points that exhibit surprising behavior when compared to the rest of the data. Characterizing, locating, and in some cases eliminating these outliers offers interesting insight about the data under scrutiny and reinforces the confidence that one may have in conclusions drawn from otherwise noisy datasets. In this paper, we describe a tuple expansion procedure which reconstructs rich information from semantically poor SQL data types such as strings, integers, and floating point numbers. We then use this procedure as the foundation of a new user-guided outlier detection framework, dBoost, which relies on inference and statistical modeling of heterogeneous data to flag suspicious fields in database tuples. We show that this novel approach achieves good classification performance, both in traditional numerical datasets and in highly non-numerical contexts such as mostly textual datasets. Our implementation is publicly available, under version 3 of the GNU General Public License.",12 p.,MIT-CSAIL-TR-2016-002,Outlier Detection in Heterogeneous Datasets using Automatic Tuple Expansion,2016-02-10T18:45:07Z,,,,,
Martin Rinard,"Long, Fan; Rinard, Martin",Program Analysis and Compilation,2016-02-18T20:45:03Z,2016-02-18T20:45:03Z,2016-02-18,http://hdl.handle.net/1721.1/101211,"We present the first systematic analysis of the characteristics of patch search spaces for automatic patch generation systems. We analyze the search spaces of two current state-of- the-art systems, SPR and Prophet, with 16 different search space configurations. Our results are derived from an analysis of 1104 different search spaces and 768 patch generation executions. Together these experiments consumed over 9000 hours of CPU time on Amazon EC2.The analysis shows that 1) correct patches are sparse in the search spaces (typically at most one correct patch per search space per defect), 2) incorrect patches that nevertheless pass all of the test cases in the validation test suite are typically orders of magnitude more abundant, and 3) leveraging information other than the test suite is therefore critical for enabling the system to successfully isolate correct patches.We also characterize a key tradeoff in the structure of the search spaces. Larger and richer search spaces that contain correct patches for more defects can actually cause systems to find fewer, not more, correct patches. We identify two reasons for this phenomenon: 1) increased validation times because of the presence of more candidate patches and 2) more incorrect patches that pass the test suite and block the discovery of correct patches. These fundamental properties, which are all characterized for the first time in this paper, help explain why past systems often fail to generate correct patches and help identify challenges, opportunities, and productive future directions for the field.",45 p.,MIT-CSAIL-TR-2016-003,An Analysis of the Search Spaces for Generate and Validate Patch Generation Systems,2016-02-18T20:45:03Z,Program repair,,,,
