dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.subject,dc.title,dc.identifier.citation,dc.rights,dc.rights.uri,dc.contributor,dc.identifier,dc.relation.replaces,dc.relation.uri,dc.relation
Dina Katabi,"Sodini, Charles; Edalat, Farinaz; Katabi, Dina; Kushman, Nate; Rahul, Hariharan",Networks & Mobile Systems,2010-02-02T23:15:08Z,2010-02-02T23:15:08Z,2008-08-17/2008-08-22,http://hdl.handle.net/1721.1/51335,"Wideband technologies in the unlicensed spectrum can satisfy the ever-increasing demands for wireless bandwidth created by emerging rich media applications. The key challenge for such systems, however, is to allow narrowband technologies that share these bands (say, 802.11 a/b/g/n, Zigbee) to achieve their normal performance, without compromising the throughput or range of the wideband network.This paper presents SWIFT, the first system where high-throughput wideband nodes are shown in a working deployment to coexist with unknown narrowband devices, while forming a network of their own. Prior work avoids narrowband devices by operating below the noise level and limiting itself to a single contiguous unused band. While this achieves coexistence, it sacrifices the throughput and operating distance of the wideband device. In contrast, SWIFT creates high throughput wireless links by weaving together non-contiguous unused frequency bands that change as narrowband devices enter or leave the environment. This design principle of cognitive aggregation allows SWIFT to achieve coexistence, while operating at normal power, and thereby obtaining higher throughput and greater operating range. We implement SWIFT on a wideband hardware platform, and evaluate it in the presence of 802.11 devices. In comparison to a baseline that coexists with narrowband devices by operating below their noise level, SWIFT is equally narrowband-friendly but achieves 3.6x-10.5x higher throughput and 6x greater range.",13 p.,MIT-CSAIL-TR-2010-001,Cognitive Radios; White Spaces; Cognitive Aggregation; Wideband; Wireless Networks,SWIFT: A Narrowband-Friendly Cognitive Wideband Network,"""Learning to Share: Narrowband-Friendly Wideband Networks"", ACM SIGCOMM 2008",,,,,,,
Tomaso Poggio,"De Vito, Ernesto; Belkin, Mikhail; Rosasco, Lorenzo",Center for Biological and Computational Learning (CBCL),2008-08-20T19:15:07Z,2008-08-20T19:15:07Z,2008-08-19,http://hdl.handle.net/1721.1/41940,"A large number of learning algorithms, for example, spectral clustering, kernel Principal Components Analysis and many manifold methods are based on estimating eigenvalues and eigenfunctions of operators defined by a similarity function or a kernel, given empirical data. Thus for the analysis of algorithms, it is an important problem to be able to assess the quality of such approximations. The contribution of our paper is two-fold:  1. We use a technique based on a concentration inequality for Hilbert spaces to provide new much simplified proofs for a number of results in spectral approximation.  2. Using these methods we provide several new results for estimating spectral properties of the graph Laplacian operator extending and strengthening results from [26].",22 p.,MIT-CSAIL-TR-2008-052; CBCL-274,perturbation theory; statistical learning theory; kernel methods; spectral methods,A Note on Perturbation Results for Learning Empirical Operators,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,
,"Dig, Danny; Marrero, John; Ernst, Michael D.",,2008-09-05T21:00:10Z,2008-09-05T21:00:10Z,2008-09-05,http://hdl.handle.net/1721.1/42832,"For several decades, programmers have relied onMooreÃ¢  s Law to improve the performance of their softwareapplications. From now on, programmers need to programthe multi-cores if they want to deliver efficient code. Inthe multi-core era, a major maintenance task will be tomake sequential programs more concurrent. What are themost common transformations to retrofit concurrency intosequential programs?We studied the source code of 5 open-source Javaprojects. We analyzed qualitatively and quantitatively thechange patterns that developers have used in order toretrofit concurrency. We found that these transformationsbelong to four categories: transformations that improve thelatency, the throughput, the scalability, or correctness of theapplications. In addition, we report on our experience ofparallelizing one of our own programs. Our findings caneducate software developers on how to parallelize sequentialprograms, and can provide hints for tool vendors aboutwhat transformations are worth automating.",10 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,concurrency; program transformation; refactoring,How do programs become more concurrent? A story of program transformations,,,,Michael Ernst; Program Analysis,local: MIT-CSAIL-TR-2008-053,,,
,"Kiezun, Adam; Guo, Philip J.; Jayaraman, Karthick; Ernst, Michael D.",,2008-09-25T19:00:06Z,2008-09-25T19:00:06Z,2008-09-10,http://hdl.handle.net/1721.1/42836,"We present a technique for finding security vulnerabilitiesin Web applications. SQL Injection (SQLI) and cross-sitescripting (XSS) attacks are widespread forms of attackin which the attacker crafts the input to the application toaccess or modify user data and execute malicious code. Inthe most serious attacks (called second-order, or persistent,XSS), an attacker can corrupt a database so as to causesubsequent users to execute malicious code.This paper presents an automatic technique for creatinginputs that expose SQLI and XSS vulnerabilities. The techniquegenerates sample inputs, symbolically tracks taintsthrough execution (including through database accesses),and mutates the inputs to produce concrete exploits. Oursis the first analysis of which we are aware that preciselyaddresses second-order XSS attacks.Our technique creates real attack vectors, has few falsepositives, incurs no runtime overhead for the deployed application,works without requiring modification of applicationcode, and handles dynamic programming-languageconstructs. We implemented the technique for PHP, in a toolArdilla. We evaluated Ardilla on five PHP applicationsand found 68 previously unknown vulnerabilities (23 SQLI,33 first-order XSS, and 12 second-order XSS).",11 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,reliability; dynamic analysis; dynamic taint,Automatic Creation of SQL Injection and Cross-Site Scripting Attacks,,,,Michael Ernst; Program Analysis,local: MIT-CSAIL-TR-2008-054,,,
David Karger,"Nikolova, Evdokia",Theory of Computation,2008-09-25T19:00:16Z,2008-09-25T19:00:16Z,2008-09-13,http://hdl.handle.net/1721.1/42837,"We consider general combinatorial optimization problems that can be formulated as minimizing the weight of a feasible solution wT x over an arbitrary feasible set. For these problems we describe a broad class of corresponding stochastic problems where the weight vector W has independent random components, unknown at the time of solution. A natural and important objective which incorporates risk in this stochastic setting, is to look for a feasible solution whose stochastic weight has a small tail or a small linear combination of mean and standard deviation. Our models can be equivalently reformulated as deterministic nonconvex programs for which no efficient algorithms are known. In this paper, we make progress on these hard problems.  Our results are several efficient general-purpose approximation schemes. They use as a black-box (exact or approximate) the solution to the underlying deterministic combinatorial problem and thus immediately apply to arbitrary combinatorial problems. For example, from an available ?-approximation algorithm to the deterministic problem, we construct a ?(1 + ?)-approximation algorithm that invokes the deterministic algorithm only a logarithmic number of times in the input and polynomial in 1/?, for any desired accuracy level ? > 0. The algorithms are based on a geometric analysis of the curvature and approximability of the nonlinear level sets of the objective functions.",25 p.,MIT-CSAIL-TR-2008-055,"approximation algorithms, combinatorial optimization, stochastic optimization, risk, nonconvex optimization, concave programming",Stochastic Combinatorial Optimization with Risk,,,,,,,,
Trevor Darrell,"Stiefelhagen, Rainer; Darrell, Trevor; Urtasun, Raquel; Geiger, Andreas",Vision,2008-09-29T20:15:10Z,2008-09-29T20:15:10Z,2008-09-26,http://hdl.handle.net/1721.1/42840,"Non-linear dimensionality reduction methods are powerful techniques to deal with high-dimensional datasets. However, they often are susceptible to local minima and perform poorly when initialized far from the global optimum, even when the intrinsic dimensionality is known a priori. In this work we introduce a prior over the dimensionality of the latent space, and simultaneously optimize both the latent space and its intrinsic dimensionality. Ad-hoc initialization schemes are unnecessary with our approach; we initialize the latent space to the observation space and automatically infer the latent dimensionality using an optimization scheme that drops dimensions in a continuous fashion. We report results applying our prior to various tasks involving probabilistic non-linear dimensionality reduction, and show that our method can outperform graph-based dimensionality reduction techniques as well as previously suggested ad-hoc initialization strategies.",8 p.,MIT-CSAIL-TR-2008-056,,Rank Priors for Continuous Non-Linear Dimensionality Reduction,,,,,,,,
Michael Ernst,"Ernst, Michael D.; Marrero, John; Dig, Danny",Program Analysis,2008-09-30T19:15:06Z,2008-09-30T19:15:06Z,2008-09-30,http://hdl.handle.net/1721.1/42841,"Parallelizing existing sequential programs to run efficiently on multicores is hard. The Java 5 packagejava.util.concurrent (j.u.c.) supports writing concurrent programs: much of the complexity of writing threads-safe and scalable programs is hidden in the library.  To use this package, programmers still need to reengineer existing code. This is tedious because it requires changing many lines of code, is error-prone because programmers can use the wrong APIs, and is omission-prone because programmers can miss opportunities to use the enhanced APIs.  This paper presents our tool, CONCURRENCER, which enables programmers to refactor sequential code into parallel code that uses j.u.c. concurrent utilities. CONCURRENCER does not require any program annotations, although the transformations are very involved: they span multiple program statements and use custom program analysis.  A find-and-replace tool can not perform such transformations.  Empirical evaluation shows that CONCURRENCER refactors code effectively: CONCURRENCER correctly identifies and applies transformations that some open-source developers overlooked, and the converted code exhibits good speedup.",12 p.,MIT-CSAIL-TR-2008-057,program transformations; concurrency; refactoring; library,Refactoring Sequential Java Code for Concurrency via Concurrent Libraries,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,
Dina Katabi,"Katabi, Dina; Gollakota, Shyamnath",Networks & Mobile Systems,2008-10-02T15:45:10Z,2008-10-02T15:45:10Z,2008-10-01,http://hdl.handle.net/1721.1/42842,"This paper presents ZigZag, an 802.11 receiver design that combats hidden terminals. ZigZag's core contribution is a new form of interference cancellation that exploits asynchrony across successive collisions. Specifically, 802.11 retransmissions, in the case of hidden terminals, cause successive collisions. These collisions have different interference-free stretches at their start, which ZigZag exploits to bootstrap its decoding. ZigZag makes no changes to the 802.11 MAC and introduces no overhead when there are no collisions. But, when senders collide, ZigZag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. We build a prototype of ZigZag in GNU Radio. In a testbed of 14 USRP nodes, ZigZag reduces the average packet loss rate at hidden terminals from 72.6% to about 0.7%.",14 p.,MIT-CSAIL-TR-2008-058,,ZigZag Decoding: Combating Hidden Terminals In Wireless Networks,,,,,,http://hdl.handle.net/1721.1/41084,http://hdl.handle.net/1721.1/41084,
Silvio Micali,"Micali, Silvio; Chen, Jing",Theory of Computation,2008-12-03T16:15:09Z,2008-12-03T16:15:09Z,2008-10-08,http://hdl.handle.net/1721.1/43715,"We put forward a new mechanism achieving a high benchmark for (both revenue and) the sum of revenue and efficiency in truly combinatorial auctions. Notably, our mechanism guarantees its performance (1) in a very adversarial collusion model; (2) for any profile of strategies surviving the iterated elimination of dominated strategies; and (3) by leveraging the knowledge that the players have about each other (in a non-Bayesian setting).Our mechanism also is computationally efficient, and preserves the players' privacy to an unusual extent.",18 p.,MIT-CSAIL-TR-2008-071,Implementation in surviving strategies; Resilient Mechanism Design; Privacy-preserving mechanisms; Equilibrium-less mechanism design; Knowledge benchmarks,Resilient Knowledge-Based  Mechanisms  For Truly Combinatorial Auctions (And Implementation in Surviving Strategies),,,,Silvio Micali; Theory of Computation,,,,MIT-CSAIL-TR-2008-059
Anant Agarwal,"Agarwal, Anant; Wentzlaff, David",Computer Architecture,2008-10-08T23:15:04Z,2008-10-08T23:15:04Z,2008-10-08,http://hdl.handle.net/1721.1/42894,"The next decade will afford us computer chips with 1,000 - 10,000 cores on a single piece of silicon. Contemporary operating systems have been designed to operate on a single core or small number of cores and hence are not well suited to manage and provide operating system services at such large scale. Managing 10,000 cores is so fundamentally different from managing two cores that the traditional evolutionary approach of operating system optimization will cease to work. The fundamental design of operating systems and operating system data structures must be rethought. This work begins by documenting the scalability problems of contemporary operating systems. These studies are used to motivate the design of a factored operating system (fos). fos is a new operating system targeting 1000+ core multicore systems where space sharing replaces traditional time sharing to increase scalability. fos is built as a collection of Internet inspired services. Each operating system service is factored into a fleet of communicating servers which in aggregate implement a system service. These servers are designed much in the way that distributed Internet services are designed, but instead of providing high level Internet services, these servers provide traditional kernel services and manage traditional kernel data structures in a factored, spatially distributed manner. The servers are bound to distinct processing cores and by doing so do not fight with end user applications for implicit resources such as TLBs and caches. Also, spatial distribution of these OS services facilitates locality as many operations only need to communicate with the nearest server for a given service.",12 p.,MIT-CSAIL-TR-2008-060,multicore; operating system design; manycore,The Case for a Factored Operating System (fos),,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",Theory of Computation,2008-10-08T20:15:07Z,2008-10-08T20:15:07Z,2008-10-08,http://hdl.handle.net/1721.1/42893,"Following Micali and Valiant [MV07.a], a mechanism is resilient if it achieves its objective without any problem of (1) equilibrium selection and (2) player collusion. To advance resilient mechanism design,We put forward a new meaningful benchmark for the COMBINED social welfare-revenue performance of any mechanism in truly combinatorial auctions.We put forward a NEW notion of implementation, much more general than the ones used so far, which we believe to be of independent interest.We put forward a new RESILIENT mechanism that, by leveraging the knowledge that the players have about each other, guarantees at least one half of our benchmark under a very general collusion model.",32 p.,MIT-CSAIL-TR-2008-059,knowledge benchmarks; implementation in surviving strategies; equilibrium-less implementation; combinatorial auctions; resilient mechanisms; collusion; truly combinatorial auctions,New Resiliency in  Truly Combinatorial Auctions  (and Implementation in Surviving Strategies),,,,,,,,MIT-CSAIL-TR-2008-041
Daniel Jackson,"Edwards, Jonathan",Software Design,2008-10-10T21:15:05Z,2008-10-10T21:15:05Z,2008-10-10,http://hdl.handle.net/1721.1/42895,"Modularity and flexibility can conflict in multi-language systems. For example, the templates commonly used to generate web pages must be manually updated when the database schema changes. Modularity can be improved by generating web pages automatically from the database schema, but it is hard for such a generator to produce the same variety of outputs that are easily achieved by ad hoc edits to a template. Ideally, such ad hoc edits would be abstracted into transformations that compose with the generator, offering both modularity and flexibility. However common customizations cannot be abstracted using the standard techniques of textual identifiers and ordinal positions. These difficulties are distilled into a challenge problem to evaluate potential solutions. A solution is proposed based on field trees, a new data model for software artifacts that provides persistent identifiers and unshifting positions within sequences. But using field trees with conventional programming languages and development environments requires more effort than the ad hoc editing they seek to supplant. Field trees are therefore extended into differential trees, which integrate artifacts and their transformations into a unified representation.",13 p.,MIT-CSAIL-TR-2008-061,,Modular Generation and Customization,,Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,
Tomaso Poggio,"Rosasco, Lorenzo; Pereverzyev, Sergei; De Vito, Ernesto",Center for Biological and Computational Learning (CBCL),2008-10-17T15:30:10Z,2008-10-17T15:30:10Z,2008-10-16,http://hdl.handle.net/1721.1/42896,"The regularization parameter choice is a fundamental problem in supervised learning since the performance of most algorithms crucially depends on the choice of one or more of such parameters. In particular a main theoretical issue regards the amount of prior knowledge on the problem needed to suitably choose the regularization parameter and obtain learning rates. In this paper we present a strategy, the balancing principle, to choose the regularization parameter without knowledge of the regularity of the target function. Such a choice adaptively achieves the best error rate. Our main result applies to regularization algorithms in reproducing kernel Hilbert space with the square loss, though we also study how a similar principle can be used in other situations. As a straightforward corollary we can immediately derive adaptive parameter choice for various kernel methods recently studied. Numerical experiments with the proposed parameter choice rules are also presented.",24 p.,MIT-CSAIL-TR-2008-062; CBCL-275,Adaptive Model Selection; Learning Theory; Inverse Problems; Regularization,Adaptive Kernel Methods Using the Balancing Principle,,,,,,,,
Dina Katabi,"Woo, Grace; Katabi, Dina; Chachulski, Szymon",Networks & Mobile Systems,2008-10-20T15:00:05Z,2008-10-20T15:00:05Z,2008-10-18,http://hdl.handle.net/1721.1/42897,"The fundamental problem of wireless video multicast is to scalably serve multiple receivers which may have very different channel characteristics. Ideally, one would like to broadcast a single stream that allows each receiver to benefit from all correctly received bits to improve its video quality. We introduce Digital Rain, a new approach to wireless video multicast that adapts to channel characteristics without any need for receiver feedback or variable codec rates. Users that capture more packets or have fewer bit errors naturally see higher video quality. Digital Rain departs from current approaches in two ways: 1) It allows a receiver to exploit video packets that may contain bit errors; 2) It builds on the theory of compressed sensing to develop robust video encoding and decoding algorithms that degrade smoothly with bit errors and packet loss. Implementation results from an indoor wireless testbed show that Digital Rain significantly improves the received video quality and the number of supported receivers.",12 p.,MIT-CSAIL-TR-2008-063,Wireless networks; Video streaming; Multicast,One Video Stream to Serve Diverse Receivers,,,,,,,,
Anant Agarwal,"Agarwal, Anant; Psota, James; Eastep, Jonathan; Konstantakopoulos, Theodoros",Computer Architecture,2008-11-14T05:00:09Z,2008-11-14T05:00:09Z,2008-11-11,http://hdl.handle.net/1721.1/43707,"On-chip interconnection networks (OCNs) such as point-to-point networks and buses form the communication backbone in systems-on-a-chip, multicore processors, and tiled processors. OCNs can consume significant portions of a chip's energy budget, so analyzing their energy consumption early in the design cycle becomes important for architectural design decisions. Although numerous studies have examined OCN implementation and performance, few have examined energy. This paper develops an analytical framework for energy estimation in OCNs and presents results based on both analytical models of communication patterns and real network traces from applications running on a tiled multicore processor. Our analytical framework supports arbitrary OCN topologies under arbitrary communication patterns while accounting for wire length, switch energy, and network contention. It is the first to incorporate the effects of communication locality and network contention, and use real traces extensively. This paper compares the energy of point-to-point networks against buses under varying degrees of communication locality. The results indicate that, for 16 or more processors, a one-dimensional and a two-dimensional point-to-point network provide 66% and 82% energy savings, respectively, over a bus assuming that processors communicate with equal likelihood. The energy savings increase for patterns which exhibit locality. For the two-dimensional point-to-point OCN of the Raw tiled microprocessor, contention contributes a maximum of just 23% of the OCN energy, using estimated values for channel, switch control logic, and switch queue buffer energy of 34.5pJ, 17pJ, and 12pJ, respectively. Our results show that the energy-delay product per message decreases with increasing processor message injection rate.",24 p.,MIT-CSAIL-TR-2008-066,on-chip networks; multicore; energy scalability,Energy Scalability of On-Chip Interconnection Networks in Multicore Architectures,,,,,,,,
John Leonard,"Benjamin, Michael R.","Robotics, Vision & Sensor Networks",2008-11-14T05:00:20Z,2008-11-14T05:00:20Z,2008-11-11,http://hdl.handle.net/1721.1/43708,This document describes seven common MOOS-IvP autonomy tools. The uHelmScope application provides a run-time scoping window into the state of an active IvP Helm executing its mission. The pMarineViewer application is a geo-based GUI tool for rendering marine vehicles and certain autonomy properties in their operational area. The uXMS application is a terminal based tool for live scoping on a MOOSDB process. The uTermCommand application is a terminal based tool for poking the MOOSDB with a set of MOOS file pre-defined variable-value pairs selectable with tab-completion of aliases from the command-line. The pEchoVar application provides a way of echoing an observed write to a variable with a new write with the same value to a different variable name. The uProcessWatch application is a way of monitoring the presence or absence of a set of MOOS processes and summarizing the collective status in a single MOOS variable. The uPokeDB application is a way of poking a MOOSDB from the command line with one or more variable-value pairs without any pre-existing configuration of a MOOS file.,46 p.,MIT-CSAIL-TR-2008-065,MOOS Poke; Marine Robotics; IvP Helm; MOOS; MOOS Scope; Helm Scope,MOOS-IvP Autonomy Tools Users Manual,,,,,,,,
Silvio Micali,"Micali, Silvio; Valiant, Paul",Theory of Computation,2008-11-14T05:00:26Z,2008-11-14T05:00:26Z,2008-11-13,http://hdl.handle.net/1721.1/43709,"Dominant-strategy truthfulness is traditionally considered the best possible solution concept in mechanism design, as it enables one to predict with confidence which strategies INDEPENDENT players will actually choose. Yet, as with any other form of equilibrium, it too can be extremely vulnerable to COLLUSION. The problem of collusion is particularly evident for UNRESTRICTED combinatorial auctions}, arguably the hardest type of auctions.We thus investigate how much revenue can be guaranteed, in unrestricted combinatorial auctions, by dominant-strategy-truthful mechanisms that are COLLUSION-RESILIENT in a very strong sense; and obtain almost matching upper- and lower-bounds.",18 p.,MIT-CSAIL-TR-2008-067,Resilient mechanism design,Resilient Mechanisms For Truly Combinatorial Auctions,,,,,,,,MIT-CSAIL-TR-2008-039; MIT-CSAIL-TR-2007-052
Nancy Lynch,"Lynch, Nancy; Pereira, Olivier; Kaynar, Dilsun; Cheung, Ling; Canetti, Ran",Theory of Computation,2008-11-24T06:00:04Z,2008-11-24T06:00:04Z,2008-11-22,http://hdl.handle.net/1721.1/43711,"For many cryptographic protocols, security relies on the assumption that adversarial entities have limited computational power. This type of security degrades progressively over the lifetime of a protocol. However, some cryptographic services, such as timestamping services or digital archives, are long-lived in nature; they are expected to be secure and operational for a very long time (i.e., super-polynomial). In such cases, security cannot be guaranteed in the traditional sense: a computationally secure protocol may become insecure if the attacker has a super-polynomial number of interactions with the protocol. This paper proposes a new paradigm for the analysis of long-lived security protocols. We allow entities to be active for a potentially unbounded amount of real time, provided they perform only a polynomial amount of work per unit of real time. Moreover, the space used by these entities is allocated dynamically and must be polynomially bounded. We propose a new notion of long-term implementation, which is an adaptation of computational indistinguishability to the long-lived setting. We show that long-term implementation is preserved under polynomial parallel composition and exponential sequential composition. We illustrate the use of this new paradigm by analyzing some security properties of the long-lived timestamping protocol of Haber and Kamat.",27 p.,MIT-CSAIL-TR-2008-068,,"Modeling Computational Security in Long-Lived Systems, Version 2",,,,,,,,
Joshua Tenenbaum,"Tenenbaum, Joshua B.; Jonas, Eric M.; Mansinghka, Vikash K.",Computational Cognitive Science,2008-11-24T16:30:26Z,2008-11-24T16:30:26Z,2008-11-24,http://hdl.handle.net/1721.1/43712,"We introduce combinational stochastic logic, an abstraction that generalizes deterministic digital circuit design (based on Boolean logic gates) to the probabilistic setting. We show how this logic can be combined with techniques from contemporary digital design to generate stateless and stateful circuits for exact and approximate sampling from a range of probability distributions. We focus on Markov chain Monte Carlo algorithms for Markov random fields, using massively parallel circuits. We implement these circuits on commodity reconfigurable logic and estimate the resulting performance in time, space and price. Using our approach, these simple and general algorithms could be affordably run for thousands of iterations on models with hundreds of thousands of variables in real time.",10 p.,MIT-CSAIL-TR-2008-069,cognitive science; robustness; Bayesian inference; artificial intelligence,Stochastic Digital Circuits for Probabilistic Inference,,,,,,,,
Tomaso Poggio,"Caponnetto, Andrea; Poggio, Tomaso; Bouvrie, Jake; Rosasco, Lorenzo; Smale, Steve",Center for Biological and Computational Learning (CBCL),2008-11-27T01:30:05Z,2008-11-27T01:30:05Z,2008-11-26,http://hdl.handle.net/1721.1/43713,"We propose a natural image representation, the neural response, motivated by the neuroscience of the visual cortex. The inner product defined by the neural response leads to a similarity measure between functions which we call the derived kernel. Based on a hierarchical architecture, we give a recursive definition of the neural response and associated derived kernel. The derived kernel can be used in a variety of application domains such as classification of images, strings of text and genomics data.",25 p.,MIT-CSAIL-TR-2008-070; CBCL-276,neuroscience; computer vision; kernels,Mathematics of the Neural Response,,,,,,,,
