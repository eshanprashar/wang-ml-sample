dc.contributor.advisor,dc.contributor.author,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.relation.ispartofseries,dc.title,dc.identifier.other,dc.description.abstract,dc.format.extent,dc.format.mimetype,dc.language.iso
"Liskov, Barbara H.; Morris, Robert T.","Ajmani, Sameer",2023-03-29T15:35:38Z,2023-03-29T15:35:38Z,2000-09,https://hdl.handle.net/1721.1/149948,MIT-LCS-TR-846,A Trusted Execution Platform for Multiparty Computation,,,,,
,"Kumar, Vinay; Poggio, Tomaso",2004-10-20T21:04:37Z,2004-10-20T21:04:37Z,2000-09-01,http://hdl.handle.net/1721.1/7264,AIM-1696; CBCL-191,Learning-Based Approach to Estimation of Morphable Model Parameters,AIM-1696; CBCL-191,"We describe the key role played by partial  evaluation in the Supercomputing Toolkit, a  parallel computing system for scientific  applications that effectively exploits the vast  amount of parallelism exposed by partial  evaluation. The Supercomputing Toolkit  parallel processor and its associated partial  evaluation-based compiler have been used  extensively by scientists at MIT, and have  made possible recent results in astrophysics  showing that the motion of the planets in our  solar system is chaotically unstable.",1037544 bytes; 218112 bytes,application/postscript; application/pdf,en_US
,"Serre, Thomas; Heisele, Bernd; Mukherjee, Sayan; Poggio, Tomaso",2004-10-20T21:03:34Z,2004-10-20T21:03:34Z,2000-09-01,http://hdl.handle.net/1721.1/7232,AIM-1697; CBCL-192,Feature Selection for Face Detection,AIM-1697; CBCL-192,We present a new method to select features for a face detection system using Support Vector Machines (SVMs). In the first step we reduce the dimensionality of the input space by projecting the data into a subset of eigenvectors. The dimension of the subset is determined by a classification criterion based on minimizing a bound on the expected error probability of an SVM. In the second step we select features from the SVM feature space by removing those that have low contributions to the decision function of the SVM.,7211022 bytes; 1034240 bytes,application/postscript; application/pdf,en_US
,"Snoeren, Alex C.; Andersen, David G.; Balakrishnan, Hari",2023-03-29T15:33:50Z,2023-03-29T15:33:50Z,2000-11,https://hdl.handle.net/1721.1/149916,MIT-LCS-TR-813,Fine-Grained Failover Using Connection Migration,,"This paper presents a set of techniques for providing fine-grained failover of long-running connections across a distributed collection of replica servers, and is especially useful for fault-tolerant and load-balanced delivery of streaming media and telephony sessions. Our system achieves connection-level failover across both local- and wide-area server replication, without requiring a front-end transport- or application-layer switch. Our approach is enabled by the recently-developed end-to-end ``connection migration'' mechanism for transport protocols such as TCP, combined with a soft-state session synchronization protocol between replica servers.   The end result is a robust, fast, and fine-grained server failover mechanism that is transparent to both the client and server applications. We describe the details of our design and Linux implementation, as well as experiments with our implementation that show that this approach to failover is an attractive way to engineer robust systems for distributing long-running streams; connections suffer relatively low performance degradation even when server redirection occurs every few seconds, and overhead is negligible when compared to standard techniques. In particular, we observe the performance impact of migrating TCP connections depends on the length of time between migration and the most recent loss-recovery event.",,,
,"Ingols, Kyle; Keidar, Idit",2023-03-29T14:42:03Z,2023-03-29T14:42:03Z,2000-11,https://hdl.handle.net/1721.1/149301,MIT-LCS-TM-611,Availability Study of Dynamic Voting Algorithms,,"Fault tolerant distributed systems often select a primary component to allow a subset of the processes to function when failures occur. The dynamic voting paradigm defines rules for selecting the primary component adaptively: when a partition occurs, if a majority of the previous primary component is connected, a new and possibly smaller primary is chosen. Several studies have shown that dynamic voting leads to more available solutions than other paradigms for maintaining a primary component. However, these studies have assumed that every attempt made by the algorithm to form a new primary component terminates successfully. Unfortunately, in real systems, this is not always the case: a change in connectivity can interrupt the algorithm whiel it is still attempting to form a new primary component; in such cases, algorithms typically block until processes can resolve the outcome of the interrupted attempt. This paper uses simulations to evaluate the effect of interruptions on the availability of dynamic voting algorithms. We study four dynamic voting algorithms, and identify two important characteristics that impact an algorithm's availability in runs with frequent connectivity changes. First, we show that the number of communication rounds exchanged in an algorithm plays a significant role in the availability achieved, especially in the degradation of availability as connectivity changes become more frequent. Second, we show that the number of processes that need to be present in order to resolve past attempts impacts the availability, especially during long runs with numerous connectivity changes.",,,
,"Fekete, Alan; Keidar, Idit",2023-03-29T14:42:00Z,2023-03-29T14:42:00Z,2000-11,https://hdl.handle.net/1721.1/149300,MIT-LCS-TM-610,A General Framework for Highly Available Services based on Group Communication,,"We present a general framework for building highly available services. The framework uses group communication to coordinate a collection of servers. Our framework is configurable, in that one can adjust parameters such as the number of servers and the extent to which they are synchronized. We analyze the scenarios that can lead to the service availability being temporarily comprised, and we discuss the tradeoffs that govern the choice of parameters.",,,
,"Thies, William F.; Viven, Frederic; Sheldon, Jeffery W.; Amarasinghe, Saman",2023-03-29T14:42:05Z,2023-03-29T14:42:05Z,2000-11,https://hdl.handle.net/1721.1/149302,MIT-LCS-TM-613,A Unified Framework for Schedule and Storage Optimization,,"We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find the best storage mapping for a given schedule, the best schedule for a given storage mapping, and the best storage mapping that is valid for all legal schedules. Our techniques combines affine scheduling techniques with occupancy vector analysis, and incorporates general affine dependencies across statements and loop nests. We formulate the constraints imposed by the data dependencies and the storage mapping as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the best occupancy vector. We consider out method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.",,,
,"Shrestha, Govinda; Amarasinghe, Saman",2023-03-29T15:33:59Z,2023-03-29T15:33:59Z,2000-11,https://hdl.handle.net/1721.1/149918,MIT-LCS-TR-815,Perspectives on the Use of the Internet in Sri Lanka,,"The survey examines the use of computers and the Internet in Sri Lanka from the perspective of the Internet Service Provider (ISP) members. It attempts to describe the general nature of IT use in terms of the availability, access, familiarity and general conditions associated with using computers and the Internet in the country.  The survey was conducted in July 1999. Questionnaires were e-mailed to 9448 ISP members in Sri Lanka, using e-mail addresses available to us at that time. Altogether, 560 members completed and returned questionnaires via e-mail to MIT's Laboratory for Computer Science.  Descriptive analysis of both quantitative and qualitative data was then conducted.    Major quantitative findings include:  *Over 60% of the respondents were members of their respective ISPs for two or less years, and over half had first used a computer sometime during the 1990-99 period. *Sixty-two percent of the respondents had sent 10 or more e-mails per week over the past six (or less) months, and 52% had received 15 or more e-mails per week during the same period. *Nearly half of the respondents used a computer at home, and 48% indicated 33.6K as the baud rate to connect their ISPs. *Seventy-eight percent of the respondents spent 1-9 hours per week sending and receiving e-mails, and a large majority (68%) spent 1-9 hours surfing the Web. *A majority of the respondents were positive about conditions in the workplace, such as the number and quality of opportunities for training and skill development, the quality of telecommunications facilities, and the quality and reliability of Internet connections. *An overwhelming majority of the respondents indicated that ISP subscriber fees, computer hardware and software costs, and telecommunications charges were generally high. *Most respondents were generally positive about 1) the quality of access to the Internet, 2) the quality of access to e-mails, Web pages and other Internet-based features, and 3) various benefits of Internet access. *Seventy-one percent of the respondents were male; nearly half were younger than 35, and a large majority were educated (with at least a high school diploma.)  Private company employees and people in business comprised over half of the respondents.  Major qualitative findings include: * It is crucially important to have faster access to information, increased communication at low costs, online-education and training, and increased efficiency in business, professional and organizational activities. * Matters of considerable concern include the low bandwidth, the high telecommunications charges, the low quality of Internet services, and the lack of organized information and databases. * Greatly needed is a raising of awareness, a change in the current regulatory environment, an open government, and a set of local information resources to support commerce.",,,
,"Antone, Matthew E.; Teller, Seth",2023-03-29T15:33:52Z,2023-03-29T15:33:52Z,2000-12,https://hdl.handle.net/1721.1/149917,MIT-LCS-TR-814,Automatic Recovery of Camera Positions in Urban Scenes,,"Accurate camera calibration is crucial to the reconstruction of three-dimensional geometry and the recovery of photometric scene properties. Calibration involves the determination of intrinsic parameters (e.g. focal length, principal point, and radial lens distortion) and extrinsic parameters (orientation and position).  In urban scenes and other environments containing sufficient geometric structure, it is possible to decouple extrinsic calibration into rotational and translational components that can be treated separately, simplifying the registration problem. Here we present such a decoupled formulation and describe methods for automatically recovering the positions of a large set of cameras given intrinsic calibration, relative rotations, and approximate positions.  Our algorithm first estimates the directions of translation (up to an unknown scale factor) between adjacent camera pairs using point features but without requiring explicit correspondence between them. This technique combines the robustness and simplicity of a Hough transform with the accuracy of Monte Carlo expectation maximization. We then find a set of distances between the pairs that produces globally-consistent camera positions. Novel uncertainty formulations and match plausibility criteria improve reliability and accuracy.  We assess our system's performance using both synthetic data and a large set of real panoramic imagery. The system produces camera positions accurate to within 5 centimeters in image networks extending over hundreds of meters.",,,
"Liskov, Barbara H.","Ahmed, Sarah",2023-03-29T15:35:45Z,2023-03-29T15:35:45Z,2001-01,https://hdl.handle.net/1721.1/149951,MIT-LCS-TR-849,A Scalable Byzantine Fault Tolerant Secure Domain Name Service,,,,,
,"Rinard, Martin; Kuncak, Viktor",2023-03-29T15:34:01Z,2023-03-29T15:34:01Z,2001-01,https://hdl.handle.net/1721.1/149919,MIT-LCS-TR-816,"Object Models, Heaps and Interpretations",,This paper explores the use of object models for specifying verifiable heap invariants.  We define a simple language based on sets and relations and illustrate its use through examples.  We give formal semantics of the laguage by translation into predicate calculus and interpretation of predicates in terms of objects and references in the program heap.,,,
,"Castro, Miguel",2023-03-29T15:34:03Z,2023-03-29T15:34:03Z,2001-01,https://hdl.handle.net/1721.1/149920,MIT-LCS-TR-817,Practical Byzantine Fault Tolerance,,"Our growing reliance on online services accessible on the Internet demands highly-available systemsthat provide correct service without interruptions. Byzantine faults such as software bugs, operatormistakes, and malicious attacks are the major cause of service interruptions. This thesis describesa new replication algorithm, BFT, that can be used to build highly-available systems that tolerateByzantine faults. It shows, for the first time, how to build Byzantine-fault-tolerant systems that canbe used in practice to implement real services because they do not rely on unrealistic assumptionsand they perform well. BFT works in asynchronous environments like the Internet, it incorporatesmechanisms to defend against Byzantine-faulty clients, and it recovers replicas proactively. Therecovery mechanism allows the algorithm to tolerate any number of faults over the lifetime of thesystem provided fewer than 1=3 of the replicas become faulty within a small windowof vulnerability.The window may increase under a denial-of-service attack but the algorithm can detect and respondto such attacks and it can also detect when the state of a replica is corrupted by an attacker.BFT has been implemented as a generic program library with a simple interface. The BFTlibrary provides a complete solution to the problem of building real services that tolerate Byzantinefaults. We used the library to implement the first Byzantine-fault-tolerant NFS file system, BFS. TheBFT library and BFS perform well because the library incorporates several important optimizations.The most important optimization is the use of symmetric cryptography to authenticate messages.Public-key cryptography, which was the major bottleneck in previous systems, is used only toexchange the symmetric keys. The performance results show that BFS performs 2% faster to 24%slower than production implementations of the NFS protocol that are not replicated. Therefore, webelieve that the BFT library can be used to build practical systems that tolerate Byzantine faults.",,,
,"Koile, Kimberle",2004-10-20T20:28:12Z,2004-10-20T20:28:12Z,2001-01-01,http://hdl.handle.net/1721.1/7072,AITR-2001-001,The Architect's Collaborator: Toward Intelligent Tools for Conceptual Design,AITR-2001-001,"In early stages of architectural design, as in  other design domains, the language used is often very abstract. In architectural design, for  example, architects and their clients use experiential terms such as ""private"" or ""open""  to describe spaces. If we are to build programs that can help designers during this  early-stage design, we must give those programs the capability to deal with concepts  on the level of such abstractions. The work reported in this thesis sought to do that,  focusing on two key questions: How are  abstract terms such as ""private"" and ""open"" translated  into physical form? How might one build a tool to assist designers with this process? The Architect's Collaborator (TAC) was built to  explore these issues. It is a design assistant that supports iterative design refinement, and  that represents and reasons about how experiential qualities are manifested in  physical form. Given a starting design and a  set of design goals, TAC explores the space of  possible designs in search of solutions that  satisfy the goals. It employs a strategy we've called  dependency-directed redesign: it evaluates a design with respect to a set of goals, then  uses an explanation of the evaluation to guide proposal and refinement of repair  suggestions; it then carries out the repair  suggestions to create new designs. A series of experiments was run to study  TAC's behavior. Issues of control structure,  goal set size, goal order, and modification operator  capabilities were explored. In addition, TAC's use as a design assistant was studied  in an experiment using a house in the  process of being redesigned. TAC's use as an  analysis tool was studied in an experiment  using Frank Lloyd Wright's Prairie houses.",20962265 bytes; 1471552 bytes,application/postscript; application/pdf,en_US
,"Alvira, Mariano; Rifkin, Ryan",2004-10-20T20:50:07Z,2004-10-20T20:50:07Z,2001-01-01,http://hdl.handle.net/1721.1/7219,AIM-2001-004; CBCL-193,An Empirical Comparison of SNoW and SVMs for Face Detection,AIM-2001-004; CBCL-193,"Impressive claims have been made for the performance of the SNoW algorithm on face detection tasks by Yang et. al. [7]. In particular, by looking at both their results and those of Heisele et. al. [3], one could infer that the SNoW system performed substantially better than an SVM-based system, even when the SVM used a polynomial kernel and the SNoW system used a particularly simplistic 'primitive' linear representation. We evaluated the two approaches in a controlled experiment, looking directly at performance on a simple, fixed-sized test set, isolating out 'infrastructure' issues related to detecting faces at various scales in large images. We found that SNoW performed about as well as linear SVMs, and substantially worse than polynomial SVMs.",1232391 bytes; 319169 bytes,application/postscript; application/pdf,en_US
,"Darrell, T.; Demirdjian, D.; Checka, N.; Felzenswalb, P.",2004-10-04T14:37:37Z,2004-10-04T14:37:37Z,2001-02-01,http://hdl.handle.net/1721.1/6075,AIM-2001-001,Plan-view Trajectory Estimation with Dense Stereo Background Models,AIM-2001-001,"In a known environment, objects may be tracked in multiple views using a set of back-ground models. Stereo-based models can be illumination-invariant, but often have undefined values which inevitably lead to foreground classification errors. We derive dense stereo models for object tracking using long-term, extended dynamic-range imagery, and by detecting and interpolating uniform but unoccluded planar regions. Foreground points are detected quickly in new images using pruned disparity search. We adopt a 'late-segmentation' strategy, using an integrated plan-view density representation. Foreground points are segmented into object regions only when a trajectory is finally estimated, using a dynamic programming-based method. Object entry and exit are optimally determined and are not restricted to special spatial zones.",5522496 bytes; 672260 bytes,application/postscript; application/pdf,en_US
,"Stoica, Ion; Morris, Robert T.; Karger, David R.; Kaashoek, M. Frans; Balakrishnan, Hari",2023-03-29T15:34:10Z,2023-03-29T15:34:10Z,2001-03,https://hdl.handle.net/1721.1/149922,MIT-LCS-TR-819,Chord: A scalable peer-to-peer lookup service for Internet applications,,,,,
,"Fu, Kevin; Sit, Emil; Smith, Kendra; Feamster, Nick",2023-03-29T15:34:08Z,2023-03-29T15:34:08Z,2001-03,https://hdl.handle.net/1721.1/149921,MIT-LCS-TR-818,Client Authentication on the Web,,,,,
,"Sadr, Javid; Sinha, Pawan",2004-10-20T20:50:11Z,2004-10-20T20:50:11Z,2001-03-01,http://hdl.handle.net/1721.1/7221,AIM-2001-006; CBCL-196,Exploring Object Perception with Random Image Structure Evolution,AIM-2001-006; CBCL-196,"We have developed a technique called RISE (Random Image Structure Evolution), by which one may systematically sample continuous paths in a high-dimensional image space. A basic RISE sequence depicts the evolution of an object's image from a random field, along with the reverse sequence which depicts the transformation of this image back into randomness. The processing steps are designed to ensure that important low-level image attributes such as the frequency spectrum and luminance are held constant throughout a RISE sequence. Experiments based on the RISE paradigm can be used to address some key open issues in object perception. These include determining the neural substrates underlying object perception, the role of prior knowledge and expectation in object perception, and the developmental changes in object perception skills from infancy to adulthood.",14196504 bytes; 1545031 bytes,application/postscript; application/pdf,en_US
,"Shelton, Christian R.",2004-10-20T20:50:06Z,2004-10-20T20:50:06Z,2001-03-20,http://hdl.handle.net/1721.1/7218,AIM-2001-002; CBCL-194,Policy Improvement for POMDPs Using Normalized Importance Sampling,AIM-2001-002; CBCL-194,"We present a new method for estimating the expected return of a POMDP from experience. The estimator does not assume any knowle ge of the POMDP and allows the experience to be gathered with an arbitrary set of policies. The return is estimated for any new policy of the POMDP. We motivate the estimator from function-approximation and importance sampling points-of-view and derive its theoretical properties. Although the estimator is biased, it has low variance and the bias is often irrelevant when the estimator is used for pair-wise comparisons.We conclude by extending the estimator to policies with memory and compare its performance in a greedy search algorithm to the REINFORCE algorithm showing an order of magnitude reduction in the number of trials required.",4576001 bytes; 768071 bytes,application/postscript; application/pdf,en_US
,"Liskov, Moses; Lysyanskeya, Anna; Micali, Silvio; Reyzin, Leonid; Smith, Adam",2023-03-29T14:42:11Z,2023-03-29T14:42:11Z,2001-04,https://hdl.handle.net/1721.1/149304,MIT-LCS-TM-615,Mutually Independent Commitment,,"We describe a new kind of commitment scheme in which two parties commit to values in a commitment stage, at the end of which we are assured that the values they have committed to cannot be correlated to one another. We call this new primitive mutually independent commitments. We present three mutually independent commitment schemes which handle single bit commitments, and which are computationally hiding and perfecting binding.",,,
