dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.rights,dc.rights.uri,dc.title,dc.description.sponsorship,dc.subject,dc.date.updated,dc.description,dc.identifier.citation,dc.language.rfc3066,dc.contributor
Leslie Kaelbling,"Glover, Jared; Kaelbling, Leslie Pack",Learning and Intelligent Systems,2013-03-29T21:30:06Z,2013-03-29T21:30:06Z,2013-03-27,http://hdl.handle.net/1721.1/78248,"A deterministic method for sequential estimation of 3-D rotations is presented. The Bingham distribution is used to represent uncertainty directly on the unit quaternion hypersphere. Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions. Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic. We present two versions of the QBF, suitable for tracking the state of first- and second-order rotating dynamical systems.",13 p.,MIT-CSAIL-TR-2013-005,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,Tracking 3-D Rotations with the Quaternion Bingham Filter,,,,,,,
Joshua Tenenbaum,"Wingate, David; Diuk, Carlos; O'Donnell, Timothy; Tenenbaum, Joshua; Gershman, Samuel",Computational Cognitive Science,2013-04-18T00:45:04Z,2013-04-18T00:45:04Z,2013-04-12,http://hdl.handle.net/1721.1/78573,"This paper describes a probabilistic framework for incorporating structured inductive biases into reinforcement learning. These inductive biases arise from policy priors, probability distributions over optimal policies. Borrowing recent ideas from computational linguistics and Bayesian nonparametrics, we define several families of policy priors that express compositional, abstract structure in a domain. Compositionality is expressed using probabilistic context-free grammars, enabling a compact representation of hierarchically organized sub-tasks. Useful sequences of sub-tasks can be cached and reused by extending the grammars nonparametrically using Fragment Grammars. We present Monte Carlo methods for performing inference, and show how structured policy priors lead to substantially faster learning in complex domains compared to methods without inductive biases.",17 p.,MIT-CSAIL-TR-2013-007,,,Compositional Policy Priors,"This work was supported by AFOSR FA9550-07-1-0075 and ONR
N00014-07-1-0937. SJG was supported by a Graduate Research Fellowship from the NSF.",,,,,,
Fredo Durand,"Levin, Anat; Glasner, Daniel; Xiong, Ying; Durand, Fredo; Freeman, William; Matusik, Wojciech; Zickler, Todd",Computer Graphics,2013-04-24T18:00:04Z,2013-04-24T18:00:04Z,2013-04-24,http://hdl.handle.net/1721.1/78590,"This manuscript completes the analysis of our SIGGRAPH 2013 paper ""Fabricating BRDFs at High Spatial Resolution Using Wave Optics"" in which photolithography fabrication was used for manipulating reflectance effects. While photolithography allows for precise reflectance control, it is costly to fabricate. Here we explore an inexpensive alternative to micro-fabrication, in the form of metallic powders. Such powders are readily available at a variety of particle sizes and morphologies. Using an analysis similar to the micro-fabrication paper, we provide guidelines for the relation between the particles' shape and size and the reflectance functions they can produce.",4 p.,MIT-CSAIL-TR-2013-008,,,High Spatial Resolution BRDFs with Metallic powders Using Wave Optics Analysis,,,,,,,
Dina Katabi,"Hassanieh, Haitham; Shi, Lixin; Abari, Omid; Hamed, Ezzeldine; Katabi, Dina",Networks & Mobile Systems,2013-06-03T23:45:02Z,2013-06-03T23:45:02Z,2013-05-22,http://hdl.handle.net/1721.1/79058,"The goal of this paper is to make sensing and decoding GHz of spectrum simple, cheap, and low power. Our thesis is simple: if we can build a technology that captures GHz of spectrum using commodity Wi-Fi radios, it will have the right cost and power budget to enable a variety of new applications such as GHz-widedynamic access and concurrent decoding of diverse technologies. This vision will change today s situation where only expensive power-hungry spectrum analyzers can capture GHz-wide spectrum. Towards this goal, the paper harnesses the sparse Fourier transform to compute the frequency representation of a sparse signal without sampling it at full bandwidth. The paper makes the following contributions. First, it presents BigBand, a receiver that can sense and decode a sparse spectrum wider than its own digital bandwidth. Second, it builds a prototype of its design using 3 USRPs that each samples the spectrum at 50 MHz, producing a device that captures 0.9 GHz -- i.e., 6x larger bandwidth than the three USRPs combined. Finally, it extends its algorithm to enable spectrum sensing in scenarios where the spectrum is not sparse.",13 p.,MIT-CSAIL-TR-2013-009,,,BigBand: GHz-Wide Sensing and Decoding on Commodity Radios,,Spectrum Sensing; Sparse Fourier Transform; Wireless; ADC; Software Radios,2013-06-03T23:45:03Z,,,,
Gerald Sussman,"Evans, Isaac; Lynch, Joseph","Robotics, Vision & Sensor Networks",2013-06-03T23:30:05Z,2013-06-03T23:30:05Z,2013-05-24,http://hdl.handle.net/1721.1/79057,"Organon is an open source system for expressing and solving complex symbolic constraints between generic entities. Our design avoids restricting the programmer s ability to phrase constraints; Organon acts purely as a framework that defines and holds together the key concepts of forms, constraints, and solvers. It has three main components: (1) Forms: Abstract representations of the entities to be constrained. (2) Constraints: Functions that symbolically express requirements on the relationships between forms as well as provide information a solver can use to improve the constraint s satisfaction. (3) Solvers: Functions which inspect instantiations of forms and manipulate them in an attempt to satisfy a set of objective constraints.",33 p.,MIT-CSAIL-TR-2013-010,,,Organon: A Symbolic Constraint Framework & Solver,,scheme; propagator; exponential solver; annealing solver,2013-06-03T23:30:06Z,,,,
Karen Sollins,"Simosa, Jorge D.",Advanced Network Architecture,2013-06-04T20:15:04Z,2013-06-04T20:15:04Z,2013-06-04,http://hdl.handle.net/1721.1/79060,"As modern networks become highly integrated, heterogeneous, and experience exponential growth, the task of network management becomes increasingly unmanageable for network administrators and designers. The Knowledge Plane (KP) is designed to support a self-managing network, given the organizational constraints of network management, as well as to create synergy and exploit commonality among network applications. In this thesis, to build an Information Plane that is suitable to the requirements of the KP, we propose a publish/subscribe system that provides a clear and systematic framework for resolving tussles in the network. To evaluate the effectiveness of this design, we configured a network of PlanetLab nodes and conducted experiments involving a variety of file sizes and source-destination pairs. The results suggest that the system's performance is not only comparable to existing file transfer services, but that the system also introduces several performance gains that are unattainable with current network architectures.",77 p.,MIT-CSAIL-TR-2013-011,Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,A Publish-Subscribe Implementation of Network Management,,,2013-06-04T20:15:04Z,MEng thesis,,,
Brian Williams,"Shroff, Ameya",Model-based Embedded and Robotic Systems,2013-06-06T22:30:03Z,2013-06-06T22:30:03Z,2013-06-06,http://hdl.handle.net/1721.1/79078,"We envision a world in which robots and humans can collaborate to perform complex tasks in real-world environments. Current motion planners successfully generate trajectories for a robot with multiple degrees of freedom, in a cluttered environment, and ensure that the robot can achieve its goal while avoiding all the obstacles in the environment. However, these planners are not practical in real world scenarios that involve unstructured, dynamic environments for a three primary reasons. First, these motion planners assume that the environment the robot is functioning in, is well-known and static, both during plan generation and plan execution. Second, these planners do not support temporal constraints, which are crucial for planning in a rapidly-changing environment and for allowing task synchronisation between the robot and other agents, like a human or even another robot. Third, the current planners do not adequately represent the requirements of the task. They often over-constrain the task description and are hence unable to take advantage of task flexibility which may aid in optimising energy efficiency or robustness. In this thesis we present Chekhov, a reactive, integrated motion planning and execution executive that addresses these shortcomings using four key innovations. First, unlike traditional planners, the planning and execution components of Chekhov are very closely integrated. This close coupling blurs the traditional, sharp boundary between the two components and allows for optimal collaboration. Second, Chekhov represents temporal constraints, which allows it to perform operations that are temporally synchronised with external events. Third, Chekhov uses an incremental search algorithm which allows it to rapidly generate a new plan if a disturbance is encountered that threatens the execution of the existing plan. Finally, unlike standard planners which generate a single reference trajectory from the start pose to the goal pose, Chekhov generates a Qualitative Control Plan using Flow Tubes that represent families of feasible trajectories and associated control policies. These flow tubes provide Chekhov with a flexibility that is extremely valuable and serve as Chekhov's first line of defence.",100 p,MIT-CSAIL-TR-2013-012,,,Reactive Integrated Motion Planning and Execution Using Chekhov,,Integrated Planning and Control; Manipulation Planning and Control,2013-06-06T22:30:03Z,MEng thesis,,en-GB,
Tomaso Poggio,"Kim, Heejung; Wohlwend, Jeremy; Leibo, Joel Z.; Poggio, Tomaso",Center for Biological and Computational Learning (CBCL),2013-06-20T17:00:04Z,2013-06-20T17:00:04Z,2013-06-18,http://hdl.handle.net/1721.1/79354,"When learning to recognize a novel body shape, e.g., a panda bear, we are not misled by changes in its pose. A ""jumping panda bear"" is readily recognized, despite having no prior visual experience with the conjunction of these concepts. Likewise, a novel pose can be estimated in an invariant way, with respect to the actor's body shape. These body and pose recognition tasks require invariance to non-generic transformations that previous models of the ventral stream do not have. We show that the addition of biologically plausible, class-specific mechanisms associating previously-viewed actors in a range of poses enables a hierarchical model of object recognition to account for this human capability. These associations could be acquired in an unsupervised manner from past experience.",10 p.,MIT-CSAIL-TR-2013-013; CBCL-312,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,Body-form and body-pose recognition with a hierarchical model of the ventral stream,,Ventral stream; Modularity; Computational neuroscience; HMAX,2013-06-20T17:00:05Z,,,,
Martin Rinard,"Carbin, Michael; Misailovic, Sasa; Rinard, Martin",Computer Architecture,2013-06-20T17:00:08Z,2013-06-20T17:00:08Z,2013-06-19,http://hdl.handle.net/1721.1/79355,"Emerging high-performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. Full detection and recovery from soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors. In this paper we present Rely, a programming language that enables developers to reason about the quantitative reliability of an application -- namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces. We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification, that characterizes the reliability of the underlying hardware components, and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely.",22 p.,MIT-CSAIL-TR-2013-014,,,Verifying Quantitative Reliability of Programs That Execute on Unreliable Hardware,"This research was supported in part by the National Science Foundation (Grants CCF-0905244, CCF-1036241, CCF-1138967, CCF-1138967, and IIS-0835652), the United States Department of Energy (Grant DE-SC0008923), and DARPA (Grants FA8650-11-C-7192, FA8750-12-2-0110).","unreliable hardware, probabilistic semantics, quantitative reliability",2013-06-20T17:00:08Z,,,en,
Nancy Lynch,"Attie, Paul C.; Lynch, Nancy A.",Theory of Computation,2013-07-08T18:15:04Z,2013-07-08T18:15:04Z,2013-07-08,http://hdl.handle.net/1721.1/79420,"We present dynamic I/O automata (DIOA), a compositional model of dynamic systems, based on I/O automata. In our model, automata can be created and destroyed dynamically, as computation proceeds. In addition, an automaton can dynamically change its signature, that is, the set of actions in which it can participate. This allows us to model mobility, by enforcing the constraint that only automata at the same location may synchronize on common actions. Our model features operators for parallel composition, action hiding, and action renaming. It also features a notion of automaton creation, and a notion of trace inclusion from one dynamic system to another, which can be used to prove that one system implements the other. Our model is hierarchical: a dynamically changing system of interacting automata is itself modeled as a single automaton that is ""one level higher."" This can be repeated, so that an automaton that represents such a dynamic system can itself be created and destroyed. We can thus model the addition and removal of entire subsystems with a single action. We establish fundamental compositionality results for DIOA: if one component is replaced by another whose traces are a subset of the former, then the set of traces of the system as a whole can only be reduced, and not increased, i.e., no new behaviors are added. That is, parallel composition, action hiding, and action renaming, are all monotonic with respect to trace inclusion. We also show that, under certain technical conditions, automaton creation is monotonic with respect to trace inclusion: if a system creates automaton Ai instead of (previously) creating automaton A'i, and the traces of Ai are a subset of the traces of A'i,then the set of traces of the overall system is possibly reduced, but not increased. Our trace inclusion results imply that trace equivalence is a congruence relation with respect to parallel composition, action hiding, and action renaming. Our trace inclusion results enable a design and refinement methodology based solely on the notion of externally visible behavior, and which is therefore independent of specific methods of establishing trace inclusion. It permits the refinement of components and subsystems in isolation from the entire system, and provides more flexibility in refinement than a methodology which is, for example, based on the monotonicity of forward simulation with respect to parallel composition. In the latter, every automaton must be refined using forward simulation, whereas in our framework different automata can be refined using different methods. The DIOA model was defined to support the analysis of mobile agent systems, in a joint project with researchers at Nippon Telegraph and Telephone. It can also be used for other forms of dynamic systems, such as systems described by means of object-oriented programs, and systems containing services with changing access permissions.",63 p.,MIT-CSAIL-TR-2013-015,,,Dynamic Input/Output Automata: a Formal and Compositional Model for Dynamic Systems,,,2013-07-08T18:15:05Z,,,,
Nancy Lynch,"Cadambe, Viveck R.; Lynch, Nancy; Medard, Muriel; Musial, Peter",Theory of Computation,2013-07-17T18:45:04Z,2013-07-17T18:45:04Z,2013-07-17,http://hdl.handle.net/1721.1/79606,"This paper considers the communication and storage costs of emulating atomic (linearizable) read/write shared memory in distributed message-passing systems. We analyze the costs of previously-proposed algorithms by Attiya, Bar-Noy, and Dolev (the ABD algorithm) and by Fan and Lynch (the LDR algorithm), and develop new coding-based algorithms that significantly reduce these costs. The paper contains three main contributions: (1) We present a new shared-memory algorithm that we call CAS, for Coded Atomic Storage. This algorithm uses erasure coding methods. (2) In a storage system with N servers that is resilient to f server failures, we show that the communication costs for the ABD and LDR algorithms, measured in terms of number of object values, are both at least f + 1, whereas the communication cost for CAS is N/(N-2f). (3) We also explicitly quantify the storage costs of the ABD, LDR, and CAS algorithms. The storage cost of the ABD algorithm, measured in terms of number of object values, is N; whereas the storage costs of the LDR and CAS algorithms are both unbounded. We present a modification of the CAS algorithm based on the idea of garbage collection. The modified version of CAS has a storage cost of (d + 1) N/(N-2f), where d in an upper bound on the number of operations that are concurrent with a read operation. Thus, if d is sufficiently small, the storage cost of CAS is lower than those of both the ABD and LDR algorithms.",24 p.,MIT-CSAIL-TR-2013-016,,,Coded Emulation of Shared Atomic Memory for Message Passing Architectures,,,2013-07-17T18:45:05Z,,,,
Tomaso Poggio,"Poggio, Tomaso; Mutch, Jim; Anselmi, Fabio; Tacchetti, Andrea; Rosasco, Lorenzo; Leibo, Joel Z.",,2013-08-12T02:30:12Z,2013-08-12T02:30:12Z,2013-08-06,http://hdl.handle.net/1721.1/79828,"Tuning properties of simple cells in cortical V1 can be described in terms of a ""universal shape"" characterized by parameter values which hold across different species. This puzzling set of findings begs for a general explanation grounded on an evolutionarily important computational function of the visual cortex. We ask here whether these properties are predicted by the hypothesis that the goal of the ventral stream is to compute for each image a ""signature"" vector which is invariant to geometric transformations, with the the additional assumption that the mechanism for continuously learning and maintaining invariance consists of the memory storage of a sequence of neural images of a few objects undergoing transformations (such as translation, scale changes and rotation) via Hebbian synapses. For V1 simple cells the simplest version of this hypothesis is the online Oja rule which implies that the tuning of neurons converges to the eigenvectors of the covariance of their input. Starting with a set of dendritic fields spanning a range of sizes, simulations supported by a direct mathematical analysis show that the solution of the associated ""cortical equation"" provides a set of Gabor-like wavelets with parameter values that are in broad agreement with the physiology data. We show however that the simple version of the Hebbian assumption does not predict all the physiological properties. The same theoretical framework also provides predictions about the tuning of cells in V4 and in the face patch AL which are in qualitative agreement with physiology data.",10 p.,MIT-CSAIL-TR-2013-019; CBCL-313,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,Does invariant recognition predict tuning of neurons in sensory cortex?,,,2013-08-12T02:30:12Z,,,,
Martin Rinard,"Long, Fan; Sidiroglou-Douskos, Stelios; Kim, Deokhwan; Rinard, Martin",Computer Architecture,2013-08-12T02:30:08Z,2013-08-12T02:30:08Z,2013-08-06,http://hdl.handle.net/1721.1/79827,"We present a system, SIFT, for generating input filters that nullify integer overflow errors associated with critical program sites such as memory allocation or block copy sites. SIFT uses a static program analysis to generate filters that discard inputs that may trigger integer overflow errors in the computations of the sizes of allocated memory blocks or the number of copied bytes in block copy operations. The generated filters are sound   if an input passes the filter, it will not trigger an integer overflow error for any analyzed site. Our results show that SIFT successfully analyzes (and therefore generates sound input filters for) 52 out of 58 memory allocation and block memory copy sites in analyzed input processing modules from five applications (VLC, Dillo, Swfdec, Swftools, and GIMP). These nullified errors include six known integer overflow vulnerabilities. Our results also show that applying these filters to 62895 real-world inputs produces no false positives. The analysis and filter generation times are all less than a second.",20 p.,MIT-CSAIL-TR-2013-018,,,Sound Input Filter Generation for Integer Overflow Errors,,Static Analysis; Integer Overflow; Filter Generation,2013-08-12T02:30:08Z,,,,
,"Jackson, Daniel",Software Design,2013-08-12T02:30:04Z,2013-08-12T02:30:04Z,2013-08-08,http://hdl.handle.net/1721.1/79826,"A research agenda in software design is outlined, focusing on the role of concepts. The notions of concepts as ""abstract affordances"" and of conceptual integrity are discussed, and a series of small examples of conceptual models is given.",17 p.,MIT-CSAIL-TR-2013-020,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,Conceptual Design of Software: A Research Agenda,,Software design; Usability; Conceptual integrity; Conceptual models,2013-08-12T02:30:05Z,,,,
Leslie Kaelbling,"Jordan, Matthew; Perez, Alejandro",Learning and Intelligent Systems,2013-08-16T18:30:03Z,2013-08-16T18:30:03Z,2013-08-15,http://hdl.handle.net/1721.1/79884,"In this paper we present a simple, computationally-efficient, two-tree variant of the RRT* algorithm along with several heuristics.",12 p.,MIT-CSAIL-TR-2013-021,Creative Commons Attribution-NonCommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,Optimal Bidirectional Rapidly-Exploring Random Trees,,,2013-08-16T18:30:03Z,,,,
Brian Williams,"Bush, Lawrence A. M.",Model-based Embedded and Robotic Systems,2018-01-30T23:45:58Z,2018-01-30T23:45:58Z,2013-08-22,http://hdl.handle.net/1721.1/113363,"Over the past several decades, technologies for remote sensing and exploration have be- come increasingly powerful but continue to face limitations in the areas of information gathering and analysis. These limitations affect technologies that use autonomous agents, which are devices that can make routine decisions independent of operator instructions. Bandwidth and other communications limitation require that autonomous differentiate between relevant and irrelevant information in a computationally efficient manner.This thesis presents a novel approach to this problem by framing it as an adaptive sensing problem. Adaptive sensing allows agents to modify their information collection strategies in response to the information gathered in real time. We developed and tested optimization algorithms that apply information guides to Monte Carlo planners. Information guides provide a mechanism by which the algorithms may blend online (realtime) and offline (previously simulated) planning in order to incorporate uncertainty into the decision- making process. This greatly reduces computational operations as well as decisional and communications overhead. We begin by introducing a 3-level hierarchy that visualizes adaptive sensing at synoptic (global), mesoscale (intermediate) and microscale (close-up) levels (a spatial hierarchy). We then introduce new algorithms for decision uncertainty minimization (DUM) and representational uncertainty minimization (RUM). Finally, we demonstrate the utility of this approach to real-world sensing problems, including bathymetric mapping and disaster relief. We also examine its potential in space exploration tasks by describing its use in a hypothetical aerial exploration of Mars. Our ultimate goal is to facilitate future large-scale missions to extraterrestrial objects for the purposes of scientific advancement and human exploration.",310 p.,MIT-CSAIL-TR-2018-003,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,Decision Uncertainty Minimization and Autonomous Information Gathering,,,2018-01-30T23:45:58Z,PhD thesis,,,Brian Williams; Model-based Embedded and Robotic Systems
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",Computation Structures,2013-07-31T18:30:05Z,2013-07-31T18:30:05Z,2013-09-01,http://hdl.handle.net/1721.1/79746,"Shared last-level caches, widely used in chip-multiprocessors (CMPs), face two fundamental limitations. First, the latency and energy of shared caches degrade as the system scales up. Second, when multiple workloads share the CMP, they suffer from interference in shared cache accesses. Unfortunately, prior research addressing one issue either ignores or worsens the other: NUCA techniques reduce access latency but are prone to hotspots and interference, and cache partitioning techniques only provide isolation but do not reduce access latency. We present Jigsaw, a technique that jointly addresses the scalability and interference problems of shared caches. Hardware lets software define shares, collections of cache bank partitions that act as virtual caches, and map data to shares. Shares give software full control over both data placement and capacity allocation. Jigsaw implements efficient hardware support for share management, monitoring, and adaptation. We propose novel resource-management algorithms and use them to develop a system-level runtime that leverages Jigsaw to both maximize cache utilization and place data close to where it is used. We evaluate Jigsaw using extensive simulations of 16- and 64-core tiled CMPs. Jigsaw improves performance by up to 2.2x (18% avg) over a conventional shared cache, and significantly outperforms state-of-the-art NUCA and partitioning techniques.",16 p.,MIT-CSAIL-TR-2013-017,Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,Jigsaw: Scalable Software-Defined Caches (Extended Version),This work was supported in part by DARPA PERFECT contract HR0011-13-2-0005 and Quanta Computer.,"cache, memory, NUCA, partitioning, isolation, multicore, virtual memory",2013-07-31T18:30:05Z,,,,
Regina Barzilay,"Kushman, Nate; Adib, Fadel; Katabi, Dina; Barzilay, Regina",Natural Language Processing,2013-09-10T21:30:03Z,2013-09-10T21:30:03Z,2013-09-10,http://hdl.handle.net/1721.1/80380,"Consider the problem of migrating a company's CRM or ERP database from one application to another, or integrating two such databases as a result of a merger. This problem requires matching two large relational schemas with hundreds and sometimes thousands of fields. Further, the correct match is likely complex: rather than a simple one-to-one alignment, some fields in the source database may map to multiple fields in the target database, and others may have no equivalent fields in the target database. Despite major advances in schema matching, fully automated solutions to large relational schema matching problems are still elusive. This paper focuses on improving the accuracy of automated large relational schema matching. Our key insight is the observation that modern database applications have a rich user interface that typically exhibits more consistency across applications than the underlying schemas. We associate UI widgets in the application with the underlying database fields on which they operate and demonstrate that this association delivers new information useful for matching large and complex relational schemas. Additionally, we show how to formalize the schema matching problem as a quadratic program, and solve it efficiently using standard optimization and machine learning techniques. We evaluate our approach on real-world CRM applications with hundreds of fields and show that it improves the accuracy by a factor of 2-4x.",12 p.,MIT-CSAIL-TR-2013-022,,,Harvesting Application Information for Industry-Scale Relational Schema Matching,,,2013-09-10T21:30:03Z,,,,
Tomaso Poggio,"Ni, Yuzhao; Frogner, Charles A.; Poggio, Tomaso A.",Center for Biological and Computational Learning (CBCL),2013-09-19T22:30:06Z,2013-09-19T22:30:06Z,2013-09-19,http://hdl.handle.net/1721.1/80815,"In this thesis, we designed and implemented a crowdsourcing system to annotatemouse behaviors in videos; this involves the development of a novel clip-based video labeling tools, that is more efficient than traditional labeling tools in crowdsourcing platform, as well as the design of probabilistic inference algorithms that predict the true labels and the workers' expertise from multiple workers' responses. Our algorithms are shown to perform better than majority vote heuristic. We also carried out extensive experiments to determine the effectiveness of our labeling tool, inference algorithms and the overall system.",69 p.,MIT-CSAIL-TR-2013-023; CBCL-314,,,Mouse Behavior Recognition with The Wisdom of Crowd,,crowdsourcing; video labeling; human computation; mouse phenotyping; action recognition,2013-09-19T22:30:06Z,,,,
Gerald Sussman,"Panchekha, Pavel; Brodsky, Micah Z. (Micah Zev)",Mathematics and Computation,2013-10-09T17:30:04Z,2013-10-09T17:30:04Z,2013-10-08,http://hdl.handle.net/1721.1/81365,"Shared mutable state is challenging to maintain in a distributed environment. We develop a technique, based on the Operational Transform, that guides independent agents into producing consistent states through inconsistent but equivalent histories of operations. Our technique, history maintenance, extends and streamlines the Operational Transform for general distributed systems. We describe how to use history maintenance to create eventually-consistent, strongly-consistent, and hybrid systems whose correctness is easy to reason about.",21 p.,MIT-CSAIL-TR-2013-024,,,Distributed Shared State with History Maintenance,,eventual consistency; distributed systems; operational transform,2013-10-09T17:30:04Z,,,,
