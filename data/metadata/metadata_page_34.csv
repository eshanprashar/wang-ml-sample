dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.other,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation,dc.subject,dc.title,dc.relation.ispartofseries,dc.contributor,dc.identifier.citation
Leslie Kaelbling,"Milch, Brian; Koller, Daphne",Learning and Intelligent Systems,2008-05-13T16:00:19Z,2008-05-13T16:00:19Z,2008-05-12,MIT-CSAIL-TR-2008-029,http://hdl.handle.net/1721.1/41530,"In some multi-agent scenarios, identifying observations that an agent can safely ignore reduces exponentially the size of the agent's strategy space and hence the time required to find a Nash equilibrium. We consider games represented using the multi-agent influence diagram (MAID) framework of Koller and Milch [2001], and analyze the extent to which information edges can be eliminated. We define a notion of a safe edge removal transformation, where all equilibria in the reduced model are also equilibria in the original model. We show that existing edge removal algorithms for influence diagrams are safe, but limited, in that they do not detect certain cases where edges can be removed safely. We describe an algorithm that produces the ""minimal"" safe reduction, which removes as many edges as possible while still preserving safety. Finally, we note that both the existing edge removal algorithms and our new one can eliminate equilibria where agents coordinate their actions by conditioning on irrelevant information. Surprisingly, in some games these ""lost"" equilibria can be preferred by all agents in the game.",16 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Multi-agent influence diagrams; Irrelevance,Ignorable Information in Multi-Agent Scenarios,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",Theory of Computation,2008-07-09T22:15:19Z,2008-07-09T22:15:19Z,2008-06,MIT-CSAIL-TR-2008-041,http://hdl.handle.net/1721.1/41877,"We put forward new benchmarks and solution concepts for Adversarial Mechanism Design, as defined by [MV07.a], and we exemplify them in the case of truly combinatorial auctions.We benchmark the combined performance (the sum of the auction's efficiency and revenue) of a truly combinatorial auction against a very relevant but private knowledge of the players: essentially, the maximum revenue that the best informed player could guarantee if he were the seller. (I.e., by offering each other player a subset of the goods for a take-it-or-leave-it price.)We achieve this natural benchmark within a factor of 2, by means of a new and probabilistic auction mechanism, in surviving strategies. That is, the above performance of our mechanism is guaranteed in any rational play, independent of any possible beliefs of the players. Indeed, our performance guarantee holds for any possible choice of strategies, so long as each player chooses a strategy among those surviving iterated elimination of dominated strategies.Our mechanism is extremely robust. Namely, its performance guarantees hold even if all but one of the players collude (together or in separate groups) in any possible but reasonable way. Essentially, the only restriction for the collective utility function of a collusive subset S of the players is the following: the collective utility increases when one member of S is allocated a ubset of the goods ""individually better"" for him and/or his ""individual price"" is smaller, while the allocations and prices of all other members of S stay the same.Our results improve on the yet unpublished ones of [MV07.b]. The second part of this paper, dealing with a more aggressive benchmark (essentially, the maximum welfare privately known to the players) is forthcoming.",17 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Knowledge Benchmarks in Adversarial Mechanism Design and Implementation in Surviving Strategies (Part I),,,
Hari Balakrishnan,"Jamieson, Kyle",Networks & Mobile Systems,2008-06-05T18:00:27Z,2008-06-05T18:00:27Z,2008-06-03,MIT-CSAIL-TR-2008-031,http://hdl.handle.net/1721.1/41857,"At ever-increasing rates, we are using wireless systems to communicatewith others and retrieve content of interest to us. Current wirelesstechnologies such as WiFi or Zigbee use forward error correction todrive bit error rates down when there are few interferingtransmissions. However, as more of us use wireless networks toretrieve increasingly rich content, interference increases inunpredictable ways. This results in errored bits, degradedthroughput, and eventually, an unusable network. We observe that thisis the result of higher layers working at the packet granularity,whereas they would benefit from a shift in perspective from wholepackets to individual symbols.From real-world experiments on a 31-node testbed of Zigbee andsoftware-defined radios, we find that often, not all of the bitsin corrupted packets share fate. Thus, today's wireless protocolsretransmit packets where only a small number of the constituent bitsin a packet are in error, wasting network resources. In thisdissertation, we will describe a physical layer that passesinformation about its confidence in each decoded symbol up to higherlayers. These SoftPHY hints have many applications, one ofwhich, more efficient link-layer retransmissions, we will describe indetail. PP-ARQ is a link-layer reliable retransmission protocolthat allows a receiver to compactly encode a request forretransmission of only the bits in a packet that are likely in error.Our experimental results show that PP-ARQ increases aggregate networkthroughput by a factor of approximately 2x under variousconditions. Finally, we will place our contributions in the contextof related work and discuss other uses of SoftPHY throughout thewireless networking stack.",153 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,802.11; 802.15.4,The SoftPHY Abstraction: from Packets to Symbols in Wireless Network Design,,,
Seth Teller,"Huang, Albert S.; Teller, Seth","Robotics, Vision & Sensor Networks",2008-06-11T20:15:09Z,2008-06-11T20:15:09Z,2008-06-06,MIT-CSAIL-TR-2008-032,http://hdl.handle.net/1721.1/41860,"We describe a new method for wide-area, non-metrical robot navigationwhich enables useful, purposeful motion indoors. Our method has twophases: a training phase, in which a human user directs a wheeledrobot with an attached camera through an environment while occasionallysupplying textual place names; and a navigation phase in which theuser specifies goal place names (again as text), and the robot issueslow-level motion control in order to move to the specified place. We show thatdifferences in the visual-field locations and scales of features matched acrosstraining and navigation can be used to construct a simple and robust controlrule that guides the robot onto and along the training motion path.Our method uses an omnidirectional camera, requires approximateintrinsic and extrinsic camera calibration, and is capable of effective motioncontrol within an extended, minimally-prepared building environment floorplan.We give results for deployment within a single building floor with 7 rooms, 6corridor segments, and 15 distinct place names.",8 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Non-Metrical Navigation Through Visual Path Control,,,
Trevor Darrell,"Yeh, Tom; Lee, John J.; Darrell, Trevor",Vision,2008-06-11T20:15:30Z,2008-06-11T20:15:30Z,2008-06-10,MIT-CSAIL-TR-2008-033,http://hdl.handle.net/1721.1/41862,"Object localization and classification are important problems incomputer vision. However, in many applications, exhaustive searchover all class labels and image locations is computationallyprohibitive. While several methods have been proposed to makeeither classification or localization more efficient, few havedealt with both tasks simultaneously. This paper proposes anefficient method for concurrent object localization andclassification based on a data-dependent multi-classbranch-and-bound formalism. Existing bag-of-featuresclassification schemes, which can be expressed as weightedcombinations of feature counts can be readily adapted to ourmethod. We present experimental results that demonstrate the meritof our algorithm in terms of classification accuracy, localizationaccuracy, and speed, compared to baseline approaches includingexhaustive search, the ISM method, and single-class branch andbound.",9 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Fast concurrent object classification and localization,,,
Karen Sollins,"Li, Ji",Advanced Network Architecture,2008-06-11T20:15:18Z,2008-06-11T20:15:18Z,2008-06-11,MIT-CSAIL-TR-2008-034,http://hdl.handle.net/1721.1/41861,"In designing and building a network like the Internet, we continue to face the problems of scale and distribution. With the dramatic expansion in scale and heterogeneity of the Internet, network management has become an increasingly difficult task. Furthermore, network applications often need to maintain efficient organization among the participants by collecting information from the underlying networks. Such individual information collection activities lead to duplicate efforts and contention for network resources.The Knowledge Plane (KP) is a new common construct that provides knowledge and expertise to meet the functional, policy and scaling requirements of network management, as well as to create synergy and exploit commonality among many network applications. To achieve these goals, we face many challenging problems, including widely distributed data collection, efficient processing of that data, wide availability of the expertise, etc.In this thesis, to provide better support for network management and large-scale network applications, I propose a knowledge plane architecture that consists of a network knowledge plane (NetKP) at the network layer, and on top of it, multiple specialized KPs (spec-KPs). The NetKP organizes agents to provide valuable knowledge and facilities about the Internet to the spec-KPs. Each spec-KP is specialized in its own area of interest. In both the NetKP and the spec-KPs, agents are organized into regions based on different sets of constraints. I focus on two key design issues in the NetKP: (1) a regionbased architecture for agent organization, in which I design an efficient and non-intrusive organization among regions that combines network topology and a distributed hash table; (2) request and knowledge dissemination, in which I design a robust and efficient broadcast and aggregation mechanism using a tree structure among regions. In the spec-KPs, I build two examples: experiment management on the PlanetLab testbed and distributed intrusion detection on the DETER testbed. The experiment results suggest a common approach driven by the design principles of the Internet and more specialized constraints can derive productive organization for network management and applications.",191 p.,; Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,broadcast; region; knowledge plane; network pnowledge plane; agent; intrusion detection; specialized knowledge plane; distributed hash table; aggregation,Agent Organization in the Knowledge Plane,,,
Chris Terman,"Carli, Roberto",Computer Architecture,2008-07-02T06:00:36Z,2008-07-02T06:00:36Z,2008-06-16,MIT-CSAIL-TR-2008-036,http://hdl.handle.net/1721.1/41874,"The flexible MIPS soft processor architecture borrows selected technologies from high-performance computing to deliver a modular, highly customizable CPU targeted towards FPGA implementations for embedded systems; the objective is to provide a more flexible architectural alternative to coprocessor-based solutions. The processor performs out-of-order execution on parallel functional units, it delivers in-order instruction commit and it is compatible with the MIPS-1 Instruction Set Architecture. Amongst many available options, the user can introduce custom instructions and matching functional units; modify existing units; change the pipelining depth within functional units to any fixed or variable value; customize instruction definitions in terms of operands, control signals and register file interaction; insert multiple redundant functional units for improved performance. The flexibility provided by the architecture allows the user to expand the processor functionality to implement instructions of coprocessor-level complexity through additional functional units. The processor design was implemented and simulated on two FPGA platforms, tested on multiple applications, and compared to three commercially available soft processor solutions in terms of features, area, clock frequency and benchmark performance.",49 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Flexible MIPS Soft Processor Architecture,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",Theory of Computation,2008-07-02T06:00:46Z,2008-07-02T06:00:46Z,2008-06-17,MIT-CSAIL-TR-2008-037,http://hdl.handle.net/1721.1/41875,None,6 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Leveraging Player Knowledge in Combinatorial Auctions (and Implementation in Surviving Strategies),,,
Eric Grimson,"Grimson, Eric; Wang, Xiaogang; Ng, Gee-Wah; Ma, Keng Teck",Vision,2008-03-17T19:15:17Z,2008-03-17T19:15:17Z,2008-06-24,MIT-CSAIL-TR-2008-015,http://hdl.handle.net/1721.1/40808,"We propose a novel nonparametric Bayesian model, Dual Hierarchical Dirichlet Processes (Dual-HDP), for trajectory analysis and semantic region modeling in surveillance settings, in an unsupervised way. In our approach, trajectories are treated as documents and observations of an object on a trajectory are treated as words in a document. Trajectories are clustered into different activities. Abnormal trajectories are detected as samples with low likelihoods. The semantic regions, which are intersections of paths commonly taken by objects, related to activities in the scene are also modeled. Dual-HDP advances the existing Hierarchical Dirichlet Processes (HDP) language model. HDP only clusters co-occurring words from documents into topics and automatically decides the number of topics. Dual-HDP co-clusters both words and documents. It learns both the numbers of word topics and document clusters from data. Under our problem settings, HDP only clusters observations of objects, while Dual-HDP clusters both observations and trajectories. Experiments are evaluated on two data sets, radar tracks collected from a maritime port and visual tracks collected from a parking lot.",12 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,"hierarchical Dirichlet processes, activity analysis, clustering, visual surveillance",Trajectory Analysis and Semantic Region Modeling Using A Nonparametric Bayesian Model,,,
Barbara Liskov,"Vandiver, Benjamin Mead",Programming Methodology,2008-07-02T06:00:10Z,2008-07-02T06:00:10Z,2008-06-30,MIT-CSAIL-TR-2008-040,http://hdl.handle.net/1721.1/41873,"This thesis describes the design, implementation, and evaluation of a replication scheme to handle Byzantine faults in transaction processing database systems. The scheme compares answers from queries and updates on multiple replicas which are off-the-shelf database systems, to provide a single database that is Byzantine fault tolerant. The scheme works when the replicas are homogeneous, but it also allows heterogeneous replication in which replicas come from different vendors. Heterogeneous replicas reduce the impact of bugs and security compromises because they are implemented independently and are thus less likely to suffer correlated failures. A final component of the scheme is a repair mechanism that can correct the state of a faulty replica, ensuring the longevity of the scheme.The main challenge in designing a replication scheme for transaction processingsystems is ensuring that the replicas state does not diverge while allowing a high degree of concurrency. We have developed two novel concurrency control protocols, commit barrier scheduling (CBS) and snapshot epoch scheduling (SES) that provide strong consistency and good performance. The two protocols provide different types of consistency: CBS provides single-copy serializability and SES provides single-copy snapshot isolation. We have implemented both protocols in the context of a replicated SQL database. Our implementation has been tested with production versions of several commercial and open source databases as replicas. Our experiments show a configuration that can tolerate one faulty replica has only a modest performance overhead (about 10-20% for the TPC-C benchmark). Our implementation successfully masks several Byzantine faults observed in practice and we have used it to find a new bug in MySQL.",174 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Detecting and Tolerating Byzantine Faults in Database Systems,,,
Silvio Micali,"Chen, Jing; Micali, Silvio",Theory of Computation,2008-07-14T19:45:18Z,2008-07-14T19:45:18Z,2008-07,MIT-CSAIL-TR-2008-042,http://hdl.handle.net/1721.1/41878,"We put forward new benchmarks and solution concepts for Adversarial Mechanism Design, as defined by [MV07.a], and we exemplify them in the case of truly combinatorial auctions.We benchmark the combined performance (the sum of the auction's effciency and revenue)of a truly combinatorial auction against a very relevant but private knowledge of the players: essentially, the maximum revenue that the best informed player could guarantee if he were the seller. (I.e., by offering each other player a subset of the goods for a take-it-or-leave-it price.) We achieve this natural benchmark within a factor of 2, by means of a new and probabilisticauction mechanism, in KNOWLINGLY SURVIVING STRATEGIES. That is, the above performance of our mechanism is guaranteed in any rational play, independent of any possible beliefs of the players. Indeed, our performance guarantee holds for any possible choice of strategies, so long as each player chooses a strategy among those surviving iterated elimination of knowingly dominated strategies.Our mechanism is extremely robust. Namely, its performance guarantees hold even if all but one of the players collude (together or in separate groups) in any possible but reasonable way. Essentially, the only restriction for the collective utility function of a collusive subset S of the players is the following: the collective utility increases when one member of S is allocated asubset of the goods ""individually better"" for him and/or his ""individual price"" is smaller, while the allocations and prices of all other members of S stay the same.Our results improve on the yet unpublished ones of [MV07.b]. The second part of this paper, dealing with a more aggressive benchmark (essentially, the maximum welfare privately known to the players) is forthcoming.",30 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Knowledge Benchmarks in Adversarial Mechanism Design (Part I) and Implementation in Surviving Strategies (Part I),,,
Gerald Sussman,"Qumsiyeh, Dany M.",Mathematics and Computation,2008-07-16T20:15:24Z,2008-07-16T20:15:24Z,2008-07-14,MIT-CSAIL-TR-2008-043,http://hdl.handle.net/1721.1/41879,"This thesis investigates the feasibility of a smart building evacuation system, capable of guiding occupants along safe paths to exits and responding to changing threats. Inspired by developments in amorphous computing, the design presented is scalable to large networks, robust to hardware and communication failure, and based on simple low-cost components. A simulation and hardware prototype demonstrate that this distributed building evacuation system is both feasible and cost effective.",82 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,sensor networks; gradient; distributed control; synchronization; threat avoidance; fire; emergency,A Distributed Building Evacuation System,,,
Trevor Darrell,"Quattoni, Ariadna; Carreras, Xavier; Collins, Michael; Darrell, Trevor",Vision,2008-07-24T20:00:14Z,2008-07-24T20:00:14Z,2008-07-23,MIT-CSAIL-TR-2008-045,http://hdl.handle.net/1721.1/41888,"Recent approaches to multi-task learning have investigated the use of a variety of matrix norm regularization schemes for promoting feature sharing across tasks.In essence, these approaches aim at extending the l1 framework for sparse single task approximation to the multi-task setting. In this paper we focus on the computational complexity of training a jointly regularized model and propose an optimization algorithm whose complexity is linear with the number of training examples and O(n log n) with n being the number of parameters of the joint model. Our algorithm is based on setting jointly regularized loss minimization as a convex constrained optimization problem for which we develop an efficient projected gradient algorithm. The main contribution of this paper is the derivation of a gradient projection method with l1âˆ’âˆž constraints that can be performed efficiently and which has convergence rates.",8 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,A Projected Subgradient Method for Scalable Multi-Task Learning,,,
Patrick Winston,"Bonawitz, Keith A",Genesis,2008-07-23T19:30:16Z,2008-07-23T19:30:16Z,2008-07-23,MIT-CSAIL-TR-2008-044,http://hdl.handle.net/1721.1/41887,"Probabilistic inference provides a unified, systematic framework for specifying and solving these problems. Recent work has demonstrated the great value of probabilistic models defined over complex, structured domains. However, our ability to imagine probabilistic models has far outstripped our ability to programmatically manipulate them and to effectively implement inference, limiting the complexity of the problems that we can solve in practice.This thesis presents Blaise, a novel framework for composable probabilistic modeling and inference, designed to address these limitations. Blaise has three components: * The Blaise State-Density-Kernel (SDK) graphical modeling language that generalizes factor graphs by: (1) explicitly representing inference algorithms (and their locality) using a new type of graph node, (2) representing hierarchical composition and repeated substructures in the state space, the interest distribution, and the inference procedure, and (3) permitting the structure of the model to change during algorithm execution. * A suite of SDK graph transformations that may be used to extend a model (e.g. to construct a mixture model from a model of a mixture component), or to make inference more effective (e.g. by automatically constructing a parallel tempered version of an algorithm or by exploiting conjugacy in a model). * The Blaise Virtual Machine, a runtime environment that can efficiently execute the stochastic automata represented by Blaise SDK graphs. Blaise encourages the construction of sophisticated models by composing simpler models, allowing the designer to implement and verify small portions of the model and inference method, and to reuse model components from one task to another. Blaise decouples the implementation of the inference algorithm from the specification of the interest distribution, even in cases (such as Gibbs sampling) where the shape of the interest distribution guides the inference. This gives modelers the freedom to explore alternate models without slow, error-prone reimplementation. The compositional nature of Blaise enables novel reinterpretations of advanced Monte Carlo inference techniques (such as parallel tempering) as simple transformations of Blaise SDK graphs.In this thesis, I describe each of the components of the Blaise modeling framework, as well as validating the Blaise framework by highlighting a variety of contemporary sophisticated models that have been developed by the Blaise user community. I also present several surprising findings stemming from the Blaise modeling framework, including that an Infinite Relational Model can be built using exactly the same inference methods as a simple mixture model, that constructing a parallel tempered inference algorithm should be a point-and-click/one-line-of-code operation, and that Markov chain Monte Carlo for probabilistic models with complicated long-distance dependencies, such as a stochastic version of Scheme, can be managed using standard Blaise mechanisms.",190 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Bayesian; MCMC,Composable Probabilistic Inference with Blaise,,,
Tomaso Poggio,"De Mol, Christine; Rosasco, Lorenzo; De Vito, Ernesto",Center for Biological and Computational Learning (CBCL),2008-07-24T20:00:33Z,2008-07-24T20:00:33Z,2008-07-24,,http://hdl.handle.net/1721.1/41889,"Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie [""Regularization and variable selection via the elastic net"" J. R. Stat. Soc. Ser. B, 67(2):301-320, 2005] for the selection of groups of correlated variables. To investigate on the statistical properties of this scheme and in particular on its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combination of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular ""elastic-net representation"" of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in ""Regularization and variable selection via the elastic net"".",32 p.,,machine learning; regularization; feature selection,Elastic-Net Regularization in Learning Theory,MIT-CSAIL-TR-2008-046; CBCL-273,,
William Freeman,"Levin, Anat; Freeman, William; Durand, Fredo",Vision,2008-07-28T16:30:15Z,2008-07-28T16:30:15Z,2008-07-28,MIT-CSAIL-TR-2008-049,http://hdl.handle.net/1721.1/41892,"Computer vision has traditionally focused on extracting structure,such as depth, from images acquired using thin-lens or pinholeoptics. The development of computational imaging is broadening thisscope; a variety of unconventional cameras do not directly capture atraditional image anymore, but instead require the jointreconstruction of structure and image information. For example, recentcoded aperture designs have been optimized to facilitate the jointreconstruction of depth and intensity. The breadth of imaging designs requires new tools to understand the tradeoffs implied bydifferent strategies. This paper introduces a unified framework for analyzing computational imaging approaches.Each sensor element is modeled as an inner product over the 4D light field.The imaging task is then posed as Bayesian inference: giventhe observed noisy light field projections and a new prior on light field signals, estimate the original light field. Under common imaging conditions, we compare theperformance of various camera designs using 2D light field simulations. Thisframework allows us to better understand the tradeoffs of each camera type and analyze their limitations.",26 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Understanding camera trade-offs through a Bayesian analysis of light field projections - A revision,,,
Nancy Lynch,"Umeno, Shinya",Theory of Computation,2008-07-28T13:30:25Z,2008-07-28T13:30:25Z,2008-07-28,MIT-CSAIL-TR-2008-048,http://hdl.handle.net/1721.1/41891,"We present a new abstraction technique, event order abstraction (EOA), for parametric safety verification of real-time systems in which ``correct orderings of events'' needed for system correctness are maintained by timing constraints on the systems' behavior. By using EOA, one can separate the task of verifying a real-time system into two parts: 1. Safety property verification of the system given that only correct event orderings occur; and 2. Derivation of timing parameter constraints for correct orderings of events in the system.The user first identifies a candidate set of bad event orders.Then, by using ordinary untimed model-checking, the user examines whether a discretized system model in which all timing constraints are abstracted away satisfies a desirable safety property under the assumption that the identified bad event orders occur in no system execution. The user uses counterexamples obtained from the model-checker to identify additional bad event orders, and repeats the process until the model-checking succeeds. In this step, the user obtains a sufficient set of bad event orders that must be excluded by timing synthesis for system correctness.Next, the algorithm presented in the paper automatically derives a set of timing parameter constraints under which the system does not exhibit the identified bad event orderings. From this step combined with the untimed model-checking step,the user obtains a sufficient set of timing parameter constraints under which the system executes correctly with respect to a given safety property.We illustrate the use of EOA with a train-gate example inspired by the general railroad crossing problem. We also summarize three other case studies, a biphase mark protocol, the IEEE 1394 root contention protocol, and the Fischer mutual exclusion algorithm.",19 p.,; Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,parametric verification; event-based approach; counter-example guided abstraction refinement (CEGAR); automatic timing synthesis,Event Order Abstraction for Parametric Real-Time System Verification,,,
Leslie Kaelbling,"Gardiol, Natalia H.; Kaelbling, Leslie Pack",Learning and Intelligent Systems,2008-08-01T21:30:16Z,2008-08-01T21:30:16Z,2008-07-29,MIT-CSAIL-TR-2008-050,http://hdl.handle.net/1721.1/41920,"We describe a method to use structured representations of the environmentâ€™s dynamics to constrain and speed up the planning process. Given a problem domain described in a probabilistic logical description language, we develop an anytime technique that incrementally improves on an initial, partial policy. This partial solution is found by ï¬rst reducing the number of predicates needed to represent a relaxed version of the problem to a minimum, and then dynamically partitioning the action space into a set of equivalence classes with respect to this minimal representation. Our approach uses the envelope MDP framework, which creates a Markov decision process out of a subset of the full state space as de- termined by the initial partial solution. This strategy permits an agent to begin acting within a restricted part of the full state space and to expand its envelope judiciously as resources permit.",17 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,Adaptive Envelope MDPs for Relational Equivalence-based Planning,,,
,"Silva, Marco da; Popovic, Jovan; Abe, Yeuhi",,2008-08-28T18:45:23Z,2008-08-28T18:45:23Z,2008-08-01,,http://hdl.handle.net/1721.1/42003,"Animating natural human motion in dynamic environments is difficult because of complex geometric and physical interactions. Simulation provides an automatic solution to parts of this problem, but it needs control systems to produce lifelike motions. This paper describes the systematic computation of controllers that can reproduce a range of locomotion styles in interactive simulations. Given a reference motion that describes the desired style, a derived control system can reproduce that style in simulation and in new environments. Because it produces high-quality motions that are both geometrically and physically consistent with simulated surroundings, interactive animation systems could begin to use this approach with more established kinematic methods.",N/A,,animation; control; robotics,Interactive Simulation of Stylized Human Locomotion,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Jovan Popovic; Computer Graphics,"ACM Transactions on Graphics, Volume 27, Issue 3"
Ronitt Rubinfeld,"Agarwal, Shivani",Theory of Computation,2008-08-14T17:15:02Z,2008-08-14T17:15:02Z,2008-08-07,,http://hdl.handle.net/1721.1/41938,"In ranking, one is given examples of order relationships among objects, and the goal is to learn from these examples a real-valued ranking function that induces a ranking or ordering over the object space. We consider the problem of learning such a ranking function in a transductive, graph-based setting, where the object space is finite and is represented as a graph in which vertices correspond to objects and edges encode similarities between objects. Building on recent developments in regularization theory for graphs and corresponding Laplacian-based learning methods, we develop an algorithmic framework for learning ranking functions on graphs. We derive generalization bounds for our algorithms in transductive models similar to those used to study other transductive learning problems, and give experimental evidence of the potential benefits of our framework.",27 p.,,Graph-based learning; Transductive learning; Ranking; Graph regularization,Transductive Ranking on Graphs,MIT-CSAIL-TR-2008-051,,
