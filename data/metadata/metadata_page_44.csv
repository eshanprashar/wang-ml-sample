dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.subject,dc.title,dc.language.rfc3066,dspace.orderedauthors,dc.rights,dc.rights.uri,dc.description,dc.description.degree,dc.contributor,dc.description.sponsorship,dc.identifier.citation,dc.publisher
Tomaso Poggio,"Alvarez, Mauricio A.; Rosasco, Lorenzo; Lawrence, Neil D.",Center for Biological and Computational Learning (CBCL),2011-06-30T19:30:08Z,2011-06-30T19:30:08Z,2011-06-30,http://hdl.handle.net/1721.1/64731,"Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.",38 p.,MIT-CSAIL-TR-2011-033; CBCL-301,learning theory; kernel methods; multi-output learning,Kernels for Vector-Valued Functions: a Review,en-US,"Alvarez, Mauricio A.; Rosasco, Lorenzo; Lawrence, Neil D.",,,,,,,,
Anant Agarwal,"Belay, Adam; Wentzlaff, David; Agarwal, Anant",Computer Architecture,2011-07-28T17:45:15Z,2011-07-28T17:45:15Z,2011-07-27,http://hdl.handle.net/1721.1/64977,"Recent trends in OS research have shown evidence that there are performance benefits to running OS services on different cores than the user applications that rely on them. We quantitatively evaluate this claim in terms of one of the most significant architectural constraints: memory performance. To this end, we have created CachEMU, an open-source memory trace generator and cache simulator built as an extension to QEMU for working with system traces. Using CachEMU, we determined that for five common Linux test workloads, it was best to run the OS close, but not too close   on the same package, but not on the same core.",6 p.,MIT-CSAIL-TR-2011-035,,Vote the OS off your Core,en-US,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,
John Leonard,"Benjamin, Michael R.",Marine Robotics,2011-08-03T17:15:13Z,2011-08-03T17:15:13Z,2011-07-28,http://hdl.handle.net/1721.1/65074,This document describes 19 MOOS-IvP autonomy tools. uHelmScope provides a run-time scoping window into the state of an active IvP Helm executing its mission. pMarineViewer is a geo-based GUI tool for rendering marine vehicles and geometric data in their operational area. uXMS is a terminal based tool for scoping on a MOOSDB process. uTermCommand is a terminal based tool for poking a MOOSDB with a set of MOOS file pre-defined variable-value pairs selectable with aliases from the command-line. pEchoVar provides a way of echoing a post to one MOOS variable with a new post having the same value to a different variable. uProcessWatch monitors the presence or absence of a set of MOOS processes and summarizes the collective status in a single MOOS variable. uPokeDB provides a way of poking the MOOSDB from the command line with one or more variable-value pairs without any pre-existing configuration of a MOOS file. uTimerScript will execute a pre-defined timed pausable script of poking variable-value pairs to a MOOSDB. pNodeReporter summarizes a platforms critical information into a single node report string for sharing beyond the vehicle. pBasicContactMgr provides a basic contact management service with the ability to generate range-dependent configurable alerts. uSimMarine provides a simple marine vehicle simulator. uSimBeaconRange and uSimContactRange provide further simulation for range-only sensors. The Alog Toolbox is a set of offline tools for analyzing and manipulating log files in the .alog format.,167 p.,MIT-CSAIL-TR-2011-036,Autonomous Marine Vehicles; Unmanned Marine Vehicles; UUVs; AUVs; USVs; IvP Helm; pHelmIvP; LAMSS; MOOS; pLogger; MOOS Scope; MOOSDB,MOOS-IvP Autonomy Tools Users Manual Release 4.2.1,en-US,,Creative Commons Attribution-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,,,,,
John Leonard,"Benjamin, Michael R.; Schmidt, Henrik; Newman, Paul; Leonard, John J.",Marine Robotics,2011-08-03T17:15:06Z,2011-08-03T17:15:06Z,2011-08-03,http://hdl.handle.net/1721.1/65073,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moos-ivp.org.",262 p.,MIT-CSAIL-TR-2011-037,Unmanned Underwater Vehicles; UUV; Unmanned Surface Vehicles; USV; Unmanned Vehicles; Autonomy; Marine Vehicles; Marine Autonomy; pHelmIvP; MOOS; Autonomous Vehicles; Autonomous Marine Vehicles; Unmanned Marine Vehicles; Multi-objective Optimization; Behavior-Based Control; Behavior-Based Architecture,An Overview of MOOS-IvP and a Users Guide to the IvP Helm - Release 4.2.1,en-US,,Creative Commons Attribution-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-sa/3.0/,,,,,,
Karen Sollins,"Woodrow, Stephen Robert",Advanced Network Architecture,2011-08-31T23:00:08Z,2011-08-31T23:00:08Z,2011-08-06,http://hdl.handle.net/1721.1/65591,"This thesis analyzes and discusses the effectiveness of social efforts to achieve collective action amongst Internet network operators in order to manage the growth of the Internet routing table. The size and rate of growth of the Internet routing table is an acknowledged challenge impeding the scalability of our BGP interdomain routing architecture. While most of the work towards a solution to this problem has focused on architectural improvements, an effort launched in the 1990s called the CIDR Report attempts to incentivize route aggregation using social forces and norms in the Internet operator community. This thesis analyzes the behavior of Internet network operators in response to the CIDR Report from 1997 to 2011 to determine whether the Report was effective in achieving this goal. While it is difficult to causally attribute aggregation behavior to appearance on the CIDR report, there is a trend for networks to improve their prefix aggregation following an appearance on the CIDR Report compared to untreated networks. This suggests that the CIDR Report did affect network aggregation behavior, although the routing table continued to grow. This aggregation improvement is most prevalent early in the study period and becomes less apparent as time goes on. Potential causes of the apparent change in efficacy of the Report are discussed and examined using Ostrom s Common Pool Resource framework. The thesis then concludes with a discussion of options for mitigating routing table growth, including the continued use of community forces to better manage the Internet routing table.",165 p.,MIT-CSAIL-TR-2011-038,,Tragedy of the routing table: An analysis of collective action amongst Internet network operators,en-US,,,,S.M. thesis,S.M.,,,,
Russ Tedrake,"Platt, Robert, Jr.; Kaelbling, Leslie; Lozano-Perez, Tomas; Tedrake, Russ",Robot Locomotion Group,2011-09-15T18:30:11Z,2011-09-15T18:30:11Z,2011-08-27,http://hdl.handle.net/1721.1/65856,"We consider the partially observable control problem where it is potentially necessary to perform complex information-gathering operations in order to localize state. One approach to solving these problems is to create plans in belief-space, the space of probability distributions over the underlying state of the system. The belief-space plan encodes a strategy for performing a task while gaining information as necessary. Most approaches to belief-space planning rely upon representing belief state in a particular way (typically as a Gaussian). Unfortunately, this can lead to large errors between the assumed density representation and the true belief state. We propose a new computationally efficient algorithm for planning in non-Gaussian belief spaces. We propose a receding horizon re-planning approach where planning occurs in a low-dimensional sampled representation of belief state while the true belief state of the system is monitored using an arbitrary accurate high-dimensional representation. Our key contribution is a planning problem that, when solved optimally on each re-planning step, is guaranteed, under certain conditions, to enable the system to gain information. We prove that when these conditions are met, the algorithm converges with probability one. We characterize algorithm performance for different parameter settings in simulation and report results from a robot experiment that illustrates the application of the algorithm to robot grasping.",22 p.,MIT-CSAIL-TR-2011-039,,A hypothesis-based algorithm for planning and control in non-Gaussian belief spaces,en-US,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,Russ Tedrake; Robot Locomotion Group,,,
Tomaso Poggio,"Isik, Leyla; Leibo, Joel Z; Poggio, Tomaso",Center for Biological and Computational Learning (CBCL),2011-09-12T16:00:13Z,2011-09-12T16:00:13Z,2011-09-10,http://hdl.handle.net/1721.1/65646,"Learning by temporal association rules such as Foldiak's trace rule is an attractive hypothesis that explains the development of invariance in visual recognition. Consistent with these rules, several recent experiments have shown that invariance can be broken by appropriately altering the visual environment but found puzzling differences in the effects at the psychophysical versus single cell level. We show a) that associative learning provides appropriate invariance in models of object recognition inspired by Hubel and Wiesel b) that we can replicate the ""invariance disruption"" experiments using these models with a temporal association learning rule to develop and maintain invariance, and c) that we can thereby explain the apparent discrepancies between psychophysics and singe cells effects. We argue that these models account for the stability of perceptual invariance despite the underlying plasticity of the system, the variability of the visual world and expected noise in the biological mechanisms.",13 p.,MIT-CSAIL-TR-2011-040; CBCL-302,"vision, object recognition",Learning and disrupting invariance in visual recognition,en-US,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,
Daniela Rus,"Julian, Brian J.; Angermann, Michael; Schwager, Mac; Rus, Daniela",Robotics (Rus),2011-07-15T17:15:17Z,2011-07-15T17:15:17Z,2011-09-25,http://hdl.handle.net/1721.1/64821,"This paper presents a scalable information theoretic approach to infer the state of an environment by distributively controlling robots equipped with sensors. The robots iteratively estimate the environment state using a recursive Bayesian filter, while continuously moving to improve the quality of the estimate by following the gradient of mutual information. Both the filter and the controller use a novel algorithm for approximating the robots' joint measurement probabilities, which combines consensus (for decentralization) and sampling (for scalability). The approximations are shown to approach the true joint measurement probabilities as the size of the consensus rounds grows or as the network becomes complete. The resulting gradient controller runs in constant time with respect to the number of robots, and linear time with respect to the number of sensor measurements and environment discretization cells, while traditional mutual information methods are exponential in all of these quantities. Furthermore, the controller is proven to be convergent between consensus rounds and, under certain conditions, is locally optimal. The complete distributed inference and coordination algorithm is demonstrated in experiments with five quad-rotor flying robots and simulations with 100 robots.",10 p.,MIT-CSAIL-TR-2011-034,,A Scalable Information Theoretic Approach to Distributed Robot Coordination,en-US,,,,,,,"This work is sponsored by the Department of the Air Force under Air Force contract number FA8721-05-C-0002. The opinions, interpretations, recommendations, and conclusions are those of the authors and are not necessarily endorsed by the United States Government. This work is also supported in part by ARO grant number W911NF-05-1-0219, ONR grant number N00014-09-1-1051, NSF grant number EFRI-0735953, ARL grant number W911NF-08-2-0004, MIT Lincoln Laboratory, the European Commission, and the Boeing Company.",In Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS 11),
Tomaso Poggio,"Mosci, Sofia; Rosasco, Lorenzo; Santoro, Matteo; Verri, Alessandro; Villa, Silvia",Center for Biological and Computational Learning (CBCL),2011-09-26T20:45:09Z,2011-09-26T20:45:09Z,2011-09-26,http://hdl.handle.net/1721.1/65964,"In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose and study a new regularizer and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art.",38 p.,MIT-CSAIL-TR-2011-041; CBCL-303,computational learning; machine learning,Nonparametric Sparsity and Regularization,en-US,,,,,,,,,
Tomaso Poggio,"Mroueh, Youssef; Poggio, Tomaso; Rosasco, Lorenzo; Slotine, Jean-Jacques E.",Center for Biological and Computational Learning (CBCL),2011-09-27T20:30:07Z,2011-09-27T20:30:07Z,2011-09-27,http://hdl.handle.net/1721.1/66085,"We study multi-category classification in the framework of computational learning theory. We show how a relaxation approach, which is commonly used in binary classification, can be generalized to the multi-class setting. We propose a vector coding, namely the simplex coding, that allows to introduce a new notion of multi-class margin and cast multi-category classification into a vector valued regression problem. The analysis of the relaxation error be quantified and the binary case is recovered as a special case of our theory. From a computational point of view we can show that using the simplex coding we can design regularized learning algorithms for multi-category classification that can be trained at a complexity which is independent to the number of classes.",3 p.,MIT-CSAIL-TR-2011-043; CBCL-305,computational learning; machine learning; convex relaxation,Multi-Class Learning: Simplex Coding And Relaxation Error,en-US,,,,,,,,,
Martin Rinard,"Long, Fan; Ganesh, Vijay; Carbin, Micheal; Sidiroglou, Stelios; Rinard, Martin",Computer Architecture,2011-10-03T21:00:06Z,2011-10-03T21:00:06Z,2011-10-03,http://hdl.handle.net/1721.1/66170,"We present a novel technique, automatic input rectification, and a prototype implementation called SOAP. SOAP learns a set of constraints characterizing typical inputs that an application is highly likely to process correctly. When given an atypical input that does not satisfy these constraints, SOAP automatically rectifies the input (i.e., changes the input so that is satisfies the learned constraints). The goal is to automatically convert potentially dangerous inputs into typical inputs that the program is highly likely to process correctly. Our experimental results show that, for a set of benchmark applications (namely, Google Picasa, ImageMagick, VLC, Swfdec, and Dillo), this approach effectively converts malicious inputs (which successfully exploit vulnerabilities in the application) into benign inputs that the application processes correctly. Moreover, a manual code analysis shows that, if an input does satisfy the learned constraints, it is incapable of exploiting these vulnerabilities. We also present the results of a user study designed to evaluate the subjective perceptual quality of outputs from benign but atypical inputs that have been automatically rectified by SOAP to conform to the learned constraints. Specifically, we obtained benign inputs that violate learned constraints, used our input rectifier to obtain rectified inputs, then paid Amazon Mechanical Turk users to provide their subjective qualitative perception of the difference between the outputs from the original and rectified inputs. The results indicate that rectification can often preserve much, and in many cases all, of the desirable data in the original input.",13 p.,MIT-CSAIL-TR-2011-044,,Automatic Input Rectification,,,,,,,,,,MIT CSAIL
Nancy Lynch,"Ghaffari, Mohsen; Lynch, Nancy; Sastry, Srikanth",Theory of Computation,2011-10-12T18:30:07Z,2011-10-12T18:30:07Z,2011-10-12,http://hdl.handle.net/1721.1/66224,"We consider the problem of leader election (LE) in single-hop radio networks with synchronized time slots for transmitting and receiving messages. We assume that the actual number n of processes is unknown, while the size u of the ID space is known, but is possibly much larger. We consider two types of collision detection: strong (SCD), whereby all processes detect collisions, and weak (WCD), whereby only non-transmitting processes detect collisions. We introduce loneliness detection (LD) as a key subproblem for solving LE in WCD systems. LD informs all processes whether the system contains exactly one process or more than one. We show that LD captures the difference in power between SCD and WCD, by providing an implementation of SCD over WCD and LD. We present two algorithms that solve deterministic and probabilistic LD in WCD systems with time costs of O(log(u/n)) and O(min(log(u/n), (log(1/epsilon)/n)), respectively, where epsilon is the error probability. We also provide matching lower bounds. We present two algorithms that solve deterministic and probabilistic LE in SCD systems with time costs of O(log u) and O(min(log u, loglog n + log(1/epsilon))), respectively, where epsilon is the error probability. We provide matching lower bounds.",37 p.,MIT-CSAIL-TR-2011-045,wireless networks,Leader Election Using Loneliness Detection,,,,,,,,"This work partially supported by the NSF under award numbers CCF-0937274, and CCF-0726514, and by AFOSR under award number FA9550-08-1-0159. This work is also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.",,
Anant Agarwal,"Hoffmann, Henry; Maggio, Martina; Santambrogio, Marco D.; Leva, Alberto; Agarwal, Anant",Computer Architecture,2011-11-14T21:30:26Z,2011-11-14T21:30:26Z,2011-11-07,http://hdl.handle.net/1721.1/67020,"Modern systems require applications to balance competing goals, e.g. achieving high performance and low power. Achieving this balance places an unrealistic burden on application programmers who must understand the power and performance implications of a variety of application and system actions (e.g. changing algorithms or allocating cores). To address this problem, we propose the Self-aware Computing framework, or SEEC. SEEC automatically and dynamically schedules actions to meet application specified goals. While other self-aware implementations have been proposed, SEEC is uniquely distinguished by its decoupled approach, which allows application and systems programmers to separately specify observations and actions, according to their expertise. SEEC s runtime decision engine observes the system and schedules actions automatically, reducing programmer burden. This general and extensible decision engine employs both control theory and machine learning to reason about previously unseen applications and actions while automatically adapting to changes in both application and system models. This paper describes the SEEC framework and evaluates it in several case studies. SEEC is used to build an adaptive system that optimizes performance per Watt for the PARSEC benchmarks on multiple machines, achieving results as least 1.65x better than a classical control system. Additional studies show how SEEC can learn optimal resource allocation online and respond to fluctuations in the underlying hardware while managing multiple applications.",12 p.,MIT-CSAIL-TR-2011-046,Adaptive computing; Self-optimizing systems; Self-adaptive computing; Autonomic Computing; Resource Allocation,SEEC: A General and Extensible Framework for Self-Aware Computing,en-US,,,,,,,,,
Martin Rinard,"Carbin, Michael; Kim, Deokhwan; Misailovic, Sasa; Rinard, Martin C.",Computer Architecture,2011-11-15T18:15:04Z,2011-11-15T18:15:04Z,2011-11-15,http://hdl.handle.net/1721.1/67031,"A number of approximate program transformations have recently emerged that enable transformed programs to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control program execution. We call such transformed programs relaxed programs -- they have been extended with additional nondeterminism to relax their semantics and offer greater execution flexibility. We present programming language constructs for developing relaxed programs and proof rules for reasoning about properties of relaxed programs. Our proof rules enable programmers to directly specify and verify acceptability properties that characterize the desired correctness relationships between the values of variables in a program's original semantics (before transformation) and its relaxed semantics. Our proof rules also support the verification of safety properties (which characterize desirable properties involving values in individual executions). The rules are designed to support a reasoning approach in which the majority of the reasoning effort uses the original semantics. This effort is then reused to establish the desired properties of the program under the relaxed semantics. We have formalized the dynamic semantics of our target programming language and the proof rules in Coq, and verified that the proof rules are sound with respect to the dynamic semantics. Our Coq implementation enables developers to obtain fully machine checked verifications of their relaxed programs.",11 p.,MIT-CSAIL-TR-2011-050,,Reasoning about Relaxed Programs,,,,,,,,,,
Frédo Durand,"Aubry, Mathieu; Paris, Sylvain; Hasinoff, Samuel W.; Kautz, Jan; Durand, Frédo",Computer Graphics,2011-11-15T16:30:09Z,2011-11-15T16:30:09Z,2011-11-15,http://hdl.handle.net/1721.1/67030,"Multi-scale manipulations are central to image editing but they are also prone to halos. Achieving artifact-free results requires sophisticated edgeaware techniques and careful parameter tuning. These shortcomings were recently addressed by the local Laplacian filters, which can achieve a broad range of effects using standard Laplacian pyramids. However, these filters are slow to evaluate and their relationship to other approaches is unclear. In this paper, we show that they are closely related to anisotropic diffusion and to bilateral filtering. Our study also leads to a variant of the bilateral filter that produces cleaner edges while retaining its speed. Building upon this result, we describe an acceleration scheme for local Laplacian filters that yields speed-ups on the order of 50x. Finally, we demonstrate how to use local Laplacian filters to alter the distribution of gradients in an image. We illustrate this property with a robust algorithm for photographic style transfer.",11 p.,MIT-CSAIL-TR-2011-049,"Computational photography, photoediting, image processing, Laplacian pyramid, bilateral filter, photographic style transfer",Fast and Robust Pyramid-based Image Processing,en-US,"Aubry, Mathieu; Paris, Sylvain; Hasinoff, Samuel W.; Kautz, Jan; Durand, Frédo",,,,,,,,
Nickolai Zeldovich,"Metreveli, Zviad; Zeldovich, Nickolai; Kaashoek, M. Frans",Parallel and Distributed Operating Systems,2011-11-28T18:30:04Z,2011-11-28T18:30:04Z,2011-11-26,http://hdl.handle.net/1721.1/67296,"CPHash is a concurrent hash table for multicore processors. CPHash partitions its table across the caches of cores and uses message passing to transfer lookups/inserts to a partition. CPHash's message passing avoids the need for locks, pipelines batches of asynchronous messages, and packs multiple messages into a single cache line transfer. Experiments on a 80-core machine with 2 hardware threads per core show that CPHash has ~1.6x higher throughput than a hash table implemented using fine-grained locks. An analysis shows that CPHash wins because it experiences fewer cache misses and its cache misses are less expensive, because of less contention for the on-chip interconnect and DRAM. CPServer, a key/value cache server using CPHash, achieves ~5% higher throughput than a key/value cache server that uses a hash table with fine-grained locks, but both achieve better throughput and scalability than memcached. Finally, the throughput of CPHash and CPServer scales near-linearly with the number of cores.",10 p.,MIT-CSAIL-TR-2011-051,,CPHash: A Cache-Partitioned Hash Table,en-US,,,,,,,This work was partially supported by Quanta Computer and by NSF award 915164.,,
,"Durand, Frédo",Computer Graphics,2011-12-14T19:45:12Z,2011-12-14T19:45:12Z,2011-12-14,http://hdl.handle.net/1721.1/67677,"The numerical calculation of integrals is central to many computer graphics algorithms such as Monte-Carlo Ray Tracing. We show that such methods can be studied using Fourier analysis. Numerical error is shown to correspond to aliasing and the link between properties of the sampling pattern and the integrand is studied. The approach also permits the unified study of image aliasing and numerical integration, by considering a multidimensional domain where some dimensions are integrated while others are sampled.",6 p.,MIT-CSAIL-TR-2011-052,Numerical Analysis; Integration; Fourier; Monte-Carlo; Aliasing; Rendering; Ray Tracing,A Frequency Analysis of Monte-Carlo and other Numerical Integration Schemes,en-US,,,,,,,,,
Nancy Lynch,"Censor-Hillel, Keren; Gilbert, Seth; Kuhn, Fabian; Lynch, Nancy; Newport, Calvin",Theory of Computation,2011-12-27T20:30:09Z,2011-12-27T20:30:09Z,2011-12-22,http://hdl.handle.net/1721.1/67885,"In this paper we study the problem of building a connected dominating set with constant degree (CCDS) in the dual graph radio network model.  This model includes two types of links:  reliable links, which
always deliver messages, and unreliable links, which sometimes fail to deliver messages.  Real networks compensate for this differing quality by deploying low-layer detection protocols to filter unreliable from reliable links.  With this in mind, we begin by presenting an algorithm that solves the CCDS problem in the dual graph model under the assumption that every process u is provided with a local ""link detector set"" consisting of every neighbor connected to u by a reliable link.  The algorithm solves the CCDS problem in O((Delta log2(n)/b) + log3(n)) rounds, with high probability, where Delta is the maximum degree in the reliable link graph, n is the network size, and b is an upper bound in bits on the message size.  The algorithm works by first building a Maximal Independent Set (MIS) in log3(n) time, and then leveraging the local topology knowledge to efficiently connect nearby MIS processes.  A natural follow up question is whether the link detector must be perfectly reliable to solve the CCDS problem.  To answer this question, we first describe an algorithm that builds a CCDS in O(Delta polylog(n)) time under the assumption of O(1) unreliable links included in each link detector set.  We then prove this algorithm to be (almost) tight by showing that the possible inclusion of only a single unreliable link in each process's local link detector set is sufficient to require Omega(Delta) rounds to solve the CCDS problem, regardless of message size.  We conclude by discussing how to apply our algorithm in the setting where the topology of reliable and unreliable links can change over time.",21 p.,MIT-CSAIL-TR-2011-053,,Structuring Unreliable Radio Networks,en-US,,,,,,,,,
Tomaso Poggio,"Tan, Cheston; Leibo, Joel Z; Poggio, Tomaso",Center for Biological and Computational Learning (CBCL),2012-06-21T19:45:06Z,2012-06-21T19:45:06Z,2012,http://hdl.handle.net/1721.1/71199,"In recent years, scientific and technological advances have produced artificial systems that have matched or surpassed human capabilities in narrow domains such as face detection and optical character recognition. However, the problem of producing truly intelligent machines still remains far from being solved. In this chapter, we first describe some of these recent advances, and then review one approach to moving beyond these limited successes---the neuromorphic approach of studying and reverse-engineering the networks of neurons in the human brain (specifically, the visual system). Finally, we discuss several possible future directions in the quest for visual intelligence.",15 p.,MIT-CSAIL-TR-2012-016; CBCL-309,Vision; Artificial intelligence,Throwing Down the Visual Intelligence Gauntlet,en-US,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,"This research was sponsored by grants from DARPA (IPTO and DSO), National Science Foundation (NSF-0640097, NSF-0827427), AFSOR-THRL (FA8650-05-C-7262). Additional support was provided by: Adobe, Honda Research Institute USA, King Abdullah University Science and Technology grant to B. DeVore, NEC, Sony and especially by the Eugene McDermott Foundation.","Machine Learning for Computer Vision (2012); eds: Cipolla R, Battiato S, Giovanni Maria F. Springer: Studies in Computational Intelligence Vol. 411.",
Frédo Durand,"Judd, Tilke; Durand, Frédo; Torralba, Antonio",Computer Graphics,2012-01-13T22:30:12Z,2012-01-13T22:30:12Z,2012-01-13,http://hdl.handle.net/1721.1/68590,"Many computational models of visual attention have been created from a wide variety of different approaches to predict where people look in images. Each model is usually introduced by demonstrating performances on new images, and it is hard to make immediate comparisons between models. To alleviate this problem, we propose a benchmark data set containing 300 natural images with eye tracking data from 39 observers to compare model performances. We calculate the performance of 10 models at predicting ground truth fixations using three different metrics. We provide a way for people to submit new models for evaluation online. We find that the Judd et al. and Graph-based visual saliency models perform best. In general, models with blurrier maps and models that include a center bias perform well. We add and optimize a blur and center bias for each model and show improvements. We compare performances to baseline models of chance, center and human performance. We show that human performance increases with the number of humans to a limit. We analyze the similarity of different models using multidimensional scaling and explore the relationship between model performance and fixation consistency. Finally, we offer observations about how to improve saliency models in the future.",22 p.,MIT-CSAIL-TR-2012-001,"fixation maps, saliency maps, vision",A Benchmark of Computational Models of Saliency to Predict Human Fixations,en-US,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,
