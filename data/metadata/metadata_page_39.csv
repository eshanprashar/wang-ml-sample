dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.title,dc.contributor.editor,dc.description,dc.description.sponsorship,dc.subject,dc.rights,dc.rights.uri,dc.identifier.citation,dc.contributor,dc.coverage.spatial,dc.coverage.temporal,dc.language.iso,eprint.grantNumber
Anant Agarwal,"Beckmann, Nathan; Eastep, Jonathan; Gruenwald, Charles, III; Kurian, George; Kasture, Harshad; Miller, Jason E.; Celio, Christopher; Agarwal, Anant",Computer Architecture,2009-11-10T18:15:03Z,2009-11-10T18:15:03Z,2009-11-09,http://hdl.handle.net/1721.1/49809,"This paper introduces the open-source Graphite distributed parallel multicore simulator infrastructure. Graphite is designed from the ground up for exploration of future multicore processors containing dozens, hundreds, or even thousands of cores. It provides high performance for fast design space exploration and software development for future processors. Several techniques are used to achieve this performance including: direct execution, multi-machine distribution, analytical modeling, and lax synchronization. Graphite is capable of accelerating simulations by leveraging several machines. It can distribute simulation of an off-the-shelf threaded application across a cluster of commodity Linux machines with no modification to the source code. It does this by providing a single, shared address space and consistent single-process image across machines. Graphite is designed to be a simulation framework, allowing different component models to be easily replaced to either model different architectures or tradeoff accuracy for performance. We evaluate Graphite from a number of perspectives and demonstrate that it can simulate target architectures containing over 1000 cores on ten 8-core servers. Performance scales well as more machines are added with near linear speedup in many cases. Simulation slowdown is as low as 41x versus native execution for some applications. The Graphite infrastructure and existing models will be released as open-source software to allow the community to simulate their own architectures and extend and improve the framework.",17 p.,MIT-CSAIL-TR-2009-056,Graphite: A Distributed Parallel Simulator for Multicores,,,,,,,,,,,,
Nancy Lynch,"Oshman, Rotem; Lynch, Nancy; Kuhn, Fabian",Theory of Computation,2009-11-12T19:30:03Z,2009-11-12T19:30:03Z,2009-11-10,http://hdl.handle.net/1721.1/49814,"In this report we investigate distributed computation in dynamic networks in which the network topology changes from round to round. We consider a worst-case model in which the communication links for each round are chosen by an adversary, and nodes do not know who their neighbors for the current round are before they broadcast their messages. The model is intended to capture mobile networks and wireless networks, in which mobility and interference render communication unpredictable. The model allows the study of the fundamental computation power of dynamic networks. In particular, it captures mobile networks and wireless networks, in which mobility and interference render communication unpredictable. In contrast to much of the existing work on dynamic networks, we do not assume that the network eventually stops changing; we require correctness and termination even in networks that change continually. We introduce a stability property called T-interval connectivity (for T >= 1), which stipulates that for every T consecutive rounds there exists a stable connected spanning subgraph. For T = 1 this means that the graph is connected in every round, but changes arbitrarily between rounds. Algorithms for the dynamic graph model must cope with these unceasing changes. We show that in 1-interval connected graphs it is possible for nodes to determine the size of the network and compute any computable function of their initial inputs in O(n^2) rounds using messages of size O(log n + d), where d is the size of the input to a single node. Further, if the graph is T-interval connected for T > 1, the computation can be sped up by a factor of T, and any function can be computed in O(n + n^2 / T) rounds using messages of size O(log n + d). We also give two lower bounds on the gossip problem, which requires the nodes to disseminate k pieces of information to all the nodes in the network. We show an Omega(n log k) bound on gossip in 1-interval connected graphs against centralized algorithms, and an Omega(n + nk / T) bound on exchanging k pieces of information in T-interval connected graphs for a restricted class of randomized distributed algorithms. The T-interval connected dynamic graph model is a novel model, which we believe opens new avenues for research in the theory of distributed computing in wireless, mobile and dynamic networks.",40 p.,MIT-CSAIL-TR-2009-058,Distributed Computation in Dynamic Networks,,,,,,,,,,,,
Silvio Micali,"Micali, Silvio; Chen, Jing",,2009-11-10T18:15:12Z,2009-11-10T18:15:12Z,2009-11-10,http://hdl.handle.net/1721.1/49810,"The currently prevailing equilibrium-based approach to mechanism design suffers from a plurality of fundamental problems, and new conceptual frameworks are needed to solve or sufficiently alleviate them. In this paper, we put forward rational robustness, a new solution concept/implementation notion that is not equilibrium-based; prove its fundamental structural theorems; and compare it with prior notions. Our notion of implementation is specifically built so as to be robust against the problem of equilibrium selection. We prove it robust against other fundamental problems as well in different papers.",22 p.,MIT-CSAIL-TR-2009-057,Rational Robustness for Mechanism Design,Theory of Computation,first draft,Work partially supported by ONR Contract Number N00014-09-1-0597.,"Mechanism Design, Implementation, Rational Robustness, Distinguishably Dominated Strategies",,,,,,,,
Anant Agarwal,"Modzelewski, Kevin; Miller, Jason; Belay, Adam; Beckmann, Nathan; Gruenwald, Charles, III; Wentzlaff, David; Youseff, Lamia; Agarwal, Anant",Computer Architecture,2009-11-20T23:45:04Z,2009-11-20T23:45:04Z,2009-11-20,http://hdl.handle.net/1721.1/49844,"Single chip processors with thousands of cores will be available in the next ten years and clouds of multicore processors afford the operating system designer thousands of cores today. Constructing operating systems for manycore and cloud systems face similar challenges. This work identifies these shared challenges and introduces our solution: a factored operating system (fos) designed to meet the scalability, faultiness, variability of demand, and programming challenges of OSâ  s for single-chip thousand-core manycore systems as well as current day cloud computers. Current monolithic operating systems are not well suited for manycores and clouds as they have taken an evolutionary approach to scaling such as adding fine grain locks and redesigning subsystems, however these approaches do not increase scalability quickly enough. fos addresses the OS scalability challenge by using a message passing design and is composed out of a collection of Internet inspired servers. Each operating system service is factored into a set of communicating servers which in aggregate implement a system service. These servers are designed much in the way that distributed Internet services are designed, but provide traditional kernel services instead of Internet services. Also, fos embraces the elasticity of cloud and manycore platforms by adapting resource utilization to match demand. fos facilitates writing applications across the cloud by providing a single system image across both future 1000+ core manycores and current day Infrastructure as a Service cloud computers. In contrast, current cloud environments do not provide a single system image and introduce complexity for the user by requiring different programming models for intra- vs inter-machine communication, and by requiring the use of non-OS standard management tools.",11 p.,MIT-CSAIL-TR-2009-059,A Unified Operating System for Clouds and Manycore: fos,,,,Infrastructure as a Service; Cloud Computing; Manycore; Operating System; Multicore,,,,,,,,
Tomaso Poggio,"Poggio, Tomaso; Rosasco, Lorenzo; Wibisono, Andre",Center for Biological and Computational Learning (CBCL),2009-12-01T21:15:05Z,2009-12-01T21:15:05Z,2009-12-01,http://hdl.handle.net/1721.1/49868,"In this paper, we study the stability and generalization properties of penalized empirical-risk minimization algorithms. We propose a set of properties of the penalty term that is sufficient to ensure uniform ?-stability: we show that if the penalty function satisfies a suitable convexity property, then the induced regularization algorithm is uniformly ?-stable. In particular, our results imply that regularization algorithms with penalty functions which are strongly convex on bounded domains are ?-stable. In view of the results in [3], uniform stability implies generalization, and moreover, consistency results can be easily obtained. We apply our results to show that â  p regularization for 1 < p <= 2 and elastic-net regularization are uniformly ?-stable, and therefore generalize.",16 p.,CBCL-284; MIT-CSAIL-TR-2009-060,Sufficient Conditions for Uniform Stability of Regularization Algorithms,,,,artificial intelligence; theory; computation; learning,,,,,,,,
Silvio Micali,"Micali, Silvio; Chen, Jing",Theory of Computation,2009-12-09T21:15:08Z,2009-12-09T21:15:08Z,2009-12-04,http://hdl.handle.net/1721.1/49869,"We show that, when the players are perfectly informed about each other, essentially all social-choice functions can be rationally robustly implemented via an extensive-form public-action mechanism that (1) is perfectly robust against collusion, (2) requires only a linear number of computation steps and communication bits, and (3) preserves the privacy of the players' types to a very high extent.",4 p.,MIT-CSAIL-TR-2009-061,Perfect and General Virtual Implementation For Perfectly Informed Players,,,,rationally robust implementation; Virtual implementation; perfectly informed players,,,,,,,,
Fredo Durand,"Durand, Fredo; Cohen, Michael; Chen, Jiawen; Paris, Sylvain; Wang, Jue; Matusik, Wojciech",Computer Graphics,2009-12-17T19:15:09Z,2009-12-17T19:15:09Z,2009-12-16,http://hdl.handle.net/1721.1/50231,"This paper introduces the video mesh, a data structure for representing video as 2.5D ""paper cutouts."" The video mesh allows interactive editing of moving objects and modeling of depth, which enables 3D effects and post-exposure camera control. The video mesh sparsely encodes optical flow as well as depth, and handles occlusion using local layering and alpha mattes. Motion is described by a sparse set of points tracked over time. Each point also stores a depth value. The video mesh is a triangulation over this point set and per-pixel information is obtained by interpolation. The user rotoscopes occluding contours and we introduce an algorithm to cut the video mesh along them. Object boundaries are refined with perpixel alpha values. The video mesh is at its core a set of texture mapped triangles, we leverage graphics hardware to enable interactive editing and rendering of a variety of effects. We demonstrate the effectiveness of our representation with a number of special effects including 3D viewpoint changes, object insertion, and depth-of-field manipulation.",9 p.,MIT-CSAIL-TR-2009-062,The Video Mesh: A Data Structure for Image-based Video Editing,,,,feature tracking; compositing; video editing; optical flow; triangle mesh,Creative Commons Attribution-Noncommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,"CHEN, J., PARIS, S., WANG, J., MATUSIK, W., COHEN, M., and DURAND, F. 2009. The Video Mesh: A Data Structure for Image-based Video Editing. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series, MIT-CSAIL-TR-2009-062.",,,,,
Whitman Richards,"Richards, Whitman; Winston, Patrick Henry; Finlayson, Mark Alan",Belief Dynamics,2009-12-17T21:15:04Z,2009-12-17T21:15:04Z,2009-12-17,http://hdl.handle.net/1721.1/50232,"Report of a Workshop held at the Wylie Center, Beverly, MA, Oct 8-10 2009",32 p.,MIT-CSAIL-TR-2009-063,Advancing Computational Models of Narrative,,,Sponsored by the AFOSR under MIT-MURI contract #FA9550-05-1-0321,literary theory; stories; narrative; computational linguistics; natural language generation; text understanding; discourse parsing,Creative Commons Attribution-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,Belief Dynamics; Whitman Richards,"Beverly, Massachusetts, United States",2009-10-08/2009-10-10,en,FA9550-05-1-0321
Saman Amarasinghe,"Amarasinghe, Saman; Rabbah, Rodric; Larsen, Samuel",Computer Architecture,2009-12-18T19:30:12Z,2009-12-18T19:30:12Z,2009-12-18,http://hdl.handle.net/1721.1/50235,"Multimedia extensions are nearly ubiquitous in today's general-purpose processors. These extensions consist primarily of a set of short-vector instructions that apply the same opcode to a vector of operands. Vector instructions introduce a data-parallel component to processors that exploit instruction-level parallelism, and present an opportunity for increased performance. In fact, ignoring a processor's vector opcodes can leave a significant portion of the available resources unused. In order for software developers to find short-vector instructions generally useful, however, the compiler must target these extensions with complete transparency and consistent performance. This paper describes selective vectorization, a technique for balancing computation across a processor's scalar and vector units. Current approaches for targeting short-vector instructions directly adopt vectorizing technology first developed for supercomputers. Traditional vectorization, however, can lead to a performance degradation since it fails to account for a processor's scalar resources. We formulate selective vectorization in the context of software pipelining. Our approach creates software pipelines with shorter initiation intervals, and therefore, higher performance. A key aspect of selective vectorization is its ability to manage transfer of operands between vector and scalar instructions. Even when operand transfer is expensive, our technique is sufficiently sophisticated to achieve significant performance gains. We evaluate selective vectorization on a set of SPEC FP benchmarks. On a realistic VLIW processor model, the approach achieves whole-program speedups of up to 1.35x over existing approaches. For individual loops, it provides speedups of up to 1.75x.",25 p.,MIT-CSAIL-TR-2009-064,Selective Vectorization for Short-Vector Instructions,,,,SIMD; Vectorization; Compiler,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",Theory of Computation,2013-04-11T20:45:03Z,2013-04-11T20:45:03Z,2009.,http://hdl.handle.net/1721.1/78359,"Modeling frameworks such as Probabilistic I/O Automata (PIOA) and Markov Decision Processes permit both probabilistic and nondeterministic choices. In order to use these frameworks to express claims about probabilities of events, one needs mechanisms for resolving nondeterministic choices. For PIOAs, nondeterministic choices have traditionally been resolved by schedulers that have perfect information about the past execution. However, these schedulers are too powerful for certain settings, such as cryptographic protocol analysis, where information must sometimes be hidden. Here, we propose a new, less powerful nondeterminism-resolution mechanism for PIOAs, consisting of tasks and local schedulers. Tasks are equivalence classes of system actions that are scheduled by oblivious, global task sequences. Local schedulers resolve nondeterminism within system components, based on local information only. The resulting task-PIOA framework yields simple notions of external behavior and implementation, and supports simple compositionality results. We also define a new kind of simulation relation, and show it to be sound for proving implementation. We illustrate the potential of the task-PIOAframework by outlining its use in verifying an Oblivious Transfer protocol.",46 p.,MIT-CSAIL-TR-2013-006,Task-Structured Probabilistic I/O Automata,,"""May 28, 2009.""",,,,,,,,,,
John Leonard,"Kaess, Michael; Dellaert, Frank; Roberts, Richard; Ila, Viorela","Robotics, Vision & Sensor Networks",2010-05-05T18:15:02Z,2010-05-05T18:15:02Z,2010-01-29,http://hdl.handle.net/1721.1/54717,"In this paper we present a novel data structure, the Bayes tree, which exploits the connections between graphical model inference and sparse linear algebra. The proposed data structure provides a new perspective on an entire class of simultaneous localization and mapping (SLAM) algorithms. Similar to a junction tree, a Bayes tree encodes a factored probability density, but unlike the junction tree it is directed and maps more naturally to the square root information matrix of the SLAM problem. This makes it eminently suited to encode the sparse nature of the problem, especially in a smoothing and mapping (SAM) context. The inherent sparsity of SAM has already been exploited in the literature to produce efficient solutions in both batch and online mapping. The graphical model perspective allows us to develop a novel incremental algorithm that seamlessly incorporates reordering and relinearization. This obviates the need for expensive periodic batch operations from previous approaches, which negatively affect the performance and detract from the intended online nature of the algorithm. The new method is evaluated using simulated and real-world datasets in both landmark and pose SLAM settings.",18 p.,MIT-CSAIL-TR-2010-021,The Bayes Tree: Enabling Incremental Reordering and Fluid Relinearization for Online Mapping,,,,sparse nonlinear optimization; sparse linear algebra; iSAM; SLAM; junction tree; probabilistic inference; graphical models; mobile robotics; smoothing and mapping,,,,,,,,
Erik Demaine,"Zadimoghaddam, Morteza; Hajiaghayi, MohammadTaghi; Bateni, MohammadHossein",Theory of Computation,2010-02-02T23:30:07Z,2010-02-02T23:30:07Z,2010-02-01,http://hdl.handle.net/1721.1/51336,"Online auction is an essence of many modern markets, particularly networked markets, in which information about goods, agents, and outcomes is revealed over a period of time, and the agents must make irrevocable decisions without knowing future information. Optimal stopping theory, especially the classic ""secretary problem"", is a powerful tool for analyzing such online scenarios which generally require optimizing an objective function over the input. The secretary problem and its generalization the ""multiple-choice secretary problem"" were under a thorough study in the literature. In this paper, we consider a very general setting of the latter problem called the ""submodular secretary problem"", in which the goal is to select k secretaries so as to maximize the expectation of a (not necessarily monotone) submodular function which defines efficiency of the selected secretarial group based on their overlapping skills. We present the first constant-competitive algorithm for this case. In a more general setting in which selected secretaries should form an independent (feasible) set in each of l given matroids as well, we obtain an O(l log^2 r)-competitive algorithm generalizing several previous results, where r is the maximum rank of the matroids. Another generalization is to consider l knapsack constraints instead of the matroid constraints, for which we present an O(l)-competitive algorithm. In a sharp contrast, we show for a more general setting of ""subadditive secretary problem, there is no o~(sqrt(n))-competitive algorithm and thus submodular functions are the most general functions to consider for constant competitiveness in our setting. We complement this result by giving a matching O(sqrt(n))-competitive algorithm for the subadditive case. At the end, we consider some special cases of our general setting as well.",19 p.,MIT-CSAIL-TR-2010-002,Submodular Secretary Problem and Extensions,,,,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,
Anant Agarwal,"Modzelewski, Kevin; Miller, Jason; Belay, Adam; Beckmann, Nathan; Gruenwald, Charles, III; Wentzlaff, David; Youseff, Lamia; Agarwal, Anant",Computer Architecture,2010-02-08T20:00:07Z,2010-02-08T20:00:07Z,2010-02-08,http://hdl.handle.net/1721.1/51381,"Cloud computers and multicore processors are two emerging classes of computational hardware that have the potential to provide unprecedented compute capacity to the average user. In order for the user to effectively harness all of this computational power, operating systems (OSes) for these new hardware platforms are needed.  Existing multicore operating systems do not scale to large numbers of cores, and do not support clouds. Consequently, current-day cloud systems push much complexity onto the user, requiring the user to manage individual Virtual Machines (VMs) and deal with many system-level concerns. In this work we describe the mechanisms and implementation of a factored operating system named fos. fos is a single system image operating system across both multicore and Infrastructure as a Service (IaaS) cloud systems.  fos tackles OS scalability challenges by factoring the OS into its component system services. Each system service is further factored into a collection of Internet-inspired servers which communicate via messaging. Although designed in a manner similar to distributed Internet services, OS services instead provide traditional kernel services such as file systems, scheduling, memory management,and access to hardware. fos also implements new classes of OS services like fault tolerance and demand elasticity. In this work, we describe our working fos implementation, and provide early performance measurements of fos for both intra-machine and inter-machine operations.",12 p.,MIT-CSAIL-TR-2010-003,An Operating System for Multicore and Clouds: Mechanisms and Implementation,,,,,,,,,,,,
Nancy Lynch,"Lynch, Nancy; Kuhn, Fabian; Kowalski, Dariusz; Khabbazian, Majid",Theory of Computation,2010-02-09T18:30:02Z,2010-02-09T18:30:02Z,2010-02-09,http://hdl.handle.net/1721.1/51667,"We analyze greedy algorithms for broadcasting messages throughout a multi-hop wireless network, using a slot-based model that includes message collisions without collision detection. Our algorithms are split formally into two pieces: a high-level piece for broadcast and a low-level piece for contention management. We accomplish the split using abstract versions of the MAC layer to encapsulate the contention management. We use two different abstract MAC layers: a basic non-probabilistic one, which our contention management algorithm implements with high probability, and a probabilistic one, which our contention management algorithm implements precisely. Using this approach, we obtain the following complexity bounds: Single-message broadcast, using the basic abstract MAC layer, takes time O(D log(n/epsilon) log(Delta)) to deliver the message everywhere with probability 1 - epsilon, where D is the network diameter, n is the number of nodes, and Delta is the maximum node degree. Single-message broadcast, using the probabilistic abstract MAC layer, takes time only O((D + log(n/epsilon)) log(Delta)). For multi-message broadcast, the bounds are O((D + k' Delta) log(n/epsilon) log(Delta)) using the basic layer and O((D + k' Delta log(n/epsilon)) log(Delta)) using the probabilistic layer,for the time to deliver a single message everywhere in the presence of at most k' concurrent messages.",42 p.,MIT-CSAIL-TR-2010-005,The Cost of Global Broadcast Using Abstract MAC Layers,,,,,,,,,,,,
Martin Rinard,"Kim, Deokhwan; Misailovic, Sasa; Rinard, Martin",Computer Architecture,2010-02-10T18:15:03Z,2010-02-10T18:15:03Z,2010-02-10,http://hdl.handle.net/1721.1/51680,"Traditional parallelizing compilers are designed to generate parallel programs that produce identical outputs as the original sequential program. The difficulty of performing the program analysis required to satisfy this goal and the restricted space of possible target parallel programs have both posed significant obstacles to the development of effective parallelizing compilers. The QuickStep compiler is instead designed to generate parallel programs that satisfy statistical accuracy guarantees. The freedom to generate parallel programs whose output may differ (within statistical accuracy bounds) from the output of the sequential program enables a dramatic simplification of the compiler and a significant expansion in the range of parallel programs that it can legally generate. QuickStep exploits this flexibility to take a fundamentally different approach from traditional parallelizing compilers. It applies a collection of transformations (loop parallelization, loop scheduling, synchronization introduction, and replication introduction) to generate a search space of parallel versions of the original sequential program. It then searches this space (prioritizing the parallelization of the most time-consuming loops in the application) to find a final parallelization that exhibits good parallel performance and satisfies the statistical accuracy guarantee. At each step in the search it performs a sequence of trial runs on representative inputs to examine the performance, accuracy, and memory accessing characteristics of the current generated parallel program. An analysis of these characteristics guides the steps the compiler takes as it explores the search space of parallel programs. Results from our benchmark set of applications show that QuickStep can automatically generate parallel programs with good performance and statistically accurate outputs. For two of the applications, the parallelization introduces noise into the output, but the noise remains within acceptable statistical bounds. The simplicity of the compilation strategy and the performance and statistical acceptability of the generated parallel programs demonstrate the advantages of the QuickStep approach.",12 p.,MIT-CSAIL-TR-2010-007,Automatic Parallelization With Statistical Accuracy Bounds,,,,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,
Anant Agarwal,"Agarwal, Anant; Miller, Jason; Beckmann, Nathan; Wentzlaff, David",Computer Architecture,2010-02-12T07:15:04Z,2010-02-12T07:15:04Z,2010-02-11,http://hdl.handle.net/1721.1/51733,"The number of cores which fit on a single chip is growing at an exponential rate while off-chip main memory bandwidth is growing at a linear rate at best. This core count to off-chip bandwidth disparity causes per-core memory bandwidth to decrease as process technology advances. Continuing per-core off-chip bandwidth reduction will cause multicore and manycore chip architects to rethink the optimal grain size of a core and the on-chip cache configuration in order to save main memory bandwidth. This work introduces an analytic model to study the tradeoffs of utilizing increased chip area for larger caches versus more cores. We focus this study on constructing manycore architectures well suited for the emerging application space of cloud computing where many independent applications are consolidated onto a single chip. This cloud computing application mix favors small, power-efficient cores. The model is exhaustively evaluated across a large range of cache and core-count configurations utilizing SPEC Int 2000 miss rates and CACTI timing and area models to determine the optimal cache configurations and the number of cores across four process nodes. The model maximizes aggregate computational throughput and is applied to SRAM and logic process DRAM caches. As an example, our study demonstrates that the optimal manycore configuration in the 32nm node for a 200 mm^2 die uses on the order of 158 cores, with each core containing a 64KB L1I cache, a 16KB L1D cache, and a 1MB L2 embedded-DRAM cache. This study finds that the optimal cache size will continue to grow as process technology advances, but the tradeoff between more cores and larger caches is a complex tradeoff in the face of limited off-chip bandwidth and the non-linearities of cache miss rates and memory controller queuing delay.",13 p.,MIT-CSAIL-TR-2010-008,Core Count vs Cache Size for Manycore Architectures in the Cloud,,,,,,,,,,,,
Anant Agarwal,"Psota, James; Agarwal, Anant; Miller, Jason; Beckmann, Nathan; Kurian, George",Computer Architecture,2010-02-12T07:15:12Z,2010-02-12T07:15:12Z,2010-02-11,http://hdl.handle.net/1721.1/51734,"Ever since industry has turned to parallelism instead of frequency scaling to improve processor performance, multicore processors have continued to scale to larger and larger numbers of cores. Some believe that multicores will have 1000 cores or more by the middle of the next decade. However, their promise of increased performance will only be reached if their inherent scaling challenges are overcome. One such major scaling challenge is the viability of efficient cache coherence with large numbers of cores. Meanwhile, recent advances in nanophotonic device manufacturing are making CMOS-integrated optics a realityâ  interconnect technology which can provide significantly more bandwidth at lower power than conventional electrical analogs. The contributions of this paper are two-fold. (1) It presents ATAC, a new manycore architecture that augments an electrical mesh network with an optical network that performs highly efficient broadcasts. (2) It introduces ACKwise, a novel directory-based cache coherence protocol that provides high performance and scalability on any large-scale manycore interconnection net- work with broadcast capability. Performance evaluation studies using analytical models show that (i) a 1024-core ATAC chip using ACKwise achieves a speedup of 3.9Ã  compared to a similarly-sized pure electrical mesh manycore with a conventional limited directory protocol; (ii) the ATAC chip with ACKwise achieves a speedup of 1.35Ã  compared to the electrical mesh chip with ACKwise; and (iii) a pure electrical mesh chip with ACKwise achieves a speedup of 2.9Ã  over the same chip using a conventional limited directory protocol.",4 p.,MIT-CSAIL-TR-2010-009,Efficient Cache Coherence on Manycore Optical Networks,,,,,,,,,,,,
John Guttag,"Zeng, Qing; Curtis, Dorothy",Networks & Mobile Systems,2010-02-25T23:45:11Z,2010-02-25T23:45:11Z,2010-02-25,http://hdl.handle.net/1721.1/51833,"Increasingly, natural language processing (NLP) techniques are being developed and utilized in a variety of biomedical domains. Part of speech tagging is a critical step in many NLP applications. Currently, we are developing a NLP tool for text simplification. As part of this effort, we set off to evaluate several part of speech (POS) taggers. We selected 120 sentences (2375 tokens) from a corpus of six types of diabetes-related health texts and asked human reviewers to tag each word in these sentences to create a ""Gold Standard."" We then tested each of the three POS taggers against the ""Gold Standard."" One tagger (dTagger) had been trained on health texts and the other two (MaxEnt and Curran & Clark) were trained on general news articles. We analyzed the errors and placed them into five categories: systematic, close, subtle, difficult source, and other. The three taggers have relatively similar rates of success: dTagger, MaxEnt, and Curran & Clark had 87%, 89% and 90% agreement with the gold standard, respectively. These rates of success are lower than published rates for these taggers. This is probably due to our testing them on a corpus that differs significantly from their training corpora. The taggers made different errors: the dTagger, which had been trained on a set of medical texts (MedPost), made fewer errors on medical terms than MaxEnt and Curran & Clark. The latter two taggers performed better on non-medical terms and we found the difference between their performance and that of dTagger was statistically significant. Our findings suggest that the three POS taggers have similar correct tagging rates, though they differ in the types of errors they make. For the task of text simplification, we are inclined to perform additional training of the Curran & Clark tagger with the Medpost corpus because both the fine grained tagging provided by this tool and the correct recognition of medical terms are equally important.",20 p.,MIT-CSAIL-TR-2010-012,Performance and error analysis of three part of speech taggers on health texts,,,,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,
Tomaso Poggio,"Poggio, Tomaso; Knoblich, Ulf; Mutch, Jim",Center for Biological and Computational Learning (CBCL),2010-02-26T20:30:02Z,2010-02-26T20:30:02Z,2010-02-26,http://hdl.handle.net/1721.1/51839,"Computational models whose organization is inspired by the cortex are increasing in both number and popularity. Current instances of such models include convolutional networks, HMAX, Hierarchical Temporal Memory, and deep belief networks. These models present two practical challenges. First, they are computationally intensive. Second, while the operations performed by individual cells, or units, are typically simple, the code needed to keep track of network connectivity can quickly become complicated, leading to programs that are difficult to write and to modify. Massively parallel commodity computing hardware has recently become available in the form of general-purpose GPUs. This helps address the first problem but exacerbates the second. GPU programming adds an extra layer of difficulty, further discouraging exploration. To address these concerns, we have created a programming framework called CNS ('Cortical Network Simulator'). CNS models are automatically compiled and run on a GPU, typically 80-100x faster than on a single CPU, without the user having to learn any GPU programming. A novel scheme for the parametric specification of network connectivity allows the user to focus on writing just the code executed by a single cell. We hope that the ability to rapidly define and run cortically-inspired models will facilitate research in the cortical modeling community. CNS is available under the GNU General Public License.",11 p.,CBCL-286; MIT-CSAIL-TR-2010-013,CNS: a GPU-based framework for simulating cortically-organized networks,,,,,,,,,,,,
Sam Madden,"Wu, Eugene; Madden, Samuel; Zhang, Yang; Jones, Evan; Curino, Carlo",Database,2010-03-15T22:15:07Z,2010-03-15T22:15:07Z,2010-03-14,http://hdl.handle.net/1721.1/52606,"In this paper, we make the case for â  databases as a serviceâ   (DaaS), with two target scenarios in mind: (i) consolidation of data management functionality for large organizations and (ii) outsourcing data management to a cloud-based service provider for small/medium organizations. We analyze the many challenges to be faced, and discuss the design of a database service we are building, called Relational Cloud. The system has been designed from scratch and combines many recent advances and novel solutions. The prototype we present exploits multiple dedicated storage engines, provides high-availability via transparent replication, supports automatic workload partitioning and live data migration, and provides serializable distributed transactions. While the system is still under active development, we are able to present promising initial results that showcase the key features of our system. The tests are based on TPC benchmarks and real-world data from epinions.com, and show our partitioning, scalability and balancing capabilities.",6 p.,MIT-CSAIL-TR-2010-014,Relational Cloud: The Case for a Database Service,,,,cloud computing; database partitioning; distributed databases; database as a service,,,,,,,,
