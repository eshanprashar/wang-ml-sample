dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.other,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation,dc.title,dc.description,dc.relation.ispartofseries,dc.subject,dc.relation.replaces,dc.relation.uri,dc.identifier.citation
Silvio Micali,"Izmalkov, Sergei; Lepinski, Matt; Micali, Silvio",Theory of Computation,2008-05-08T16:30:24Z,2008-05-08T16:30:24Z,2007-03,MIT-CSAIL-TR-2008-028,http://hdl.handle.net/1721.1/41527,"Privacy and trust affect our strategic thinking, yet they have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism ---by disregarding the players' privacy, or assuming trust in a mediator--- may not be realistic and fail to reach the mechanism's objectives. We thus investigate implementations of a new type.We put forward the notion of a perfect implementation of a normal-form mechanism M: in essence, an extensive-form mechanism exactly preserving all strategic properties of M, WITHOUT relying on a trusted mediator or violating the privacy of the players. We prove that ANY normal-form mechanism can be perfectly implemented by a PUBLIC mediator using envelopes and an envelope-randomizing device (i.e., the same tools used for running fair lotteries or tallying secret votes). Differently from a trusted mediator, a public one only performs prescribed public actions, so that everyone can verify that he is acting properly, and never learns any information that should remain private.",42 p.,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,Perfect Implementation of Normal-Form Mechanisms,,,,,,
Rodney Brooks,"Torres-Jara, Eduardo",Humanoid Robotics,2007-03-03T15:01:47Z,2007-03-03T15:01:47Z,2007-03-02,MIT-CSAIL-TR-2007-015,http://hdl.handle.net/1721.1/36371,"This thesis presents an effective alternative to the traditionalapproach to robotic manipulation. In our approach, manipulation ismainly guided by tactile feedback as opposed to vision. Themotivation comes from the fact that manipulating an object impliescoming in contact with it, consequently, directly sensing physicalcontact seems more important than vision to control theinteraction of the object and the robot. In this work, thetraditional approach of a highly precise arm and vision systemcontrolled by a model-based architecture is replaced by one thatuses a low mechanical impedance arm with dense tactile sensing andexploration capabilities run by a behavior-based architecture.The robot OBRERO has been built to implement this approach. Newtactile sensing technology has been developed and mounted on therobot's hand. These sensors are biologically inspired and presentmore adequate features for manipulation than those of state of theart tactile sensors. The robot's limb was built with compliantactuators, which present low mechanical impedance, to make theinteraction between the robot and the environment safer than thatof a traditional high-stiffness arm. A new actuator was created tofit in the hand size constraints. The reduced precision ofOBRERO's limb is compensated by the capability of explorationgiven by the tactile sensors, actuators and the softwarearchitecture.The success of this approach is shown by picking up objects in anunmodelled environment. This task, simple for humans, has been achallenge for robots. The robot can deal with new, unmodelledobjects. OBRERO can come gently in contact, explore, lift, andplace the object in a different location. It can also detectslippage and external forces acting on an object while it is held.Each one of these steps are done by using tactile feedback. Thistask can be done with very light objects with no fixtures and onslippery surfaces.",172 p.,,Sensitive Manipulation,PhD thesis,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Manipulation; Compliant tactile sensors; Compliant hand; Small series elastic actuators,,,
Brian Williams,"Block, Stephen",Model-based Embedded and Robotic Systems,2007-03-06T12:41:49Z,2007-03-06T12:41:49Z,2007-03-05,MIT-CSAIL-TR-2007-016,http://hdl.handle.net/1721.1/36372,"Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and to uncertainty. Previous work developed contingent, temporally flexible plans. These plans provide robustness to uncertain activity durations, through flexible timing constraints, and robustness to plan failure, through alternate approaches to achieving a task. Robust execution of contingent, temporally flexible plans consists of two phases. First, in the plan extraction phase, the executive chooses between the functionally redundant methods in the plan to select an execution sequence that satisfies the temporal bounds in the plan. Second, in the plan execution phase, the executive dispatches the plan, using the temporal flexibility to schedule activities dynamically.Previous contingent plan execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. Likewise, a computation bottleneck may also occur because a single agent conducts all processing.This thesis introduces a robust, distributed executive for temporally flexible plans, called Distributed-Kirk, or D-Kirk. To execute a plan, D-Kirk first distributes the plan between the participating agents, by creating a hierarchical ad-hoc network and by mapping the plan onto this hierarchy. Second, the plan is reformulated using a distributed, parallel algorithm into a form amenable to fast dispatching. Finally, the plan is dispatched in a distributed fashion.We then extend the D-Kirk distributed executive to handle contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. A temporally consistent plan is extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN.At all stages of D-Kirk, the communication load is spread over all agents, thus eliminating the communication bottleneck. In particular, D-Kirk reduces the peak communication complexity of the plan execution phase by a factor of O(A/e'), where e' is the number of edges per node in the dispatchable plan, determined by the branching factor of the input plan, and A is the number of agents involved in executing the plan.In addition, the distributed algorithms employed by D-Kirk reduce the computational load on each agent and provide opportunities for parallel processing, thus increasing efficiency. In particular, D-Kirk reduces the average computational complexity of plan dispatching from O(eN^3) in the centralized case, to typical values of O(eN^2) per node and O(eN^3/A) per agent in the distributed case, where N is the number of nodes in the plan and e is the number of edges per node in the input plan.Both of the above results were confirmed empirically using a C++ implementation of D-Kirk on a set of parameterized input plans. The D-Kirk implementation was also tested in a realistic application where it was used to control a pair of robotic manipulators involved in a cooperative assembly task.",178 p.,,"Distributed Method Selection and Dispatching of Contingent, Temporally Flexible Plans",SM thesis,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,
Howard Shrobe,"Bachrach, Jonathan; Beal, Jacob",AIRE,2007-03-14T18:21:48Z,2007-03-14T18:21:48Z,2007-03-14,MIT-CSAIL-TR-2007-017,http://hdl.handle.net/1721.1/36840,"Programmability is a major challenge in spatial computing, anaggregate control problem found in domains such as sensor networks,swarm robotics, and modular robotics.  We address this challenge witha model of a spatial computer (Proto Abstract Machine) and adistributed operating system, ProtoKernel, which implements PAMapproximately.  ProtoKernel has been demonstrated on platforms inthree spatial computing domains: sensor networks, swarm robotics, andmodular robotics.",5 p.,,Building Spatial Computers,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,amorphous medium; amorphous computing,,,
Michael Ernst,"Zibin, Yoav; Potanin, Alex; Artzi, Shay; Kiezun, Adam; Ernst, Michael D.",Program Analysis,2007-03-16T21:01:46Z,2007-03-16T21:01:46Z,2007-03-16,MIT-CSAIL-TR-2007-018,http://hdl.handle.net/1721.1/36850,"A compiler-checked immutability guarantee provides useful documentation, facilitates reasoning, and enables optimizations. This paper presents Immutability Generic Java (IGJ), a novel language extension that expresses immutability without changing Java’s syntax by building upon Java’s generics and annotation mechanisms. In IGJ, each class has one additional generic parameter that is Immutable, Mutable, or ReadOnly. IGJ guarantees both reference immutability (only mutable references can mutate an object) and object immutability (an immutable reference points to an immutable object). IGJ is the first proposal for enforcing object immutability, and its reference immutability is more expressive than previous work. IGJ also permits covariant changes of generic arguments in a type-safe manner, e.g., a readonly list of integers is a subtype of a readonly list of numbers. IGJ extends Java’s type system with a few simple rules. We formalize this type system and prove it sound. Our IGJ compiler works by type-erasure and generates byte-code that can be executed on any JVM without runtime penalty.",12 p.,,Object and Reference Immutability using Java Generics,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,
Tomaso Poggio,"Rifkin, Ryan; Bouvrie, Jake; Schutte, Ken; Chikkerur, Sharat; Kouh, Minjoon; Ezzat, Tony; Poggio, Tomaso",Center for Biological and Computational Learning (CBCL),2007-03-22T11:21:47Z,2007-03-22T11:21:47Z,2007-03-21,MIT-CSAIL-TR-2007-019; CBCL-267,http://hdl.handle.net/1721.1/36865,"A preliminary set of experiments are described in which a biologically-inspired computer vision system (Serre, Wolf et al. 2005; Serre 2006; Serre, Oliva et al. 2006; Serre, Wolf et al. 2006) designed for visual object recognition was applied to the task of phonetic classification. During learning, the systemprocessed 2-D wideband magnitude spectrograms directly as images, producing a set of 2-D spectrotemporal patch dictionaries at different spectro-temporal positions, orientations, scales, and of varying complexity. During testing, features were computed by comparing the stored patches with patches fromnovel spectrograms. Classification was performed using a regularized least squares classifier (Rifkin, Yeo et al. 2003; Rifkin, Schutte et al. 2007) trained on the features computed by the system. On a 20-classTIMIT vowel classification task, the model features achieved a best result of 58.74% error, compared to 48.57% error using state-of-the-art MFCC-based features trained using the same classifier. This suggests that hierarchical, feed-forward, spectro-temporal patch-based architectures may be useful for phonetic analysis.",17 p.,,"Phonetic Classification Using Hierarchical, Feed-forward, Spectro-temporal Patch-based Architectures",,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,phonetic classification; hierarchical models; regularized least-squares; spectrotemporal patches,http://hdl.handle.net/1721.1/35835,http://hdl.handle.net/1721.1/35835,
Michael Ernst,"Artzi, Shay; Kiezun, Adam; Glasser, David; Ernst, Michael D.",Program Analysis,2007-03-26T11:21:46Z,2007-03-26T11:21:46Z,2007-03-23,MIT-CSAIL-TR-2007-020,http://hdl.handle.net/1721.1/36880,"Knowing which method parameters may be mutated during a method's execution is useful for many software engineering tasks. We present an approach to discovering parameter immutability, in which several lightweight, scalable analyses are combined in stages, with each stage rening the overall result. The resulting analysis is scalable and combines the strengths of its component  analyses. As one of the component analyses, we present a novel, dynamic mutability analysis and show how its results can be improved by random input generation. Experimental results on programs of up to 185 kLOC show that, compared to previous approaches, our approach increases both scalability and overall accuracy.",17 p.,,Combined Static and Dynamic Mutability Analysis,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,
Trevor Darrell,"Urtasun, Raquel; Darrell, Trevor",Vision,2007-03-29T11:21:46Z,2007-03-29T11:21:46Z,2007-03-28,MIT-CSAIL-TR-2007-021,http://hdl.handle.net/1721.1/36901,"Supervised learning is difficult with high dimensional input spacesand very small training sets, but accurate classification may bepossible if the data lie on a low-dimensional manifold.  GaussianProcess Latent Variable Models can discover low dimensional manifoldsgiven only a small number of examples, but learn a latent spacewithout regard for class labels.  Existing methods for discriminativemanifold learning (e.g., LDA, GDA) do constrain the class distributionin the latent space, but are generally deterministic and may notgeneralize well with limited training data.  We introduce a method forGaussian Process Classification using latent variable models trainedwith discriminative priors over the latent space, which can learn adiscriminative latent space from a small training set.",8 p.,,Discriminative Gaussian Process Latent Variable Model for Classification,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Gaussian Processes; Classification; Latent Variable Models; Machine Learning,,,
Rodney Brooks,"Aryananda, Lijin",Humanoid Robotics,2007-04-04T17:21:51Z,2007-04-04T17:21:51Z,2007-04-03,MIT-CSAIL-TR-2007-022,http://hdl.handle.net/1721.1/37144,"This thesis presents an integrated framework and implementation for Mertz, an expressive robotic creature for exploring the task of face recognition through natural interaction in an incremental and unsupervised fashion.  The goal of this thesis is to advance toward a framework which would allow robots to incrementally ``get to know'' a set of familiar individuals in a natural and extendable way.  This thesis is motivated by the increasingly popular goal of integrating robots in the home.  In order to be effective in human-centric tasks, the robots must be able to not only recognize each family member, but also to learn about the roles of various people in the household.In this thesis, we focus on two particular limitations of the current technology.  Firstly, most of face recognition research concentrate on the supervised classification problem.  Currently, one of the biggest problems in face recognition is how to generalize the system to be able to recognize new test data that vary from the training data.  Thus, until this problem is solved completely, the existing supervised approaches may require multiple manual introduction and labelling sessions to include training data with enough variations. Secondly, there is typically a large gap between research prototypes and commercial products, largely due to lack of robustness and scalability to different environmental settings.In this thesis, we propose an unsupervised approach which wouldallow for a more adaptive system which can incrementally update thetraining set with more recent data or new individuals over time.Moreover, it gives the robots a more natural {\em socialrecognition} mechanism to learn not only to recognize each person'sappearance, but also to remember some relevant contextualinformation that the robot observed during previous interactionsessions. Therefore, this thesis focuses on integrating anunsupervised and incremental face recognition system within aphysical robot which interfaces directly with humans through naturalsocial interaction.  The robot autonomously detects, tracks, andsegments face images during these interactions and automaticallygenerates a training set for its face recognition system.  Moreover,in order to motivate robust solutions and address scalabilityissues, we chose to put the robot, Mertz, in unstructured publicenvironments to interact with naive passersby, instead of with onlythe researchers within the laboratory environment.While an unsupervised and incremental face recognition system is acrucial element toward our target goal, it is only a part of thestory.  A face recognition system typically receives eitherpre-recorded face images or a streaming video from a static camera.As illustrated an ACLU review of a commercial face recognitioninstallation, a security application which interfaces with thelatter is already very challenging.  In this case, our target goalis a robot that can recognize people in a home setting. Theinterface between robots and humans is even more dynamic.  Both therobots and the humans move around.We present the robot implementation and its unsupervised incremental face recognition framework.  We describe analgorithm for clustering local features extracted from a large set of automatically generated face data.  We demonstrate the robot's capabilities and limitations in a series of experiments at a public lobby. In a final experiment, the robot interacted with a few hundred individuals in an eight day period and generated a training set of over a hundred thousand face images. We evaluate the clustering algorithm performance across a range of parameters on this automatically generated training data and also the Honda-UCSD video face database. Lastly, we present some recognition results using the self-labelled clusters.",244 p.,,A Few Days of A Robot's Life in the Human's World: Toward Incremental Individual Recognition,PhD thesis,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Humanoid robotic; Human robot interaction; Face recognition,,,
Howard Shrobe,"Shrobe, Howard; Laddaga, Robert; Balzer, Robert; Goldman, Neil; Wile, Dave; Tallis, Marcelo; Hollebeek, Tim; Egyed, Alexander",AIRE,2007-04-10T21:41:47Z,2007-04-10T21:41:47Z,2007-04-10,MIT-CSAIL-TR-2007-023,http://hdl.handle.net/1721.1/37151,"Information systems form the backbones of the critical infrastructures of modern societies. Unfortunately, these systems are highly vulnerable to attacks that can result in enormous damage. Furthermore, traditional approaches to information security have not provided all the protections necessary to defeat and recover from a concerted attack; in particular, they are largely irrelevant to the problem of defending against attacks launched by insiders.This paper describes two related systems PMOP and AWDRAT that were developed during the DARPA Self Regenerative Systems program. PMOP defends against insider attacks while AWDRAT is intended to detect compromises to software systems. Both rely on self-monitoring, diagnosis and self-adaptation. We describe both systems and show the results of experiments with each.",10 p.,,Self-Adaptive Systems for Information Survivability: PMOP and AWDRAT,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Information Survivability; Model Based Diagnosis; Adaptive Software,,,
Gerald Sussman,"Beal, Jacob",Mathematics and Computation,2007-04-12T20:21:45Z,2007-04-12T20:21:45Z,2007-04-12,,http://hdl.handle.net/1721.1/37152,"Principles for Engineered EmergenceIt is difficult to establish engineering control over the behavior ofaggregates of unreliable devices with complicated interactionpatterns.  I take a linguistic view of this problem, searching formechanisms that simplify the composition and abstraction ofcomplicated behaviors.  From my work on various problems of aggregatecontrol in cognitive architectures and spatial computing, I havenoticed common themes in mechanisms that solve them.  From these, Iextract four principles which seem to help in engineering robustaggregate behavior---self-scaling, sparseness, gradual degradation,and failure simplification---and give examples of how they can beexploited.",28 p.,,Principles for Engineered Emergence (slides),,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,artificial intelligence; amorphous computing,,,
William Freeman,"Torralba, Antonio; Fergus, Rob; Freeman, William T.",Vision,2007-04-24T14:01:48Z,2007-04-24T14:01:48Z,2007-04-23,MIT-CSAIL-TR-2007-024,http://hdl.handle.net/1721.1/37291,"The human visual system is remarkably tolerant to degradations in image resolution: in a scene recognition task, human performance is similar whether $32 \times 32$ color images or multi-mega pixel images are used. With small images, even object recognition and segmentation is performed robustly by the visual system, despite the object being unrecognizable in isolation. Motivated by these observations, we explore the space of 32x32 images using a database of 10^8 32x32 color images gathered from the Internet using image search engines. Each image is loosely labeled with one of the 70,399 non-abstract nouns in English, as  listed in the Wordnet lexical database. Hence the image database represents a dense sampling of all object categories and scenes. With this dataset, we use nearest neighbor methods to perform objectrecognition across the 10^8 images.",9 p.,,Tiny images,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Recognition; Nearest neighbors methods; Image databases,,,
Tomaso Poggio,"Rifkin, Ryan M.; Lippert, Ross A.",Center for Biological and Computational Learning (CBCL),2007-05-01T16:01:50Z,2007-05-01T16:01:50Z,2007-05-01,MIT-CSAIL-TR-2007-025; CBCL-268,http://hdl.handle.net/1721.1/37318,"This is a collection of information about regularized least squares (RLS). The facts here are not “new results”, but we have not seen them usefully collected together before. A key goal of this work is to demonstrate that with RLS, we get certain things “for free”: if we can solve a single supervised RLS problem, we can search for a good regularization parameter lambda at essentially no additional cost.The discussion in this paper applies to “dense” regularized least squares, where we work with matrix factorizations of the data or kernel matrix. It is also possible to work with iterative methods such as conjugate gradient, and this is frequently the method of choice for large data sets in high dimensions with very few nonzero dimensions per point, such as text classifciation tasks. The results discussed here do not apply to iterative methods, which have different design tradeoffs.We present the results in greater detail than strictly necessary, erring on the side of showing our work. We hope that this will be useful to people trying to learn more about linear algebra manipulations in the machine learning context.",8 p.,,Notes on Regularized Least Squares,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"machine learning, linear algebra",,,
Gerald Sussman,"Beal, Jacob",Mathematics and Computation,2007-05-15T17:41:55Z,2007-05-15T17:41:55Z,2007-05-15,MIT-CSAIL-TR-2007-026,http://hdl.handle.net/1721.1/37336,"We can evaluate models of natural intelligence, as well as theirindividual components, by using a model of hardware and developmentcosts, ignoring almost all the details of biology.  The basic argumentis that neither the gross anatomy of the brain nor the behavior ofindividual cells nor the behavior of the whole poses sufficientconstraint on the algorithms that might run within the brain, but thatthe process of engineering an intelligence under this cost model posessimilar challenges to those faced by a human growing from a singlecell to an adult.  This will allow us to explore architectural ideasfreely, yet retain confidence that when a system works, the principlesallowing it to work are likely to be similar to those that allow humanintelligence to work.",4 p.,,Developmental Cost for Models of Intelligence,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,cognitive architectures; artificial intelligence,,,AAAI 2007 Workshop on Evaluating Architectures for Intelligence
Hal Abelson,"Abelson, Hal",Mathematics and Computation,2007-05-21T12:41:49Z,2007-05-21T12:41:49Z,2007-05-19,,http://hdl.handle.net/1721.1/37585,"This paper traces the genesis of the MIT OpenCourseWare project from its initial strategic precursors in 1999 and 2000, through its launch in 2001 and its subsequent evolution.  The story told here illuminates the interplay among institutional leadership, and strategic planning, and with university culture in launching major educational technology enterprises.  It also shows how initiatives can evolve in unexpected ways, and can even surpass their initial goals.  The paper concludes with an overview of challenges facing OpenCourseWare in moving from the end of its production ramp-up and towards sustainability.",16 p.,,The Creation of OpenCourseWare at MIT,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"educational technology, open educational resources",,,J. Science Education and Technology
Dina Katabi,"Woo, Grace Rusi; Kheradpour, Pouya; Katabi, Dina",Networks & Mobile Systems,2007-05-29T18:21:50Z,2007-05-29T18:21:50Z,2007-05-29,MIT-CSAIL-TR-2007-027,http://hdl.handle.net/1721.1/37587,"Wireless networks can suffer from high packet loss rates.  This paper shows that the loss rate can be significantly reduced by exposing information readily available at the physical layer. We make the physical layer convey an estimate of its confidence that a particular bit is ``0'' or ``1'' to the higher layers.  When used with cooperative design, this information dramatically improves the throughput of the wireless network. Access points that hear the same transmission combine their information to correct bits in a packet with minimal overhead. Similarly, a receiver may combine multiple erroneous transmissions to recover a correct packet.  We analytically prove that our approach minimizes the errors in packet recovery.  We also experimentally demonstrate its benefits using a testbed of GNU software radios. The results show that our approach can reduce loss rate by up to 10x in comparison with the current approach, and significantly outperforms prior cooperation proposals.",12 p.,,Beyond the Bits: Cooperative Packet Recovery Using Physical Layer Information,PhD thesis,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,
Howard Shrobe,"Shrobe, Howard; Knight, Thomas; Hon, Andre de",AIRE,2007-05-30T18:01:22Z,2007-05-30T18:01:22Z,2007-05-30,MIT-CSAIL-TR-2007-028,http://hdl.handle.net/1721.1/37589,"The last 20 years have led to unprecedented improvements in chipdensity and system performance fueled mainly by Moore's Law.  Duringthe same time, system and application software have bloated, leadingto unmanageable complexity, vulnerability to attack, rigidity and lackof robustness and accountability. These problems arise from the factthat all key elements of the computational environment, from hardwarethrough system software and middleware to application code regard theworld as consisting of unconstrained ``raw seething bits''.  No elementof the entire stack is responsible for enforcing over-archingconventions of memory structuring or access control.  Outsiders mayeasily penetrate the system by exploiting vulnerabilities (e.g. bufferoverflows) arising from this lack of basic constraints. Attacks arenot easily contained, whether they originate from the clever outsiderwho penetrates the defenses or from the insider who exploits existingprivileges.  Finally, because there are no facilities for tracing theprovenance of data, even when an attack is detected, it is difficultif not impossible to tell which data are traceable to the attack andwhat data may still be trusted. We have abundant computational resources allowing us to fix thesecritical problems using a combination of hardware, system software,and programming language technology: In this report, we describe theTIARAproject, which is using these resources to design a newcomputer system thatis less vulnerable, more tolerant of intrusions, capable of recoveryfrom attacks, and accountable for their actions.  TIARA provides thesecapabilities without significant impact on overall system performance.  Itachieves these goals through the judicious use of a modest amountof extra, but reasonably generable purpose, hardware that is dedicatedto tracking the provenance of data at a very fine grained level, toenforcing access control policies, and to constructing a coherentobject-oriented model of memory.  This hardware runs in parallel withthe main data-paths of the system and operates on a set of extra bitstagging each word with data-type, bounds, access control andprovenance information. Operations that violate the intendedinvariants are trapped, while normal results are tagged withinformation derived from the tags of the input operands.This hardware level provides fine-grained support for a series ofsoftware layers that enable a variety of comprehensive access controlpolicies, self-adaptive computing, and fine-grained recoveryprocessing.  The first of these software layers establishes aconsistent object-oriented level of computing while higher layersestablish wrappers that may not be bypassed, access controls, dataprovenance tracking.  At the highest level we create the ``planlevel'' of computing in which code is executed in parallel with anabstract model (or executable specification) of the system that checkswhether the code behaves as intended.",18 p.,,"TIARA:  Trust Management, Intrusion-tolerance, Accountability, and Reconstitution Architecture",,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Security; Intrusion Tolerance; Tagged Architecture,,,
David Clark,"Lee, George J.",Advanced Network Architecture,2007-06-05T14:21:56Z,2007-06-05T14:21:56Z,2007-06-04,MIT-CSAIL-TR-2007-031,http://hdl.handle.net/1721.1/37595,"This thesis presents a new approach to root cause localization and fault diagnosis in the Internet based on a Common Architecture for Probabilistic Reasoning in the Internet (CAPRI) in which distributed, heterogeneous diagnostic agents efficiently conduct diagnostic tests and communicate observations, beliefs, and knowledge to probabilistically infer the cause of network failures.  Unlike previous systems that can only diagnose a limited set of network component failures using a limited set of diagnostic tests, CAPRI provides a common, extensible architecture for distributed diagnosis that allows experts to improve the system by adding new diagnostic tests and new dependency knowledge.To support distributed diagnosis using new tests and knowledge, CAPRI must overcome several challenges including the extensible representation and communication of diagnostic information, the description of diagnostic agent capabilities, and efficient distributed inference.  Furthermore, the architecture must scale to support diagnosis of a large number of failures using many diagnostic agents.  To address these challenges, this thesis presents a probabilistic approach to diagnosis based on an extensible, distributed component ontology to support the definition of new classes of components and diagnostic tests; a service description language for describing new diagnostic capabilities in terms of their inputs and outputs; and a message processing procedure for dynamically incorporating new information from other agents, selecting diagnostic actions, and inferring a diagnosis using Bayesian inference and belief propagation.To demonstrate the ability of CAPRI to support distributed diagnosis of real-world failures, I implemented and deployed a prototype network of agents on Planetlab for diagnosing HTTP connection failures.  Approximately 10,000 user agents and 40 distributed regional and specialist agents on Planetlab collect information from over 10,000 users and diagnose over 140,000 failures using a wide range of active and passive tests, including DNS lookup tests, connectivity probes, Rockettrace measurements, and user connection histories.  I show how to improve accuracy and cost by learning new dependency knowledge and introducing new diagnostic agents.  I also show that agents can manage the cost of diagnosing many similar failures by aggregating related requests and caching observations and beliefs.",222 p.,,CAPRI: A Common Architecture for Distributed Probabilistic Internet Fault Diagnosis,PhD thesis,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,
Una-May O'Reilly,"Salameh, Lynne Rafik",Humanoid Robotics,2007-06-06T12:21:53Z,2007-06-06T12:21:53Z,2007-06-05,MIT-CSAIL-TR-2007-032,http://hdl.handle.net/1721.1/37597,"Analog designers are interested in optimization tools which automate the process of circuit sizing. Geometric programming, which uses posynomial models of MOSFET parameters, represents one such tool. Genetic algorithms have been used to evolve posynomial models for geometric programs, with a reasonable mean error when modeling MOSFET parameters. By visualizing MOSFET data using two dimensional plots, this thesis investigates the behavior of various MOSFET small and large signal parameters and consequently proposes a lower bound on the maximum error, which a posynomial cannot improve upon. It then investigates various error metrics which can be used to balance the mean and maximum errors generated by posynomial MOSFET models. Finally, the thesis uses empirical data to verify the existence of the lower bound, and compares the maximum error from various parameters modeled by the genetic algorithm and by monomial fitting. It concludes that posynomial MOSFET models suffer from inherent inaccuracies. Additionally, although genetic algorithms improve on the maximum model error, the improvement, in general, does not vastly surpass results obtained through monomial fitting, which is a less computationally intensive method. Genetic algorithms are hence best used when modeling partially convex MOSFET parameters, such as r0 .",90 p,,An Analysis of Posynomial MOSFET Models Using Genetic Algorithms and Visualization,MEng thesis,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,limiations; convex optimization,,,
Peter Szolovits,"Perry, John C.",Clinical Decision-Making,2007-06-13T11:01:54Z,2007-06-13T11:01:54Z,2007-06-12,MIT-CSAIL-TR-2007-033,http://hdl.handle.net/1721.1/37599,"A longstanding controversy in philosophy is whether decision-making isgoverned by reason or emotion.  I study the role of physiologicalresponses in the decision-making process within the realm of financialmarkets, where both the environment and decisions---trades---aremeasurable.In an experiment performed on a regional stock exchange, mycollaborators and I record six different types of physiologicalsignals---skin conductance/galvanic skin response (SCR/GSR), bloodvolume pulse (BVP), electrocardiogram (ECG),electroencephalogram (EEG), electromyogram (EMG), andtemperature (Temp)---of monetarily motivated professionals making highpressure decisions.  From these signals I estimate underlyingphysiological features, such as heart rate,changes in body temperature, and amplitude of SCR, which are proxy foraffect.  Simultaneously, we record real-time market information whichthe specialists process and which serves as the basis for theirdecisions, as well as recording their decisions and outcomes.In a sample of eight market-makers, I find statistically significantdifferences in mean skin conductance response and cardiovascularvariables during transient market events relative to no-market-eventcontrol intervals.  In addition, I find a strong relationship betweentrading decisions and physiological responses.  Using regression, Idemonstrate that heart rate variability can statisticallysignificantly improve predictions of trading decisions, although notby much.",215 p.,,The Psychophysiology of Risk Processing and Decision Making at a Regional Stock Exchange,PhD thesis,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,,,,
