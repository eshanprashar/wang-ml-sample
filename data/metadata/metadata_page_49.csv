dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.title,dc.date.updated,dc.relation.isreplacedby,dc.relation.uri,dc.rights,dc.rights.uri,dc.subject,dc.relation.replaces,dc.contributor,dc.publisher,dc.identifier.citation
Julie A Shah,"Kim, Been; Rudin, Cynthia; Shah, Julie",Interactive Robotics Group,2014-05-27T18:15:05Z,2014-05-27T18:15:05Z,2014-05-26,http://hdl.handle.net/1721.1/87548,"We present a general framework for Bayesian case-based reasoning and prototype classification and clustering -- Latent Case Model (LCM). LCM learns the most representative prototype observations of a dataset by performing joint inference on cluster prototypes and features. Simultaneously, LCM pursues sparsity by learning subspaces, the sets of few features that play important roles in characterizing the prototypes. The prototype and subspace representation preserves interpretability in high dimensional data. We validate the approach preserves classification accuracy on standard data sets, and verify through human subject experiments that the output of LCM produces statistically significant improvements in participants' performance on a task requiring an understanding of clusters within a dataset.",10 p.,MIT-CSAIL-TR-2014-011,Latent Case Model: A Generative Approach for  Case-Based Reasoning and Prototype Classification,2014-05-27T18:15:05Z,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",Theory of Computation,2014-06-10T21:00:02Z,2014-06-10T21:00:02Z,2014-06-09,http://hdl.handle.net/1721.1/87727,"We consider rationality and rationalizability for normal-form games of incomplete information in which the players have possibilistic beliefs about their opponents. In this setting, we prove that the strategies compatible with the players being level-k rational coincide with the strategies surviving a natural k-step iterated elimination procedure. We view the latter strategies as the (level-k) rationalizable ones in our possibilistic setting.",10 p.,MIT-CSAIL-TR-2014-013,Possibilistic Beliefs and Higher-Level Rationality,2014-06-10T21:00:03Z,,,,,,,,,
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",Theory of Computation,2014-06-09T20:15:07Z,2014-06-09T20:15:07Z,2014-06-09,http://hdl.handle.net/1721.1/87710,"We consider rationality and rationalizability for normal-form games of incomplete information in which the players have possibilistic beliefs about their opponents. In this setting, we prove that the strategies compatible with the players being level-k rational coincide with the strategies surviving a natural k-step iterated elimination procedure. We view the latter strategies as the (level-k) rationalizable ones in our possibilistic setting.",10 p.,MIT-CSAIL-TR-2014-012,Possibilistic Beliefs and Higher-Level Rationality,2014-06-09T20:15:07Z,,,,,,,,,
Saman Amarasinghe,"Ding, Yufei; Ansel, Jason; Veeramachaneni, Kalyan; Shen, Xipeng; O'Reilly, Una-May; Amarasinghe, Saman",Computer Architecture,2014-06-23T21:45:03Z,2014-06-23T21:45:03Z,2014-06-23,http://hdl.handle.net/1721.1/88083,"Empirical autotuning is increasingly being used in many domains to achieve optimized performance in a variety of different execution environments. A daunting challenge faced by such autotuners is input sensitivity, where the best autotuned configuration may vary with different input sets. In this paper, we propose a two level solution that: first, clusters to find input sets that are similar in input feature space; then, uses an evolutionary autotuner to build an optimized program for each of these clusters; and, finally, builds an adaptive overhead aware classifier which assigns each input to a specific input optimized program. Our approach addresses the complex trade-off between using expensive features, to accurately characterize an input, and cheaper features, which can be computed with less overhead. Experimental results show that by adapting to different inputs one can obtain up to a 3x speedup over using a single configuration for all inputs.",14 p.,MIT-CSAIL-TR-2014-014,Autotuning Algorithmic Choice for Input Sensitivity,2014-06-23T21:45:03Z,,,,,,,,,
Nancy Lynch,"Cadambe, Viveck R.; Lynch, Nancy; Medard, Muriel; Musial, Peter",Theory of Computation,2014-08-06T18:00:06Z,2014-08-06T18:00:06Z,2014-08-01,http://hdl.handle.net/1721.1/88551,"This paper considers the communication and storage costs of emulating atomic (linearizable) multi-writer multi-reader shared memory in distributed message-passing systems. The paper contains three main contributions: (1) We present a atomic shared-memory emulation algorithm that we call Coded Atomic Storage (CAS). This algorithm uses erasure coding methods. In a storage system with 'N' servers that is resilient to 'f' server failures, we show that the communication cost of CAS is N/(N-2f) . The storage cost of CAS is unbounded. (2) We present a modification of the CAS algorithm known as CAS with Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer 'd' and has a bounded storage cost. We show that in every execution where the number of write operations that are concurrent with a read operation is no bigger than 'd', the CASGC algorithm with parameter 'd' satisfies atomicity and liveness. We explicitly characterize the storage cost of CASGC, and show that it has the same communication cost as CASGC. (3) We describe an algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS) algorithm that achieves a smaller communication cost than CAS and CASGC. In particular, CCOAS incurs read and write communication costs of N/(N-f) measured in terms of number of object values. We also discuss drawbacks of CCOAS as compared with CAS and CASGC.",28 p.,MIT-CSAIL-TR-2014-015,A Coded Shared Atomic Memory Algorithm for Message Passing Architectures,2014-08-06T18:00:06Z,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Piselli, Paolo; Rinard, Martin",Program Analysis,2014-10-22T21:30:07Z,2014-10-22T21:30:07Z,2014-08-11,http://hdl.handle.net/1721.1/91148,"We present pDNA, a system for automatically transferring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using three donor applications to eliminate seven errors in four recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer overflow errors at critical memory allocation sites. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",14 p.,MIT-CSAIL-TR-2014-024,Automatic Error Elimination by Multi-Application Code Transfer,2014-10-22T21:30:07Z,MIT-CSAIL-TR-2014-026,http://hdl.handle.net/1721.1/91150,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,Automatic Program Repair,,,,
Martin Rinard,"Achour, Sara; Rinard, Martin",Computer Architecture,2014-08-19T21:00:06Z,2014-08-19T21:00:06Z,2014-08-19,http://hdl.handle.net/1721.1/88926,"We present Topaz, a new task-based language for computations that execute on approximate computing platforms that may occasionally produce arbitrarily inaccurate results. The Topaz implementation maps approximate tasks onto the approximate machine and integrates the approximate results into the main computation, deploying a novel outlier detection and reliable reexecution mechanism to prevent unacceptably inaccurate results from corrupting the overall computation. Topaz therefore provides the developers of approximate hardware with substantial freedom in producing designs with little or no precision or accuracy guarantees. Experimental results from our set of benchmark applications demonstrate the effectiveness of Topaz and the Topaz implementation in enabling developers to productively exploit emerging approximate hardware platforms.",41 p.,MIT-CSAIL-TR-2014-016,Energy-Efficient Approximate Computation in Topaz,2014-08-19T21:00:07Z,,,,,,,,,
Seth Teller,"Park, Jun-geun; Teller, Seth","Robotics, Vision & Sensor Networks",2014-08-26T20:30:04Z,2014-08-26T20:30:04Z,2014-08-26,http://hdl.handle.net/1721.1/89075,"Indoor localization -- a device's ability to determine its location within an extended indoor environment -- is a fundamental enabling capability for mobile context-aware applications. Many proposed applications assume localization information from GPS, or from WiFi access points. However, GPS fails indoors and in urban canyons, and current WiFi-based methods require an expensive, and manually intensive, mapping, calibration, and configuration process performed by skilled technicians to bring the system online for end users. We describe a method that estimates indoor location with respect to a prior map consisting of a set of 2D floorplans linked through horizontal and vertical adjacencies. Our main contribution is the notion of ""path compatibility,"" in which the sequential output of a classifier of inertial data producing low-level motion estimates (standing still, walking straight, going upstairs, turning left etc.) is examined for agreement with the prior map. Path compatibility is encoded in an HMM-based matching model, from which the method recovers the user s location trajectory from the low-level motion estimates. To recognize user motions, we present a motion labeling algorithm, extracting fine-grained user motions from sensor data of handheld mobile devices. We propose ""feature templates,"" which allows the motion classifier to learn the optimal window size for a specific combination of a motion and a sensor feature function. We show that, using only proprioceptive data of the quality typically available on a modern smartphone, our motion labeling algorithm classifies user motions with 94.5% accuracy, and our trajectory matching algorithm can recover the user's location to within 5 meters on average after one minute of movements from an unknown starting location. Prior information, such as a known starting floor, further decreases the time required to obtain precise location estimate.",14 p.,MIT-CSAIL-TR-2014-017,Motion Compatibility for Indoor Localization,2014-08-26T20:30:04Z,,,,,Indoor localization; Inertial sensing; Motion classification; Trajectory matching; Sensor fusion; Route networks; Conditional random fields; Hidden Markov models,,,,
Daniel Jackson,"Milicevic, Aleksandar; Near, Joseph P.; Kang, Eunsuk; Jackson, Daniel",Software Design,2014-09-03T18:15:05Z,2014-09-03T18:15:05Z,2014-09-02,http://hdl.handle.net/1721.1/89157,"The last decade has seen a dramatic growth in the use of constraint solvers as a computational mechanism, not only for analysis and synthesis of software, but also at runtime. Solvers are available for a variety of logics but are generally restricted to first-order formulas. Some tasks, however, most notably those involving synthesis, are inherently higher order; these are typically handled by embedding a first-order solver (such as a SAT or SMT solver) in a domain-specific algorithm. Using strategies similar to those used in such algorithms, we show how to extend a first-order solver (in this case Kodkod, a model finder for relational logic used as the engine of the Alloy Analyzer) so that it can handle quantifications over higher-order structures. The resulting solver is sufficiently general that it can be applied to a range of problems; it is higher order, so that it can be applied directly, without embedding in another algorithm; and it performs well enough to be competitive with specialized tools on standard benchmarks. Although the approach is demonstrated for a particular relational logic, the principles behind it could be applied to other first-order solvers. Just as the identification of first-order solvers as reusable backends advanced the performance of specialized tools and simplified their architecture, factoring out higher-ordersolvers may bring similar benefits to a new class of tools.",15 p.,MIT-CSAIL-TR-2014-018,Alloy*: A Higher-Order Relational Constraint Solver,2014-09-03T18:15:05Z,,,,,,,,,
Nickolai Zeldovich,"Boyd-Wickizer, Silas; Kaashoek, M. Frans; Morris, Robert; Zeldovich, Nickolai",Parallel and Distributed Operating Systems,2014-09-16T19:30:05Z,2014-09-16T19:30:05Z,2014-09-16,http://hdl.handle.net/1721.1/89653,"Existing techniques (e.g., RCU) can achieve good multi-core scaling for read-mostly data, but for update-heavy data structures only special-purpose techniques exist. This paper presents OpLog, a general-purpose library supporting good scalability for update-heavy data structures. OpLog achieves scalability by logging each update in a low-contention per-core log; it combines logs only when required by a read to the data structure. OpLog achieves generality by logging operations without having to understand them, to ease application to existing data structures. OpLog can further increase performance if the programmer indicates which operations can be combined in the logs. An evaluation shows how to apply OpLog to three update-heavy Linux kernel data structures. Measurements on a 48-core AMD server show that the result significantly improves the performance of the Apache web server and the Exim mail server under certain workloads.",13 p.,MIT-CSAIL-TR-2014-019,OpLog: a library for scaling update-heavy data structures,2014-09-16T19:30:05Z,,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Piselli, Paolo; Rinard, Martin",Program Analysis,2014-10-22T21:30:11Z,2014-10-22T21:30:11Z,2014-09-30,http://hdl.handle.net/1721.1/91149,"We present pDNA, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",15 p.,MIT-CSAIL-TR-2014-025,Automatic Error Elimination by Multi-Application Code Transfer,2014-10-22T21:30:12Z,MIT-CSAIL-TR-2014-026,http://hdl.handle.net/1721.1/91150,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,automatic patching; software self-healing,,,,
Armando Solar-Lezama,"Rose, Eva",Computer-Aided Programming,2014-10-02T21:45:04Z,2014-10-02T21:45:04Z,2014-10-01,http://hdl.handle.net/1721.1/90560,"Our goal is to present a completed, semantic formalization of the Jeeves privacy language evaluation engine, based on the original Jeeves constraint semantics defined by Yang et al at POPL12, but sufficiently strong to support a first complete implementation thereof. Specifically, we present and implement a syntactically and semantically completed concrete syntax for Jeeves that meets the example criteria given in the paper. We also present and implement the associated translation to J, but here formulated by a completed and decompositional operational semantic formulation. Finally, we present an enhanced and decompositional, non-substitutional operational semantic formulation and implementation of the J evaluation engine (the dynamic semantics) with privacy constraints. In particular, we show how implementing the constraints can be defined as a monad, and evaluation can be defined as monadic operation on the constraint environment. The implementations are all completed in Haskell, utilizing its almost one-to-one capability to transparently reflect the underlying semantic reasoning when formalized this way. In practice, we have applied the ""literate"" program facility of Haskell to this report, a feature that enables the source LATEX to also serve as the source code for the implementation (skipping the report-parts as comment regions). The implementation is published as a github project.",56 p.,MIT-CSAIL-TR-2014-020,Constraint Generation for the Jeeves Privacy Language,2014-10-02T21:45:04Z,,,,,,,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",Program Analysis,2014-10-22T21:30:14Z,2014-10-22T21:30:14Z,2014-10-02,http://hdl.handle.net/1721.1/91150,"We present pDNA, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",16 p.,MIT-CSAIL-TR-2014-026,Automatic Error Elimination by Multi-Application Code Transfer,2014-10-22T21:30:14Z,,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,automatic program repair,MIT-CSAIL-TR-2014-025; MIT-CSAIL-TR-2014-024,,,
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Rinard, Martin",Program Analysis,2014-10-02T21:45:09Z,2014-10-02T21:45:09Z,2014-10-02,http://hdl.handle.net/1721.1/90561,"We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of CP to transfer code across applications to eliminate otherwise fatal integer and buffer over- flow errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to eliminate software errors via the successful transfer of correct code across applications.",16 p.,MIT-CSAIL-TR-2014-021,Automatic Error Elimination by Multi-Application Code Transfer,2014-10-02T21:45:09Z,,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,automatic program repair,,,,
Boris Katz,"Borchardt, Gary; Katz, Boris; Nguyen, Hong-Linh; Felshin, Sue; Senne, Ken; Wang, Andy",Infolab,2014-10-08T20:45:02Z,2014-10-08T20:45:02Z,2014-10-08,http://hdl.handle.net/1721.1/90812,"This report describes the Analyst's Assistant, a software system for language-interactive, collaborative user-system interpretation of events, specifically targeting vehicle events that can be recognized on the basis of vehicle track data. The Analyst's Assistant utilizes language not only as a means of interaction, but also as a basis for internal representation of scene information, background knowledge, and results of interpretation. Building on this basis, the system demonstrates emerging intelligent systems techniques related to event recognition, summarization of events, partitioning of subtasks between user and system, and handling of language and graphical references to scene entities during interactive analysis.",73 p.,MIT-CSAIL-TR-2014-022,An Analyst's Assistant for the Interpretation of Vehicle Track Data,2014-10-08T20:45:03Z,,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,
Brian Williams,"Wang, David; Williams, Brian C.",Model-based Embedded and Robotic Systems,2014-10-24T18:15:04Z,2014-10-24T18:15:04Z,2014-10-24,http://hdl.handle.net/1721.1/91170,"Planning for and controlling a network of interacting devices requires a planner that accounts for the automatic timed transitions of devices while meeting deadlines and achieving durative goals. For example, a planner for an imaging satellite with a camera intolerant of exhaust would need to determine that opening a valve causes a chain reaction that ignites the engine, and thus needs to shield its camera. While planners exist that support deadlines and durative goals, currently, no planners can handle automatic timed transitions. We present tBurton, a temporal planner that supports these features while additionally producing a temporally least-commitment plan. tBurton uses a divide and conquer approach: dividing the problem using causal-graph decomposition and conquering each factor with heuristic forward search. The `sub-plans' from each factor are unified in a conflict directed search, guided by the causal graph structure. We describe why tBurton is fast and efficient and present its efficacy on benchmarks from the International Planning Competition.",7 p.,MIT-CSAIL-TR-2014-027,tBurton: A Divide and Conquer Temporal Planner,2014-10-24T18:15:04Z,,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,timed automata; timed concurrent automata; temporal planning; simple temporal network,,,,
,"Feizi, Soheil; Duffy, Ken; Kellis, Manolis; Medard, Muriel",,2014-12-04T18:30:07Z,2014-12-04T18:30:07Z,2014-12-02,http://hdl.handle.net/1721.1/92031,"Several models exist for diffusion of signals across biological, social, or engineered networks. However, the inverse problem of identifying the source of such propagated information appears more difficult even in the presence of multiple network snapshots, and especially for the single-snapshot case, given the many alternative, often similar, progression of diffusion that may lead to the same observed snapshots. Mathematically, this problem can be undertaken using a diffusion kernel that represents diffusion processes in a given network, but computing this kernel is computationally challenging in general. Here, we propose a path-based network diffusion kernel which considers edge-disjoint shortest paths among pairs of nodes in the network and can be computed efficiently for both homogeneous and heterogeneous continuous-time diffusion models. We use this network diffusion kernel to solve the inverse diffusion problem, which we term Network Infusion (NI), using both likelihood maximization and error minimization. The minimum error NI algorithm is based on an asymmetric Hamming premetric function and can balance between false positive and false negative error types. We apply this framework for both single-source and multi-source diffusion, for both single-snapshot and multi-snapshot observations, and using both uninformative and informative prior probabilities for candidate source nodes. We also provide proofs that under a standard susceptible-infected diffusion model, (1) the maximum-likelihood NI is mean-field optimal for tree structures or sufficiently sparse Erdos-Renyi graphs, (2) the minimum-error algorithm is mean-field optimal for regular tree structures, and (3) for sufficiently-distant sources, the multi-source solution is mean-field optimal in the regular tree structure. Moreover, we provide techniques to learn diffusion model parameters such as observation times. We apply NI to several synthetic networks and compare its performance to centrality-based and distance-based methods for Erdos-Renyi graphs, power-law networks, symmetric and asymmetric grids. Moreover, we use NI in two real-world applications. First, we identify the news sources for 3,553 stories in the Digg social news network, and validate our results based on annotated information, that was not provided to our algorithm. Second, we use NI to identify infusion hubs of human diseases, defined as gene candidates that can explain the connectivity pattern of disease-related genes in the human regulatory network. NI identifies infusion hubs of several human diseases including T1D, Parkinson, MS, SLE, Psoriasis and Schizophrenia. We show that, the inferred infusion hubs are biologically relevant and often not identifiable using the raw p-values.",45 p.,,Network Infusion to Infer Information Sources in Networks,2014-12-04T18:30:07Z,,,Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,Manolis Kellis; Computational Biology (Kellis),MIT CSAIL,
,"Gombolay, Matthew; Golen, Toni; Shah, Neel; Shah, Julie",,2014-12-16T22:00:06Z,2014-12-16T22:00:06Z,2014-12-16,http://hdl.handle.net/1721.1/92354,"Labor and Delivery is a complex clinical service requiring the support of highly trained healthcare professionals from Obstetrics, Anesthesiology, and Neonatology and the access to a finite set of valuable resources. In the United States, the rate of cesarean sections on labor floors is approximately twice as high as considered appropriate for patient care. We analyze one month of data from a Boston-area hospital to assess how well the labor and delivery process can be modelled with tools from queueing theory. We find that the labor and delivery process is highly amenable to analysis under queueing theory models. We also investigate the problem of high cesarean section rates and the potential effects of resource utilization of lowering the rate of cesarean section.",10 p.,,Queueing Theory Analysis of Labor & Delivery at a Tertiary Care Center,2014-12-16T22:00:07Z,,,Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,Obstetrics; Healthcare; Queueing Theory; Operations Research,,Julie A Shah; Interactive Robotics Group,MIT CSAIL,
Patrick Winston,"Finlayson, Mark Alan",Genesis,2014-12-30T21:45:10Z,2014-12-30T21:45:10Z,2014-12-30,http://hdl.handle.net/1721.1/92563,"This archive contains supplementary materials for the article titled ""A Survey of Corpora in Computational and Cognitive Narrative Science"" by Mark A. Finlayson, published in the journal *Sprache und Datenverarbeitung*. The archive contains two files. The first file is the raw bibliographic data of the survey, containing 2600+ citations. The second file is a spreadsheet with the coded features of each corpus, plus the analyses that underlie sections 3 & 4 of the paper.",1172839 bytes,,"Supplementary Materials for ""A Survey of Corpora in Computational and Cognitive Narrative Science""",2014-12-30T21:45:10Z,,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,
Nick Roy,"Banerjee, Ashis Gopal; Roy, Nicholas","Robotics, Vision & Sensor Networks",2015-01-21T19:45:08Z,2015-01-21T19:45:08Z,2015-01-21,http://hdl.handle.net/1721.1/93099,"It is challenging to obtain online solutions of large-scale integer linear programming (ILP) problems that occur frequently in slightly different forms during planning for autonomous systems. We refer to such ILP problems as repeated ILP problems. The branch-and-bound (BAB) algorithm is commonly used to solve ILP problems, and a significant amount of computation time is expended in solving numerous relaxed linear programming (LP) problems at the nodes of the BAB trees. We observe that the relaxed LP problems, both within a particular BAB tree and across multiple trees for repeated ILP problems, are similar to each other in the sense that they contain almost the same number of constraints, similar objective function and constraint coefficients, and an identical number of decision variables. We present a boosting tree-based regression technique for learning a set of functions that map the objective function and the constraints to the decision variables of such a system of similar LP problems; this enables us to efficiently infer approximately optimal solutions of the repeated ILP problems. We provide theoretical performance guarantees on the predicted values and demonstrate the effectiveness of the algorithm in four representative domains involving a library of benchmark ILP problems, aircraft carrier deck scheduling, vehicle routing, and vehicle control.",48 p.,MIT-CSAIL-TR-2015-001,Efficiently Solving Repeated Integer Linear Programming Problems by Learning Solutions of Similar Linear Programming Problems using Boosting Trees,2015-01-21T19:45:09Z,,,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,"Combinatorial optimization, linear programming, regression, boosting trees, planning",,,,
