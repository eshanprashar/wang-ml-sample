dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.rights,dc.rights.uri,dc.subject,dc.title,dc.contributor,dc.description.sponsorship,dc.language.rfc3066,dc.format.mimetype,dc.description
Li-Shiuan Peh,"Krishna, Tushar; Beckmann, Bradford M.; Peh, Li-Shiuan; Reinhardt, Steven K.",Computer Architecture,2011-03-14T19:45:24Z,2011-03-14T19:45:24Z,2011-03-14,http://hdl.handle.net/1721.1/61695,"Future many-core chips will require an on-chip network that can support broadcasts and multicasts at good power-performance. A vanilla on-chip network would send multiple unicast packets for each broadcast packet, resulting in latency, throughput and power overheads. Recent research in on-chip multicast support has proposed forking of broadcast/multicast packets within the network at the router buffers, but these techniques are far from ideal, since they increase buffer occupancy which lowers throughput, and packets incur delay and power penalties at each router. In this work, we analyze an ideal broadcast mesh; show the substantial gaps between state-of-the-art multicast NoCs and the ideal; then propose BOOM, which comprises a WHIRL routing protocol that ideally load balances broadcast traffic, a mXbar multicast crossbar circuit that enables multicast traversal at similar energy-delay as unicasts, and speculative bypassing of buffering for multicast flits. Together, they enable broadcast packets to approach the delay, energy, and throughput of the ideal fabric. Our simulations show BOOM realizing an average network latency that is 5% off ideal, attaining 96% of ideal throughput, with energy consumption that is 9% above ideal. Evaluations using synthetic traffic show BOOM achieving a latency reduction of 61%, throughput improvement of 63%, and buffer power reduction of 80% as compared to a baseline broadcast. Simulations with PARSEC benchmarks show BOOM reducing average request and network latency by 40% and 15% respectively.",12 p.,MIT-CSAIL-TR-2011-013,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,multicore,BOOM: Broadcast Optimizations for On-chip Meshes,,,,,
Arvind,"Newton, Ryan; Chen, Chih-Ping; Marlow, Simon",Computation Structures,2011-03-22T17:00:16Z,2011-03-22T17:00:16Z,2011-03-22,http://hdl.handle.net/1721.1/61759,"Intel Concurrent Collections (CnC) is a parallel programming model in which a network of steps (functions) communicate through message-passing as well as a limited form of shared memory. This paper describes a new implementation of CnC for Haskell. Compared to existing parallel programming models for Haskell, CnC occupies a useful point in the design space: pure and deterministic like Evaluation Strategies, but more explicit about granularity and the structure of the parallel computation, which affords the programmer greater control over parallel performance. We present results on 4, 8, and 32-core machines demonstrating parallel speedups over 20x on non-trivial benchmarks.",21 p.,MIT-CSAIL-TR-2011-015,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,dataflow computation; task graphs; I-structures,Intel Concurrent Collections for Haskell,Arvind; Computation Structures,,,,
Anant Agarwal,"Hoffmann, Henry; Maggio, Martina; Santambrogio, Marco D.; Leva, Alberto; Agarwal, Anant",Computer Architecture,2011-03-24T21:15:14Z,2011-03-24T21:15:14Z,2011-03-24,http://hdl.handle.net/1721.1/61950,"This paper presents SEEC, a self-aware programming model, designed to reduce programming effort in modern multicore systems. In the SEEC model, application programmers specify application goals and progress, while systems programmers separately specify actions system software and hardware can take to affect an application (e.g. resource allocation). The SEEC runtime monitors applications and dynamically selects actions to meet application goals optimally (e.g. meeting performance while minimizing power consumption). The SEEC runtime optimizes system behavior for the application rather than requiring the application programmer to optimize for the system. This paper presents a detailed discussion of the SEEC model and runtime as well as several case studies demonstrating their benefits. SEEC is shown to optimize performance per Watt for a video encoder, find optimal resource allocation for an application with complex resource usage, and maintain the goals of multiple applications in the face of environmental fluctuations.",14 p.,MIT-CSAIL-TR-2011-016,,,"Self-adaptive, self-optimizing, self-tuning, self-*, power-aware",SEEC: A Framework for Self-aware Management of Multicore Resources,,,,,
Anant Agarwal,"Lau, Eric; Miller, Jason E; Choi, Inseok; Yeung, Donald; Amarasinghe, Saman; Agarwal, Anant",Computer Architecture,2011-03-25T21:15:08Z,2011-03-25T21:15:08Z,2011-03-25,http://hdl.handle.net/1721.1/61978,"As the push for parallelism continues to increase the number of cores on a chip, and add to the complexity of system design, the task of optimizing performance at the application level becomes nearly impossible for the programmer. Much effort has been spent on developing techniques for optimizing performance at runtime, but many techniques for modern processors employ the use of speculative threads or performance counters. These approaches result in stolen cycles, or the use of an extra core, and such expensive penalties put demanding constraints on the gains provided by such methods. While processors have grown in power and complexity, the technology for small, efficient cores has emerged. We introduce the concept of Partner Cores for maximizing hardware power efficiency; these are low-area, low-power cores situated on-die, tightly coupled to each main processor core. We demonstrate that such cores enable performance improvement without incurring expensive penalties, and carry out potential applications that are impossible on a traditional chip multiprocessor.",7 p.,MIT-CSAIL-TR-2011-017,,,self-aware; adaptive,Multicore Performance Optimization Using Partner Cores,,,,,
Nickolai Zeldovich,"Boneh, Dan; Mazieres, David; Popa, Raluca Ada",Parallel and Distributed Operating Systems,2011-03-31T20:15:08Z,2011-03-31T20:15:08Z,2011-03-30,http://hdl.handle.net/1721.1/62006,"Remote storage of data has become an increasingly attractive and advantageous option, especially due to cloud systems. While encryption protects the data, it does not hide the access pattern to the data. A natural solution is to access remote storage using an Oblivious RAM (ORAM) which provably hides all access patterns. While ORAM is asymptotically efficient, the best existing scheme (Pinkas and Reinman, Crypto'10) still has considerable overhead for a practical implementation: for M stored items, it stores 4 times and sometimes 6 times more items remotely, requires O(log2 M) round trips to storage server per request, and periodically blocks all data requests to shuffle all storage (which is a lengthy process). In this paper, we first define a related notion to ORAM, oblivious storage (OS), which captures more accurately and naturally the security setting of remote storage. Then, we propose a new ORAM/OS construction that solves the practicality issues just outlined: it has a storage constant of ~ 1, achieves O(1) round trips to the storage server per request, and allows requests to happen concurrently with shuffle without jeopardizing security. Our construction consists of a new organization of server memory into a flat main part and a hierarchical shelter, a client-side index for rapidly locating identifiers at the server, a new shelter serving requests concurrent with the shuffle, and a data structure for locating items efficiently in a partially shuffled storage.",18 p.,MIT-CSAIL-TR-2011-018,,,access patterns; data privacy,Remote Oblivious Storage: Making Oblivious RAM Practical,,,,,
Anant Agarwal,"Maggio, Martina; Hoffmann, Henry; Santambrogio, Marco D.; Agarwal, Anant; Leva, Alberto",Computer Architecture,2011-04-01T19:30:09Z,2011-04-01T19:30:09Z,2011-04-01,http://hdl.handle.net/1721.1/62020,"Autonomic computing systems are capable of adapting their behavior and resources thousands of times a second to automatically decide the best way to accomplish a given goal despite changing environmental conditions and demands. Different decision mechanisms are considered in the literature, but in the vast majority of the cases a single technique is applied to a given instance of the problem. This paper proposes a comparison of some state of the art approaches for decision making, applied to a self-optimizing autonomic system that allocates resources to a software application, which provides direct performance feedback at runtime. The Application Heartbeats framework is used to provide the sensor data (feedback), and a variety of decision mechanisms, from heuristics to control-theory and machine learning, are investigated. The results obtained with these solutions are compared by means of case studies using standard benchmarks.",10 p.,MIT-CSAIL-TR-2011-019,,,,A Comparison of Autonomic Decision Making Techniques,,,,,
Fredo Durand,"Levin, Anat; Weiss, Yair; Durand, Fredo; Freeman, William T.",Vision,2011-04-04T15:45:25Z,2011-04-04T15:45:25Z,2011-04-04,http://hdl.handle.net/1721.1/62035,"In blind deconvolution one aims to estimate from an input blurred image y a sharp image x and an unknown blur kernel k. Recent research shows that a key to success is to consider the overall shape of the posterior distribution p(x, k|y) and not only its mode. This leads to a distinction between MAPx,k strategies which estimate the mode pair x, k and often lead to undesired results, and MAPk strategies which select the best k while marginalizing over all possible x images. The MAPk principle is significantly more robust than the MAPx,k one, yet, it involves a challenging marginalization over latent images. As a result, MAPk techniques are considered complicated, and have not been widely exploited. This paper derives a simple approximated MAPk algorithm which involves only a modest modification of common MAPx,k algorithms. We show that MAPk can, in fact, be optimized easily, with no additional computational complexity.",12 p.,MIT-CSAIL-TR-2011-020,,,,Efficient Marginal Likelihood Optimization in Blind Deconvolution,,,,,
Nancy Lynch,"Radeva, Tsvetomira; Lynch, Nancy",Theory of Computation,2011-04-21T18:15:25Z,2011-04-21T18:15:25Z,2011-04-14,http://hdl.handle.net/1721.1/62295,"Partial Reversal (PR) is a link reversal algorithm which ensures that the underlying graph structure is destination-oriented and acyclic. These properties of PR make it useful in routing protocols and algorithms for solving leader election and mutual exclusion. While proofs exist to establish the acyclicity property of PR, they rely on assigning labels to either the nodes or the edges in the graph. In this work we present simpler direct proof of the acyclicity property of partial reversal without using any external or dynamic labeling mechanism. First, we provide a simple variant of the PR algorithm, and show that it maintains acyclicity. Next, we present a binary relation which maps the original PR algorithm to the new algorithm, and finally, we conclude that the acyclicity proof applies to the original PR algorithm as well.",20 p.,MIT-CSAIL-TR-2011-022,,,Partial Reversal; Link Reversal; Graph Algorithms,Partial Reversal Acyclicity,,,,,
Tomaso Poggio,"Chikkerur, Sharat; Poggio, Tomaso",Center for Biological and Computational Learning (CBCL),2011-04-21T18:15:06Z,2011-04-21T18:15:06Z,2011-04-14,http://hdl.handle.net/1721.1/62293,"The HMAX model is a biologically motivated architecture for computer vision whose components are in close agreement with existing physiological evidence. The model is capable of achieving close to human level performance on several rapid object recognition tasks. However, the model is computationally bound and has limited engineering applications in its current form. In this report, we present several approximations in order to increase the efficiency of the HMAX model. We outline approximations at several levels of the hierarchy and empirically evaluate the trade-offs between efficiency and accuracy. We also explore ways to quantify the representation capacity of the model.",12 p.,MIT-CSAIL-TR-2011-021; CBCL-298,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,"object recognition, approximation",Approximations in the HMAX Model,,,,,
Hal Abelson,"Kagal, Lalana; Jacobi, Ian; Khandelwal, Ankesh",Decentralized Information Group,2011-04-21T18:15:15Z,2011-04-21T18:15:15Z,2011-04-16,http://hdl.handle.net/1721.1/62294,"The Semantic Web is a distributed model for publishing, utilizing and extending structured information using Web protocols. One of the main goals of this technology is to automate the retrieval and integration of data and to enable the inference of interesting results. This automation requires logics and rule languages that make inferences, choose courses of action, and answer questions. The openness of the Web, however, leads to several issues including the handling of inconsistencies, integration of diverse information, and the determination of the quality and trustworthiness of the data. AIR is a Semantic Web-based rule language that provides this functionality while focusing on generating and tracking explanations for its inferences and actions as well as conforming to Linked Data principles. AIR supports Linked Rules, which allow rules to be combined, re-used and extended in a manner similar to Linked Data. Additionally, AIR explanations themselves are Semantic Web data so they can be used for further reasoning. In this paper we present an overview of AIR, discuss its potential as a Web rule language by providing examples of how its features can be leveraged for different inference requirements, and describe how justifications are represented and generated.",10 p.,MIT-CSAIL-TR-2011-023,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,Gasping for AIR   Why we need Linked Rules and Justifications on the Semantic Web,,"This material is based upon work supported by the National Science Foundation under Award No. CNS-0831442, by the Air Force Office of Scientific Research under Award No. FA9550-09-1-0152, and by Intelligence Advanced Research Projects Activity under Award No. FA8750-07-2-0031.",,,
Silvio Micali,"Chen, Jinc; Micali, Silvio",Theory of Computation,2011-04-22T18:45:10Z,2011-04-22T18:45:10Z,2011-04-22,http://hdl.handle.net/1721.1/62301,"Fifty years ago, Vickrey published his famous mechanism for auctioning a single good in limited supply. The main property of Vickrey's mechanism is efficiency in dominant strategies. In absence of collusion, this is a wonderful efficiency guarantee. We note, however, that collusion is far from rare in auctions, and if some colluders exist and have some wrong beliefs, then the Vickrey mechanism dramatically loses its efficiency. Accordingly, we put forward a new mechanism that, despite unconstrained collusion, guarantees efficiency by providing a richer set of strategies and ensuring that it is dominant for every player to reveal truthfully not only his own valuation, but also with whom he is colluding, if he is indeed colluding with someone else. Our approach meaningfully bypasses prior impossibility proofs.",10 p.,MIT-CSAIL-TR-2011-025,,,Collusion; Auctions; Efficiency,Collusive Dominant-Strategy Truthfulness,,,,,
,"Jayaraman, Karthick; Ganesh, Vijay; Tripunitara, Mahesh; Rinard, Martin C.; Chapin, Steve J.",Program Analysis,2011-04-28T20:30:11Z,2011-04-28T20:30:11Z,2011-04-27,http://hdl.handle.net/1721.1/62562,"Administrative role-based access control (ARBAC) is the first comprehensive administrative model proposed for role-based access control (RBAC). ARBAC has several features for designing highly expressive policies, but current work has not highlighted the utility of these expressive policies. In this report, we present a case study of designing an ARBAC policy for a bank comprising 18 branches. Using this case study we provide an assessment about the features of ARBAC that are likely to be used in realistic policies.",6 p.,MIT-CSAIL-TR-2011-026,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,ARBAC policy; computer security; access control,ARBAC Policy for a Large Multi-National Bank,,,en-US,,
Srini Devadas,"Shim, Keun Sup; Cho, Myong Hyon; Lis, Mieszko; Khan, Omer; Devadas, Srinivas",Computation Structures,2011-05-03T21:15:13Z,2011-05-03T21:15:13Z,2011-05-02,http://hdl.handle.net/1721.1/62580,"Directory-based cache coherence is a popular mechanism for chip multiprocessors and multicores. The directory protocol, however, requires multicast for invalidation messages and the collection of acknowledgement messages, which can be expensive in terms of latency and network traffic. Furthermore, the size of the directory increases with the number of cores. We present Library Cache Coherence (LCC), which requires neither broadcast/multicast for invalidations nor waiting for invalidation acknowledgements. A library is a set of timestamps that are used to auto-invalidate shared cache lines, and delay writes on the lines until all shared copies expire. The size of library is independent of the number of cores. By removing the complex invalidation process of directory-based cache coherence protocols, LCC generates fewer network messages. At the same time, LCC also allows reads on a cache block to take place while a write to the block is being delayed, without breaking sequential consistency. As a result, LCC has 1.85X less average memory latency than a MESI directory-based protocol on our set of benchmarks, even with a simple timestamp choosing algorithm; moreover, our experimental results on LCC with an ideal timestamp scheme (though not implementable) show the potential of further improvement for LCC with more sophisticated timestamp schemes.",6 p.,MIT-CSAIL-TR-2011-027,,,multicore; cache coherence,Library Cache Coherence,,,en-US,,
David Clark,"Heikkinen, Mikko V. J.; Berger, Arthur W.",Advanced Network Architecture,2011-05-03T21:15:05Z,2011-05-03T21:15:05Z,2011-05-03,http://hdl.handle.net/1721.1/62579,"We compare Web traffic characteristics of mobile- versus fixed-access end-hosts, where herein the term ""mobile"" refers to access via cell towers, using for example the 3G/UMTS standard, and the term ""fixed"" includes Wi-Fi access. It is well-known that connection speeds are in general slower over mobile-access networks, and also that often there is higher packet loss. We were curious whether this leads mobile-access users to have smaller connections. We examined the distribution of the number of bytes-per-connection, and packet loss from a sampling of logs from servers of Akamai Technologies. We obtained 149 million connections, across 57 countries. The mean bytes-per-connection was typically larger for fixed-access: for two-thirds of the countries, it was at least one-third larger. Regarding distributions, we found that the difference between the bytes-per-connection for mobile- versus fixed-access, as well as the packet loss, was statistically significant for each of the countries; however the visual difference in plots is typically small. For some countries, mobile-access had the larger connections. As expected, mobile-access often had higher loss than fixed-access, but the reverse pertained for some countries. Typically packet loss increased during the busy period of the day, when mobile-access had a larger increase. Comparing our results from 2010 to those from 2009 of the same time period, we found that connections have become a bit smaller.",20 p.,MIT-CSAIL-TR-2011-028,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,Comparison of User Traffic Characteristics on Mobile-Access versus Fixed-Access Networks,,,en-US,,
Patrick Winston,"Finlayson, Mark Alan; Kulkarni, Nidhi",Genesis,2011-05-09T19:30:09Z,2011-05-09T19:30:09Z,2011-05-09,http://hdl.handle.net/1721.1/62792,"Contains the source code and data necessary to run all computations described in the following two papers: Finlayson, Mark A. and Kulkarni, Nidhi (2011) ""Detecting Multi-Word Expressions improves Word Sense Disambiguation"", in Proceedings of the 2011 Workshop on Multiword Expressions, held at ACL'2011 in Portland, OR; Kulkarni, Nidhi and Finlayson, Mark A. (2011) ""jMWE: A Java Toolkit for Detecting Multi-Word Expressions"" in Proceedings of the 2011 Workshop on Multiword Expressions, held at ACL'2011 in Portland, OR.",45241266 bytes,,,,Multi-word expressions; MWE; Java; Supplementary material,Source code and data for MWE'2011 papers,,,,application/zip,
David Clark,"García, Rubén",Advanced Network Architecture,2011-05-11T07:15:47Z,2011-05-11T07:15:47Z,2011-05-10,http://hdl.handle.net/1721.1/62812,"Supplemental materials for the master thesis ""Understanding the Performance of Broadband Networks Through the Statistical Analysis of Speed Tests"", by Rubén García, submitted in May 2011 for the S.M. in Technology and Policy. Supplemental materials include: Source_code: Folder containing the source code for the statistical analysis of NDT speed tests, written for the R statistical package; NDT_data: Folder containing the following datasets (1) ndt4.h5: Initial NDT data that we used for the analysis; (2) ndt3.h5: Reduced version of the ndt4 dataset (same tests but less variables), also contains the 'whois' file that we combine with the NDT data in order to add location information; (3) comcast-ndt.h5: dataset containing the speed tests of a controlled experiment that we ran using different test durations; Aggregated_datasets: Versions of the ndt4.h5 dataset aggregated by IP and by Autonomous System.",241484228 bytes,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,Web100; NDT,Understanding the Performance of Broadband Networks through the Statistical Analysis of Speed Tests - Supplemental materials,,,,,
Tomaso Poggio,"Mroueh, Youssef; Poggio, Tomaso; Rosasco, Lorenzo",Center for Biological and Computational Learning (CBCL),2011-06-03T15:15:06Z,2011-06-03T15:15:06Z,2011-06-03,http://hdl.handle.net/1721.1/63175,In this work we discuss a regularization framework to solve multi-category when the classes are described by an underlying class taxonomy. In particular we discuss how to learn the class taxonomy while learning a multi-category classifier.,5 p.,MIT-CSAIL-TR-2011-029; CBCL-299,,,computational learning; classification,Regularization Predicts While Discovering Taxonomy,,"This research was sponsored by grants from DARPA (IPTO and DSO), National Science Foundation (NSF-0640097, NSF-0827427), AFSOR-THRL (FA8650-05-C-7262) Additional support was provided by: Adobe, Honda Research Institute USA, King Abdullah University Science and Technology grant to B. DeVore, NEC, Sony and especially by the Eugene McDermott Foundation",en-US,,
Karen Sollins,"Guo, Nina X.",Advanced Network Architecture,2011-06-07T21:15:04Z,2011-06-07T21:15:04Z,2011-06-07,http://hdl.handle.net/1721.1/63260,"This thesis analyzes scalable information-sharing network management. It looks into one of the large problems in network management today: finding information across different network domains. Information-sharing network management is a method to solving the problem, though it is important to make it scalable. The solution proposed uses the Publish-Subscribe Internet Routing Paradigm (PSIRP) inter-domain design as the base structure. The design borrows from Border Gateway Protocol ideas and uses the Chord protocol as one of the key methods of finding information. The conclusion after analyzing the scalability of PSIRP is that its use of Chord gives it an advantage that allows a O(log^2 N) tradeoff between performance and distribution.",56 p.,MIT-CSAIL-TR-2011-030,,,"network management, scalability, information-centric networking, inter-domain routing",Scalable Information-Sharing Network Management,,,en-US,,MEng thesis
Tomaso Poggio,"Isik, Leyla; Leibo, Joel Z.; Mutch, Jim; Lee, Sang Wan; Poggio, Tomaso",Center for Biological and Computational Learning (CBCL),2011-06-20T19:45:08Z,2011-06-20T19:45:08Z,2011-06-17,http://hdl.handle.net/1721.1/64621,"We present a peripheral vision model inspired by the cortical architecture discovered by Hubel and Wiesel. As with existing cortical models, this model contains alternating layers of simple cells, which employ tuning functions to increase specificity, and complex cells, which pool over simple cells to increase invariance. To extend the traditional cortical model, we introduce the option of eccentricity-dependent pooling and tuning parameters within a given model layer. This peripheral vision system can be used to model physiological data where receptive field sizes change as a function of eccentricity. This gives the user flexibility to test different theories about filtering and pooling ranges in the periphery. In a specific instantiation of the model, pooling and tuning parameters can increase linearly with eccentricity to model physiological data found in different layers of the visual cortex. Additionally, it can be used to introduce pre-cortical model layers such as retina and LGN. We have tested the model s response with different parameters on several natural images to demonstrate its effectiveness as a research tool. The peripheral vision model presents a useful tool to test theories about crowding, attention, visual search, and other phenomena of peripheral vision.",13 p.,MIT-CSAIL-TR-2011-031; CBCL-300,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,"peripheral vision, computational neuroscience, HMAX",A hierarchical model of peripheral vision,,"This work was supported by the following grants: NSF-0640097, NSF-0827427, NSF-0645960, DARPA-DSO, AFSOR FA8650-50-C-7262, AFSOR FA9550-09-1-0606.",en-US,,
Robert Morris,"Boyd-Wickizer, Silas; Kaashoek, M. Frans; Morris, Robert; Zeldovich, Nickolai",Parallel and Distributed Operating Systems,2011-06-28T21:45:22Z,2011-06-28T21:45:22Z,2011-06-28,http://hdl.handle.net/1721.1/64698,"Multicore chips will have large amounts of fast on-chip cache memory, along with relatively slow DRAM interfaces. The on-chip cache memory, however, will be fragmented and spread over the chip; this distributed arrangement is hard for certain kinds of applications to exploit efficiently, and can lead to needless slow DRAM accesses. First, data accessed from many cores may be duplicated in many caches, reducing the amount of distinct data cached. Second, data in a cache distant from the accessing core may be slow to fetch via the cache coherence protocol. Third, software on each core can only allocate space in the small fraction of total cache memory that is local to that core. A new approach called software cache unification (SCU) addresses these challenges for applications that would be better served by a large shared cache. SCU chooses the on-chip cache in which to cache each item of data. As an application thread reads data items, SCU moves the thread to the core whose on-chip cache contains each item. This allows the thread to read the data quickly if it is already on-chip; if it is not, moving the thread causes the data to be loaded into the chosen on-chip cache. A new file cache for Linux, called MFC, uses SCU to improve performance of file-intensive applications, such as Unix file utilities. An evaluation on a 16-core AMD Opteron machine shows that MFC improves the throughput of file utilities by a factor of 1.6. Experiments with a platform that emulates future machines with less DRAM throughput per core shows that MFC will provide benefit to a growing range of applications.",13 p.,MIT-CSAIL-TR-2011-032,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,A Software Approach to Unifying Multicore Caches,,"This material is based upon work supported by the National Science
Foundation under grant number 0915164.",en-US,,
