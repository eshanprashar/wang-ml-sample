dc.contributor.advisor,dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.subject,dc.title,dc.date.updated,dc.rights,dc.rights.uri,dc.description,dc.relation.replaces,dc.relation.uri,dc.relation.isreplacedby,dc.identifier.citation
Hari Balakrishnan,"Deng, Shuo; Sivaraman, Anirudh; Balakrishnan, Hari",Networks & Mobile Systems,2016-03-09T00:00:06Z,2016-03-09T00:00:06Z,2016-02-25,http://hdl.handle.net/1721.1/101636,"This paper presents Delphi, a mobile software controller that helps applications select the best network among available choices for their data transfers. Delphi optimizes a specified objective such as transfer completion time, or energy per byte transferred, or the monetary cost of a transfer. It has four components: a performance predictor that uses features gathered by a network monitor, and a traffic profiler to estimate transfer sizes near the start of a transfer, all fed into a network selector that uses the prediction and transfer size estimate to optimize an objective.For each transfer, Delphi either recommends the  best  single network to use, or recommends Multi-Path TCP (MPTCP), but crucially selects the network for MPTCP s  primary subflow . The choice of primary subflow has a strong impact onthe transfer completion time, especially for short transfers.We designed and implemented Delphi in Linux. It requires no application modifications. Our evaluation shows that Delphi reduces application network transfer time by 46% for Web browsing and by 49% for video streaming, comparedwith Android s default policy of always using Wi-Fi when it is available. Delphi can also be configured to achieve high throughput while being battery-efficient: in this configuration, it achieves 1.9x the throughput of Android s default policy while only consuming 6% more energy.",14 p.,MIT-CSAIL-TR-2016-004,MPTCP; Network Selection; Wi-Fi; LTE,Delphi: A Software Controller for Mobile Network Selection,2016-03-09T00:00:06Z,,,,,,,
Leslie Kaelbling,"Kawaguchi, Kenji",Learning and Intelligent Systems,2016-05-24T20:45:08Z,2016-05-24T20:45:08Z,2016-05-23,http://hdl.handle.net/1721.1/102665,"In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.",26 p.,MIT-CSAIL-TR-2016-005,Optimization; Neural Network; Machine Learning; High Dimension; Convex; Non-convex; Local minimum; Global minimum; Saddle point; Critical point,Deep Learning without Poor Local Minima,2016-05-24T20:45:08Z,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,
Leslie Kaelbling,"Kawaguchi, Kenji",Learning and Intelligent Systems,2016-06-01T22:00:14Z,2016-06-01T22:00:14Z,2016-05-26,http://hdl.handle.net/1721.1/102796,"This thesis discusses novel principles to improve the theoretical analyses of a class of methods, aiming to provide theoretically driven yet practically useful methods. The thesis focuses on a class of methods, called bound-based search, which includes several planning algorithms (e.g., the A* algorithm and the UCT algorithm), several optimization methods (e.g., Bayesian optimization and Lipschitz optimization), and some learning algorithms (e.g., PAC-MDP algorithms). For Bayesian optimization, this work solves an open problem and achieves an exponential convergence rate. For learning algorithms, this thesis proposes a new analysis framework, called PAC-RMDP, and improves the previous theoretical bounds. The PAC-RMDP framework also provides a unifying view of some previous near-Bayes optimal and PAC-MDP algorithms. All proposed algorithms derived on the basis of the new principles produced competitive results in our numerical experiments with standard benchmark tests.",87 p.,MIT-CSAIL-TR-2016-006,PAC-MDP; AI planning; Global optimization,Towards Practical Theory: Bayesian Optimization and Optimal Exploration,2016-06-01T22:00:15Z,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,SM thesis,,,,
Karen Sollins,"Jing, Yuxin",Advanced Network Architecture,2016-06-28T22:15:02Z,2016-06-28T22:15:02Z,2016-06-28,http://hdl.handle.net/1721.1/103381,"This thesis seeks to test and evaluate the effects of in-­network storage in novel proposed Internet architectures in terms of their performance. In a world where more and more people are mobile and connected to the Internet, we look at how the added variable of user mobility can affect how these architectures perform under different loads. Evaluating the effects of in­-network storage and caching in these novel architectures will provide another facet to understanding how viable of an alternative they would be to the current TCP/IP paradigm of today's Internet. In Named Data Networking, where the storage is used to directly cache content, we see its use of storage impact the locality of where things are, while in MobilityFirst, where storage is used to cache chunks to provide robust delivery, we look at how its different layers work together in a mobility event.",58 p.,MIT-CSAIL-TR-2016-009,in-network storage; mobility; scalability,Evaluating Caching Mechanisms In Future Internet Architectures,2016-06-28T22:15:02Z,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International,http://creativecommons.org/licenses/by-nc-sa/4.0/,MEng thesis,,,,
Karen Sollins,"Xu, Shidan",Advanced Network Architecture,2016-06-28T21:30:08Z,2016-06-28T21:30:08Z,2016-06-28,http://hdl.handle.net/1721.1/103379,"This project involves learning to predict users' mobility within the network topology. Topological mobility, as opposed to physical mobility, can be substantial as a user switches from LTE to wifi network, while moving minimally physically. Our dataset consists of email IMAP logs as they document associated client IP addresses, as well as the clients' identifiers. Prediction for online mobility is of particular interest to the networks community. If we can predict online mobility with high probability, then new network architecture can be designed to optimize the caching system by minimizing resending packets. We used various approaches and techniques to model the user's behavior, including probabilistic programming, regression, neural nets, and clustering algorithms. We compare and contrast how models differ in their prediction accuracy, speed of convergence, and algorithmic complexity.",60 p.,MIT-CSAIL-TR-2016-007,,Modeling Network User Behavior: Various Approaches,2016-06-28T21:30:09Z,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,MEng thesis,,,,
Martin Rinard,"Long, Fan; Amidon, Peter; Rinard, Martin",Program Analysis and Compilation,2017-05-02T21:45:03Z,2017-05-02T21:45:03Z,2016-07-08,http://hdl.handle.net/1721.1/108619,"We present a new system, Genesis, that processes sets of human patches to automatically infer code transforms and search spaces for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the resulting complete Genesis patch generation system working with real-world patches and errors collected from top 1000 github Java software development projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from successful patches.",26 p.,MIT-CSAIL-TR-2017-008,,Automatic Inference of Code Transforms and Search Spaces for Automatic Patch Generation Systems,2017-05-02T21:45:04Z,,,,MIT-CSAIL-TR-2016-010,http://hdl.handle.net/1721.1/103556,,
Martin Rinard,"Long, Fan; Amidon, Peter; Rinard, Martin",Program Analysis and Compilation,2016-07-08T20:15:07Z,2016-07-08T20:15:07Z,2016-07-08,http://hdl.handle.net/1721.1/103556,"We present a new system, Genesis, that processes sets of human patches to automatically infer code transforms and search spaces for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the resulting complete Genesis patch generation system working with real-world patches and errors collected from top 1000 github Java software development projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from successful patches.",14 p.,MIT-CSAIL-TR-2016-010,,Automatic Inference of Code Transforms and Search Spaces for Automatic Patch Generation Systems,2016-07-08T20:15:08Z,,,,,http://hdl.handle.net/1721.1/108619,MIT-CSAIL-TR-2017-008,
Hari Balakrishnan,"Perry, Jonathan; Balakrishnan, Hari; Shah, Devavrat",Networks & Mobile Systems,2016-08-15T20:00:07Z,2016-08-15T20:00:07Z,2016-08-15,http://hdl.handle.net/1721.1/103920,"Rapid convergence to a desired allocation of network resources to endpoint traffic has been a long-standing challenge for packet-switched networks. The reason for this is that congestion control decisions are distributed across the endpoints, which vary their offered load in response to changes in application demand and network feedback on a packet-by-packet basis. We propose a different approach for datacenter networks, flowlet control, in which congestion control decisions are made at the granularity of a flowlet, not a packet. With flowlet control, allocations have to change only when flowlets arrive or leave. We have implemented this idea in a system called Flowtune using a centralized allocator that receives flowlet start and end notifications from endpoints. The allocator computes optimal rates using a new, fast method for network utility maximization, and updates endpoint congestion-control parameters. Experiments show that Flowtune outperforms DCTCP, pFabric, sfqCoDel, and XCP on tail packet delays in various settings, converging to optimal rates within a few packets rather than over several RTTs. Our implementation of Flowtune handles 10.4x more throughput per core and scales to 8x more cores than Fastpass, for an 83-fold throughput gain.",15 p.,MIT-CSAIL-TR-2016-011,,Flowtune: Flowlet Control for Datacenter Networks,2016-08-15T20:00:08Z,,,,,,,
Karen Sollins,"Rock, Colleen T.",Advanced Network Architecture,2016-09-26T22:15:03Z,2016-09-26T22:15:03Z,2016-09-26,http://hdl.handle.net/1721.1/104385,"The problem we address in this thesis is to uncover the design elements in a network architecture design that may open it up to denial of service (DoS) attacks and to expose the tradeoffs in mitigating those DoS opportunities. We take as our candidate network architecture design the Future Internet Architecture project MobilityFirst. MobilityFirst's overarching goal, driven by increasingly available wireless communication, is the support of mobility in an Internet architecture. At its core, MobilityFirst separates identification from location, as distinct from the current Internet architecture, and postulates the existence of globally unique, flat identifiers. In order to support mobility in this context, it also postulates a global name resolution service (GNRS). In this thesis we examine three alternative designs for the GNRS and the opportunities they expose for DoS attacks. We consider each one in depth analytically. As an example, we then study one particular attack in depth and are forced to conclude that approaches to mitigating this attack would have significant negative impact on the support of mobility thus exposing the dilemma in such system design tradeoffs.",68 p.,MIT-CSAIL-TR-2016-012,DMap; GMap; Auspice,Examining Key Mobility Resources through Denial of Service Attacks on proposed Global Name Resolution Services,2016-09-26T22:15:03Z,,,MEng thesis,,,,
Nickolai Zeldovich,"Lazar, David; Zeldovich, Nickolai",Parallel and Distributed Operating Systems,2016-10-26T16:00:07Z,2016-10-26T16:00:07Z,2016-10-05,http://hdl.handle.net/1721.1/105093,"Alpenhorn is the first system for initiating an encrypted connection between two users that provides strong privacy and forward secrecy guarantees for metadata (i.e., information about which users connected to each other) and that does not require out-of-band communication other than knowing the other user's Alpenhorn username (email address). This resolves a significant shortcoming in all prior works on private messaging, which assume an out-of-band key distribution mechanism. Alpenhorn's design builds on three ideas. First, Alpenhorn provides each user with an address book of friends that the user can call to establish a connection. Second, when a user adds a friend for the first time, Alpenhorn ensures the adversary does not learn the friend's identity, by using identity-based encryption in a novel wayto privately determine the friend's public key. Finally, when calling a friend, Alpenhorn ensures forward secrecy of metadata by storing pairwise shared secrets in friends' address books, and evolving them over time, using a new keywheel construction. Alpenhorn relies on a number of servers, but operates in an anytrust model, requiring just one of the servers to be honest. We implemented a prototype of Alpenhorn, and integrated it into the Vuvuzela private messaging system (which did not previously provide privacy or forward secrecy of metadata when initiating conversations). Experimental results show that Alpenhorn can scale to many users, supporting 10 million users on three Alpenhorn servers with an average call latency of 150 seconds and a client bandwidth overhead of 3.7 KB/sec.",17 p.,MIT-CSAIL-TR-2016-013,,Alpenhorn: Bootstrapping Secure Communication without Leaking Metadata,2016-10-26T16:00:07Z,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,"David Lazar and Nickolai Zeldovich. Alpenhorn: Bootstrapping Secure Communication without Leaking Metadata. In Proceedings of the 12th Symposium on Operating Systems Design and Implementation (OSDI), Savannah, GA, Nov. 2016."
Brian Williams,"Yu, Peng",Model-based Embedded and Robotic Systems,2017-02-07T23:00:15Z,2017-02-07T23:00:15Z,2016-10-14,http://hdl.handle.net/1721.1/106886,"Over-subscription, that is, being assigned too many tasks or requirements that are too demanding, is commonly encountered in temporal planning problems. As human beings, we often want to do more than we can, ask for things that may not be available, while underestimating how long it takes to perform each task. It is often difficult for us to detect the causes of failure in such situations and then find resolutions that are effective. We can greatly benefit from tools that assist us by looking out for these plan failures, by identifying their root causes, and by proposing preferred resolutions to these failures that lead to feasible plans. In recent literature, several approaches have been developed to resolve such over-subscribed problems, which are often framed as over-constrained scheduling, configuration design or optimal planning problems. Most of them take an all-or-nothing approach, in which over-subscription is resolved through suspending constraints or dropping goals. While helpful, in real-world scenarios, we often want to preserve our plan goals as much possible. As human beings, we know that slightly weakening the requirements of a travel plan, or replacing one of its destinations with an alternative one is often sufficient to resolve an over-subscription problem, no matter if the requirement being weakened is the duration of a deep-sea survey being planned for, or the restaurant cuisine for a dinner date. The goal of this thesis is to develop domain independent relaxation algorithms that perform this type of slight weakening of constraints, which we will formalize as continuous relaxation, and to embody them in a computational aid, Uhura, that performs tasks akin to an experienced travel agent or ocean scientists. In over-subscribed situations, Uhura helps us diagnose the causes of failure, suggests alternative plans, and collaborates with us in order to resolve conflicting requirements in the most preferred way. Most importantly, the algorithms underlying Uhura supports the weakening, instead of suspending, of constraints and variable domains in a temporally flexible plan. The contribution of this thesis is two-fold. First, we developed an algorithmic framework, called Best-first Conflict-Directed Relaxation (BCDR), for performing plan relaxation. Second, we use the BCDR framework to perform relaxation for several different families of plan representations involving different types of constraints. These include temporal constraints, chance constraints and variable domain constraints, and we incorporate several specialized conflict detection and resolution algorithms in support of the continuous weakening of them. The key idea behind BCDR's approach to continuous relaxation is to generalize the concepts of discrete conflicts and relaxations, first introduced by the model-based diagnosis community, to hybrid conflicts and relaxations, which denote minimal inconsistencies and minimal relaxations to both discrete and continuous relaxable constraints.",197 p.,MIT-CSAIL-TR-2017-001,planning; scheduling; constraint relaxation; conflict-directed search,Collaborative Diagnosis of Over-Subscribed Temporal Plans,2017-02-07T23:00:15Z,,,PhD thesis,,,,"Collaborative Diagnosis of Over-Subscribed Temporal Plans, Peng Yu, PhD Thesis, Massachusetts Institute of Technology, 2016"
Patrick Winston,"Finlayson, Mark Alan",Genesis,2016-11-08T23:00:04Z,2016-11-08T23:00:04Z,2016-11-08,http://hdl.handle.net/1721.1/105270,"On March 30 & 31, 2015, an international group of twenty-three researchers with expertise in linguistic annotation convened in Sunny Isles Beach, Florida to discuss problems with and potential solutions for the state of linguistic annotation tooling. The participants comprised 14 researchers from the U.S. and 9 from outside the U.S., with 7 countries and 4 continents represented, and hailed from fields and specialties including computational linguistics, artificial intelligence, speech processing, multi-modal data processing, clinical & medical natural language processing, linguistics, documentary linguistics, sign-language linguistics, corpus linguistics, and the digital humanities. The motivating problem of the workshop was the balkanization of annotation tooling, namely, that even though linguistic annotation requires sophisticated tool support to efficiently generate high-quality data, the landscape of tools for the field is fractured, incompatible, inconsistent, and lacks key capabilities. The overall goal of the workshop was to chart the way forward, centering on five key questions: (1) What are the problems with current tool landscape? (2) What are the possible benefits of solving some or all of these problems? (3) What capabilities are most needed? (4) How should we go about implementing these capabilities? And, (5) How should we ensure longevity and sustainability of the solution? I surveyed the participants before their arrival, which provided significant raw material for ideas, and the workshop discussion itself resulted in identification of ten specific classes of problems, five sets of most-needed capabilities. Importantly, we identified annotation project managers in computational linguistics as the key recipients and users of any solution, thereby succinctly addressing questions about the scope and audience of potential solutions. We discussed management and sustainability of potential solutions at length. The participants agreed on sixteen recommendations for future work. This technical report contains a detailed discussion of all these topics, a point-by-point review of the discussion in the workshop as it unfolded, detailed information on the participants and their expertise, and the summarized data from the surveys.",61 p.,MIT-CSAIL-TR-2016-014,Linguistic Annotation; Annotation Tools & Resources; Natural Language Processing,Report on the 2015 NSF Workshop on Unified Annotation Tooling,2016-11-08T23:00:04Z,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,
Patrick Winston,"Eisenberg, Joshua D.; Finlayson, Mark A.",Genesis,2016-11-10T20:45:06Z,2016-11-10T20:45:06Z,2016-11-09,http://hdl.handle.net/1721.1/105279,"This archive contains the code and data for the workshop article ""Automatic Identification of Narrative Diegesis and Point of View,"" published in 2016 in the 2nd Workshop for Computing News Storylines (CNewsStory 2016), co-located with EMNLP 2016 in Austin, TX. The root of the archive contains a README file which explains the archive contents. Furthermore, the archive can be imported directly into the Eclipse IDE as a project encapsulating the executable code required to reproduce the results of the paper; the code compiles with Java 1.8. The archive also contains a copy of the final version of the paper for reference.",224 MiB,,,"Data and Code for ""Automatic Identification of Narrative Diegesis and Point of View""",2016-11-10T20:45:06Z,,,,,,,
Frans Kaashoek,"Chajed, Tej; Gjengset, Jon; Kaashoek, M. Frans; Mickens, James; Morris, Robert; Zeldovich, Nickolai",Parallel and Distributed Operating Systems,2016-12-12T23:30:04Z,2016-12-12T23:30:04Z,2016-12-08,http://hdl.handle.net/1721.1/105802,"In principle, the web should provide the perfect stage for user-generated content, allowing users to share their data seamlessly with other users across services and applications. In practice, the web fragments a user's data over many sites, each exposing only limited APIs for sharing. This paper describes Oort, a new cloud storage system that organizes data primarily by user rather than by application or web site. Oort allows users to choose which web software to use with their data and which other users to share it with, while giving applications powerful tools to query that data. Users rent space from providers that cooperate to provide a global, federated, general-purpose storage system. To support large-scale, multi-user applications such as Twitter and e-mail, Oort provides global queries that find and combine data from relevant users across all providers. Oort makes global query execution efficient by recognizing and merging similar queries issued by many users' application instances, largely eliminating the per-user factor in the global complexity of queries. Our evaluation predicts that an Oort implementation could handle traffic similar to that seen by Twitter using a hundred cooperating Oort servers, and that applications with other sharing patterns, like e-mail, can also be executed efficiently.",14 p.,MIT-CSAIL-TR-2016-015,cloud storage; global queries; user-centric storage; sharing; web development,Oort: User-Centric Cloud Storage with Global Queries,2016-12-12T23:30:04Z,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,
Brian Williams,"Lane, Spencer Dale",Model-based Embedded and Robotic Systems,2016-12-15T23:15:10Z,2016-12-15T23:15:10Z,2016-12-14,http://hdl.handle.net/1721.1/105848,"Communication is the key to effective teamwork regardless of whether the team members are humans or machines. Much of the communication that makes human teams so effective is non-verbal; they are able to recognize the actions that the other team members are performing and take their own actions in order to assist. A robotic team member should be able to make the same inferences, observing the state of the environment and inferring what actions are being taken. In this thesis I introduce a novel approach to the combined problem of activity recognition and propositional monitoring. This approach breaks down the problem into smaller sub-tasks. First, the raw sensor input is parsed into simple, easy to understand primitive semantic relationships known as qualitative spatial relations (QSRs). These primitives are then combined to estimate the state of the world in the same language used by most planners, planning domain definition language (PDDL) propositions. Both the primitives and propositions are combined to infer the status of the actions that the human is taking. I describe an algorithm for solving each of these smaller problems and describe the modeling process for a variety of tasks from an abstracted electronic component assembly (ECA) scenario. I implemented this scenario on a robotic testbed and collected data of a human performing the example actions.",80 p.,MIT-CSAIL-TR-2016-016,"hybrid systems, filtering, activity recognition, qualitative spatial relations",Propositional and Activity Monitoring Using Qualitative Spatial Reasoning,2016-12-15T23:15:10Z,,,SM thesis,,,,
Howard Shrobe,"Khan, M. Taimoor; Serpanos, Dimitrios; Shrobe, Howard",Cybersecurity,2016-12-15T23:15:03Z,2016-12-15T23:15:03Z,2016-12-15,http://hdl.handle.net/1721.1/105847,"We present a run-time security monitor that detects both known and unknown cyber attacks by checking that the run-time behavior of the application is consistent with the expected behavior modeled by an application specification. This is crucial because, even if the implementation is consistent with its specification, the application may still be vulnerable due to flaws in the supporting infrastructure. This run-time security monitor is sound and complete, eliminating false alarms, as well as efficient, so that it does not limit run-time application performance and so that it supports real-time systems. Importantly, this monitor is readily applicable to both legacy and new system platforms.The security monitor takes as input the application specification and the application implementation, which may be expressed in different languages. The security monitor detects attacks by systematically comparing the application execution and specification behaviors at run-time, even though they operate at two different levels of abstraction. We define the denotational semantics of the specification language and prove that the monitor is sound and complete, i.e. if the application is consistent with its specification, the security monitor will produce no false alarms (soundness) and that it will detect any deviation of the application from the behavior sanctioned by the specification language (completeness). Importantly, the application specification language enables the description of known or potential attack plans, enabling not only attack detection but attack characterization as well.",42 p.,MIT-CSAIL-TR-2016-017,,Sound and Complete Runtime Security Monitor for Application Software,2016-12-15T23:15:03Z,Creative Commons Attribution-NonCommercial 4.0 International,http://creativecommons.org/licenses/by-nc/4.0/,,,,,
Tomas Lozano-Perez,"Anders, Ariel; Kaelbling, Leslie; Lozano-Perez, Tomas",Learning and Intelligent Systems,2017-04-28T19:45:06Z,2017-04-28T19:45:06Z,2017-01-30,http://hdl.handle.net/1721.1/108510,"A crucial challenge in robotics is achieving reliable results in spite of sensing and control uncertainty. A prominent strategy for dealing with uncertainty is to construct a feedback policy, where actions are chosen as a function of the current state estimate. However, constructing such policies is computationally very difficult. An alternative strategy is conformant planning which finds open-loop action sequences that achieve the goal for all input states and action outcomes. In this work, we investigate the conformant planning approach to robot manipulation. In particular, we tackle the problem of pushing multiple objects simultaneously to achieve a specified arrangement. Conformant planning is a belief-state planning problem. A belief state is the set of all possible states of the world, and the goal is to find a sequence of actions that will bring an initial belief state to a goal belief state To do forward belief-state planning, we created a deterministic belief-state transition model from supervised learning based on physics simulations. A key pitfall in conformant planning is that the complexity of the belief state tends to increase with each operation, making it increasingly harder to compute the effect of actions. This work explores the idea that we can construct conformant plans for robot manipulation by only using actions resulting in compact belief states.",8 pp.,MIT-CSAIL-TR-2017-007,manipulation; robotics; machine learning; belief space; planning; uncertainty,Planning Robust Strategies for Constructing Multi-object Arrangements,2017-04-28T19:45:06Z,,,,,,,
John Leonard,"Rosen, David M.; Carlone, Luca; Bandeira, Afonso S.; Leonard, John J.",Marine Robotics,2017-02-07T23:00:06Z,2017-02-07T23:00:06Z,2017-02-05,http://hdl.handle.net/1721.1/106885,"Many important geometric estimation problems naturally take the form of synchronization over the special Euclidean group: estimate the values of a set of unknown poses given noisy measurements of a subset of their pairwise relative transforms. Examples of this class include the foundational problems of pose-graph simultaneous localization and mapping (SLAM) (in robotics), camera motion estimation (in computer vision), and sensor network localization (in distributed sensing), among others. This inference problem is typically formulated as a nonconvex maximum-likelihood estimation that is computationally hard to solve in general. Nevertheless, in this paper we present an algorithm that is able to efficiently recover certifiably globally optimal solutions of the special Euclidean synchronization problem in a non-adversarial noise regime. The crux of our approach is the development of a semidefinite relaxation of the maximum-likelihood estimation whose minimizer provides an exact MLE so long as the magnitude of the noise corrupting the available measurements falls below a certain critical threshold; furthermore, whenever exactness obtains, it is possible to verify this fact a posteriori, thereby certifying the optimality of the recovered estimate. We develop a specialized optimization scheme for solving large-scale instances of this semidefinite relaxation by exploiting its low-rank, geometric, and graph-theoretic structure to reduce it to an equivalent optimization problem defined on a low-dimensional Riemannian manifold, and then design a Riemannian truncated-Newton trust-region method to solve this reduction efficiently. Finally, we combine this fast optimization approach with a simple rounding procedure to produce our algorithm, SE-Sync. Experimental evaluation on a variety of simulated and real-world pose-graph SLAM datasets shows that SE-Sync is capable of recovering certifiably globally optimal solutions when the available measurements are corrupted by noise up to an order of magnitude greater than that typically encountered in robotics and computer vision applications, and does so more than an order of magnitude faster than the Gauss-Newton-based approach that forms the basis of current state-of-the-art techniques.","49 pages, 20 figures",MIT-CSAIL-TR-2017-002,Simultaneous localization and mapping (SLAM); Maximum-likelihood estimation; Convex relaxation; Low-rank semidefinite programming; Riemannian optimization,SE-Sync: A Certifiably Correct Algorithm for Synchronization over the Special Euclidean Group,2017-02-07T23:00:06Z,,,,,,,
Saman Amarasinghe,"Kjolstad, Fredrik; Kamil, Shoaib; Chou, Stephen; Lugato, David; Amarasinghe, Saman",Computer Architecture,2017-02-21T22:00:07Z,2017-02-21T22:00:07Z,2017-02-17,http://hdl.handle.net/1721.1/107013,"Tensor and linear algebra is pervasive in data analytics and the physical sciences. Often the tensors, matrices or even vectors are sparse. Computing expressions involving a mix of sparse and dense tensors, matrices and vectors requires writing kernels for every operation and combination of formats of interest. The number of possibilities is infinite, which makes it impossible to write library code for all. This problem cries out for a compiler approach. This paper presents a new technique that compiles compound tensor algebra expressions combined with descriptions of tensor formats into efficient loops. The technique is evaluated in a prototype compiler called taco, demonstrating competitive performance to best-in-class hand-written codes for tensor and matrix operations.",14 p.,MIT-CSAIL-TR-2017-003,Tensor Algebra; Linear Algebra; Compiler; C++ Library,The Tensor Algebra Compiler,2017-02-21T22:00:07Z,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,
Vinod Vaikuntanathan,"Micali, Silvio; Vaikuntanathan, Vinod",Theory of Computation,2017-04-06T23:00:04Z,2017-04-06T23:00:04Z,2017-03-31,http://hdl.handle.net/1721.1/107927,"We construct a Byzantine Agreement protocol that tolerates t < n/2 corruptions, is very efficient in terms of the number of rounds and the number of bits of communication, and satisfies a strong notion of robustness called player replaceability (defined in [Mic16]). We provide an analysis of our protocol when executed on real-world networks such as the ones employed in the bitcoin protocol.",8 p.,MIT-CSAIL-TR-2017-004,Byzantine Agreement; Cryptography; Honest Majority,Optimal and Player-Replaceable Consensus with an Honest Majority,2017-04-06T23:00:04Z,,,,,,,
