dc.contributor.author,dc.contributor.other,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.other,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.format.mimetype,dc.language.iso,dc.relation.ispartofseries,dc.title,dc.subject,dc.contributor.advisor
"Gassend, B.; O'Donnell, C. W.; Thies, W.; Lee, A.; van Dijk, M.; Devadas, S.",Computation Structures,2005-12-22T02:37:06Z,2005-12-22T02:37:06Z,2005-10-06,MIT-CSAIL-TR-2005-060; MIT-LCS-TR-1003,http://hdl.handle.net/1721.1/30571,"Our goal is to develop a state-of-the-art predictor with an intuitive and biophysically-motivated energy model through the use of Hidden Markov Support Vector Machines (HM-SVMs), a recent innovation in the field of machine learning.  We focus on the prediction of alpha helices in proteins and show that using HM-SVMs, a simple 7-state HMM with 302 parameters can achieve a Q_alpha value of 77.6% and a SOV_alpha value of 73.4%.  We briefly describe how our method can be generalized to predicting beta strands and sheets.",15 p.; 18110378 bytes; 702915 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Secondary Structure Prediction of All-Helical Proteins Using Hidden Markov Support Vector Machines,,
"Ajmani, Sameer; Liskov, Barbara; Shrira, Liuba; Curtis, Dorothy",Programming Methodology,2005-12-22T02:37:15Z,2005-12-22T02:37:15Z,2005-10-06,MIT-CSAIL-TR-2005-062; MIT-LCS-TR-1005,http://hdl.handle.net/1721.1/30572,"Upgrading the software of long-lived, highly-available distributedsystems is difficult.  It is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  Instead, upgrades must happengradually, and there may be long periods of time when different nodesrun different software versions and need to communicate usingincompatible protocols.  We present a methodology and infrastructurethat make it possible to upgrade distributed systems automatically whilelimiting service disruption.  We introduce new ways to reason aboutcorrectness in a multi-version system. We also describe a prototypeimplementation that supports automatic upgrades with modest overhead.",14 p.; 26794595 bytes; 1207166 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems,,
"Das, Sanmay",,2005-12-22T02:37:21Z,2005-12-22T02:37:21Z,2005-10-07,MIT-CSAIL-TR-2005-063; AIM-2005-028; CBCL-255,http://hdl.handle.net/1721.1/30573,"This paper introduces algorithms for learning how to trade usinginsider (superior) information in Kyle's model of financial markets.Prior results in finance theory relied on the insider having perfectknowledge of the structure and parameters of the market. I show herethat it is possible to learn the equilibrium trading strategy whenits form is known even without knowledge of the parameters governingtrading in the model. However, the rate of convergence toequilibrium is slow, and an approximate algorithm that does notconverge to the equilibrium strategy achieves better utility whenthe horizon is limited. I analyze this approximate algorithm fromthe perspective of reinforcement learning and discuss the importanceof domain knowledge in designing a successful learning algorithm.",15 p.; 16523384 bytes; 613212 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning to Trade with Insider Information,AI,
"Zhang, MIchael; Asanovic, Krste",Computer Architecture,2005-12-22T02:37:29Z,2005-12-22T02:37:29Z,2005-10-10,MIT-CSAIL-TR-2005-064; MIT-LCS-TR-1006,http://hdl.handle.net/1721.1/30574,"Future CMPs will have more cores and greater onchip cache capacity. The on-chip cache can either be divided into separate private L2 caches for each core, or treated as a large shared L2 cache. Private caches provide low hit latency but low capacity, while shared caches have higher hit latencies but greater capacity. Victim replication was previously introduced as a way of reducing the average hit latency of a shared cache by allowing a processor to make a replica of a primary cache victim in its local slice of the global L2 cache. Although victim replication performs well on multithreaded and single-threaded codes, it performs worse than the private scheme for multiprogrammed workloads where there is little sharing between the different programs running at the same time. In this paper, we propose victim migration, which improves on victim replication by adding an additional set of migration tags on each node which are used to implement an exclusive cache policy for replicas. When a replica has been created on a remote node, it is not also cached on the home node, but only recorded in the migration tags. This frees up space on the home node to store shared global lines or replicas for the local processor. We show that victim migration performs better than private, shared, and victim replication schemes across a range of single threaded, multithreaded, and multiprogrammed workloads, while using less area than a private cache design. Victim migration provides a reduction in average memory access latency of up to 10% over victim replication.",17 p.; 18487877 bytes; 796263 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Victim Migration: Dynamically Adapting Between Private and Shared CMP Caches,,
"Geiger, Gadi; Amara, Domenic G",,2005-12-22T02:37:36Z,2005-12-22T02:37:36Z,2005-10-18,MIT-CSAIL-TR-2005-065; AIM-2005-029; CBCL-256,http://hdl.handle.net/1721.1/30575,"Previous studies have shown that dyslexic individuals who supplement windowed reading practice with intensive small-scale hand-eye coordination tasks exhibit marked improvement in their reading skills. Here we examine whether similar hand-eye coordination activities, in the form of artwork performed by children in kindergarten, first and second grades, could reduce the number of students at-risk for reading problems. Our results suggest that daily hand-eye coordination activities significantly reduce the number of students at-risk. We believe that the effectiveness of these activities derives from their ability to prepare the students perceptually for reading.",0 p.; 14865502 bytes; 6633149 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Towards the Prevention of Dyslexia,AI; dyslexia; prevention,
"Torlak, Emina; van Dijk, Marten; Gassend, Blaise; Jackson, Daniel; Devadas, Srinivas",Software Design,2005-12-22T02:37:42Z,2005-12-22T02:37:42Z,2005-10-19,MIT-CSAIL-TR-2005-066; MIT-LCS-TR-1007,http://hdl.handle.net/1721.1/30576,"Knowledge flow analysis offers a simple and flexible way to find flaws in security protocols. A protocol is described by a collection of rules constraining the propagation of knowledge amongst principals. Because this characterization corresponds closely to informal descriptions of protocols, it allows a succinct and natural formalization; because it abstracts away message ordering, and handles communications between principals and applications of cryptographic primitives uniformly, it is readily represented in a standard logic. A generic framework in the Alloy modelling language is presented, and instantiated for two standard protocols, and a new key management scheme.",23 p.; 22738148 bytes; 915258 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Knowledge Flow Analysis for Security Protocols,,
"Lippert, Ross; Rifkin, Ryan",,2005-12-22T02:40:10Z,2005-12-22T02:40:10Z,2005-10-20,MIT-CSAIL-TR-2005-067; AIM-2005-030; CBCL-257,http://hdl.handle.net/1721.1/30577,"We consider regularized least-squares (RLS) with a Gaussian kernel. Weprove that if we let the Gaussian bandwidth $\sigma \rightarrow\infty$ while letting the regularization parameter $\lambda\rightarrow 0$, the RLS solution tends to a polynomial whose order iscontrolled by the relative rates of decay of $\frac{1}{\sigma^2}$ and$\lambda$: if $\lambda = \sigma^{-(2k+1)}$, then, as $\sigma \rightarrow\infty$, the RLS solution tends to the $k$th order polynomial withminimal empirical error.  We illustrate the result with an example.",1 p.; 7286963 bytes; 527607 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Asymptotics of Gaussian Regularized Least-Squares,AI; machine learning; regularization,
"Drake, Matthew; Hoffmann, Hank; Rabbah, Rodric; Amarasinghe, Saman",Computer Architecture,2005-12-22T02:40:17Z,2005-12-22T02:40:17Z,2005-10-22,MIT-CSAIL-TR-2005-068; MIT-LCS-TM-652,http://hdl.handle.net/1721.1/30578,"Image and video codecs are prevalent in multimedia applications, ranging from embedded systems, to desktop computers, to high-end servers such as HDTV editing consoles. It is not uncommon however that developers create (from scratch) and customize their codec implementations for each of the architecture targets they intend their coders and decoders to run on. This practice is time consuming anderror prone, leading to code that is not malleable or portable.  In this paper we describe an implementation of the MPEG-2 codec using the StreamIt programming language. StreamIt is an architecture-independent stream language that aims to improve programmer productivity, while concomitantly exposing the inherent parallelism and communication topology of the application. We describe why MPEG is a good match forthe streaming programming model, and illustrate the malleability of the implementation using a simple modification to the decoder to support alternate color compression formats. StreamIt allows for modular application development, which also reduces the complexity of the debugging process since stream components can be verifiedindependently. This in turn leads to greater programmer productivity. We implement a fully functional MPEG-2 decoder in StreamIt. The decoder was developed in eight weeks by a single student programmer who did not have any prior experience with MPEG or other video codecs. Many of the MPEG-2 components were subsequently reused to assemble a JPEG codec.",15 p.; 22871205 bytes; 786081 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,MPEG-2 in a Stream Programming Language,,
"Nguyen, Huu Hai; Rinard, Martin",Computer Architecture,2005-12-22T02:40:21Z,2005-12-22T02:40:21Z,2005-10-26,MIT-CSAIL-TR-2005-069; MIT-LCS-TR-1008,http://hdl.handle.net/1721.1/30579,"We present and evaluate a new memory management technique foreliminating memory leaks in programs with dynamic memoryallocation. This technique observes the execution of the program on asequence of training inputsto find m-bounded allocation sites,which have the property that at any time during the execution of theprogram, the program accesses at most only the last m objects allocated atthat site. The technique then transforms the program to usecyclic memory allocation at that site: it preallocates a buffercontaining m objects of the type allocated at that site, with eachallocation returning the next object in the buffer. At the end of thebuffer the allocations wrap back around to the first object.  Cyclicallocation eliminates any memory leak at the allocation site - thetotal amount of memory required to hold all of the objects everallocated at the site is simply $m$ times the object size.We evaluate our technique by applying it to several widely-used opensource programs.  Our results show that it is able to successfullyeliminate important memory leaks in these programs.  A potentialconcern is that the estimated bounds m may be too small, causing theprogram to overlay live objects in memory.  Our results indicate thatour bounds estimation technique is quite accurate in practice,providing incorrect results for only one of the 160 m-bounded sitesthat it identifies. To evaluate the potential impact ofoverlaying live objects, we artificially reduce the bounds at$m$-bounded sites and observe the resulting behavior.The resulting overlayingof live objects often does not affect thefunctionality of the program at all; even when it does impairpart of the functionality, the program does not fail andis still able to acceptably deliver the remaining functionality.",10 p.; 20338887 bytes; 816035 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Cyclic Memory Allocation to Eliminate Memory Leaks,,
"Rahul, Hariharan; Kasbekar, Mangesh; Sitaraman, Ramesh; Berger, Arthur",Networks and Mobile Systems,2005-12-22T02:40:26Z,2005-12-22T02:40:26Z,2005-11-01,MIT-CSAIL-TR-2005-070; MIT-LCS-TR-1009,http://hdl.handle.net/1721.1/30580,"Prior analyses of the benefits of routing overlays are based onplatforms consisting of nodes located primarily in North America, onthe academic Internet, and at the edge of the network. This paper isthe first global study of the benefits of overlays on the commercialInternet in terms of round trip latencies and availability, usingmeasurements from diverse ISPs over 1100 locations (77 countries, 630cities and 6 continents).Our study shows that while overlays provide some improvements in North America, their benefits are especially significant for paths withAsian endpoints.  Regarding practical considerations in constructingoverlay routes, we show that an algorithm that randomly chooses asmall number of alternate redundant paths achieves an availability ofover 99.5%. We also propose and evaluate a simple predictive schemethat achieves almost optimal latency using only 2-3 paths, and thatthis is achievable with surprisingly persistent routing choices.",23 p.; 23260667 bytes; 831725 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Towards Realizing the Performance and Availability Benefits of a Global Overlay Network,,
"Lepinski, Matthew; Micali, Silvio",Cryptography and Information Security,2005-12-22T02:40:29Z,2005-12-22T02:40:29Z,2005-11-02,MIT-CSAIL-TR-2005-071; MIT-LCS-TM-653,http://hdl.handle.net/1721.1/30581,"In their paper, ""Rational Secure Computation and Ideal Mechanism Design,"" Izmalkov, Lepinski and Micali show that any one-shot mediated game can be simulated by the players themselves, without the help of a trusted mediator, using physical envelopes and a ballot-box. We show that communication between the players is not essential to the ILM protocol. That is, we provide a protocol for rational secure function evaluation (Rational SFE) where the players just send a set of envelopes to a referee who simply performs a sequence of publicly verifiable actions. That is, the players can ""subcontract"" all of the computation to an untrusted referee. In addition to providing a communication structure that more closely matches the ideal game, our protocol also enables us to better simulate mediated games in which abort is not a dominated action.",6 p.; 8137326 bytes; 296971 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Subcontracted Rational SFE,,
"Wies, Thomas; Kuncak, Viktor; Lam, Patrick; Podelski, Andreas; Rinard, Martin",Computer Architecture,2005-12-22T02:40:34Z,2005-12-22T02:40:34Z,2005-11-03,MIT-CSAIL-TR-2005-072; MIT-LCS-TR-1010,http://hdl.handle.net/1721.1/30582,"We introduce field constraint analysis, a new  technique for verifying data structure invariants.  A  field constraint for a field is a formula specifying a set of objects  to which the field can point.  Field constraints enable  the application of decidable logics to data structures  which were originally beyond the scope of these logics, by verifying the  backbone of the data structure and then verifying  constraints on fields that cross-cut the backbone in  arbitrary ways.  Previously, such cross-cutting fields  could only be verified when they were uniquely determined by  the backbone, which significantly limited the range of  analyzable data structures.  Our field constraint analysis permits \\emph{non-deterministic} field  constraints on cross-cutting fields, which allows to verify  invariants of data structures such as skip lists.  Non-deterministic  field constraints also enable the verification of invariants between  data structures, yielding an expressive generalization of static  type declarations.  The generality of our field constraints requires new  techniques, which are orthogonal to the traditional use of  structure simulation.  We present one such technique and  prove its soundness.  We have implemented this technique  as part of a symbolic shape analysis deployed in  the context of the Hob system for verifying data structure  consistency.  Using this implementation we were able to  verify data structures that were previously beyond the  reach of similar techniques.",23 p.; 22680509 bytes; 928319 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Field Constraint Analysis,,
"Andoni, Alexandr; Indyk, Piotr",,2005-12-22T02:40:39Z,2005-12-22T02:40:39Z,2005-11-04,MIT-CSAIL-TR-2005-073; AIM-2005-031,http://hdl.handle.net/1721.1/30583,"We present an algorithm for c-approximate nearest neighbor problem in a d-dimensional Euclidean space,  achieving query time ofO(dn^{1/c^2+o(1)}) and space O(dn + n^{1+1/c^2+o(1)}).",12 p.; 11656417 bytes; 559939 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New LSH-based Algorithm for Approximate Nearest Neighbor,AI; locality sensitive hashing; nearest neighbor; high dimensions,
"Hajiaghayi, MohammadTaghi; Kortsarz, Guy; Salavatipour, Mohammad R.",Theory of Computation,2006-01-05T20:37:59Z,2006-01-05T20:37:59Z,2005-11-15,MIT-CSAIL-TR-2006-001,http://hdl.handle.net/1721.1/30601,"In the buy-at-bulk $k$-Steiner tree (or rent-or-buy$k$-Steiner tree) problem we are given a graph $G(V,E)$ with a setof terminals $T\subseteq V$ including a particular vertex $s$ calledthe root, and an integer $k\leq |T|$. There are two cost functionson the edges of $G$, a buy cost $b:E\longrightarrow \RR^+$ and a rentcost $r:E\longrightarrow \RR^+$. The goal is to find a subtree $H$ of$G$ rooted at $s$ with at least $k$ terminals so that the cost$\sum_{e\in H} b(e)+\sum_{t\in T-s} dist(t,s)$ is minimize, where$dist(t,s)$ is the distance from $t$ to $s$ in $H$ with respect tothe $r$ cost. Our main result is  an $O(\log^5 n)$-approximation forthe buy-at-bulk $k$-Steiner tree problem.To achieve this we also design an approximation algorithm forbicriteria $k$-Steiner tree. In the bicriteria $k$-Steiner tree problem weare given a graph $G$ with edge costs $b(e)$ and distance costs$r(e)$ over the edges, and an integer $k$. Our goal is to find aminimum cost (under $b$-cost) $k$-Steiner tree such that thediameter under $r$-cost is at most some given bound $D$. An$(\alpha,\beta)$-approximation finds a subgraph of diameter at most$\alpha\cdot {D}$ (with respect to $r$) and cost with respect to$b$ of at most $\beta\cdot opt$ where $opt$ is the minimum cost ofany solution with diameter at most $D$. Marathe et al \cite{ravi}gave an $(O(\log n),O(\log n))$-approximation algorithm for thebicriteria Steiner tree problem. Their algorithm does not extend tothe bicriteria $k$-Steiner tree problem.Our algorithm for the buy-at-bulk $k$-Steiner tree problem relies on an$(O(\log^2 n),O(\log^4 n))$-approximation algorithm we develop for the(shallow-light) bicriteria  $k$-Steiner tree problem, which is ofindependent interest. Indeed, this is also one of the main tools we use to obtainthe first polylogarithmic approximation algorithm for non-uniformmulticommodity buy-at-bulk~\cite{HKS}.",14 p.; 18505768 bytes; 766175 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Approximating Buy-at-Bulk k-Steiner trees,,Erik Demaine
"Monteleoni, Claire; Jaakkola, Tommi",,2005-12-22T02:40:44Z,2005-12-22T02:40:44Z,2005-11-17,MIT-CSAIL-TR-2005-074; AIM-2005-032,http://hdl.handle.net/1721.1/30584,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,8 p.; 10189026 bytes; 760649 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Online Learning of Non-stationary Sequences,AI; online learning; regret bounds; non-stationarity; HMM; wireless networks,
"Dasgupta, Sanjoy; Kalai, Adam Tauman; Monteleoni, Claire",,2005-12-22T02:40:49Z,2005-12-22T02:40:49Z,2005-11-17,MIT-CSAIL-TR-2005-075; AIM-2005-033,http://hdl.handle.net/1721.1/30585,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",15 p.; 11491832 bytes; 599624 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Analysis of Perceptron-Based Active Learning,AI; active learning; perceptron; label-complexity; mistake bound; selective sampling,
"Uzuner, Ozlem",,2005-12-22T02:41:36Z,2005-12-22T02:41:36Z,2005-11-18,MIT-CSAIL-TR-2005-077; AITR-2005-004,http://hdl.handle.net/1721.1/30587,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",216 p.; 179019584 bytes; 5410679 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Identifying Expression Fingerprints using Linguistic Information,AI; natural language processing; syntactic information; content; expression,
"Zeng, Gang; Paris, Sylvain; Quan, Long; Sillion, Francois",Computer Graphics,2005-12-22T02:41:10Z,2005-12-22T02:41:10Z,2005-11-18,MIT-CSAIL-TR-2005-076; MIT-LCS-TR-1011,http://hdl.handle.net/1721.1/30586,"We introduce a new surface representation, the patchwork, to extend the problem of surface reconstruction from multiple images. A patchwork is the combination of several patches that are built one by one. This design potentially allows the reconstruction of an object of arbitrarily large dimensions while preserving a fine level of detail. We formally demonstrate that this strategy leads to a spatial complexity independent of the dimensions of the reconstructed object, and to a time complexity linear with respect to the object area. The former property ensures that we never run out of storage (memory) and the latter means that reconstructing an object can be done in a reasonable amount of time. In addition, we show that the patchwork representation handles equivalently open and closed surfaces whereas most of the existing approaches are limited to a specific scenario (open or closed surface but not both).Most of the existing optimization techniques can be cast into this framework. To illustrate the possibilities offered by this approach, we propose two applications that expose how it dramatically extends a recent accurate graph-cut technique. We first revisit the popular carving techniques. This results in a well-posed reconstruction problem that still enjoys the tractability of voxel space. We also show how we can advantageously combine several image-driven criteria to achieve a finely detailed geometry by surface propagation. The above properties of the patchwork representation and reconstruction are extensively demonstrated on real image sequences.",35 p.; 126753884 bytes; 4493080 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Accurate and Scalable Surface Representation and Reconstruction from Images,,
"Hajiaghayi, MohammadTaghi; Kortsarz, Guy; Salavatipour, Mohammad R.",Theory of Computation,2006-01-05T20:42:38Z,2006-01-05T20:42:38Z,2005-11-26,MIT-CSAIL-TR-2006-002,http://hdl.handle.net/1721.1/30602,"We consider the non-uniform multicommodity buy-at-bulknetworkdesign problem. In this problem we are given a graph $G(V,E)$withtwo cost functions on the edges, a buy cost $b:E\longrightarrow \RR^+$and a rent cost$r:E\longrightarrow\RR^+$, and a set of source-sink pairs$s_i,t_i\in V$ ($1\leq i\leq \alpha$)with each pair $i$ having a positivedemand $\delta_i$. Our goal is to designa minimum cost network $G(V,E')$such that for every $1\leq i\leq\alpha$,  $s_i$ and $t_i$ are in thesameconnected component in $G(V,E')$. Thetotal cost of $G(V,E')$ is the sum ofbuy costs of the edges in $E'$plus sum of total demand going through everyedge in $E'$ times therent cost of that edge. Since the costs of differentedges can bedifferent, we say that the problem is non-uniform. Thefirstnon-trivial approximation algorithm for this problem is due toCharikarand Karagiozova (STOC' 05) whose algorithm has anapproximation guarantee of$\exp(O(\sqrt{\log n\log\log n}))$,when all $\delta_i=1$ and$\exp(O(\sqrt{\log N\log\log N}))$ for the generaldemand case where $N$ isthe sum of all demands. We improve upon this result, bypresenting the firstpolylogarithmic (specifically, $O(\log^4 n)$ for unit demandsand $O(\log^4N)$ for the general demands)approximation for this problem. The algorithmrelies on a recent result\cite{HKS1} for the buy-at-bulk $k$-Steiner treeproblem.",16 p.; 19386222 bytes; 818306 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Polylogarithmic Approximation Algorithm for Non-Uniform Multicommodity Buy-at-Bulk,,Erik Demaine
"Ajmani, Sameer",Programming Methodology,2005-12-22T02:42:02Z,2005-12-22T02:42:02Z,2005-11-30,MIT-CSAIL-TR-2005-078; MIT-LCS-TR-1012,http://hdl.handle.net/1721.1/30589,"Upgrading the software of long-lived, highly-available distributed systems is difficult. It is not possible to upgrade all the nodes in a system at once, since some nodes may be unavailable and halting the system for an upgrade is unacceptable. Instead, upgrades may happen gradually, and there may be long periods of time when different nodes are running different software versions and need to communicate using incompatible protocols. We present a methodology and infrastructure that address these challenges and make it possible to upgrade distributed systems automatically while limiting service disruption.Our methodology defines how to enable nodes to interoperate across versions, how to preserve the state of a system across upgrades, and how to schedule an upgrade so as to limit service disruption. The approach is modular: defining an upgrade requires understanding only the new software and the version it replaces.The upgrade infrastructure is a generic platform for distributing and installing software while enabling nodes to interoperate across versions. The infrastructure requires no access to the system source code and is transparent: node software is unaware that different versions even exist. We have implemented a prototype of the infrastructure called Upstart that intercepts socket communication using a dynamically-linked C++ library. Experiments show that Upstart has low overhead and works well for both local-area and Internet systems.",164 p.; 135376801 bytes; 5539474 bytes,application/postscript; application/pdf,en_US,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems,,
