dc.contributor.author,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.other,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.format.mimetype,dc.language.iso,dc.relation.ispartofseries,dc.subject,dc.title,dc.contributor.advisor
"Nagpal, Radhika",2004-10-20T20:28:28Z,2004-10-20T20:28:28Z,2001-06-01,AITR-2001-008,http://hdl.handle.net/1721.1/7076,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",118 p.; 27221557 bytes; 1541086 bytes,application/postscript; application/pdf,en_US,AITR-2001-008,AI; self-organisation; multi agent; developmental biology; amorphous computing,Programmable Self-Assembly: Constructing Global Shape using Biologically-inspire,
"Frank, Matthew; Lee, Walter; Amarasinghe, Saman",2023-03-29T14:42:21Z,2023-03-29T14:42:21Z,2001-07,,https://hdl.handle.net/1721.1/149308,"This paper presents SUDS (Software Un-Do Systems), a data speculation system for Raw processors. SUDS manages specultation in software. Thekey to managing speculation in software is to use the compiler to minimize the number of data items that need to be managed in runtime. Managing speculation in software enables Raw processors to achieve good performance on integer applications without sacrificing chip area for speculation hardware. This additional area can instead be devoted to additional computer resources, improving the performance of dense matrix and media applications.",,,,MIT-LCS-TM-619,,A Software Framework for Supporting General Purpose Applications on RAW Computation Fabrics,"Lee, Walter; Amarasinghe, Saman"
"Alvira, Mariano; Paris, Jim; Rifkin, Ryan",2004-10-20T21:03:36Z,2004-10-20T21:03:36Z,2001-07-01,AIM-2001-012; CBCL-199,http://hdl.handle.net/1721.1/7234,"We design and implement a system that recommends musicians to listeners. The basic idea is to keep track of what artists a user listens to, to find other users with similar tastes, and to recommend other artists that these similar listeners enjoy. The system utilizes a client-server architecture, a web-based interface, and an SQL database to store and process information. We describe Audiomomma-0.3, a proof-of-concept implementation of the above ideas.",10 p.; 2186561 bytes; 257129 bytes,application/postscript; application/pdf,en_US,AIM-2001-012; CBCL-199,AI,The Audiomomma Music Recommendation System,
"Chan, Nicholas T.; Dahan, Ely; Lo, Andrew W.; Poggio, Tomaso",2004-10-20T21:03:35Z,2004-10-20T21:03:35Z,2001-07-01,AIM-2001-013; CBCL-200,http://hdl.handle.net/1721.1/7233,"Market prices are well known to efficiently collect and aggregate diverse information regarding the value of commodities and assets. The role of markets has been particularly suitable to pricing financial securities. This article provides an alternative application of the pricing mechanism to marketing research - using pseudo-securities markets to measure preferences over new product concepts. Surveys, focus groups, concept tests and conjoint studies are methods traditionally used to measure individual and aggregate preferences. Unfortunately, these methods can be biased, costly and time-consuming to conduct. The present research is motivated by the desire to efficiently measure preferences and more accurately predict new product success, based on the efficiency and incentive-compatibility of security trading markets. The article describes a novel market research method, pro-vides insight into why the method should work, and compares the results of several trading experiments against other methodologies such as concept testing and conjoint analysis.",3069806 bytes; 287156 bytes,application/postscript; application/pdf,en_US,AIM-2001-013; CBCL-200,AI,Experimental Markets for Product Concepts,
"Russell, Richard; Sinha, Pawan",2004-10-20T21:03:39Z,2004-10-20T21:03:39Z,2001-07-01,AIM-2001-014; CBCL-201,http://hdl.handle.net/1721.1/7235,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",13 p.; 9714300 bytes; 2612761 bytes,application/postscript; application/pdf,en_US,AIM-2001-014; CBCL-201,AI; Image matching; vector quantization; Minkowski metric,Perceptually-based Comparison of Image Similarity Metrics,
"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:41Z,2004-10-20T21:03:41Z,2001-07-25,AIM-2001-015; CBCL-202,http://hdl.handle.net/1721.1/7236,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,17 p.; 14931961 bytes; 3219314 bytes,application/postscript; application/pdf,en_US,AIM-2001-015; CBCL-202,AI; Scene classification; Navigation; scene representation,Recognizing Indoor Scenes,
"Kuncak, Viktor; Lam, Patrick; Rinard, Martin",2023-03-29T15:34:21Z,2023-03-29T15:34:21Z,2001-08,,https://hdl.handle.net/1721.1/149925,,,,,MIT-LCS-TR-822,,Roles Are Really Great!,
"Thies, William F.; Karczmarek, Michael; Amarasinghe, Saman",2023-03-29T14:42:24Z,2023-03-29T14:42:24Z,2001-08,,https://hdl.handle.net/1721.1/149309,"We characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly important. The StreaMIT language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreaMIT compiler aims to improve the performance of streaming applications via stream-specific analyses and optimizations. In this paper, we motivate, describe and justify the language features of StreaMIT, which include: a structured model of streams, a messaging system for control, a re-initialization mechanism, and a natural textual syntax. We also present a means of reasoning about time in terms of ""information flow"": a concept that we believe is fundamental to the streaming domain. Using this concept, we give a formal semantics for StreaMIT's messaging system, as well as a simple algorithm for detecting deadlock and buffer overlow.",,,,MIT-LCS-TM-620,,StreaMIT: A Language for Streaming Applications,
"Zollei, Lilla",2004-10-20T20:28:33Z,2004-10-20T20:28:33Z,2001-08-01,AITR-2002-001,http://hdl.handle.net/1721.1/7078,"The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation.   Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",128 p.; 21043480 bytes; 1712245 bytes,application/postscript; application/pdf,en_US,AITR-2002-001,AI; registration; medical imaging,2D-3D Rigid-Body Registration of X-Ray Fluoroscopy and CT Images,
"Shelton, Christian Robert",2004-10-01T14:00:04Z,2004-10-01T14:00:04Z,2001-08-01,AITR-2001-003; CBCL-204,http://hdl.handle.net/1721.1/5568,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",108 p.; 10551422 bytes; 1268632 bytes,application/postscript; application/pdf,en_US,AITR-2001-003; CBCL-204,AI; reinforcement learning; RL; importance sampling; estimation; market-making,Importance Sampling for Reinforcement Learning with Multiple Objectives,
"Sinha, Pawan; Torralba, Antonio",2004-10-20T21:03:43Z,2004-10-20T21:03:43Z,2001-08-01,AIM-2001-017; CBCL-203,http://hdl.handle.net/1721.1/7237,"Brightness judgments are a key part of the primate brain's visual analysis of the environment. There is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. However, it is unclear precisely how a region's context influences its perceived brightness. Recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. This work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. Here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. We find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. These results point to the important role of low-level mechanisms in determining brightness percepts.",17 p.; 5391865 bytes; 331759 bytes,application/postscript; application/pdf,en_US,AIM-2001-017; CBCL-203,AI; brightness perception; perceptual organization; local mechanisms,Role of Low-level Mechanisms in Brightness Perception,
"Beal, Jacob",2004-10-04T14:37:46Z,2004-10-04T14:37:46Z,2001-08-13,AIM-2001-016,http://hdl.handle.net/1721.1/6082,I present an algorithm which allows two agents to generate a simple language based only on observations of a shared environment. Vocabulary and roles for the language are learned in linear time. Communication is robust and degrades gradually as complexity increases. Dissimilar modes of experience will lead to a shared kernel vocabulary.,35 p.; 6622296 bytes; 264031 bytes,application/postscript; application/pdf,en_US,AIM-2001-016,AI; Adaptive Learning Hash-coding communication architecture algorithm,An Algorithm for Bootstrapping Communications,
"Yeo, Gene; Poggio, Tomaso",2004-10-20T21:03:45Z,2004-10-20T21:03:45Z,2001-08-25,AIM-2001-018; CBCL-206,http://hdl.handle.net/1721.1/7238,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",17 p.; 6552074 bytes; 816114 bytes,application/postscript; application/pdf,en_US,AIM-2001-018; CBCL-206,AI; multiclass; SVM; feature selection; SRBCT; tumors,Multiclass Classification of SRBCTs,
"Tan, Godfrey; Mui, Allen; Guttag, John V.; Balakrishnan, Hari",2023-03-29T15:34:32Z,2023-03-29T15:34:32Z,2001-09,,https://hdl.handle.net/1721.1/149929,"There is increasing interest in wireless ad hoc networks built from portable devices equipped with short-range wireless network interfaces. This paper addresses issues related to internetworking such networks to form larger ÔøΩscatternets.ÔøΩ  Within the constraints imposed by the emerging standard Bluetooth link layer and MAC protocol, we describe an efficient online topology formation algorithm, called TSF (Tree Scatternet Formation) to build scatternets. TSF connects nodes in a tree structure that simplifies packet routing and scheduling. The design allows nodes to arrive and leave arbitrarily, incrementally building the topology and healing partitions when they occur. We present simulation results that show that TSF has low tree formation latency and also generates an efficient topology for forwarding packets.",,,,MIT-LCS-TR-826,,Forming Scatternets from Bluetooth Personal Area Networks,
"Lynch, Nancy A.; Segala, Roberto; Vaandrager, Frits",2023-03-29T15:34:34Z,2023-03-29T15:34:34Z,2001-09,,https://hdl.handle.net/1721.1/149930,"Hybrid systems are systems that exhibit a combination of discrete and continuous behavior. Typical hybrid systems include computer components, which operate in discrete program steps, and real-world components, whose behavior over time intervals evolves according to physical constraints. Important examples of hybrid systems include automated transportation systems, robotics systems, process control systems, systems of embedded devices, and mobile computing systems. Such systems can be very complex, and very difficult to describe and analyze. This paper presents the Hybrid Input/Output Automaton (HIOA) modeling framework, a basic mathematical framework to support description and analysis of hybrid systems. An important feature of this model is its support for decomposing hybrid system descriptions. In particular, the framework includes a notion of external behavior for a hybrid I/O automaton, which captures its discrete and continuous interactions with its environment. The framework also defines what it means for one HIOA to implement another, based on an inclusion relationship between their external behavior sets, and defines a notion of simulation, which provides a sufficient condition for demonstrating implementation relationships. The framework also includes a composition operation for HIOAs, which respects external behavior, and a notion of receptiveness, which implies that an HIOA does not block the passage of time. The framework is intended to support analysis methods from both computer science and control theory. This work is a simplification of an earlier version of the HIOA model [49, 50]. The main simplification in the new model is a clearer separation between the mechanisms used to model discrete and continuous interaction between components. In particular, the new model removes the dual use of external variables for discrete and continuous interaction.",,,,MIT-LCS-TR-827a,,Hybrid I/O Automata*,
"Teller, Seth",2023-03-29T15:34:30Z,2023-03-29T15:34:30Z,2001-09,,https://hdl.handle.net/1721.1/149928,"We describe the design considerations underlying a system for scalable, automated capture of precisely controlled imagery in urban scenes. The system operates for architectural scenes in which, from every camera position, some  two vanishing points are visible. It has been used to capture thousands of controlled images in outdoor environments spanning hundreds of meters. The proposed system architecture forms the foundation for a future, fully robotic outdoor mapping capability for urban areas, analogous to existing, satellite-based robotic mapping systems which acquire images and models of natural terrain.  Four key ideas distinguish our approach from other methods. First, our sensor acquires georeferencing metadata with every image, enabling related images to be efficiently identified and registered. Second, the sensor acquires omni-directional images; we show strong experimental evidence that such images are fundamentally more powerful observations than conventional (narrow-FOV) images. Third, the system uses a probabilistic, projective error formulation to account for uncertainty. By treating measurement error in an appropriate depth-free framework, and by deferring decisions about camera calibration and scene structure until many noisy observations can be fused, the system achieves superior robustness and accuracy. Fourth, the system's computational requirements scale linearly in the input size, the area of the acquisition region, and the size of the output model. This is in contrast to most previous methods, which either assume constant-size inputs or exhibit quadratic running time (or worse) asymptotically. These attributes enable the system to operate in a regime of scale and physical extent which is unachievable by any other method, whether manual or automated. Consequently, it can acquire the most complex calibrated terrestrial image sets in existence, while operating faster thanany existing manual or algorithmic method.",,,,MIT-LCS-TR-825,,"Scalable, Controlled Imagery Capture in Urban Environments",
"Rennie, Jason D. M.",2004-10-20T20:28:16Z,2004-10-20T20:28:16Z,2001-09-01,AITR-2001-004,http://hdl.handle.net/1721.1/7074,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",49 p.; 2017370 bytes; 687421 bytes,application/postscript; application/pdf,en_US,AITR-2001-004,AI; naive bayes; text; classification; feature selection,Improving Multi-class Text Classification with Naive Bayes,
"Hong, Won",2004-10-20T20:28:24Z,2004-10-20T20:28:24Z,2001-09-01,AITR-2001-007,http://hdl.handle.net/1721.1/7075,"This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",225 p.; 66603884 bytes; 4629577 bytes,application/postscript; application/pdf,en_US,AITR-2001-007,AI; Robotics; Soil Modeling,"Modeling, Estimation, and Control of Robot-Soil Interactions",
"Bryson, Joanna J.",2004-10-20T20:29:10Z,2004-10-20T20:29:10Z,2001-09-01,AITR-2002-003,http://hdl.handle.net/1721.1/7080,"All intelligence relies on search --- for  example, the search for an intelligent agent's  next action. Search is only likely to succeed in resource-bounded agents if they have already  been biased towards finding the right answer.  In artificial agents, the primary source of bias is engineering.   This dissertation describes an approach,  Behavior-Oriented Design (BOD) for  engineering complex agents. A complex agent  is one that must arbitrate between potentially  conflicting goals or behaviors.  Behavior-oriented design builds on work in  behavior-based and hybrid architectures for agents, and the object  oriented approach to software engineering.   The primary contributions of this dissertation  are:     1.The BOD architecture: a modular  architecture with each module providing  specialized representations to facilitate  learning.    This includes one pre-specified module  and representation for action selection or  behavior arbitration. The specialized    representation underlying BOD action  selection is Parallel-rooted, Ordered,  Slip-stack Hierarchical (POSH) reactive plans.     2.The BOD development process: an  iterative process that alternately scales the  agent's capabilities then optimizes the agent  for    simplicity, exploiting tradeoffs between the  component representations. This ongoing  process for controlling complexity not only    provides bias for the behaving agent, but  also facilitates its maintenance and  extendibility.   The secondary contributions of this  dissertation include two implementations of  POSH action selection, a procedure for  identifying useful idioms in agent architectures and  using them to distribute knowledge across  agent paradigms, several examples of  applying BOD idioms to established architectures, an  analysis and comparison of the attributes and  design trends of a large number of agent architectures, a comparison of biological  (particularly mammalian) intelligence to  artificial agent architectures, a novel model of primate transitive inference, and many other  examples of BOD agents and BOD  development.",232 p.; 4544378 bytes; 1027952 bytes,application/postscript; application/pdf,en_US,AITR-2002-003,AI,Intelligence by Design: Principles of Modularity and Coordination for Engineerin,
"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:49Z,2004-10-20T21:03:49Z,2001-09-01,AIM-2001-020; CBCL-205,http://hdl.handle.net/1721.1/7239,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",27 p.; 40187890 bytes; 5238575 bytes,application/postscript; application/pdf,en_US,AIM-2001-020; CBCL-205,AI; context; image statistics; Bayesian reasoning; recognition; focus of attention,Contextual Priming for Object Detection,
