dc.contributor.advisor,dc.contributor.author,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.description.abstract,dc.format.extent,dc.relation.ispartofseries,dc.subject,dc.title,dc.contributor.other,dc.description,dc.type,dc.description.degree,dc.contributor.department,dc.rights,dc.rights.uri,dspace.orderedauthors,dc.identifier.citation,dc.date.submitted,dc.description.sponsorship
"Robotics, Vision & Sensor Networks; John Leonard","Benjamin, Michael R.; Newman, Paul M.; Schmidt, Henrik; Leonard, John J.",2009-08-20T18:30:06Z,2009-08-20T18:30:06Z,2009-08-20,http://hdl.handle.net/1721.1/46361,"This document describes how to extend the suite of MOOS applications and IvP Helm behaviors distributed with the MOOS-IvP software bundle from www.moos-ivp.org. It covers (a) a straw-man repository with a place-holder MOOS application and IvP Behavior, with a working CMake build structure, (b) a brief overview of the MOOS application class with an example application, (c) an overview of the IvP Behavior class with an example behavior, and (d) the IvPBuild Toolbox for generation of objective functions within behaviors.",102 p.,MIT-CSAIL-TR-2009-037,UUV; Behavior Based Control; Unmanned Vehicles; Multi-objective Optimization; Autonomous Marine Vehicles; Behavior Based Architecture; Autonomous Vehicles; AUV; Arbitration; MOOSDB; Unmanned Marine Vehicles; Action Selection; Multi-Objective Optimization; Autonomous Helm; USV; Unmanned Surface Vehicles; MOOS; Behaviors; Artificial Intelligence; Autonomous Decision Making; Underwater Vehicles; ZAIC,Extending a MOOS-IvP Autonomy System and Users Guide to the IvPBuild Toolbox,,,,,,,,,,,
Peter Szolovits,"Hug, Caleb",2009-08-26T18:15:07Z,2009-08-26T18:15:07Z,2009-08-26,http://hdl.handle.net/1721.1/46690,"The modern intensive care unit (ICU) has become a complex, expensive, data-intensive environment. Caregivers maintain an overall assessment of their patients based on important observations and trends. If an advanced monitoring system could also reliably provide a systemic interpretation of a patient's observations it could help caregivers interpret these data more rapidly and perhaps more accurately. In this thesis I use retrospective analysis of mixed medical/surgical intensive care patients to develop predictive models. Logistic regression is applied to 7048 development patients with several hundred candidate variables. These candidate variables range from simple vitals to long term trends and baseline deviations. Final models are selected by backward elimination on top cross-validated variables and validated on 3018 additional patients. The real-time acuity score (RAS) that I develop demonstrates strong discrimination ability for patient mortality, with an ROC area (AUC) of 0.880. The final model includes a number of variables known to be associated with mortality, but also computationally intensive variables absent in other severity scores. In addition to RAS, I also develop secondary outcome models that perform well at predicting pressor weaning (AUC=0.825), intraaortic balloon pump removal (AUC=0.816), the onset of septic shock (AUC=0.843), and acute kidney injury (AUC=0.742). Real-time mortality prediction is a feasible way to provide continuous risk assessment for ICU patients. RAS offers similar discrimination ability when compared to models computed once per day, based on aggregate data over that day. Moreover, RAS mortality predictions are better at discrimination than a customized SAPS II score (Day 3 AUC=0.878 vs AUC=0.849, p < 0.05). The secondary outcome models also provide interesting insights into patient responses to care and patient risk profiles. While models trained for specifically recognizing secondary outcomes consistently outperform the RAS model at their specific tasks, RAS provides useful baseline risk estimates throughout these events and in some cases offers a notable level of predictive utility.",315 p.,MIT-CSAIL-TR-2009-038,"Severity of Illness Index; Real-Time Systems; Decision Support Systems, Clinical; Pattern Recognition, Automated; Intensive Care; Hospital Mortality; Data Interpretation, Statistical; Artificial Intelligence; Decision Making, Computer-Assisted",Detecting Hazardous Intensive Care Patient Episodes Using Real-time Mortality Models,Clinical Decision-Making,PhD thesis,Thesis,Ph.D. in Computer Science,"Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science",,,,,,
Martin Rinard,"Ganesh, Vijay; Singh, Rishabh; Near, Joseph P.; Rinard, Martin",2009-08-26T22:00:06Z,2009-08-26T22:00:06Z,2009-08-26,http://hdl.handle.net/1721.1/46691,"We present AvatarSAT, a SAT solver that uses machine-learning classifiers to automatically tune the heuristics of an off-the-shelf SAT solver on a per-instance basis. The classifiers use features of both the input and conflict clauses to select parameter settings for the solver's tunable heuristics. On a randomly selected set of SAT problems chosen from the 2007 and 2008 SAT competitions, AvatarSAT is, on average, over two times faster than MiniSAT based on the geometric mean speedup measure and 50% faster based on the arithmeticmean speedup measure. Moreover, AvatarSAT is hundreds to thousands of times faster than MiniSAT on many hard SAT instances and is never more than twenty times slower than MiniSAT on any SAT instance.",7 p.,MIT-CSAIL-TR-2009-039,self-tuning; machine learning; SAT solvers,AvatarSAT: An Auto-tuning Boolean SAT Solver,Computer Architecture,,,,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,
Barbara Liskov,"Cheng, Winnie Wing-Yee",2009-08-27T22:00:05Z,2009-08-27T22:00:05Z,2009-08-27,http://hdl.handle.net/1721.1/46700,"Private and confidential information is increasingly stored online and increasingly being exposed due to human errors as well as malicious attacks. Information leaks threaten confidentiality, lead to lawsuits, damage enterprise reputations, and cost billion of dollars. While distributed computing architectures provide data and service integration, they also create information flow control problems due to the interaction complexity among service providers. A main problem is the lack of an appropriate programming model to capture expected information flow behaviors in these large distributed software infrastructures. This research tackles this problem by proposing a programming methodology and enforcement platform for application developers to protect and share their sensitive data. We introduce Aeolus, a new platform intended to make it easier to build distributed applications that avoid the unauthorized release of information. The Aeolus security model is based on information flow control but differs from previous work in ways that we believe make it easier to use and understand. In addition, Aeolus provides a number of new mechanisms (anonymous closures, compound tags, boxes, and shared volatile state) to ease the job of writing applications. This thesis provides examples to show how Aeolus features support secure distributed applications. It describes the system design issues and solutions in designing a prototype implementation and presents performance results that show our platform has low overhead.",177 p.,MIT-CSAIL-TR-2009-040,privacy; security; distributed systems; information flow control; integrity; secrecy; confidentiality,Information Flow for Secure Distributed Applications,Programming Methodology,PhD thesis,Thesis,,"Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science",,,,,,
Seth Teller,"Moore, David; Olson, Edwin; Huang, Albert",2009-09-04T15:45:08Z,2009-09-04T15:45:08Z,2009-09-02,http://hdl.handle.net/1721.1/46708,"We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, targeted at real-time robotics applications. LCM is comprised of several components: a data type specification language, a message passing system, logging/playback tools, and real-time analysis tools. LCM provides a platform- and language-independent type specification language. These specifications can be compiled into platform and language specific implementations, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. Messages can be transmitted between different processes using LCM's message-passing system, which implements a publish/subscribe model. LCM's implementation is notable in providing low-latency messaging and eliminating the need for a central communications ""hub"". This architecture makes it easy to mix simulated, recorded, and live data sources. A number of logging, playback, and traffic inspection tools simplify common development and debugging tasks. LCM is targeted at robotics and other real-time systems where low latency is critical; its messaging model permits dropping messages in order to minimize the latency of new messages. In this paper, we explain LCM's design, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots.",15 p.,MIT-CSAIL-TR-2009-041,message passing; interprocess communication; robotics middleware; real-time systems,Lightweight Communications and Marshalling for Low-Latency Interprocess Communication,"Robotics, Vision & Sensor Networks",,,,,,,,,,
Martin Rinard,"Hoffmann, Henry; Misailovic, Sasa; Sidiroglou, Stelios; Agarwal, Anant; Rinard, Martin",2009-09-04T15:45:14Z,2009-09-04T15:45:14Z,2009-09-03,http://hdl.handle.net/1721.1/46709,"Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are designed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. We present a new general technique, code perforation, for automatically augmenting existing computations with the capability of trading off accuracy in return for performance. In contrast to existing approaches, which typically require the manual development of new algorithms, our implemented SpeedPress compiler can automatically apply code perforation to existing computations with no developer intervention whatsoever. The result is a transformed computation that can respond almost immediately to a range of increased performancedemands while keeping any resulting output distortion within acceptable user-defined bounds. We have used SpeedPress to automatically apply code perforation to applications from the PARSEC benchmark suite. The results show that the transformed applications can run as much as two to three times faster than the original applications while distorting the output by less than 10%. Because the transformed applications can operate successfully at many points in the performance/accuracy tradeoff space, they can (dynamically and on demand) navigate the tradeoff space to either maximize performance subject to a given accuracy constraint, or maximize accuracy subject to a given performance constraint. We also demonstrate the SpeedGuard runtime system which uses code perforation to enable applications to automatically adapt to challenging execution environments such as multicore machines that suffer core failures or machines that dynamically adjust the clock speed to reduce power consumption or to protect the machine from overheating.",19 p.,MIT-CSAIL-TR-2009-042,,"Using Code Perforation to Improve Performance, Reduce Energy Consumption, and Respond to Failures",Computer Architecture,,,,,,,"Hoffmann, Henry; Misailovic, Sasa; Sidiroglou, Stelios; Agarwal, Anant; Rinard, Martin",,,
Gerald Sussman,"Beal, Jacob; Indurkhya, Sagar",2009-09-04T21:30:14Z,2009-09-04T21:30:14Z,2009-09-04,http://hdl.handle.net/1721.1/46710,"This code and data is publicly listed code for the LOLCAT Method developed by Sagar Indurkhya and Jacob Beal, in the paper: ""Reaction factoring and bipartite update graphs accelerate the Gillespie algorithm for large-scale biochemical systems.""",,,Computational Systems Biology; Gillespie Algorithm; Stochastic Simulation Algorithm; Bioinformatics,Code for LOLCAT Method (Variant of Gillespie Algorithm),Mathematics and Computation,,,,,Creative Commons Attrbution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,
,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2009-09-17T18:30:10Z,2009-09-17T18:30:10Z,2009-09-12,http://hdl.handle.net/1721.1/46722,A progress report describing the application of policy gradient and policy search by dynamic programming methods to an aircraft collision avoidance problem inspired by the requirements of next-generation TCAS.,11 p.,MIT-CSAIL-TR-2009-043,,Finding aircraft collision-avoidance strategies using policy search methods,Learning and Intelligent Systems,,,,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,
Nicholas Roy,"Roy, Nicholas; He, Ruijie",2009-09-28T21:00:15Z,2009-09-28T21:00:15Z,2009-09-23,http://hdl.handle.net/1721.1/46820,"Online, forward-search techniques have demonstrated promising results for solving problems in partially observable environments. These techniques depend on the ability to efficiently search and evaluate the set of beliefs reachable from the current belief. However, enumerating or sampling action-observation sequences to compute the reachable beliefs is computationally demanding; coupled with the need to satisfy real-time constraints, existing online solvers can only search to a limited depth. In this paper, we propose that policies can be generated directly from the distribution of the agent's posterior belief. When the underlying state distribution is Gaussian, and the observation function is an exponential family distribution, we can calculate this distribution of beliefs without enumerating the possible observations. This property not only enables us to plan in problems with large observation spaces, but also allows us to search deeper by considering policies composed of multi-step action sequences. We present the Posterior Belief Distribution (PBD) algorithm, an efficient forward-search POMDP planner for continuous domains, demonstrating that better policies are generated when we can perform deeper forward search.",12 p.,MIT-CSAIL-TR-2009-044,,Efficient POMDP Forward Search by Predicting the Posterior Belief Distribution,"Robotics, Vision & Sensor Networks",,,,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,
Frans Kaashoek,"Lesniewski-Laas, Chris; Kaashoek, M. Frans",2009-09-28T21:00:08Z,2009-09-28T21:00:08Z,2009-09-24,http://hdl.handle.net/1721.1/46819,"Decentralized systems, such as distributed hash tables, are subject to the Sybil attack, in which an adversary creates many false identities to increase its influence. This paper proposes a routing protocol for a distributed hash table that is strongly resistant to the Sybil attack. This is the first solution to this problem with sublinear run time and space usage. The protocol uses the social connections between users to build routing tables that enable Sybil-resistant distributed hash table lookups. With a social network of N well-connected honest nodes, the protocol can tolerate up to O(N/log N) ""attack edges"" (social links from honest users to phony identities). This means that an adversary has to fool a large fraction of the honest users before any lookups will fail. The protocol builds routing tables that contain O(N log^(3/2) N) entries per node. Lookups take O(1) time. Simulation results, using social network graphs from LiveJournal, Flickr, and YouTube, confirm the analytical results.",14 p.,MIT-CSAIL-TR-2009-045,dht; security; sybil; overlay; distributed hash table,Whanaungatanga: Sybil-proof routing with social networks,Parallel and Distributed Operating Systems,,,,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,
Tomaso Poggio,"Chikkerur, Sharat; Poggio, Tomaso; Serre, Thomas",2009-10-06T22:45:05Z,2009-10-06T22:45:05Z,2009-10-02,http://hdl.handle.net/1721.1/49415,"The human visual system can recognize several thousand object categories irrespective of their position and size. This combination of selectivity and invariance is built up gradually across several stages of visual processing. However, the recognition of multiple objects in cluttered visual scenes presents a difficult problem for human as well as machine vision systems. The human visual system has evolved to perform two stages of visual processing: a pre-attentive parallel processing stage, in which the entire visual field is processed at once and a slow serial attentive processing stage, in which aregion of interest in an input image is selected for ""specialized"" analysis by an attentional spotlight. We argue that this strategy evolved to overcome the limitation of purely feed forward processing in the presence of clutter and crowding. Using a Bayesian model of attention along with a hierarchical model of feed forward recognition on a data set of real world images, we show that this two stage attentive processing can improve recognition in cluttered and crowded conditions.",12 p.,CBCL-279; MIT-CSAIL-TR-2009-046,,Attentive processing improves object recognition,Center for Biological and Computational Learning (CBCL),,,,,,,,,,
Tomaso Poggio,"Chikkerur, Sharat; Serre, Thomas; Poggio, Tomaso",2009-10-06T22:45:10Z,2009-10-06T22:45:10Z,2009-10-03,http://hdl.handle.net/1721.1/49416,"The past four decades of research in visual neuroscience has generated a large and disparate body of literature on the role of attention [Itti et al., 2005]. Although several models have been developed to describe specific properties of attention, a theoretical framework that explains the computational role of attention and is consistent with all known effects is still needed. Recently, several authors have suggested that visual perception can be interpreted as a Bayesian inference process [Rao et al., 2002, Knill and Richards, 1996, Lee and Mumford, 2003]. Within this framework, topdown priors via cortical feedback help disambiguate noisy bottom-up sensory input signals. Building on earlier work by Rao [2005], we show that this Bayesian inference proposal can be extended to explain the role and predict the main properties of attention: namely to facilitate the recognition of objects in clutter. Visual recognition proceeds by estimating the posterior probabilities for objects and their locations within an image via an exchange of messages between ventral and parietal areas of the visual cortex. Within this framework, spatial attention is used to reduce the uncertainty in feature information; feature-based attention is used to reduce the uncertainty in location information. In conjunction, they are used to recognize objects in clutter. Here, we find that several key attentional phenomena such such as pop-out, multiplicative modulation and change in contrast response emerge naturally as a property of the network. We explain the idea in three stages. We start with developing a simplified model of attention in the brain identifying the primary areas involved and their interconnections. Secondly, we propose a Bayesian network where each node has direct neural correlates within our simplified biological model. Finally, we elucidate the properties of the resulting model, showing that the predictions are consistent with physiological and behavioral evidence.",18 p.,CBCL-280; MIT-CSAIL-TR-2009-047,,A Bayesian inference theory of attention: neuroscience and algorithms,Center for Biological and Computational Learning (CBCL),,,,,,,,,,
Rob Miller,"Miller, Rob; Karger, David; Marcus, Adam; Bernstein, Michael",2009-10-13T12:31:44Z,2009-10-13T12:31:44Z,2009-10-07,http://hdl.handle.net/1721.1/49426,"To find interesting, personally relevant web content, we often rely on friends and colleagues to pass links along as they encounter them. In this paper, we study and augment link-sharing via e-mail, the most popular means of sharing web content today. Armed with survey data indicating that active sharers of novel web content are often those that actively seek it out, we present FeedMe, a plug-in for Google Reader that makes directed sharing of content a more salient part of the user experience. Our survey research indicates that sharing is moderated by concern about relevancy to the recipient, a desire to send only novel content to the recipient, and the effort required to share. FeedMe allays these concerns by recommending friends who may be interested in seeing the content, providing information on what the recipient has seen and how many emails they have received recently, and giving recipients the opportunity to provide lightweight feedback when they appreciate shared content. FeedMe introduces a novel design space for mixed-initiative social recommenders: friends who know the user voluntarily vet the material on the userâ  s behalf. We present a two week field experiment (N=60) demonstrating that FeedMeâ  s recommendations and social awareness features made it easier and more enjoyable to share content that recipients appreciated and would not have found otherwise.",10 p.,MIT-CSAIL-TR-2009-048,friendsourcing; Social link sharing; blogs; RSS,Understanding and Supporting Directed Content Sharing on the Web,User Interface Design,,,,,,,,,,
Tomaso Poggio,"Shakhnarovich, Greg; Bouvrie, Jake; Rosasco, Lorenzo; Smale, Steve",2009-10-13T12:31:06Z,2009-10-13T12:31:06Z,2009-10-09,http://hdl.handle.net/1721.1/49425,"In these notes we focus on the concept of Shannon entropy in an attempt to provide a systematic way of assessing the discrimination properties of the neural response, and quantifying the role played by the number of layers and the number of templates.",6 p.,CBCL-281; MIT-CSAIL-TR-2009-049,computer vision; artificial intelligence; neuroscience; computation,Notes on the Shannon Entropy of the Neural Response,Center for Biological and Computational Learning (CBCL),,,,,,,,,,
Tomaso Poggio,"Rosasco, Lorenzo; Verri, Alessandro; Santoro, Matteo; Mosci, Sofia; Villa, Silvia",2009-10-14T21:00:10Z,2009-10-14T21:00:10Z,2009-10-14,http://hdl.handle.net/1721.1/49428,"In this paper we propose a general framework to characterize and solve the optimization problems underlying a large class of sparsity based regularization algorithms. More precisely, we study the minimization of learning functionals that are sums of a differentiable data term and a convex non differentiable penalty. These latter penalties have recently become popular in machine learning since they allow to enforce various kinds of sparsity properties in the solution. Leveraging on the theory of Fenchel duality and subdifferential calculus, we derive explicit optimality conditions for the regularized solution and propose a general iterative projection algorithm whose convergence to the optimal solution can be proved. The generality of the framework is illustrated, considering several examples of regularization schemes, including l1 regularization (and several variants), multiple kernel learning and multi-task learning. Finally, some features of the proposed framework are empirically studied.",28 p.,MIT-CSAIL-TR-2009-050; CBCL-282,computation; learning,Iterative Projection Methods for Structured Sparsity Regularization,Center for Biological and Computational Learning (CBCL),,,,,,,,,,
Ted Adelson,"Adelson, Edward H.; Torralba, Antonio; Fleming, Roland W.",2009-10-22T17:30:07Z,2009-10-22T17:30:07Z,2009-10-22,http://hdl.handle.net/1721.1/49511,,56 p.,MIT-CSAIL-TR-2009-051,specular; shape perception; Vision,Shape from Sheen,Vision,,,,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,
Tomaso Poggio,"Yu, Xinlin; Steele, Andrew D.; Khilnani, Vinita; Garrote, Estibaliz; Jhuang, Hueihan; Serre, Thomas; Poggio, Tomaso",2009-11-03T20:30:21Z,2009-11-03T20:30:21Z,2009-10-26,http://hdl.handle.net/1721.1/49527,"We describe a trainable computer vision system enabling the automated analysis of complex mouse behaviors. We provide software and a very large manually annotated video database used for training and testing the system. Our system outperforms leading commercial software and performs on par with human scoring, as measured from the ground-truth manual annotations of thousands of clips of freely behaving animals. We show that the home-cage behavior profiles provided by the system is sufficient to accurately predict the strain identity of individual animals in the case of two standard inbred and two non-standard mouse strains. Our software should complement existing sensor-based automated approaches and help develop an adaptable, comprehensive, high-throughput, fine-grained, automated analysis of rodent behavior.",27 p.,CBCL-283; MIT-CSAIL-TR-2009-052,animal monitoring; rodents,Automated home-cage behavioral phenotyping of mice,Center for Biological and Computational Learning (CBCL),,,,,Creative Commons Attribution-Noncommercial 3.0 Unported,http://creativecommons.org/licenses/by-nc/3.0/,,,,
Polina Golland,"Golland, Polina; Lashkari, Danial",2009-11-03T20:30:11Z,2009-11-03T20:30:11Z,2009-11-03,http://hdl.handle.net/1721.1/49526,"In this paper, we present a generative model for co-clustering and develop algorithms based on the mean field approximation for the corresponding modeling problem. These algorithms can be viewed as generalizations of the traditional model-based clustering; they extend hard co-clustering algorithms such as Bregman co-clustering to include soft assignments. We show empirically that these model-based algorithms offer better performance than their hard-assignment counterparts, especially with increasing problem complexity.",9 p.,MIT-CSAIL-TR-2009-054,,Co-Clustering with Generative Models,Vision,,,,,,,,,,
Gerald Sussman,"Radul, Alexey",2009-11-03T20:30:05Z,2009-11-03T20:30:05Z,2009-11-03,http://hdl.handle.net/1721.1/49525,"I propose a shift in the foundations of computation. Practically all ideas of general-purpose computation today are founded either on execution of sequences of atomic instructions, i.e., assembly languages, or on evaluation of tree-structured expressions, i.e., most higher level programming languages. Both have served us well in the past, but it is increasingly clear that we need something more. I suggest that we can build general-purpose computation on propagation of information through networks of stateful cells interconnected with stateless autonomous asynchronous computing elements. Various forms of this general idea have been used with great success for various special purposes; perhaps the most immediate example is constraint propagation in constraint satisfaction systems. These special-purpose systems, however, are all complex and all different, and neither compose well, nor interoperate well, nor generalize well. A foundational layer is missing. The key insight in this work is that a cell should not be seen as storing a value, but as accumulating information about a value. The cells should never forget information -- such monotonicity prevents race conditions in the behavior of the network. Monotonicity of information need not be a severe restriction: for example, carrying reasons for believing each thing makes it possible to explore but thenpossibly reject tentative hypotheses, thus appearing to undo something, while maintaining monotonicity. Accumulating information is a broad enough design principle to encompass arbitrary computation. The object of this dissertation is therefore to architect a general-purpose computing system based on propagation networks; to subsume expression evaluation under propagation just as instruction execution is subsumed under expression evaluation; to demonstrate that a general-purpose propagation system can recover all the benefits that have been derived from special-purpose propagation systems, allow them to compose andinteroperate, and offer further expressive power beyond what we have known in the past; and finally to contemplate the lessons that such a fundamental shift can teach us about the deep nature of computation.",174 p.,MIT-CSAIL-TR-2009-053,,Propagation Networks: A Flexible and Expressive Substrate for Computation,Mathematics and Computation,PhD thesis,,Doctor of Philosophy,Massachusetts Institute of Technology Department of Electrical Engineering and Computer Science,Creative Commons Attribution-Noncommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,2009-09,"My graduate career in general, and this work in particular, have been sponsored in part by a National Science Foundation Graduate Research Fellowship, by the Disruptive Technology Office as part of the AQUAINT Phase 3 research program, by the Massachusetts Institute of Technology, by Google, Inc., and by the National Science Foundation Cybertrust (05-518) program."
Anant Agarwal,"Agarwal, Anant; Santambrogio, Marco D.; Wingate, David; Eastep, Jonathan",2009-11-09T21:30:03Z,2009-11-09T21:30:03Z,2009-11-09,http://hdl.handle.net/1721.1/49808,"As multicore processors become increasingly prevalent, system complexity is skyrocketing. The advent of the asymmetric multicore compounds this -- it is no longer practical for an average programmer to balance the system constraints associated with today's multicores and worry about new problems like asymmetric partitioning and thread interference. Adaptive, or self-aware, computing has been proposed as one method to help application and system programmers confront this complexity. These systems take some of the burden off of programmers by monitoring themselves and optimizing or adapting to meet their goals. This paper introduces an open-source self-aware synchronization library for multicores and asymmetric multicores called Smartlocks. Smartlocks is a spin-lock library that adapts its internal implementation during execution using heuristics and machine learning to optimize toward a user-defined goal, which may relate to performance, power, or other problem-specific criteria. Smartlocks builds upon adaptation techniques from prior work like reactive locks, but introduces a novel form of adaptation designed for asymmetric multicores that we term lock acquisition scheduling. Lock acquisition scheduling is optimizing which waiter will get the lock next for the best long-term effect when multiple threads (or processes) are spinning for a lock. Our results demonstrate empirically that lock scheduling is important for asymmetric multicores and that Smartlocks significantly outperform conventional and reactive locks for asymmetries like dynamic variations in processor clock frequencies caused by thermal throttling events.",16 p.,MIT-CSAIL-TR-2009-055,,Smartlocks: Self-Aware Synchronization through Lock Acquisition Scheduling,Computer Architecture,,,,,,,,,,
