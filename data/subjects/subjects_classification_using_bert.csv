dc.contributor.advisor,dc.contributor.author,dc.date.accessioned,dc.date.available,dc.date.issued,dc.identifier.uri,dc.relation.ispartofseries,dc.title,dc.description.abstract,dc.identifier.other,dc.format.extent,dc.format.mimetype,dc.language.iso,dc.subject,dc.contributor.other,dc.description.sponsorship,dc.rights,dc.rights.uri,dc.description,dc.identifier.citation,dc.contributor,dc.relation.isversionof,dc.description.degree,dc.relation.isreplacedby,dc.relation.uri,dc.relation.replaces,dc.relation,dc.date.updated,dc.identifier,dc.relation.isreferencedby,dc.contributor.department,dc.type,dspace.orderedauthors,dc.date.submitted,dc.contributor.editor,dc.coverage.spatial,dc.coverage.temporal,eprint.grantNumber,dc.relation.requires,dc.language.rfc3066,dc.publisher,dc.relation.haspart,dc.identifier.oclc,year,research_paper_id,text,predicted_topic
,"Papageorgiou, Constantine P.",2004-10-01T13:59:58Z,2004-10-01T13:59:58Z,2000-05-01,http://hdl.handle.net/1721.1/5566,AITR-1685; CBCL-186,A Trainable System for Object Detection in Images and Video Sequences,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",AITR-1685; CBCL-186,128 p.; 72537763 bytes; 15910731 bytes,application/postscript; application/pdf,en_US,AI; MIT; Artificial Intelligence; object detection; pattern recognition; people detection; face detection; car detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,13,"a trainable system for object detection in images and video sequences this thesis presents a general, trainable  system for object detection in static images  and video sequences. the core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. the system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  to detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. this system is  applied to face, people, and car detection with  excellent results. for our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  in addition, we present a real-time version of  the system that is currently running in a  daimlerchrysler experimental vehicle. as part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. we find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. we also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",object recognition/detection
,"Evgeniou, Theodoros; Pontil, Massimiliano",2004-10-20T20:48:37Z,2004-10-20T20:48:37Z,2000-05-01,http://hdl.handle.net/1721.1/7169,AIM-1681; CBCL-184,A Note on the Generalization Performance of Kernel Classifiers with Margin,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,AIM-1681; CBCL-184,9 p.; 1149066 bytes; 253797 bytes,application/postscript; application/pdf,en_US,AI; MIT; Artificial Intelligence; missing data; mixture models; statistical learning; EM algorithm; neural networks; kernel classifiers; Support Vector Machine; regularization networks; statistical learning theory; V-gamma dimension.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,15,a note on the generalization performance of kernel classifiers with margin we present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. support vector machine classifiers (svm) stem out of this class of machines. the bounds are derived through computations of the $v_gamma$ dimension of a family of loss functions where the svm one belongs to. bounds that use functions of margin distributions (i.e. functions of the slack variables of svm) are derived.,high performance computing
,"Sezgin, Tevfik Metin",2004-10-20T20:28:30Z,2004-10-20T20:28:30Z,2001-05-01,http://hdl.handle.net/1721.1/7077,AITR-2001-009,Feature Point Detection and Curve Approximation for Early Processing of Freehand Sketches,"Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for  feature detection in strokes and apply this technique using two approaches to  signal processing, one using simple average based thresholding and a second  using scale space.",AITR-2001-009,82 p.; 10553461 bytes; 5067939 bytes,application/postscript; application/pdf,en_US,AI; Feature Point Detection; Curve Approximation; Freehand Sketching,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,55,"feature point detection and curve approximation for early processing of freehand sketches freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. we are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. one of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. we demonstrate how multiple sources of information can be combined for  feature detection in strokes and apply this technique using two approaches to  signal processing, one using simple average based thresholding and a second  using scale space.",object recognition/detection
,"Banks, Jessica",2004-10-20T20:28:07Z,2004-10-20T20:28:07Z,2001-05-01,http://hdl.handle.net/1721.1/7070,AITR-2001-005,Design and Control of an Anthropomorphic Robotic Finger with Multi-point Tactile Sensation,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",AITR-2001-005,88 p.; 17699541 bytes; 1837341 bytes,application/postscript; application/pdf,en_US,AI; tactile sensation; finger; robot; anthropomorphic; skin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,56,"design and control of an anthropomorphic robotic finger with multi-point tactile sensation the goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. we investigate this problem through the fabrication and simple control of a planar 2-dof robotic finger inspired by anatomic consistency, self-containment, and adaptability. the robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   the integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. however, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. these constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  in this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. to this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. this thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. the results of behavioral experiments with a simple tactilely-modulated control scheme are also described. the hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",robotics
,"Ucko, Aaron Mark",2004-10-20T20:28:09Z,2004-10-20T20:28:09Z,2001-05-01,http://hdl.handle.net/1721.1/7071,AITR-2001-006,Predicate Dispatching in the Common Lisp Object System,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",AITR-2001-006,74 p.; 2463955 bytes; 977046 bytes,application/postscript; application/pdf,en_US,AI; predicate dispatching; Common Lisp; CLOS; Weyl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,57,"predicate dispatching in the common lisp object system i have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the common lisp object system (clos). to demonstrate its utility, i used predicate dispatching to enhance weyl, a computer algebra system which doubles as a clos library. my result is dispatching-enhanced weyl (dew), a computer algebra system that i have demonstrated to be well suited for both users and programmers.",language models
,"Ho, Purdy",2004-10-20T20:48:40Z,2004-10-20T20:48:40Z,2001-05-31,http://hdl.handle.net/1721.1/7171,AIM-2001-010; CBCL-197,Rotation Invariant Real-time Face Detection and Recognition System,"In this report, a face recognition system that is capable of detecting and recognizing frontal and rotated faces was developed. Two face recognition methods focusing on the aspect of pose invariance are presented and evaluated - the whole face approach and the component-based approach. The main challenge of this project is to develop a system that is able to identify faces under different viewing angles in realtime. The development of such a system will enhance the capability and robustness of current face recognition technology.  The whole-face approach recognizes faces by classifying a single feature vector consisting of the gray values of the whole face image. The component-based approach  first locates the facial components and extracts them. These components are normalized and combined into a single feature vector for classification. The Support Vector Machine (SVM) is used as the classifier for both approaches. Extensive tests with respect to the robustness against pose changes are performed on a  database that includes faces rotated up to about 40 degrees in depth. The component-based approach clearly outperforms the whole-face approach on all tests. Although this approach isproven to be more reliable, it is still too slow for real-time applications. That is the reason why a real-time face recognition system using the whole-face approach is implemented to recognize people in color video sequences.",AIM-2001-010; CBCL-197,24 p.; 12501066 bytes; 896203 bytes,application/postscript; application/pdf,en_US,AI; vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,60,"rotation invariant real-time face detection and recognition system in this report, a face recognition system that is capable of detecting and recognizing frontal and rotated faces was developed. two face recognition methods focusing on the aspect of pose invariance are presented and evaluated - the whole face approach and the component-based approach. the main challenge of this project is to develop a system that is able to identify faces under different viewing angles in realtime. the development of such a system will enhance the capability and robustness of current face recognition technology.  the whole-face approach recognizes faces by classifying a single feature vector consisting of the gray values of the whole face image. the component-based approach  first locates the facial components and extracts them. these components are normalized and combined into a single feature vector for classification. the support vector machine (svm) is used as the classifier for both approaches. extensive tests with respect to the robustness against pose changes are performed on a  database that includes faces rotated up to about 40 degrees in depth. the component-based approach clearly outperforms the whole-face approach on all tests. although this approach isproven to be more reliable, it is still too slow for real-time applications. that is the reason why a real-time face recognition system using the whole-face approach is implemented to recognize people in color video sequences.",face detection
,"Nagpal, Radhika",2004-10-20T20:28:28Z,2004-10-20T20:28:28Z,2001-06-01,http://hdl.handle.net/1721.1/7076,AITR-2001-008,Programmable Self-Assembly: Constructing Global Shape using Biologically-inspire,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",AITR-2001-008,118 p.; 27221557 bytes; 1541086 bytes,application/postscript; application/pdf,en_US,AI; self-organisation; multi agent; developmental biology; amorphous computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,61,"programmable self-assembly: constructing global shape using biologically-inspire in this thesis i present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. the global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). i provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  with this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. examples  include flat layered shapes, all plane euclidean constructions, and a  variety of tessellation patterns. in contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. the cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. i show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  the language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. the language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of d'arcy thompson's famous coordinate  transformations. the thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",language models
,"Alvira, Mariano; Paris, Jim; Rifkin, Ryan",2004-10-20T21:03:36Z,2004-10-20T21:03:36Z,2001-07-01,http://hdl.handle.net/1721.1/7234,AIM-2001-012; CBCL-199,The Audiomomma Music Recommendation System,"We design and implement a system that recommends musicians to listeners. The basic idea is to keep track of what artists a user listens to, to find other users with similar tastes, and to recommend other artists that these similar listeners enjoy. The system utilizes a client-server architecture, a web-based interface, and an SQL database to store and process information. We describe Audiomomma-0.3, a proof-of-concept implementation of the above ideas.",AIM-2001-012; CBCL-199,10 p.; 2186561 bytes; 257129 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,63,"the audiomomma music recommendation system we design and implement a system that recommends musicians to listeners. the basic idea is to keep track of what artists a user listens to, to find other users with similar tastes, and to recommend other artists that these similar listeners enjoy. the system utilizes a client-server architecture, a web-based interface, and an sql database to store and process information. we describe audiomomma-0.3, a proof-of-concept implementation of the above ideas.",language models
,"Chan, Nicholas T.; Dahan, Ely; Lo, Andrew W.; Poggio, Tomaso",2004-10-20T21:03:35Z,2004-10-20T21:03:35Z,2001-07-01,http://hdl.handle.net/1721.1/7233,AIM-2001-013; CBCL-200,Experimental Markets for Product Concepts,"Market prices are well known to efficiently collect and aggregate diverse information regarding the value of commodities and assets. The role of markets has been particularly suitable to pricing financial securities. This article provides an alternative application of the pricing mechanism to marketing research - using pseudo-securities markets to measure preferences over new product concepts. Surveys, focus groups, concept tests and conjoint studies are methods traditionally used to measure individual and aggregate preferences. Unfortunately, these methods can be biased, costly and time-consuming to conduct. The present research is motivated by the desire to efficiently measure preferences and more accurately predict new product success, based on the efficiency and incentive-compatibility of security trading markets. The article describes a novel market research method, pro-vides insight into why the method should work, and compares the results of several trading experiments against other methodologies such as concept testing and conjoint analysis.",AIM-2001-013; CBCL-200,3069806 bytes; 287156 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,64,"experimental markets for product concepts market prices are well known to efficiently collect and aggregate diverse information regarding the value of commodities and assets. the role of markets has been particularly suitable to pricing financial securities. this article provides an alternative application of the pricing mechanism to marketing research - using pseudo-securities markets to measure preferences over new product concepts. surveys, focus groups, concept tests and conjoint studies are methods traditionally used to measure individual and aggregate preferences. unfortunately, these methods can be biased, costly and time-consuming to conduct. the present research is motivated by the desire to efficiently measure preferences and more accurately predict new product success, based on the efficiency and incentive-compatibility of security trading markets. the article describes a novel market research method, pro-vides insight into why the method should work, and compares the results of several trading experiments against other methodologies such as concept testing and conjoint analysis.",privacy/ethics
,"Russell, Richard; Sinha, Pawan",2004-10-20T21:03:39Z,2004-10-20T21:03:39Z,2001-07-01,http://hdl.handle.net/1721.1/7235,AIM-2001-014; CBCL-201,Perceptually-based Comparison of Image Similarity Metrics,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",AIM-2001-014; CBCL-201,13 p.; 9714300 bytes; 2612761 bytes,application/postscript; application/pdf,en_US,AI; Image matching; vector quantization; Minkowski metric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,65,"perceptually-based comparison of image similarity metrics the image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. two norms used commonly for this purpose are l1 and l2, which are specific instances of the minkowski metric. however, there is often not a principled reason for selecting one norm over the other. one way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. with this goal, we examined perceptual preferences for images retrieved on the basis of the l1 versus the l2 norm. these images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. in both conditions the subjects showed a consistent preference for images matched using the l1 metric. these results suggest that, in the domain of natural images of the kind we have used, the l1 metric may better capture human notions of image similarity.",image classification
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:41Z,2004-10-20T21:03:41Z,2001-07-25,http://hdl.handle.net/1721.1/7236,AIM-2001-015; CBCL-202,Recognizing Indoor Scenes,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,AIM-2001-015; CBCL-202,17 p.; 14931961 bytes; 3219314 bytes,application/postscript; application/pdf,en_US,AI; Scene classification; Navigation; scene representation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,66,recognizing indoor scenes we propose a scheme for indoor place identification based on the recognition of global scene views. scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. the holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. we demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. we develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,object recognition/detection
,"Zollei, Lilla",2004-10-20T20:28:33Z,2004-10-20T20:28:33Z,2001-08-01,http://hdl.handle.net/1721.1/7078,AITR-2002-001,2D-3D Rigid-Body Registration of X-Ray Fluoroscopy and CT Images,"The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation.   Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",AITR-2002-001,128 p.; 21043480 bytes; 1712245 bytes,application/postscript; application/pdf,en_US,AI; registration; medical imaging,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,69,"2d-3d rigid-body registration of x-ray fluoroscopy and ct images the registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. in applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. we propose a mutual information- based registration algorithm to establish the proper align- ment. for optimization purposes, we compare the perfor- mance of the non-gradient powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other parzen windowing to carry out probability density approximation.   our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3d-3d registra- tion problems to the 2d-3d scenario, which obviates the need for the generation of full drrs at each iteration of pose op- timization. this facilitates a considerable savings in compu- tation expense. we also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. registration results are presented on uoroscopy and ct datasets of a plastic pelvis and a real skull, and on a high-resolution ct- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",language models
,"Shelton, Christian Robert",2004-10-01T14:00:04Z,2004-10-01T14:00:04Z,2001-08-01,http://hdl.handle.net/1721.1/5568,AITR-2001-003; CBCL-204,Importance Sampling for Reinforcement Learning with Multiple Objectives,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",AITR-2001-003; CBCL-204,108 p.; 10551422 bytes; 1268632 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; RL; importance sampling; estimation; market-making,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,70,"importance sampling for reinforcement learning with multiple objectives this thesis considers three complications that arise from applying reinforcement learning to a real-world application. in the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  we employ importance sampling (likelihood ratios) to achieve good performance in partially observable markov decision processes with few data. our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. it can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. we present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. we demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. the thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",high performance computing
,"Sinha, Pawan; Torralba, Antonio",2004-10-20T21:03:43Z,2004-10-20T21:03:43Z,2001-08-01,http://hdl.handle.net/1721.1/7237,AIM-2001-017; CBCL-203,Role of Low-level Mechanisms in Brightness Perception,"Brightness judgments are a key part of the primate brain's visual analysis of the environment. There is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. However, it is unclear precisely how a region's context influences its perceived brightness. Recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. This work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. Here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. We find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. These results point to the important role of low-level mechanisms in determining brightness percepts.",AIM-2001-017; CBCL-203,17 p.; 5391865 bytes; 331759 bytes,application/postscript; application/pdf,en_US,AI; brightness perception; perceptual organization; local mechanisms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,71,"role of low-level mechanisms in brightness perception brightness judgments are a key part of the primate brain's visual analysis of the environment. there is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. however, it is unclear precisely how a region's context influences its perceived brightness. recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. this work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. we find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. these results point to the important role of low-level mechanisms in determining brightness percepts.",image classification
,"Beal, Jacob",2004-10-04T14:37:46Z,2004-10-04T14:37:46Z,2001-08-13,http://hdl.handle.net/1721.1/6082,AIM-2001-016,An Algorithm for Bootstrapping Communications,I present an algorithm which allows two agents to generate a simple language based only on observations of a shared environment. Vocabulary and roles for the language are learned in linear time. Communication is robust and degrades gradually as complexity increases. Dissimilar modes of experience will lead to a shared kernel vocabulary.,AIM-2001-016,35 p.; 6622296 bytes; 264031 bytes,application/postscript; application/pdf,en_US,AI; Adaptive Learning Hash-coding communication architecture algorithm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,72,an algorithm for bootstrapping communications i present an algorithm which allows two agents to generate a simple language based only on observations of a shared environment. vocabulary and roles for the language are learned in linear time. communication is robust and degrades gradually as complexity increases. dissimilar modes of experience will lead to a shared kernel vocabulary.,language models
,"Yeo, Gene; Poggio, Tomaso",2004-10-20T21:03:45Z,2004-10-20T21:03:45Z,2001-08-25,http://hdl.handle.net/1721.1/7238,AIM-2001-018; CBCL-206,Multiclass Classification of SRBCTs,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",AIM-2001-018; CBCL-206,17 p.; 6552074 bytes; 816114 bytes,application/postscript; application/pdf,en_US,AI; multiclass; SVM; feature selection; SRBCT; tumors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,73,"multiclass classification of srbcts a novel approach to multiclass tumor classification using artificial neural networks (anns) was introduced in a recent paper cite{khan2001}. the method successfully classified and diagnosed small, round blue cell tumors (srbcts) of childhood into four distinct categories, neuroblastoma (nb), rhabdomyosarcoma (rms), non-hodgkin lymphoma (nhl) and the ewing family of tumors (ews), using cdna gene expression profiles of samples that included both tumor biopsy material and cell lines. we report that using an approach similar to the one reported by yeang et al cite{yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. we report the performances of 3 binary classifiers (k-nearest neighbors (knn), weighted-voting (wv), and support vector machines (svm)) with 3 feature selection techniques (golub's signal to noise (sn) ratios cite{golub99}, fisher scores (fsc) and mukherjee's svm feature selection (svmfs))cite{sayan98}.",privacy/ethics
,"Rennie, Jason D. M.",2004-10-20T20:28:16Z,2004-10-20T20:28:16Z,2001-09-01,http://hdl.handle.net/1721.1/7074,AITR-2001-004,Improving Multi-class Text Classification with Naive Bayes,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",AITR-2001-004,49 p.; 2017370 bytes; 687421 bytes,application/postscript; application/pdf,en_US,AI; naive bayes; text; classification; feature selection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,77,"improving multi-class text classification with naive bayes there are numerous text documents available in electronic form. more and more are becoming available every day. such documents represent a massive amount of information that is easily accessible. seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. the accuracy and our understanding of such systems greatly influences their usefulness. in this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. we begin by clarifying the assumptions made in the derivation of naive bayes, noting basic properties and proposing ways for its extension and improvement. next, we investigate the quality of naive bayes parameter estimates and their impact on classification. our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with naive bayes using error-correcting output codes. we use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. greater understanding of naive bayes and the properties of text allows us to make better use of it in text classification.",high performance computing
,"Hong, Won",2004-10-20T20:28:24Z,2004-10-20T20:28:24Z,2001-09-01,http://hdl.handle.net/1721.1/7075,AITR-2001-007,"Modeling, Estimation, and Control of Robot-Soil Interactions","This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",AITR-2001-007,225 p.; 66603884 bytes; 4629577 bytes,application/postscript; application/pdf,en_US,AI; Robotics; Soil Modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,78,"modeling, estimation, and control of robot-soil interactions this thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. to this end, we design our manipulator with a flat plate for easy modeling of interactions. by using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  there are a number of existing earth pressure models. these models typically provide estimates of force which are in uncertain relation to the true force. a recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. in addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. this graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  the system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. the results are compared with friction angles obtained from direct shear tests. the results highlight a number of key points. common assumptions are made in soil modeling. most notably, the mohr-coulomb failure law and perfectly plastic behavior. in the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. this has ramifications for any study of friction done at low stresses. in addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. after accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",robotics
,"Bryson, Joanna J.",2004-10-20T20:29:10Z,2004-10-20T20:29:10Z,2001-09-01,http://hdl.handle.net/1721.1/7080,AITR-2002-003,Intelligence by Design: Principles of Modularity and Coordination for Engineerin,"All intelligence relies on search --- for  example, the search for an intelligent agent's  next action. Search is only likely to succeed in resource-bounded agents if they have already  been biased towards finding the right answer.  In artificial agents, the primary source of bias is engineering.   This dissertation describes an approach,  Behavior-Oriented Design (BOD) for  engineering complex agents. A complex agent  is one that must arbitrate between potentially  conflicting goals or behaviors.  Behavior-oriented design builds on work in  behavior-based and hybrid architectures for agents, and the object  oriented approach to software engineering.   The primary contributions of this dissertation  are:     1.The BOD architecture: a modular  architecture with each module providing  specialized representations to facilitate  learning.    This includes one pre-specified module  and representation for action selection or  behavior arbitration. The specialized    representation underlying BOD action  selection is Parallel-rooted, Ordered,  Slip-stack Hierarchical (POSH) reactive plans.     2.The BOD development process: an  iterative process that alternately scales the  agent's capabilities then optimizes the agent  for    simplicity, exploiting tradeoffs between the  component representations. This ongoing  process for controlling complexity not only    provides bias for the behaving agent, but  also facilitates its maintenance and  extendibility.   The secondary contributions of this  dissertation include two implementations of  POSH action selection, a procedure for  identifying useful idioms in agent architectures and  using them to distribute knowledge across  agent paradigms, several examples of  applying BOD idioms to established architectures, an  analysis and comparison of the attributes and  design trends of a large number of agent architectures, a comparison of biological  (particularly mammalian) intelligence to  artificial agent architectures, a novel model of primate transitive inference, and many other  examples of BOD agents and BOD  development.",AITR-2002-003,232 p.; 4544378 bytes; 1027952 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,79,"intelligence by design: principles of modularity and coordination for engineerin all intelligence relies on search --- for  example, the search for an intelligent agent's  next action. search is only likely to succeed in resource-bounded agents if they have already  been biased towards finding the right answer.  in artificial agents, the primary source of bias is engineering.   this dissertation describes an approach,  behavior-oriented design (bod) for  engineering complex agents. a complex agent  is one that must arbitrate between potentially  conflicting goals or behaviors.  behavior-oriented design builds on work in  behavior-based and hybrid architectures for agents, and the object  oriented approach to software engineering.   the primary contributions of this dissertation  are:     1.the bod architecture: a modular  architecture with each module providing  specialized representations to facilitate  learning.    this includes one pre-specified module  and representation for action selection or  behavior arbitration. the specialized    representation underlying bod action  selection is parallel-rooted, ordered,  slip-stack hierarchical (posh) reactive plans.     2.the bod development process: an  iterative process that alternately scales the  agent's capabilities then optimizes the agent  for    simplicity, exploiting tradeoffs between the  component representations. this ongoing  process for controlling complexity not only    provides bias for the behaving agent, but  also facilitates its maintenance and  extendibility.   the secondary contributions of this  dissertation include two implementations of  posh action selection, a procedure for  identifying useful idioms in agent architectures and  using them to distribute knowledge across  agent paradigms, several examples of  applying bod idioms to established architectures, an  analysis and comparison of the attributes and  design trends of a large number of agent architectures, a comparison of biological  (particularly mammalian) intelligence to  artificial agent architectures, a novel model of primate transitive inference, and many other  examples of bod agents and bod  development.",language models
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:49Z,2004-10-20T21:03:49Z,2001-09-01,http://hdl.handle.net/1721.1/7239,AIM-2001-020; CBCL-205,Contextual Priming for Object Detection,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",AIM-2001-020; CBCL-205,27 p.; 40187890 bytes; 5238575 bytes,application/postscript; application/pdf,en_US,AI; context; image statistics; Bayesian reasoning; recognition; focus of attention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,80,"contextual priming for object detection there is general consensus that context can be a rich source of information about an object's identity, location and scale. in fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. the resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",object recognition/detection
,"Dror, Ron O.; Adelson, Edward H.; Willsky, Alan S.",2004-10-08T20:36:32Z,2004-10-08T20:36:32Z,2001-09-01,http://hdl.handle.net/1721.1/6656,AIM-2001-023,Surface Reflectance Estimation and Natural Illumination Statistics,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",AIM-2001-023,22 p.; 7750699 bytes; 706071 bytes,application/postscript; application/pdf,en_US,AI; reflectance; lighting; BRDF; surface; illumination statistics; natural images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,81,"surface reflectance estimation and natural illumination statistics humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. we develop a machine vision system to perform similar recognition tasks automatically. reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. we have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. a human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. we develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. we also develop an automatic feature selection method.",image classification
,"Lee, Lily",2004-10-08T20:36:33Z,2004-10-08T20:36:33Z,2001-09-01,http://hdl.handle.net/1721.1/6657,AIM-2001-019,Gait Dynamics for Recognition and Classification,"This paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. Our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. We claim that our gait dynamics representation is rich enough for the task of recognition and classification. The use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. In addition, preliminary results are shown on gender classification using our gait dynamics features.",AIM-2001-019,12 p.; 1128480 bytes; 92054 bytes,application/postscript; application/pdf,en_US,AI; gait; recognition; gender classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,82,"gait dynamics for recognition and classification this paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. we claim that our gait dynamics representation is rich enough for the task of recognition and classification. the use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. we demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. in addition, preliminary results are shown on gender classification using our gait dynamics features.",privacy/ethics
,"Miller, Erik G.; Tieu, Kinh; Stauffer, Chris P.",2004-10-08T20:36:37Z,2004-10-08T20:36:37Z,2001-09-01,http://hdl.handle.net/1721.1/6659,AIM-2001-021,Learning Object-Independent Modes of Variation with Feature Flow Fields,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",AIM-2001-021,9 p.; 8233900 bytes; 814636 bytes,application/postscript; application/pdf,en_US,AI; Invariance; Optical Flow; Color Constancy; Object Recognition; image manifold,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,83,"learning object-independent modes of variation with feature flow fields we present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. these modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   we develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. we stress that it is learning a ""parameterization"", not just the parameter values, of the data. we then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. the model is superior to previous models of color change in describing non-linear color changes due to lighting.",language models
,"Yu, Angela J.; Giese, Martin A.; Poggio, Tomaso A.",2004-10-20T21:03:51Z,2004-10-20T21:03:51Z,2001-09-01,http://hdl.handle.net/1721.1/7240,AIM-2001-022; CBCL-207,Biologically Plausible Neural Circuits for Realization of Maximum Operations,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",AIM-2001-022; CBCL-207,28 p.; 2197042 bytes; 930880 bytes,application/postscript; application/pdf,en_US,AI; maximum operation; invariance; recurrent inhibition; shunting inhibition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,84,"biologically plausible neural circuits for realization of maximum operations object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. recent theoretical studies postulate a non-linear pooling function, such as the maximum (max) operation could be fundamental in achieving such invariance. in this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the max operation. four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. we derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",object recognition/detection
,"Taycher, Leonid; Darrell, Trevor",2004-10-08T20:36:35Z,2004-10-08T20:36:35Z,2001-09-01,http://hdl.handle.net/1721.1/6658,AIM-2001-024,Range Segmentation Using Visibility Constraints,"Visibility constraints can aid the segmentation of foreground objects observed with multiple range images. In our approach, points are defined as foreground if they can be determined to occlude some {em empty space} in the scene. We present an efficient algorithm to estimate foreground points in each range view using explicit epipolar search. In cases where the background pattern is stationary, we show how visibility constraints from other views can generate virtual background values at points with no valid depth in the primary view. We demonstrate the performance of both algorithms for detecting people in indoor office environments.",AIM-2001-024,10 p.; 15686301 bytes; 1574798 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,85,"range segmentation using visibility constraints visibility constraints can aid the segmentation of foreground objects observed with multiple range images. in our approach, points are defined as foreground if they can be determined to occlude some {em empty space} in the scene. we present an efficient algorithm to estimate foreground points in each range view using explicit epipolar search. in cases where the background pattern is stationary, we show how visibility constraints from other views can generate virtual background values at points with no valid depth in the primary view. we demonstrate the performance of both algorithms for detecting people in indoor office environments.",object recognition/detection
,"Arkoudas, Konstantine",2004-10-08T20:36:40Z,2004-10-08T20:36:40Z,2001-10-05,http://hdl.handle.net/1721.1/6661,AIM-2001-025,Type-alpha DPLs,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",AIM-2001-025,27 p.; 1766438 bytes; 815435 bytes,application/postscript; application/pdf,en_US,AI; Deduction; formal proofs; semantics; proof checking; soundness; logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,86,"type-alpha dpls this paper introduces denotational proof languages (dpls). dpls are languages for presenting, discovering, and checking formal proofs. in particular, in this paper we discus type-alpha dpls---a simple class of dpls  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  type-alpha dpls allow for lucid proof presentation and for efficient proof checking, but not for proof search.  type-omega dpls allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. we do not study type-omega dpls here.   we start by listing some common characteristics of dpls. we  then illustrate with a particularly simple example: a toy  type-alpha dpl called par, for deducing parities. we present the abstract syntax of par, followed by two  different kinds of formal semantics: evaluation and denotational.  we then relate the two semantics and show how proof checking  becomes tantamount to evaluation. we proceed to develop the  proof theory of par, formulating and studying certain  key notions such as observational equivalence that pervade all dpls.   we then present ndl, a type-alpha dpl for classical zero-order  natural deduction. our presentation of ndl mirrors that of par,  showing how every basic concept that was introduced in par resurfaces in ndl. we present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that dpl proofs are  readable, writable, and concise. next we contrast dpls to typed logics based  on the curry-howard isomorphism, and discuss the distinction between pure and augmented dpls. finally we consider the issue of  implementing dpls, presenting an implementation of par in sml and one in athena, and end with some concluding remarks.",language models
,"Arkoudas, Konstantine",2004-10-08T20:36:42Z,2004-10-08T20:36:42Z,2001-10-16,http://hdl.handle.net/1721.1/6662,AIM-2001-027,Type-omega DPLs,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",AIM-2001-027,62 p.; 3794425 bytes; 787916 bytes,application/postscript; application/pdf,en_US,AI; deduction; computation; proof search; soundness; logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,87,"type-omega dpls type-omega dpls (denotational proof languages) are languages for proof presentation and search that offer strong soundness guarantees. lcf-type systems such as hol offer similar guarantees, but their soundness relies heavily on static type systems. by contrast, dpls  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. this is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   every type-omega dpl properly contains a type-alpha dpl, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. methods arise naturally  via parametric abstraction over type-alpha proofs. in that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. the type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   methods are statically closed over lexical environments, but dynamically scoped over assumption bases. they can take other methods as arguments, they can iterate, and they can branch conditionally. these capabilities,  in tandem with the bifurcated syntax of type-omega dpls and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   we demonstrate every major feature of type-omega dpls by defining and studying ndl-omega, a higher-order, lexically scoped, call-by-value type-omega dpl for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. we start by illustrating how type-alpha dpls naturally lead to type-omega dpls by way of abstraction; present the formal syntax and semantics of ndl-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega dpls; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega dpls as general programming languages; show that dpls do not have to be type-less by formulating a static hindley-milner polymorphic type system for ndl-omega; discuss some idiosyncrasies of type-omega dpls such as the potential divergence of proof checking; and compare type-omega dpls to other approaches to proof presentation and discovery. finally, a complete implementation of ndl-omega in sml-nj is given for users who want to run the examples and experiment with the language.",language models
,"Rennie, Jason D. M.; Rifkin, Ryan",2004-10-20T21:03:52Z,2004-10-20T21:03:52Z,2001-10-16,http://hdl.handle.net/1721.1/7241,AIM-2001-026; CBCL-210,Improving Multiclass Text Classification with the Support Vector Machine,"We compare Naive Bayes and Support Vector Machines on the task of multiclass text classification. Using a variety of approaches to combine the underlying binary classifiers, we find that SVMs substantially outperform Naive Bayes. We present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets. We develop a new indicator of binary performance to show that the SVM's lower multiclass error is a result of its improved binary performance. Furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties.",AIM-2001-026; CBCL-210,14 p.; 1240992 bytes; 1091543 bytes,application/postscript; application/pdf,en_US,AI; text classification; support vector machine; multiclass classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,88,"improving multiclass text classification with the support vector machine we compare naive bayes and support vector machines on the task of multiclass text classification. using a variety of approaches to combine the underlying binary classifiers, we find that svms substantially outperform naive bayes. we present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets. we develop a new indicator of binary performance to show that the svm's lower multiclass error is a result of its improved binary performance. furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties.",high performance computing
,"Fleming, Roland W.; Dror, Ron O.; Adelson, Edward H.",2004-10-08T20:36:44Z,2004-10-08T20:36:44Z,2001-10-21,http://hdl.handle.net/1721.1/6663,AIM-2001-032,How do Humans Determine Reflectance Properties under Unknown Illumination?,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",AIM-2001-032,9 p.; 7609556 bytes; 945959 bytes,application/postscript; application/pdf,en_US,AI; illumination; reflectance; natural image statistics; human vision; BRDF,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,89,"how do humans determine reflectance properties under unknown illumination? under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. untextured materials such as these have different surface reflectance properties, including lightness and gloss. with single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. in order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. we found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. the spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",image classification
,"Dror, Ron O.; Edward H. Adelson,; Willsky, Alan S.",2004-10-08T20:36:53Z,2004-10-08T20:36:53Z,2001-10-21,http://hdl.handle.net/1721.1/6664,AIM-2001-033,Recognition of Surface Reflectance Properties from a Single Image under Unknown Real-World Illumination,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",AIM-2001-033,9 p.; 5961528 bytes; 831200 bytes,application/postscript; application/pdf,en_US,AI; illumination; reflectance; computer vision; geometry; natural image statistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,90,"recognition of surface reflectance properties from a single image under unknown real-world illumination this paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. we demonstrate performance of our algorithm for surfaces of arbitrary geometry. reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",image classification
,"Ostrovsky, Yuri; Cavanagh, Patrick; Sinha, Pawan",2004-10-20T21:03:57Z,2004-10-20T21:03:57Z,2001-11-05,http://hdl.handle.net/1721.1/7243,AIM-2001-029; CBCL-209,Perceiving Illumination Inconsistencies in Scenes,"The human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. Here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. Contrary to predictions from previous studies [Enns and Rensink, 1990; Sun and Perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. Our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. Specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",AIM-2001-029; CBCL-209,13 p.; 3418249 bytes; 947913 bytes,application/postscript; application/pdf,en_US,AI; Illumination; natural scene perception; lighting direction; pop-out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,92,"perceiving illumination inconsistencies in scenes the human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. contrary to predictions from previous studies [enns and rensink, 1990; sun and perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",high performance computing
,"Torralba, Antonio; Sinha, Pawan",2004-10-20T21:03:55Z,2004-10-20T21:03:55Z,2001-11-05,http://hdl.handle.net/1721.1/7242,AIM-2001-028; CBCL-208,Detecting Faces in Impoverished Images,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",AIM-2001-028; CBCL-208,14 p.; 20987363 bytes; 1810477 bytes,application/postscript; application/pdf,en_US,AI; Face detection; image resolution; contrast negation; vertical inversion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,93,"detecting faces in impoverished images the ability to detect faces in images is of critical ecological significance. it is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. here we address the question of how the visual system classifies images into face and non-face patterns. we focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",face detection
,"Corduneanu, Adrian; Jaakkola, Tommi",2004-10-08T20:37:18Z,2004-10-08T20:37:18Z,2001-11-08,http://hdl.handle.net/1721.1/6679,AIM-2001-030,Stable Mixing of Complete and Incomplete Information,"An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n^3) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",AIM-2001-030,9 p.; 1207127 bytes; 733599 bytes,application/postscript; application/pdf,en_US,AI; semi-supervised learning; incomplete data; EM; stable estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,94,"stable mixing of complete and incomplete information an increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. standard algorithms such as em (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. we provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. this approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. the approach readily applies to any graphical model in o(n^3) time where n is the number of parameters. we use the naive bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",language models
,"Arkoudas, Konstantine",2004-10-08T20:37:19Z,2004-10-08T20:37:19Z,2001-11-13,http://hdl.handle.net/1721.1/6680,AIM-2001-031,Simplifying transformations for type-alpha certificates,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",AIM-2001-031,45 p.; 2306816 bytes; 532283 bytes,application/postscript; application/pdf,en_US,AI; deduction; proofs; simplifiation; proof optimization; deduction complexity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,95,"simplifying transformations for type-alpha certificates this paper presents an algorithm for simplifying ndl deductions. an array of simplifying transformations are rigorously defined. they are shown to be terminating, and to respect the formal semantis of the language. we also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. we present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. all of the given transformations are fully implemented in sml-nj. the complete code listing is presented, along with explanatory comments. finally, although the transformations given here are defined for ndl, we point out that they can be applied to any type-alpha dpl that satisfies a few simple conditions.",language models
,"Torralba, Antonio; Oliva, Aude",2004-10-20T21:04:45Z,2004-10-20T21:04:45Z,2001-12-01,http://hdl.handle.net/1721.1/7267,AIM-2001-036; CBCL-213,Global Depth Perception from Familiar Scene Structure,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",AIM-2001-036; CBCL-213,22 p.; 40226611 bytes; 7425856 bytes,application/postscript; application/pdf,en_US,AI; depth; monocular; scale selection; natural images; scene recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,98,"global depth perception from familiar scene structure in the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. the interpretation of shading, edges and junctions may provide a 3d model of the scene but it will not inform about the actual ""size"" of the space. one possible source of information for absolute depth estimation is the image size of known objects. however, this is computationally complex due to the difficulty of the object recognition process. here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. the shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. we demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. we illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",object recognition/detection
,"Riesenhuber, Maximilian",2004-10-20T21:04:38Z,2004-10-20T21:04:38Z,2001-12-10,http://hdl.handle.net/1721.1/7265,AIM-2001-034; CBCL-211,"Generalization over contrast and mirror reversal, but not figure-ground reversal, in an ""edge-based","Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",AIM-2001-034; CBCL-211,3 p.; 798352 bytes; 91696 bytes,application/postscript; application/pdf,en_US,AI; AI; computational neuroscience; object recognition; macaque; IT; invariance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,99,"generalization over contrast and mirror reversal, but not figure-ground reversal, in an ""edge-based baylis & driver (nature neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (it) to various stimulus transformations. they report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. this finding is taken to demonstrate that ``the selectivity of it neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (riesenhuber & poggio, nature neuroscience, 1999). in this memo, i show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. this suggests for it cell tuning that the possible contributions of explicit edge assignment processes postulated in (baylis & driver, 2001) might be smaller than expected.",object recognition/detection
,"Yip, Andrew; Sinha, Pawan",2004-10-20T21:04:40Z,2004-10-20T21:04:40Z,2001-12-13,http://hdl.handle.net/1721.1/7266,AIM-2001-035; CBCL-212,Role of color in face recognition,"One of the key challenges in face perception lies in determining the contribution of different cues to face identification. In this study, we focus on the role of color cues. Although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. Here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. Under such conditions, recognition performance with color images is significantly better than that with grayscale images. Our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",AIM-2001-035; CBCL-212,12 p.; 1469164 bytes; 237772 bytes,application/postscript; application/pdf,en_US,AI; Face recognition; color; low-resolution; grayscale,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,100,"role of color in face recognition one of the key challenges in face perception lies in determining the contribution of different cues to face identification. in this study, we focus on the role of color cues. although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. under such conditions, recognition performance with color images is significantly better than that with grayscale images. our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",face detection
,"Beal, Jacob",2004-10-20T20:29:08Z,2004-10-20T20:29:08Z,2002-01-01,http://hdl.handle.net/1721.1/7079,AITR-2002-002,Generating Communications Systems Through Shared Context,"In a distributed model of intelligence, peer components need to communicate with one another. I present a system which enables two agents connected by a thick twisted bundle of wires to bootstrap a simple communication system from observations of a shared environment. The agents learn a large vocabulary of symbols, as well as inflections on those symbols which allow thematic role-frames to be transmitted. Language acquisition time is rapid and linear in the number of symbols and inflections. The final communication system is robust and performance degrades gradually in the face of problems.",AITR-2002-002,58 p.; 7876447 bytes; 588901 bytes,application/postscript; application/pdf,en_US,AI; distributed amorphous human intelligence genesis robust communication network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,103,"generating communications systems through shared context in a distributed model of intelligence, peer components need to communicate with one another. i present a system which enables two agents connected by a thick twisted bundle of wires to bootstrap a simple communication system from observations of a shared environment. the agents learn a large vocabulary of symbols, as well as inflections on those symbols which allow thematic role-frames to be transmitted. language acquisition time is rapid and linear in the number of symbols and inflections. the final communication system is robust and performance degrades gradually in the face of problems.",language models
,"Darrell, Trevor; Checka, Neal; Oh, Alice; Morency, Louis-Philippe",2004-10-08T20:37:23Z,2004-10-08T20:37:23Z,2002-01-01,http://hdl.handle.net/1721.1/6682,AIM-2002-001,Exploring Vision-Based Interfaces: How to Use Your Head in Dual Pointing Tasks,"The utility of vision-based face tracking for dual pointing tasks is evaluated. We first describe a 3-D face tracking technique based on real-time parametric motion-stereo, which is non-invasive, robust, and self-initialized. The tracker provides a real-time estimate of a ?frontal face ray? whose intersection with the display surface plane is used as a second stream of input for scrolling or pointing, in paral-lel with hand input. We evaluated the performance of com-bined head/hand input on a box selection and coloring task: users selected boxes with one pointer and colors with a second pointer, or performed both tasks with a single pointer. We found that performance with head and one hand was intermediate between single hand performance and dual hand performance. Our results are consistent with previously reported dual hand conflict in symmetric pointing tasks, and suggest that a head-based input stream should be used for asymmetric control.",AIM-2002-001,1 p.; 1612360 bytes; 298580 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,104,"exploring vision-based interfaces: how to use your head in dual pointing tasks the utility of vision-based face tracking for dual pointing tasks is evaluated. we first describe a 3-d face tracking technique based on real-time parametric motion-stereo, which is non-invasive, robust, and self-initialized. the tracker provides a real-time estimate of a ?frontal face ray? whose intersection with the display surface plane is used as a second stream of input for scrolling or pointing, in paral-lel with hand input. we evaluated the performance of com-bined head/hand input on a box selection and coloring task: users selected boxes with one pointer and colors with a second pointer, or performed both tasks with a single pointer. we found that performance with head and one hand was intermediate between single hand performance and dual hand performance. our results are consistent with previously reported dual hand conflict in symmetric pointing tasks, and suggest that a head-based input stream should be used for asymmetric control.",face detection
,"Freeman, William T.; Zhang, Hao",2004-10-08T20:37:26Z,2004-10-08T20:37:26Z,2002-01-10,http://hdl.handle.net/1721.1/6683,AIM-2002-002,Shape-Time Photography,"We introduce a new method to describe, in a single image, changes in  shape over time. We acquire both range and image information with a  stationary stereo camera. From the pictures taken, we display a  composite image consisting of the image data from the  surface closest to the camera at every pixel. This reveals the 3-d  relationships over time by easy-to-interpret occlusion relationships  in the composite image. We call the composite a shape-time  photograph.   Small errors in depth measurements cause artifacts in the shape-time  images. We correct most of these using a Markov network to estimate  the most probable front surface, taking into account the depth  measurements, their uncertainties, and layer continuity assumptions.",AIM-2002-002,6 p.; 6494953 bytes; 11283819 bytes,application/postscript; application/pdf,en_US,AI; video summarization; stereo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,105,"shape-time photography we introduce a new method to describe, in a single image, changes in  shape over time. we acquire both range and image information with a  stationary stereo camera. from the pictures taken, we display a  composite image consisting of the image data from the  surface closest to the camera at every pixel. this reveals the 3-d  relationships over time by easy-to-interpret occlusion relationships  in the composite image. we call the composite a shape-time  photograph.   small errors in depth measurements cause artifacts in the shape-time  images. we correct most of these using a markov network to estimate  the most probable front surface, taking into account the depth  measurements, their uncertainties, and layer continuity assumptions.",image classification
,"Poggio, Tomaso; Rifkin, Ryan; Mukherjee, Sayan; Rakhlin, Alex",2004-10-20T21:04:57Z,2004-10-20T21:04:57Z,2002-03-01,http://hdl.handle.net/1721.1/7268,AIM-2002-003; CBCL-214,Bagging Regularizes,"Intuitively, we expect that averaging --- or  bagging --- different regressors with low correlation should  smooth their behavior and be somewhat similar to regularization. In this  note we make this intuition precise. Using an almost classical  definition of stability, we prove that a certain form of averaging  provides generalization bounds with a rate of convergence of the  same order as Tikhonov regularization --- similar to fashionable RKHS-based learning algorithms.",AIM-2002-003; CBCL-214,7 p.; 906324 bytes; 285651 bytes,application/postscript; application/pdf,en_US,AI; Bagging; stability; regularization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,118,"bagging regularizes intuitively, we expect that averaging --- or  bagging --- different regressors with low correlation should  smooth their behavior and be somewhat similar to regularization. in this  note we make this intuition precise. using an almost classical  definition of stability, we prove that a certain form of averaging  provides generalization bounds with a rate of convergence of the  same order as tikhonov regularization --- similar to fashionable rkhs-based learning algorithms.",image classification
,"Knoblich, Ulf; Riesenhuber, Maximilan",2004-10-20T21:04:59Z,2004-10-20T21:04:59Z,2002-03-15,http://hdl.handle.net/1721.1/7269,AIM-2002-004; CBCL-215,Stimulus Simplification and Object Representation: A Modeling Study,"Tsunoda et al. (2001) recently studied the  nature of object representation in monkey  inferotemporal cortex using a combination of  optical imaging and extracellular recordings.  In particular, they examined IT neuron  responses to complex natural objects and  ""simplified"" versions thereof. In that study, in  42% of the cases, optical imaging revealed a  decrease in the number of activation patches  in IT as stimuli were ""simplified"". However, in  58% of the cases, ""simplification"" of the  stimuli actually led to the appearance of  additional activation patches in IT. Based on  these results, the authors propose a scheme  in which an object is represented by  combinations of active and inactive columns  coding for individual features.  We examine the patterns of activation caused  by the same stimuli as used by Tsunoda et al.  in our model of object recognition in cortex  (Riesenhuber 99). We find that object-tuned  units can show a pattern of appearance and  disappearance of features identical to the  experiment. Thus, the data of Tsunoda et al.  appear to be in quantitative agreement with a  simple object-based representation in which  an object's identity is coded by its similarities  to reference objects. Moreover, the agreement  of simulations and experiment suggests that  the simplification procedure used by Tsunoda  (2001) is not necessarily an accurate method  to determine neuronal tuning.",AIM-2002-004; CBCL-215,7 p.; 1367647 bytes; 641994 bytes,application/postscript; application/pdf,en_US,AI; computational neuroscience object recognition representation simplification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,120,"stimulus simplification and object representation: a modeling study tsunoda et al. (2001) recently studied the  nature of object representation in monkey  inferotemporal cortex using a combination of  optical imaging and extracellular recordings.  in particular, they examined it neuron  responses to complex natural objects and  ""simplified"" versions thereof. in that study, in  42% of the cases, optical imaging revealed a  decrease in the number of activation patches  in it as stimuli were ""simplified"". however, in  58% of the cases, ""simplification"" of the  stimuli actually led to the appearance of  additional activation patches in it. based on  these results, the authors propose a scheme  in which an object is represented by  combinations of active and inactive columns  coding for individual features.  we examine the patterns of activation caused  by the same stimuli as used by tsunoda et al.  in our model of object recognition in cortex  (riesenhuber 99). we find that object-tuned  units can show a pattern of appearance and  disappearance of features identical to the  experiment. thus, the data of tsunoda et al.  appear to be in quantitative agreement with a  simple object-based representation in which  an object's identity is coded by its similarities  to reference objects. moreover, the agreement  of simulations and experiment suggests that  the simplification procedure used by tsunoda  (2001) is not necessarily an accurate method  to determine neuronal tuning.",object recognition/detection
,"Sullivan, Gregory T.",2004-10-08T20:37:46Z,2004-10-08T20:37:46Z,2002-03-22,http://hdl.handle.net/1721.1/6686,AIM-2002-005,"Advanced Programming Language Features for Executable Design Patterns ""Better Patterns Through Reflection","The Design Patterns book [GOF95] presents  24 time-tested patterns that consistently appear in well-designed  software systems. Each pattern is presented with a description of the  design problem the pattern addresses, as well as sample  implementation code and design considerations. This paper explores how the  patterns from the ""Gang of Four'', or ""GOF'' book, as it is often called,  appear when similar problems are addressed using a dynamic,  higher-order, object-oriented programming language. Some of the  patterns disappear -- that is, they are supported directly by language features,  some patterns are simpler or have a different focus, and some are  essentially unchanged.",AIM-2002-005,45 p.; 1734113 bytes; 322829 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,121,"advanced programming language features for executable design patterns ""better patterns through reflection the design patterns book [gof95] presents  24 time-tested patterns that consistently appear in well-designed  software systems. each pattern is presented with a description of the  design problem the pattern addresses, as well as sample  implementation code and design considerations. this paper explores how the  patterns from the ""gang of four'', or ""gof'' book, as it is often called,  appear when similar problems are addressed using a dynamic,  higher-order, object-oriented programming language. some of the  patterns disappear -- that is, they are supported directly by language features,  some patterns are simpler or have a different focus, and some are  essentially unchanged.",language models
,"Finney, Sarah; Gardiol, Natalia H.; Kaelbling, Leslie Pack; Oates, Tim",2004-10-08T20:37:45Z,2004-10-08T20:37:45Z,2002-04-10,http://hdl.handle.net/1721.1/6685,AIM-2002-006,Learning with Deictic Representation,"Most reinforcement learning methods operate  on propositional representations of the world state. Such  representations are often intractably large and generalize poorly. Using  a deictic representation is believed to be a viable  alternative: they promise generalization while allowing the use of  existing reinforcement-learning methods. Yet, there  are few experiments on learning with deictic representations reported  in the literature. In this paper we explore the effectiveness of two  forms of deictic representation and a naive propositional  representation in a simple blocks-world domain. We find,  empirically, that the deictic representations actually worsen performance.  We conclude with a discussion of possible causes of these  results and strategies for more effective learning in domains with objects.",AIM-2002-006,41 p.; 5712208 bytes; 1294450 bytes,application/postscript; application/pdf,en_US,AI; Reinforcement Learning; Partial Observability; Representations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,124,"learning with deictic representation most reinforcement learning methods operate  on propositional representations of the world state. such  representations are often intractably large and generalize poorly. using  a deictic representation is believed to be a viable  alternative: they promise generalization while allowing the use of  existing reinforcement-learning methods. yet, there  are few experiments on learning with deictic representations reported  in the literature. in this paper we explore the effectiveness of two  forms of deictic representation and a naive propositional  representation in a simple blocks-world domain. we find,  empirically, that the deictic representations actually worsen performance.  we conclude with a discussion of possible causes of these  results and strategies for more effective learning in domains with objects.",object recognition/detection
,"Knoblich, Ulf; Freedman, David J.; Riesenhuber, Maximilian",2004-10-20T21:05:01Z,2004-10-20T21:05:01Z,2002-04-18,http://hdl.handle.net/1721.1/7270,AIM-2002-007; CBCL-216,Categorization in IT and PFC: Model and Experiments,"In a recent experiment, Freedman et al.  recorded from inferotemporal (IT) and prefrontal cortices (PFC) of monkeys  performing a ""cat/dog"" categorization task (Freedman 2001 and  Freedman, Riesenhuber, Poggio, Miller 2001). In this paper we analyze the tuning properties of view-tuned  units in our HMAX model of object recognition in cortex (Riesenhuber  1999) using the same paradigm and stimuli  as in the experiment. We then compare the simulation results to the monkey  inferotemporal neuron population data. We find that view-tuned  model IT units that were trained without any explicit category  information can show category-related tuning as observed in the  experiment. This suggests that the tuning properties of experimental IT  neurons might primarily be shaped by bottom-up stimulus-space  statistics, with little influence of top-down task-specific  information. The population of experimental PFC neurons, on the other hand,  shows tuning properties that cannot be explained just by stimulus  tuning. These analyses are compatible with a model of object recognition  in cortex (Riesenhuber 2000)  in which a population of shape-tuned  neurons provides a general basis for neurons tuned to  different recognition tasks.",AIM-2002-007; CBCL-216,11 p.; 1497623 bytes; 678374 bytes,application/postscript; application/pdf,en_US,AI; categorization IT PFC computational neuroscience model HMAX,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,125,"categorization in it and pfc: model and experiments in a recent experiment, freedman et al.  recorded from inferotemporal (it) and prefrontal cortices (pfc) of monkeys  performing a ""cat/dog"" categorization task (freedman 2001 and  freedman, riesenhuber, poggio, miller 2001). in this paper we analyze the tuning properties of view-tuned  units in our hmax model of object recognition in cortex (riesenhuber  1999) using the same paradigm and stimuli  as in the experiment. we then compare the simulation results to the monkey  inferotemporal neuron population data. we find that view-tuned  model it units that were trained without any explicit category  information can show category-related tuning as observed in the  experiment. this suggests that the tuning properties of experimental it  neurons might primarily be shaped by bottom-up stimulus-space  statistics, with little influence of top-down task-specific  information. the population of experimental pfc neurons, on the other hand,  shows tuning properties that cannot be explained just by stimulus  tuning. these analyses are compatible with a model of object recognition  in cortex (riesenhuber 2000)  in which a population of shape-tuned  neurons provides a general basis for neurons tuned to  different recognition tasks.",object recognition/detection
,"Steinbach, Carl",2004-10-20T20:29:42Z,2004-10-20T20:29:42Z,2002-05-01,http://hdl.handle.net/1721.1/7093,AITR-2002-007,A Reinforcement-Learning Approach to Power Management,"We describe an adaptive, mid-level approach  to the wireless device power management problem. Our approach  is based on reinforcement learning, a machine learning  framework for autonomous agents. We describe how our  framework can be applied to the power management problem in both  infrastructure and ad~hoc wireless networks. From this thesis we conclude that  mid-level power management policies can outperform low-level policies and  are more convenient to implement than high-level policies. We also  conclude that power management policies need to adapt to the  user and network, and that a mid-level power management framework  based on reinforcement learning fulfills these requirements.",AITR-2002-007,41 p.; 8457203 bytes; 989455 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; power management; wireless networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,129,"a reinforcement-learning approach to power management we describe an adaptive, mid-level approach  to the wireless device power management problem. our approach  is based on reinforcement learning, a machine learning  framework for autonomous agents. we describe how our  framework can be applied to the power management problem in both  infrastructure and ad~hoc wireless networks. from this thesis we conclude that  mid-level power management policies can outperform low-level policies and  are more convenient to implement than high-level policies. we also  conclude that power management policies need to adapt to the  user and network, and that a mid-level power management framework  based on reinforcement learning fulfills these requirements.",language models
,"Huang, Andrew ""bunnie""",2004-10-08T20:38:06Z,2004-10-08T20:38:06Z,2002-05-26,http://hdl.handle.net/1721.1/6694,AIM-2002-008,Keeping Secrets in Hardware: the Microsoft Xbox(TM) Case Study,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",AIM-2002-008,15 p.; 837733 bytes; 527464 bytes,application/postscript; application/pdf,en_US,AI; Tamper-resistant hardware; Microsoft Xbox; Cryptography; Privacy; Public Key Algos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,130,"keeping secrets in hardware: the microsoft xbox(tm) case study this paper discusses the hardware foundations of the cryptosystem employed by the xbox(tm) video game console from microsoft. a secret boot block overlay is buried within a system asic. this secret boot block decrypts and verifies portions of an external flash-type rom. the presence of the secret boot block is camouflaged by a decoy boot block in the external rom. the code contained within the secret boot block is transferred to the cpu in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. the paper concludes with recommendations for improving the xbox security system. one lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance fpgas.",privacy/ethics
,"Huang, Andrew ""bunnie""",2004-10-20T20:29:51Z,2004-10-20T20:29:51Z,2002-06-01,http://hdl.handle.net/1721.1/7096,AITR-2002-006,ADAM: A Decentralized Parallel Computer Architecture Featuring Fast Thread and Data Migration and a Uniform Hardware Abstraction,"The furious pace of Moore's Law is driving  computer architecture into a realm where the the speed of light is the  dominant factor in system latencies. The number of clock cycles to span  a chip are increasing, while the number of bits that can be accessed  within a clock cycle is decreasing. Hence, it is becoming more  difficult to hide latency. One alternative solution is to reduce latency by  migrating threads and data, but the overhead of existing  implementations has previously made migration an unserviceable solution so  far.  I present an architecture, implementation, and  mechanisms that reduces the overhead of migration to the point where  migration is a viable supplement to other latency hiding  mechanisms, such as multithreading. The architecture is abstract,  and presents programmers with a simple, uniform fine-grained  multithreaded parallel programming model with implicit memory management. In  other words, the spatial nature and implementation details (such as  the number of processors) of a parallel machine are entirely hidden from  the programmer. Compiler writers are  encouraged to devise programming languages for the machine that guide a  programmer to express their ideas in terms of objects, since objects exhibit  an inherent physical locality of data and code. The machine  implementation can then leverage this locality to automatically distribute  data and threads across the physical machine by using a set of  high performance migration mechanisms.  An implementation of this architecture could  migrate a null thread in 66 cycles -- over a factor of 1000 improvement  over previous work. Performance also scales well; the time  required to move a typical thread is only 4 to 5 times that of a null  thread. Data migration performance is similar, and scales  linearly with data block size. Since the performance of the migration  mechanism is on par with that of an L2 cache, the implementation  simulated in my work has no data caches and relies instead on  multithreading and the migration mechanism to hide and reduce access  latencies.",AITR-2002-006,299 p.; 13404896 bytes; 2307234 bytes,application/postscript; application/pdf,en_US,AI; HPC parallel computer architecture queues fault tolerance programmability ADAM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,137,"adam: a decentralized parallel computer architecture featuring fast thread and data migration and a uniform hardware abstraction the furious pace of moore's law is driving  computer architecture into a realm where the the speed of light is the  dominant factor in system latencies. the number of clock cycles to span  a chip are increasing, while the number of bits that can be accessed  within a clock cycle is decreasing. hence, it is becoming more  difficult to hide latency. one alternative solution is to reduce latency by  migrating threads and data, but the overhead of existing  implementations has previously made migration an unserviceable solution so  far.  i present an architecture, implementation, and  mechanisms that reduces the overhead of migration to the point where  migration is a viable supplement to other latency hiding  mechanisms, such as multithreading. the architecture is abstract,  and presents programmers with a simple, uniform fine-grained  multithreaded parallel programming model with implicit memory management. in  other words, the spatial nature and implementation details (such as  the number of processors) of a parallel machine are entirely hidden from  the programmer. compiler writers are  encouraged to devise programming languages for the machine that guide a  programmer to express their ideas in terms of objects, since objects exhibit  an inherent physical locality of data and code. the machine  implementation can then leverage this locality to automatically distribute  data and threads across the physical machine by using a set of  high performance migration mechanisms.  an implementation of this architecture could  migrate a null thread in 66 cycles -- over a factor of 1000 improvement  over previous work. performance also scales well; the time  required to move a typical thread is only 4 to 5 times that of a null  thread. data migration performance is similar, and scales  linearly with data block size. since the performance of the migration  mechanism is on par with that of an l2 cache, the implementation  simulated in my work has no data caches and relies instead on  multithreading and the migration mechanism to hide and reduce access  latencies.",high performance computing
,"Brown, Jeremy Hanford",2004-10-20T20:06:13Z,2004-10-20T20:06:13Z,2002-06-01,http://hdl.handle.net/1721.1/6910,AITR-2002-005,"Sparsely Faceted Arrays: A Mechanism Supporting Parallel Allocation, Communication, and Garbage Collection","Conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. In this  thesis, I introduce sparsely faceted arrays (SFAs), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. Sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the Cray T3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. A sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. By providing simple semantics and  making efficient use of memory, SFAs enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. I present example applications which use  SFAs, and describe and evaluate simple hardware mechanisms for  implementing SFAs.  Keeping track of which nodes have allocated  facets for a particular SFA is an important task that suggests the  need for automatic memory management, including garbage collection.  To address this need, I first argue that conventional tracing  techniques such as mark/sweep and copying GC are inherently unscalable in  parallel systems. I then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. I also discuss opportunities  for hardware support of this garbage collection strategy.  I have implemented a high-level hardware/OS  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. I describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of SFAs and SFA-aware garbage collection. Simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",AITR-2002-005,115 p.; 3145524 bytes; 677754 bytes,application/postscript; application/pdf,en_US,AI; sparsely faceted arrays; shared memory; garbage collection; data structures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,138,"sparsely faceted arrays: a mechanism supporting parallel allocation, communication, and garbage collection conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. in this  thesis, i introduce sparsely faceted arrays (sfas), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the cray t3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. a sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. by providing simple semantics and  making efficient use of memory, sfas enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. i present example applications which use  sfas, and describe and evaluate simple hardware mechanisms for  implementing sfas.  keeping track of which nodes have allocated  facets for a particular sfa is an important task that suggests the  need for automatic memory management, including garbage collection.  to address this need, i first argue that conventional tracing  techniques such as mark/sweep and copying gc are inherently unscalable in  parallel systems. i then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. i also discuss opportunities  for hardware support of this garbage collection strategy.  i have implemented a high-level hardware/os  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. i describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of sfas and sfa-aware garbage collection. simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",language models
,"Kim, Adlar J.; Shelton, Christian R.",2004-10-20T21:05:02Z,2004-10-20T21:05:02Z,2002-06-01,http://hdl.handle.net/1721.1/7271,AIM-2002-009; CBCL-217,Modeling Stock Order Flows and Learning Market-Making from Data,"Stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. In this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  The sequence of buys and sells for a  particular stock, the order flow, we model as an Input-Output Hidden Markov  Model fit to historical data. When combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. Our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. We  demonstrate learning results for two separate real stocks.",AIM-2002-009; CBCL-217,7 p.; 2119856 bytes; 1370177 bytes,application/postscript; application/pdf,en_US,AI; input/output HMM; market-making; reinforcement learning; stock order flow model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,139,"modeling stock order flows and learning market-making from data stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. in this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  the sequence of buys and sells for a  particular stock, the order flow, we model as an input-output hidden markov  model fit to historical data. when combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. we  demonstrate learning results for two separate real stocks.",language models
,"III, Teodoro Arvizo",2004-10-20T20:29:40Z,2004-10-20T20:29:40Z,2002-06-01,http://hdl.handle.net/1721.1/7092,AITR-2002-004,A Virtual Machine for a Type-omega Denotational Proof Language,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",AITR-2002-004,106 p.; 2935187 bytes; 816842 bytes,application/postscript; application/pdf,en_US,AI; virtual machine; SECD; SECD machine; denotational proof language; Athena,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,140,"a virtual machine for a type-omega denotational proof language in this thesis, i designed and implemented a  virtual machine (vm) for a monomorphic  variant of athena, a type-omega denotational  proof language (dpl). this machine  attempts to maintain the minimum state required to evaluate athena phrases. this  thesis also includes the design and  implementation of a compiler for  monomorphic athena that compiles to the vm.  finally, it includes details on my  implementation of a read-eval-print loop that  glues together the vm core and the compiler  to provide a full, user-accessible  interface to monomorphic athena. the athena  vm provides the same basis for dpls that the  secd machine does for pure, functional  programming and the warren abstract machine does for prolog.",language models
,"Werfel, Justin",2004-10-08T20:38:09Z,2004-10-08T20:38:09Z,2002-07-01,http://hdl.handle.net/1721.1/6696,AIM-2002-010,Implementing Universal Computation in an Evolutionary System,"Evolutionary algorithms are a common tool in  engineering and in the study of natural  evolution. Here we take their use in a new  direction by showing how they can be made to  implement a universal computer. We  consider populations of individuals with  genes whose values are the variables of  interest. By allowing them to interact with one  another in a specified environment with  limited resources, we demonstrate the ability  to construct any arbitrary logic circuit. We  explore models based on the limits of small  and large populations, and show examples of  such a system in action, implementing a  simple logic circuit.",AIM-2002-010,17 p.; 3942374 bytes; 1153028 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,146,"implementing universal computation in an evolutionary system evolutionary algorithms are a common tool in  engineering and in the study of natural  evolution. here we take their use in a new  direction by showing how they can be made to  implement a universal computer. we  consider populations of individuals with  genes whose values are the variables of  interest. by allowing them to interact with one  another in a specified environment with  limited resources, we demonstrate the ability  to construct any arbitrary logic circuit. we  explore models based on the limits of small  and large populations, and show examples of  such a system in action, implementing a  simple logic circuit.",language models
,"Schneider, Robert; Riesenhuber, Maximilian",2004-10-20T20:48:51Z,2004-10-20T20:48:51Z,2002-08-01,http://hdl.handle.net/1721.1/7178,AIM-2002-011; CBCL-218,A Detailed Look at Scale and Translation Invariance in a Hierarchical Neural Model of Visual Object Recognition,"The HMAX model has recently been proposed  by Riesenhuber & Poggio as a  hierarchical  model of position- and size-invariant object  recognition in visual cortex. It has also turned  out to model successfully a number of other  properties of the ventral visual stream (the  visual pathway thought to be crucial for object  recognition in cortex), and particularly of (view-tuned) neurons in macaque inferotemporal  cortex, the brain area at the top of the ventral  stream. The original modeling study only  used ``paperclip'' stimuli, as in the  corresponding physiology experiment, and did  not explore systematically how model units'  invariance properties depended on model  parameters. In this study, we aimed at a  deeper understanding of the inner workings of  HMAX and its performance for various  parameter settings and ``natural'' stimulus  classes. We examined HMAX responses for  different stimulus sizes and positions  systematically and found a dependence of  model units' responses on stimulus position  for which a quantitative description is offered.  Interestingly, we find that scale invariance  properties of hierarchical neural models are  not independent of stimulus class, as  opposed to translation invariance, even  though both are affine transformations within  the image plane.",AIM-2002-011; CBCL-218,12 p.; 2137337 bytes; 1062341 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,150,"a detailed look at scale and translation invariance in a hierarchical neural model of visual object recognition the hmax model has recently been proposed  by riesenhuber & poggio as a  hierarchical  model of position- and size-invariant object  recognition in visual cortex. it has also turned  out to model successfully a number of other  properties of the ventral visual stream (the  visual pathway thought to be crucial for object  recognition in cortex), and particularly of (view-tuned) neurons in macaque inferotemporal  cortex, the brain area at the top of the ventral  stream. the original modeling study only  used ``paperclip'' stimuli, as in the  corresponding physiology experiment, and did  not explore systematically how model units'  invariance properties depended on model  parameters. in this study, we aimed at a  deeper understanding of the inner workings of  hmax and its performance for various  parameter settings and ``natural'' stimulus  classes. we examined hmax responses for  different stimulus sizes and positions  systematically and found a dependence of  model units' responses on stimulus position  for which a quantitative description is offered.  interestingly, we find that scale invariance  properties of hierarchical neural models are  not independent of stimulus class, as  opposed to translation invariance, even  though both are affine transformations within  the image plane.",object recognition/detection
,"Giese, Martin Alexander; Poggio, Tomaso",2004-10-20T21:05:04Z,2004-10-20T21:05:04Z,2002-08-01,http://hdl.handle.net/1721.1/7272,AIM-2002-012; CBCL-219,Biologically Plausible Neural Model for the Recognition of Biological Motion and Actions,"The visual recognition of complex movements  and actions is crucial for communication and  survival in many species. Remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. In recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  However, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  This paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. The  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. The  model makes specific predictions that  motivate new experiments.",AIM-2002-012; CBCL-219,26 p.; 3562724 bytes; 2540946 bytes,application/postscript; application/pdf,en_US,AI; biological motion; action recognition; visual pathways; hierarchical processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,151,"biologically plausible neural model for the recognition of biological motion and actions the visual recognition of complex movements  and actions is crucial for communication and  survival in many species. remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. in recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  however, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  this paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. the  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. the  model makes specific predictions that  motivate new experiments.",language models
,"Giese, M.A.; Xie, X.",2004-10-20T21:05:06Z,2004-10-20T21:05:06Z,2002-08-01,http://hdl.handle.net/1721.1/7273,AIM-2002-013; CBCL-220,Exact Solution of the Nonlinear Dynamics of Recurrent Neural Mechanisms for Direction Selectivity,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",AIM-2002-013; CBCL-220,7 p.; 2554351 bytes; 1165357 bytes,application/postscript; application/pdf,en_US,AI; direction; visual cortex; nonlinear dynamics; lurching waves; stability; recurre,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,152,"exact solution of the nonlinear dynamics of recurrent neural mechanisms for direction selectivity different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. the mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  we present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. these solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",privacy/ethics
,"Steck, Harald; Jaakkola, Tommi S.",2004-10-08T20:38:20Z,2004-10-08T20:38:20Z,2002-09-01,http://hdl.handle.net/1721.1/6702,AIM-2002-014,On the Dirichlet Prior and Bayesian Regularization,"A common objective in learning a model from  data is to recover its network structure, while the model  parameters are of minor interest. For example, we may wish to recover  regulatory networks from high-throughput data sources. In this paper  we examine how Bayesian regularization using a Dirichlet prior over the  model parameters affects the learned model structure in a  domain with discrete variables. Surprisingly, a weak prior in the  sense of smaller equivalent sample size leads to a strong  regularization of the model structure (sparse graph) given a sufficiently  large data set. In particular, the empty graph is obtained in the  limit of a vanishing strength of prior belief. This is  diametrically opposite to what one may expect in this limit, namely the  complete graph from an (unregularized) maximum likelihood estimate.  Since the prior affects the parameters as expected, the prior strength  balances a ""trade-off"" between regularizing the parameters or the  structure of the model. We demonstrate the benefits of optimizing this  trade-off in the sense of predictive accuracy.",AIM-2002-014,11 p.; 3152389 bytes; 1414851 bytes,application/postscript; application/pdf,en_US,AI; Regularization; Dirichlet Prior,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,155,"on the dirichlet prior and bayesian regularization a common objective in learning a model from  data is to recover its network structure, while the model  parameters are of minor interest. for example, we may wish to recover  regulatory networks from high-throughput data sources. in this paper  we examine how bayesian regularization using a dirichlet prior over the  model parameters affects the learned model structure in a  domain with discrete variables. surprisingly, a weak prior in the  sense of smaller equivalent sample size leads to a strong  regularization of the model structure (sparse graph) given a sufficiently  large data set. in particular, the empty graph is obtained in the  limit of a vanishing strength of prior belief. this is  diametrically opposite to what one may expect in this limit, namely the  complete graph from an (unregularized) maximum likelihood estimate.  since the prior affects the parameters as expected, the prior strength  balances a ""trade-off"" between regularizing the parameters or the  structure of the model. we demonstrate the benefits of optimizing this  trade-off in the sense of predictive accuracy.",high performance computing
,"Freeman, William T.; Torralba, Antonio",2004-10-08T20:38:34Z,2004-10-08T20:38:34Z,2002-09-01,http://hdl.handle.net/1721.1/6704,AIM-2002-016,Shape Recipes: Scene Representations that Refer to the Image,"The goal of low-level vision is to estimate an  underlying scene, given an observed image. Real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. We propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. Shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. We describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) Shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",AIM-2002-016,12 p.; 2606902 bytes; 1497926 bytes,application/postscript; application/pdf,en_US,AI; scene representation; shape; stereo; shape recipes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,156,"shape recipes: scene representations that refer to the image the goal of low-level vision is to estimate an  underlying scene, given an observed image. real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. we propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. we describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",image classification
,"Tappen, Marshall F.; Freeman, William T.; Adelson, Edward H.",2004-10-08T20:38:21Z,2004-10-08T20:38:21Z,2002-09-01,http://hdl.handle.net/1721.1/6703,AIM-2002-015,Recovering Intrinsic Images from a Single Image,"We present an algorithm that uses multiple  cues to recover shading and reflectance intrinsic images from a single  image. Using both color information and a classifier trained to  recognize gray-scale patterns, each image derivative is classified as being  caused by shading or a change in the surface's reflectance.  Generalized Belief Propagation is then used to propagate information from  areas where the correct classification is clear to areas where it is  ambiguous. We also show results on real images.",AIM-2002-015,12 p.; 1591784 bytes; 1186163 bytes,application/postscript; application/pdf,en_US,AI; intrinisic images; reflectance estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,157,"recovering intrinsic images from a single image we present an algorithm that uses multiple  cues to recover shading and reflectance intrinsic images from a single  image. using both color information and a classifier trained to  recognize gray-scale patterns, each image derivative is classified as being  caused by shading or a change in the surface's reflectance.  generalized belief propagation is then used to propagate information from  areas where the correct classification is clear to areas where it is  ambiguous. we also show results on real images.",image classification
,"Kumar, Vinay P.",2004-10-01T14:00:07Z,2004-10-01T14:00:07Z,2002-09-01,http://hdl.handle.net/1721.1/5569,AITR-2002-008; CBCL-221,Towards Man-Machine Interfaces: Combining Top-down Constraints with Bottom-up Learning in Facial Analysis,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",AITR-2002-008; CBCL-221,68 p.; 21293042 bytes; 2473001 bytes,application/postscript; application/pdf,en_US,AI; Facial Expression Recognition; Pose Estimation; Viseme Recognition; SVM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,158,"towards man-machine interfaces: combining top-down constraints with bottom-up learning in facial analysis this thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. from a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  we investigate these questions in the realm  of facial analysis. we propose the use of a linear morphable model  (lmm) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). we apply a supervised learning  method based on support vector machine (svm) regression for  estimating the parameters of lmms directly from pixel-based representations of  faces. we combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",high performance computing
,"Dror, Ron O.",2004-10-20T20:29:53Z,2004-10-20T20:29:53Z,2002-10-01,http://hdl.handle.net/1721.1/7097,AITR-2002-009,Surface Reflectance Recognition and Real-World Illumination Statistics,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",AITR-2002-009,195 p.; 7366082 bytes; 3656634 bytes,application/postscript; application/pdf,en_US,AI; illumination; reflectance; natural image statistics; vision; materials,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,163,"surface reflectance recognition and real-world illumination statistics humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. traditional computer vision systems cannot solve this problem at all. recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. a mirrored sphere, for example, produces a different image in every environment. to make matters worse, two surfaces with different reflectance properties could produce identical images. the mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. this thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   real-world illumination is complex, with light typically incident on a surface from every direction. we find, however, that real-world illumination patterns are not arbitrary. they exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   these properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. we construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. for surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",image classification
,"Van Eepoel, John M.",2004-10-20T20:29:56Z,2004-10-20T20:29:56Z,2002-10-22,http://hdl.handle.net/1721.1/7098,AITR-2002-010,Achieving Real-Time Mode Estimation through Offline Compilation,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",AITR-2002-010,321 p.; 20495512 bytes; 7253655 bytes,application/postscript; application/pdf,en_US,AI; mode estimation; compilation; model-based; reasoning; autonomy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,164,"achieving real-time mode estimation through offline compilation as exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. the spacecraft tasked with completing  these missions are becoming progressively more complex. this  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. a solution to this problem lies in the  development of an advanced fault management system. fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. the two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. these rules are developed by  having a human reason through the interactions between spacecraft  components. this process is limited by the number of interactions a  human can reason about correctly. in the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. this approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. we propose a fault detection engine, compiled mode  estimation (cme) that unifies the strengths of the rule-based and model-based approaches. cme uses a compiled model to determine spacecraft  behavior more accurately. reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. these are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. these rules  enable a human to inspect the diagnostic consequences of cme.  additionally, cme is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  the implementation of this engine has been tested against the near  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. this evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",language models
,"Sussman, Gerald Jay; Wisdom, Jack",2004-10-08T20:38:38Z,2004-10-08T20:38:38Z,2002-11-01,http://hdl.handle.net/1721.1/6707,AIM-2002-018,The Role of Programming in the Formulation of Ideas,"Classical mechanics is deceptively simple. It  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. To address this problem we  use computational techniques to communicate a deeper  understanding of Classical Mechanics. Computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. Expressing the methods in a computer language forces them to be  unambiguous and computationally effective. The task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. Also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",AIM-2002-018,18 p.; 1180238 bytes; 786910 bytes,application/postscript; application/pdf,en_US,AI; Education; Mechanics; Functional Programming; Symbolic Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,170,"the role of programming in the formulation of ideas classical mechanics is deceptively simple. it  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. to address this problem we  use computational techniques to communicate a deeper  understanding of classical mechanics. computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. expressing the methods in a computer language forces them to be  unambiguous and computationally effective. the task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",language models
,"Wisdom, Jack",2004-10-08T20:38:37Z,2004-10-08T20:38:37Z,2002-11-01,http://hdl.handle.net/1721.1/6706,AIM-2002-017,Swimming in Space-Time,"Cyclic changes in the shape of a quasi-rigid  body on a curved manifold can lead to net translation and/or  rotation of the body in the manifold. Presuming space-time is a  curved manifold as portrayed by general relativity, translation in space can  be accomplished simply by cyclic changes in the shape of a body,  without any thrust or external forces.",AIM-2002-017,32 p.; 6370543 bytes; 473755 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,171,"swimming in space-time cyclic changes in the shape of a quasi-rigid  body on a curved manifold can lead to net translation and/or  rotation of the body in the manifold. presuming space-time is a  curved manifold as portrayed by general relativity, translation in space can  be accomplished simply by cyclic changes in the shape of a body,  without any thrust or external forces.",language models
,"Sudderth, Erik B.; Ihler, Alexander T.; Freeman, William T.; Willsky, Alan S.",2004-10-04T14:15:28Z,2004-10-04T14:15:28Z,2002-12-01,http://hdl.handle.net/1721.1/5932,AIM-2002-020,Nonparametric Belief Propagation and Facial Appearance Estimation,"In many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. Stochastic methods such as particle filters  provide an appealing alternative. However, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. Drawing on ideas from regularized particle  filters and belief propagation (BP), this paper develops a  nonparametric belief propagation (NBP) algorithm applicable to  general graphs. Each NBP iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. The algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. Thus, NBP  extends particle filtering methods to the more general vision  problems that graphical models can describe. We apply the NBP  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",AIM-2002-020,10 p.; 3701870 bytes; 2537534 bytes,application/postscript; application/pdf,en_US,AI; graphical model; belief propagation; nonparametric inference; vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,176,"nonparametric belief propagation and facial appearance estimation in many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-gaussian distributions.  there exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. stochastic methods such as particle filters  provide an appealing alternative. however, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. drawing on ideas from regularized particle  filters and belief propagation (bp), this paper develops a  nonparametric belief propagation (nbp) algorithm applicable to  general graphs. each nbp iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. the algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. thus, nbp  extends particle filtering methods to the more general vision  problems that graphical models can describe. we apply the nbp  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",face detection
,"Beal, Jacob",2004-10-04T14:15:29Z,2004-10-04T14:15:29Z,2002-12-01,http://hdl.handle.net/1721.1/5933,AIM-2002-021,Leaderless Distributed Hierarchy Formation,"I present a system for robust leaderless  organization of an amorphous network into hierarchical clusters. This  system, which assumes that nodes are spatially embedded and can only  talk to neighbors within a given radius, scales to networks of arbitrary  size and converges rapidly. The amount of data stored at each  node is logarithmic in the diameter of the network, and the hierarchical  structure produces an addressing scheme such that there is an  invertible relation between distance and address for any pair of nodes.  The system adapts automatically to stopping failures, network  partition, and reorganization.",AIM-2002-021,27 p.; 7370490 bytes; 1660395 bytes,application/postscript; application/pdf,en_US,AI; amorphous computing hierarchy leaderless distributed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,177,"leaderless distributed hierarchy formation i present a system for robust leaderless  organization of an amorphous network into hierarchical clusters. this  system, which assumes that nodes are spatially embedded and can only  talk to neighbors within a given radius, scales to networks of arbitrary  size and converges rapidly. the amount of data stored at each  node is logarithmic in the diameter of the network, and the hierarchical  structure produces an addressing scheme such that there is an  invertible relation between distance and address for any pair of nodes.  the system adapts automatically to stopping failures, network  partition, and reorganization.",language models
,"Bouvrie, Jake V.",2004-10-08T20:38:35Z,2004-10-08T20:38:35Z,2002-12-01,http://hdl.handle.net/1721.1/6705,AIM-2002-022,Multiple Resolution Image Classification,"Binary image classifiction is a problem that  has received much attention  in recent years. In this paper we evaluate a  selection of popular  techniques in an effort to find a feature set/ classifier combination which  generalizes well to full resolution image data.  We then apply that system  to images at one-half through one-sixteenth  resolution, and consider the  corresponding error rates. In addition, we  further observe generalization  performance as it depends on the number of  training images, and lastly,  compare the system's best error rates to that  of a human performing an  identical classification task given teh same  set of test images.",AIM-2002-022,1054982 bytes; 824527 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,178,"multiple resolution image classification binary image classifiction is a problem that  has received much attention  in recent years. in this paper we evaluate a  selection of popular  techniques in an effort to find a feature set/ classifier combination which  generalizes well to full resolution image data.  we then apply that system  to images at one-half through one-sixteenth  resolution, and consider the  corresponding error rates. in addition, we  further observe generalization  performance as it depends on the number of  training images, and lastly,  compare the system's best error rates to that  of a human performing an  identical classification task given teh same  set of test images.",image classification
,"Mukherjee, Sayan; Niyogi, Partha; Poggio, Tomaso; Rifkin, Ryan",2004-08-31T18:12:01Z,2004-08-31T18:12:01Z,2002-12-01,http://hdl.handle.net/1721.3/5507,AIM-2002-024; CBCL-223,Statistical Learning: Stability is Sufficient for Generalization and Necessary and Sufficient for Consistency of Empirical Risk Minimization,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",AIM-2002-024; CBCL-223,24 p.; 1854466 bytes; 400508 bytes,application/postscript; application/pdf,en_US,AI; Theory of Learning; Great Discoveries; Consistency; ERM; Stability,,,,,revised July 2003,,,,,,,,,,,,,,,,,,,,,,,,,2002,179,"statistical learning: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization solutions of learning problems by empirical  risk  minimization (erm) need to be consistent, so  that they  may be predictive. they also need to be well-posed, so  that they can be used robustly. we show that  a statistical form  of well-posedness, defined in terms of the  key property of  l-stability, is necessary and sufficient for  consistency of erm.",image classification
,"Torralba, Antonio; Freeman, William T.",2004-10-08T20:38:07Z,2004-10-08T20:38:07Z,2002-12-01,http://hdl.handle.net/1721.1/6695,AIM-2002-019,Properties and Applications of Shape Recipes,"In low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. The  approach proposed here is to let the image itself bear as much of the  representational burden as possible. In many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. The scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. We illustrate the use of this  representation for encoding shape information. We show how  this representation has appealing properties such as locality and  slow variation across space and scale. These properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",AIM-2002-019,9 p.; 4798019 bytes; 3236270 bytes,application/postscript; application/pdf,en_US,AI; shape from X; scene representation; shape recipes; stereo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,180,"properties and applications of shape recipes in low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. the  approach proposed here is to let the image itself bear as much of the  representational burden as possible. in many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. the scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. we illustrate the use of this  representation for encoding shape information. we show how  this representation has appealing properties such as locality and  slow variation across space and scale. these properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",object recognition/detection
,"Perez-Breva, Luis; Yoshimi, Osamu",2004-10-20T20:48:55Z,2004-10-20T20:48:55Z,2002-12-01,http://hdl.handle.net/1721.1/7181,AIM-2002-023; CBCL-222,Model Selection in Summary Evaluation,"A difficulty in the design of automated text  summarization   algorithms is in the objective evaluation.  Viewing summarization   as a tradeoff between length and  information content, we introduce   a technique based on a hierarchy of  classifiers to rank, through   model selection, different summarization  methods. This summary   evaluation technique allows for broader  comparison of   summarization methods than the traditional  techniques of summary   evaluation. We present an empirical study  of two simple, albeit   widely used, summarization methods that  shows the different usages   of this automated task-based evaluation  system and confirms the   results obtained with human-based  evaluation methods over smaller   corpora.",AIM-2002-023; CBCL-222,1739841 bytes; 1972183 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,181,"model selection in summary evaluation a difficulty in the design of automated text  summarization   algorithms is in the objective evaluation.  viewing summarization   as a tradeoff between length and  information content, we introduce   a technique based on a hierarchy of  classifiers to rank, through   model selection, different summarization  methods. this summary   evaluation technique allows for broader  comparison of   summarization methods than the traditional  techniques of summary   evaluation. we present an empirical study  of two simple, albeit   widely used, summarization methods that  shows the different usages   of this automated task-based evaluation  system and confirms the   results obtained with human-based  evaluation methods over smaller   corpora.",privacy/ethics
,"Grossman, J.P.",2004-10-20T20:00:24Z,2004-10-20T20:00:24Z,2002-12-05,http://hdl.handle.net/1721.1/6828,AITR-2002-011,Design and Evaluation of the Hamal Parallel Computer,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",AITR-2002-011,186 p.; 14854547 bytes; 6844439 bytes,application/postscript; application/pdf,en_US,AI; parallel; network; simulation; hashing; multithreading; synchronization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,182,"design and evaluation of the hamal parallel computer parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. associated with such large systems is a new set of  design challenges. many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  first, a scalable memory system is required. second, the network messaging  protocol must be fault-tolerant. third, the overheads of thread creation,  thread management and synchronization must be extremely low.  this thesis presents the complete system design for hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. we develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. a number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the hamal architecture. we determine implementation parameters  for the messaging protocol which optimize performance. a discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  our simulations of hamal demonstrate the effectiveness of its thread  management and synchronization primitives. in particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",high performance computing
,"Srebro, Nathan; Jaakkola, Tommi",2004-10-08T20:38:40Z,2004-10-08T20:38:40Z,2003-01-15,http://hdl.handle.net/1721.1/6708,AIM-2003-001,Generalized Low-Rank Approximations,"We study the frequent problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving {\\em weighted} low rank approximation problems, which, unlike simple matrix factorization problems, do not admit a closed form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low rank representation, and extend the formulation to non-Gaussian noise models such as classification (collaborative filtering).",AIM-2003-001,10 p.; 2061103 bytes; 911431 bytes,application/postscript; application/pdf,en_US,AI; svd pca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,192,"generalized low-rank approximations we study the frequent problem of approximating a target matrix with a matrix of lower rank. we provide a simple and efficient (em) algorithm for solving {\\em weighted} low rank approximation problems, which, unlike simple matrix factorization problems, do not admit a closed form solution in general. we analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low rank representation, and extend the formulation to non-gaussian noise models such as classification (collaborative filtering).",high performance computing
,"Adler, Aaron D.",2004-10-20T20:31:48Z,2004-10-20T20:31:48Z,2003-02-01,http://hdl.handle.net/1721.1/7103,AITR-2003-004,Segmentation and Alignment of Speech and Sketching in a Design Environment,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",AITR-2003-004,193 p.; 34430522 bytes; 46149955 bytes,application/postscript; application/pdf,en_US,AI; sketch; design; multimodal; disambiguation; segmentation; alignment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,195,"segmentation and alignment of speech and sketching in a design environment sketches are commonly used in the early stages of  design. our previous system allows users to sketch mechanical systems that  the computer interprets. however, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. this  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. toward this end,  subjects were recorded while they sketched and talked. these  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. these rules represent the knowledge that  the computer needs to perform segmentation and alignment. the  rules successfully interpreted the 24 data sets that they were given.",speech/audio recognition
,"Kim, Philip Mjong-Hyon Shin",2004-10-20T20:31:34Z,2004-10-20T20:31:34Z,2003-02-05,http://hdl.handle.net/1721.1/7099,AITR-2003-001,"Understanding Subsystems in Biology through Dimensionality Reduction, Graph Partitioning and Analytical Modeling","Biological systems exhibit rich and complex behavior through the orchestrated interplay of a large array of components. It is hypothesized that separable subsystems with some degree of functional autonomy exist; deciphering their independent behavior and functionality would greatly facilitate understanding the system as a whole. Discovering and analyzing such subsystems are hence pivotal problems in the quest to gain a quantitative understanding of complex biological systems.  In this work, using approaches from machine learning, physics and graph theory, methods for the identification and analysis of such subsystems were developed. A novel methodology, based on a recent machine learning algorithm known as non-negative matrix factorization (NMF), was developed to discover such subsystems in a set of large-scale gene expression data. This set of subsystems was then used to predict functional relationships between genes, and this approach was shown to score significantly higher than conventional methods when benchmarking them against existing databases. Moreover, a mathematical treatment was developed to treat simple network subsystems based only on their topology (independent of particular parameter values). Application to a problem of experimental interest demonstrated the need for extentions to the conventional model to fully explain the experimental data.  Finally, the notion of a subsystem was evaluated from a topological perspective. A number of different protein networks were examined to analyze their topological properties with respect to separability, seeking to find separable subsystems. These networks were shown to exhibit separability in a nonintuitive fashion, while the separable subsystems were of strong biological significance. It was demonstrated that the separability property found was not due to incomplete or biased data, but is likely to reflect biological structure.",AITR-2003-001,124 p.; 14826182 bytes; 3860263 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,196,"understanding subsystems in biology through dimensionality reduction, graph partitioning and analytical modeling biological systems exhibit rich and complex behavior through the orchestrated interplay of a large array of components. it is hypothesized that separable subsystems with some degree of functional autonomy exist; deciphering their independent behavior and functionality would greatly facilitate understanding the system as a whole. discovering and analyzing such subsystems are hence pivotal problems in the quest to gain a quantitative understanding of complex biological systems.  in this work, using approaches from machine learning, physics and graph theory, methods for the identification and analysis of such subsystems were developed. a novel methodology, based on a recent machine learning algorithm known as non-negative matrix factorization (nmf), was developed to discover such subsystems in a set of large-scale gene expression data. this set of subsystems was then used to predict functional relationships between genes, and this approach was shown to score significantly higher than conventional methods when benchmarking them against existing databases. moreover, a mathematical treatment was developed to treat simple network subsystems based only on their topology (independent of particular parameter values). application to a problem of experimental interest demonstrated the need for extentions to the conventional model to fully explain the experimental data.  finally, the notion of a subsystem was evaluated from a topological perspective. a number of different protein networks were examined to analyze their topological properties with respect to separability, seeking to find separable subsystems. these networks were shown to exhibit separability in a nonintuitive fashion, while the separable subsystems were of strong biological significance. it was demonstrated that the separability property found was not due to incomplete or biased data, but is likely to reflect biological structure.",high performance computing
,"Chklovski, Timothy",2004-10-20T20:31:36Z,2004-10-20T20:31:36Z,2003-02-12,http://hdl.handle.net/1721.1/7100,AITR-2003-002,Using Analogy to Acquire Commonsense Knowledge from Human Contributors,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",AITR-2003-002,173 p.; 4895337 bytes; 1809437 bytes,application/postscript; application/pdf,en_US,AI; knowledge acquisition; knowledge capture; analogy; natural language; reasoning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,197,"using analogy to acquire commonsense knowledge from human contributors the goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. in the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. to acquire knowledge  from contributors not trained in knowledge engineering,  i take the following four steps: (i) develop a knowledge  representation (kr) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (ka) based on a theoretical analysis of  effectiveness of ka with this approach, and (iv) test the  kr model and the effectiveness of the cumulative  analogy algorithms empirically. to investigate  effectiveness of cumulative analogy for ka empirically,  learner, an open source system for ka by cumulative  analogy has been implemented, deployed, and  evaluated. (the site ""1001 questions,"" is available at  http://teach-computers.org/learner.html). learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a ka topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. then learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" because similarity between topics  is computed based on what is already known about  them, learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. by summing evidence for and against  posing any given question, learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. the  ka power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. i  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a ka method. empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",language models
,"Peshkin, Leonid",2004-10-20T20:31:39Z,2004-10-20T20:31:39Z,2003-02-14,http://hdl.handle.net/1721.1/7101,AITR-2003-003,Reinforcement Learning by Policy Search,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",AITR-2003-003,144 p.; 26942112 bytes; 1735254 bytes,application/postscript; application/pdf,en_US,AI; POMDP; policy search; adaptive systems; reinforcement learning; adaptive behavior,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,198,"reinforcement learning by policy search one objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. the  environment's transformations can be modeled as a markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable markov decision processes  (pomdps). while the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  in this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. this means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. the learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  the set of policies is constrained by the architecture of  the agent's controller. pomdps require a controller to have  a memory. we investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. for these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. we address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",high performance computing
,"Steck, Harald; Jaakkola, Tommi S.",2004-10-08T20:38:42Z,2004-10-08T20:38:42Z,2003-02-25,http://hdl.handle.net/1721.1/6709,AIM-2003-002,(Semi-)Predictive Discretization During Model Selection,"In this paper, we present an approach to discretizing  multivariate continuous data while learning the  structure of a graphical model. We derive the joint  scoring function from the principle of predictive  accuracy, which inherently ensures the optimal trade-off between goodness of fit and model complexity  (including the number of discretization levels). Using  the so-called finest grid implied by the data, our scoring  function depends only on the number of data points in  the various discretization levels. Not only can it be  computed efficiently, but it is also independent of the  metric used in the continuous space. Our experiments  with gene expression data show that discretization  plays a crucial role regarding the resulting network  structure.",AIM-2003-002,15 p.; 4299414 bytes; 910469 bytes,application/postscript; application/pdf,en_US,AI; Discretization; Graphical models,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,199,"(semi-)predictive discretization during model selection in this paper, we present an approach to discretizing  multivariate continuous data while learning the  structure of a graphical model. we derive the joint  scoring function from the principle of predictive  accuracy, which inherently ensures the optimal trade-off between goodness of fit and model complexity  (including the number of discretization levels). using  the so-called finest grid implied by the data, our scoring  function depends only on the number of data points in  the various discretization levels. not only can it be  computed efficiently, but it is also independent of the  metric used in the continuous space. our experiments  with gene expression data show that discretization  plays a crucial role regarding the resulting network  structure.",privacy/ethics
,"Geiger, Gadi; Ezzat, Tony; Poggio, Tomaso",2004-10-20T21:05:09Z,2004-10-20T21:05:09Z,2003-02-28,http://hdl.handle.net/1721.1/7275,AIM-2003-003; CBCL-224,Perceptual Evaluation of Video-Realistic Speech,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",AIM-2003-003; CBCL-224,17 p.; 1515741 bytes; 1358361 bytes,application/postscript; application/pdf,en_US,AI; visual speech; speech animation; face animation; image morphing; lip reading,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,200,"perceptual evaluation of video-realistic speech abstract  with many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. we describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called mary 101.  two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""intelligibility tests"").  subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  the same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. however,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. we conclude  from the two experiments that the animation of mary  101 is adequate for providing a percept of a talking  head. however, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  in addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. in  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. the implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",image classification
,"Jarudi, Izzat N.; Sinha, Pawan",2004-10-20T21:05:07Z,2004-10-20T21:05:07Z,2003-03-01,http://hdl.handle.net/1721.1/7274,AIM-2003-004; CBCL-225,Relative Contributions of Internal and External Features to Face Recognition,"The central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. Notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. Past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. The results obtained are  therefore most relevant for understanding the  identification of faces at close range. However, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. Here, we report experiments  designed to address this issue. Our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  While we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. These  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",AIM-2003-004; CBCL-225,12 p.; 1448956 bytes; 677551 bytes,application/postscript; application/pdf,en_US,AI; Face recognition; features; low resolution; degraded images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,206,"relative contributions of internal and external features to face recognition the central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. the results obtained are  therefore most relevant for understanding the  identification of faces at close range. however, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. here, we report experiments  designed to address this issue. our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  while we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. these  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",face detection
,"Beal, Jacob",2004-10-08T20:38:43Z,2004-10-08T20:38:43Z,2003-03-17,http://hdl.handle.net/1721.1/6710,AIM-2003-007,Leveraging Learning and Language Via Communication Bootstrapping,"In a Communication Bootstrapping system, peer  components with different perceptual worlds invent symbols and syntax  based on correlations between their percepts. I propose that  Communication Bootstrapping can also be used to acquire functional  definitions of words and causal reasoning knowledge. I illustrate this  point with several examples, then sketch the architecture of a  system in progress which attempts to execute this task.",AIM-2003-007,13 p.; 2403922 bytes; 651138 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,207,"leveraging learning and language via communication bootstrapping in a communication bootstrapping system, peer  components with different perceptual worlds invent symbols and syntax  based on correlations between their percepts. i propose that  communication bootstrapping can also be used to acquire functional  definitions of words and causal reasoning knowledge. i illustrate this  point with several examples, then sketch the architecture of a  system in progress which attempts to execute this task.",language models
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.; Rubin, Mark A.",2004-10-08T20:38:46Z,2004-10-08T20:38:46Z,2003-03-19,http://hdl.handle.net/1721.1/6711,AIM-2003-005,Context-Based Vision System for Place and Object Recognition,"While navigating in an environment, a vision system  has to be able to recognize where it is and what the  main objects in the scene are. In this paper we present  a context-based vision system for place and object  recognition. The goal is to identify familiar locations  (e.g., office 610, conference room 941, Main Street), to  categorize new environments (office, corridor, street)  and to use that information to provide contextual priors  for object recognition (e.g., table, chair, car, computer). We present a low-dimensional global image representation that provides  relevant information for place recognition and  categorization, and how such contextual information  introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60  locations (indoors and outdoors) and to suggest the  presence and locations of more than 20 different object  types. The algorithm has been integrated into a mobile system that provides real-time feedback to the  user.",AIM-2003-005,9 p.; 7141251 bytes; 2104025 bytes,application/postscript; application/pdf,en_US,AI; context-based vision; place recognition; object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,208,"context-based vision system for place and object recognition while navigating in an environment, a vision system  has to be able to recognize where it is and what the  main objects in the scene are. in this paper we present  a context-based vision system for place and object  recognition. the goal is to identify familiar locations  (e.g., office 610, conference room 941, main street), to  categorize new environments (office, corridor, street)  and to use that information to provide contextual priors  for object recognition (e.g., table, chair, car, computer). we present a low-dimensional global image representation that provides  relevant information for place recognition and  categorization, and how such contextual information  introduces strong priors that simplify object recognition.  we have trained the system to recognize over 60  locations (indoors and outdoors) and to suggest the  presence and locations of more than 20 different object  types. the algorithm has been integrated into a mobile system that provides real-time feedback to the  user.",object recognition/detection
,"Alvarado, Christine; Teevan, Jaime; Ackerman, Mark S.; Karger, David",2004-10-08T20:38:49Z,2004-10-08T20:38:49Z,2003-04-15,http://hdl.handle.net/1721.1/6713,AIM-2003-006,Surviving the Information Explosion: How People Find Their Electronic Information,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",AIM-2003-006,9 p.; 980296 bytes; 422112 bytes,application/postscript; application/pdf,en_US,AI; information seeking; search; orienteering; context; Semantic Web,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,212,"surviving the information explosion: how people find their electronic information we report on a study of how people look for information within email, files, and the web. when locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. they appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. we found this behavior was especially true for people with unstructured information organization. we discuss the implications of our findings for the design of personal information management tools.",object recognition/detection
,"Beal, Jacob",2004-10-08T20:38:47Z,2004-10-08T20:38:47Z,2003-04-15,http://hdl.handle.net/1721.1/6712,AIM-2003-011,Persistent Nodes for Reliable Memory in Geographically Local Networks,"A Persistent Node is a redundant distributed mechanism for storing a key/value pair reliably in a geographically local network. In this paper, I develop a method of establishing Persistent Nodes in an amorphous matrix. I address issues of construction, usage, atomicity guarantees and reliability in the face of stopping failures. Applications include routing, congestion control, and data storage in gigascale networks.",AIM-2003-011,19 p.; 5503051 bytes; 1849500 bytes,application/postscript; application/pdf,en_US,AI; robust atomic distributed amorphous,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,213,"persistent nodes for reliable memory in geographically local networks a persistent node is a redundant distributed mechanism for storing a key/value pair reliably in a geographically local network. in this paper, i develop a method of establishing persistent nodes in an amorphous matrix. i address issues of construction, usage, atomicity guarantees and reliability in the face of stopping failures. applications include routing, congestion control, and data storage in gigascale networks.",high performance computing
,"Grauman, Kristen; Shakhnarovich, Gregory; Darrell, Trevor",2004-10-08T20:38:51Z,2004-10-08T20:38:51Z,2003-04-17,http://hdl.handle.net/1721.1/6714,AIM-2003-008,Inferring 3D Structure with a Statistical Image-Based Shape Model,"We present an image-based approach to infer 3D  structure parameters using a probabilistic ""shape+structure''  model. The 3D shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. Bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. We  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. Model matching and  parameter inference are done entirely in the image domain and require no  explicit 3D construction. Our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. Using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3D  locations of 19 joints on the body based on observed silhouette  contours from real images.",AIM-2003-008,17 p.; 6362014 bytes; 9371703 bytes,application/postscript; application/pdf,en_US,AI; 3D structure; statistical shape model; multi-view imagery; pose estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,214,"inferring 3d structure with a statistical image-based shape model we present an image-based approach to infer 3d  structure parameters using a probabilistic ""shape+structure''  model. the 3d shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. we  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. model matching and  parameter inference are done entirely in the image domain and require no  explicit 3d construction. our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3d  locations of 19 joints on the body based on observed silhouette  contours from real images.",language models
,"Christoudias, Chris Mario; Morency, Louis-Philippe; Darrell, Trevor",2004-10-08T20:38:54Z,2004-10-08T20:38:54Z,2003-04-18,http://hdl.handle.net/1721.1/6716,AIM-2003-010,Light Field Morphable Models,"Statistical shape and texture appearance models are  powerful image representations, but previously had  been restricted to 2D or simple 3D shapes. In this paper  we present a novel 3D morphable model based on  image-based rendering techniques, which can  represent complex lighting conditions, structures, and  surfaces. We describe how to construct a manifold of  the multi-view appearance of an object class using light  fields and show how to match a 2D image of an object  to a point on this manifold. In turn we use the  reconstructed light field to render novel views of the  object. Our technique overcomes the limitations of  polygon based appearance models and uses light  fields that are acquired in real-time.",AIM-2003-010,1375810 bytes; 716555 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,215,"light field morphable models statistical shape and texture appearance models are  powerful image representations, but previously had  been restricted to 2d or simple 3d shapes. in this paper  we present a novel 3d morphable model based on  image-based rendering techniques, which can  represent complex lighting conditions, structures, and  surfaces. we describe how to construct a manifold of  the multi-view appearance of an object class using light  fields and show how to match a 2d image of an object  to a point on this manifold. in turn we use the  reconstructed light field to render novel views of the  object. our technique overcomes the limitations of  polygon based appearance models and uses light  fields that are acquired in real-time.",image classification
,"Shakhnarovich, Gregory; Viola, Paul; Darrell, Trevor",2004-10-08T20:38:53Z,2004-10-08T20:38:53Z,2003-04-18,http://hdl.handle.net/1721.1/6715,AIM-2003-009,Fast Pose Estimation with Parameter Sensitive Hashing,"Example-based methods are effective for parameter  estimation problems when the underlying system is simple or the  dimensionality of the input is low. For complex and high-dimensional  problems such as pose estimation, the number of required examples and the  computational complexity rapidly becme prohibitively high. We  introduce a new algorithm that learns a set of hashing functions that  efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for  locality-sensitive hashing, which finds approximate neighbors in time  sublinear in the number of examples. This method depends critically  on the choice of hash functions; we show how to find the set of hash  functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and  accurately estimate the articulated pose of human figures from a large  database of example images.",AIM-2003-009,12 p.; 5030222 bytes; 6836715 bytes,application/postscript; application/pdf,en_US,AI; parameter estimation; nearest neighbor; locally weighted learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,216,"fast pose estimation with parameter sensitive hashing example-based methods are effective for parameter  estimation problems when the underlying system is simple or the  dimensionality of the input is low. for complex and high-dimensional  problems such as pose estimation, the number of required examples and the  computational complexity rapidly becme prohibitively high. we  introduce a new algorithm that learns a set of hashing functions that  efficiently index examples relevant to a particular estimation task.  our algorithm extends a recently developed method for  locality-sensitive hashing, which finds approximate neighbors in time  sublinear in the number of examples. this method depends critically  on the choice of hash functions; we show how to find the set of hash  functions that are optimally relevant to a particular estimation problem.  experiments demonstrate that the resulting algorithm, which we call parameter-sensitive hashing, can rapidly and  accurately estimate the articulated pose of human figures from a large  database of example images.",high performance computing
,"Morency, Louis-Philippe",2004-10-20T20:31:42Z,2004-10-20T20:31:42Z,2003-05-01,http://hdl.handle.net/1721.1/7102,AITR-2003-006,Stereo-Based Head Pose Tracking Using Iterative Closest Point and Normal Flow Constraint,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",AITR-2003-006,60 p.; 5276045 bytes; 2896854 bytes,application/postscript; application/pdf,en_US,AI; Head pose estimation; Stereo processing; Cursor control; 3D model acquisition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,223,"stereo-based head pose tracking using iterative closest point and normal flow constraint in this text, we present two stereo-based head tracking  techniques along with a fast 3d model acquisition  system. the first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. we integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. our system can  automatically segment and track a user's head under large rotation and illumination variations. precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  the second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. our iterative hybrid tracker combines  constraints from the icp (iterative closest point)  algorithm and normal flow constraint. this new  technique is more precise for small movements and  noisy depth than icp alone, and more robust for large  movements than the normal flow constraint alone. we present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  the 3d model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3d mesh. 3d views are registered with shape  alignment based on our iterative hybrid tracker. we  reconstruct the 3d model using a new cubic ray  projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. we  present experiments to test the accuracy of our  approach on 3d face modelling using real-time stereo  images.",face detection
,"Beal, Jacob",2004-10-08T20:38:56Z,2004-10-08T20:38:56Z,2003-05-01,http://hdl.handle.net/1721.1/6717,AIM-2003-012,A Robust Amorphous Hierarchy from Persistent Nodes,"For a very large network deployed in space with only nearby nodes able to talk to each other, we want to do tasks like robust routing and data storage. One way to organize the network is via a hierarchy, but hierarchies often have a few critical nodes whose death can disrupt organization over long distances. I address this with a system of distributed aggregates called Persistent Nodes, such that spatially local failures disrupt the hierarchy in an area proportional to the diameter of the failure. I describe and analyze this system, which has been implemented in simulation.",AIM-2003-012,12 p.; 3383342 bytes; 1922951 bytes,application/postscript; application/pdf,en_US,AI; amorphous distributed fault tolerant gigascale,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,225,"a robust amorphous hierarchy from persistent nodes for a very large network deployed in space with only nearby nodes able to talk to each other, we want to do tasks like robust routing and data storage. one way to organize the network is via a hierarchy, but hierarchies often have a few critical nodes whose death can disrupt organization over long distances. i address this with a system of distributed aggregates called persistent nodes, such that spatially local failures disrupt the hierarchy in an area proportional to the diameter of the failure. i describe and analyze this system, which has been implemented in simulation.",language models
,"Martin, Martin C.",2004-10-08T20:38:57Z,2004-10-08T20:38:57Z,2003-05-01,http://hdl.handle.net/1721.1/6718,AIM-2003-014,The Essential Dynamics Algorithm: Essential Results,"This paper presents a novel algorithm for learning in a  class of stochastic Markov decision processes (MDPs)  with continuous state and action spaces that trades  speed for accuracy. A transform of the stochastic MDP  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. In this transformed MDP, the calculation of  values is greatly simplified. The online algorithm  estimates the model of the transformed MDP and  simultaneously does policy search against it. Bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. The algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic MDP, using less domain knowledge. All  code used in the experiments is available on the  project's web site.",AIM-2003-014,12 p.; 1085830 bytes; 303781 bytes,application/postscript; application/pdf,en_US,AI; Reinforcement learning; bicycle; policy search; markov decision processes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,226,"the essential dynamics algorithm: essential results this paper presents a novel algorithm for learning in a  class of stochastic markov decision processes (mdps)  with continuous state and action spaces that trades  speed for accuracy. a transform of the stochastic mdp  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. in this transformed mdp, the calculation of  values is greatly simplified. the online algorithm  estimates the model of the transformed mdp and  simultaneously does policy search against it. bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. the algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic mdp, using less domain knowledge. all  code used in the experiments is available on the  project's web site.",high performance computing
,"Grauman, Kristen",2004-10-20T20:31:53Z,2004-10-20T20:31:53Z,2003-05-22,http://hdl.handle.net/1721.1/7104,AITR-2003-007,A Statistical Image-Based Shape Model for Visual Hull Reconstruction and 3D Structure Inference,"We present a statistical image-based shape +  structure model for Bayesian visual hull reconstruction  and 3D structure inference. The 3D shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. Bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. We show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. The proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3D models under  various noise conditions are shown. We further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the Bayesian  reconstruction process. Model matching and parameter  inference are done entirely in the image domain and  require no explicit 3D construction. Our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. Using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3D locations of 19 joints on the  body based on observed silhouette contours from real images.",AITR-2003-007,60 p.; 14619811 bytes; 42799632 bytes,application/postscript; application/pdf,en_US,AI; visual hull; 3D structure; shape model; Bayesian inference,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,227,"a statistical image-based shape model for visual hull reconstruction and 3d structure inference we present a statistical image-based shape +  structure model for bayesian visual hull reconstruction  and 3d structure inference. the 3d shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. we show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. the proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3d models under  various noise conditions are shown. we further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the bayesian  reconstruction process. model matching and parameter  inference are done entirely in the image domain and  require no explicit 3d construction. our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3d locations of 19 joints on the  body based on observed silhouette contours from real images.",language models
,"Wehowsky, Andreas F.",2004-10-20T20:31:59Z,2004-10-20T20:31:59Z,2003-05-30,http://hdl.handle.net/1721.1/7106,AITR-2003-012,Safe Distributed Coordination of Heterogeneous Robots through Dynamic Simple Temporal Networks,"Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman-Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",AITR-2003-012,95 p.; 3611933 bytes; 908879 bytes,application/postscript; application/pdf,en_US,AI; model-based autonomy; distributed planning; distributed constraint satisfaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,228,"safe distributed coordination of heterogeneous robots through dynamic simple temporal networks research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. robotic execution languages such as raps, esl, and tdl improve robustness by managing functionally redundant procedures for achieving goals. the model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. this thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. we make two key contributions. first, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. this enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. second, we introduce a distributed temporal planner, called dtp, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed bellman-ford shortest path algorithm. the implementation of dtp has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",robotics
,"Das, Sanmay",2004-10-01T14:00:08Z,2004-10-01T14:00:08Z,2003-06-01,http://hdl.handle.net/1721.1/5570,AITR-2003-005; CBCL-226,Intelligent Market-Making in Artificial Financial Markets,"This thesis describes and evaluates a market-making  algorithm for setting prices in financial markets with asymmetric  information, and analyzes the properties of artificial markets in which the  algorithm is used. The core of our algorithm is a technique for  maintaining an online probability density estimate of the underlying  value of a stock. Previous theoretical work on market-making has  led to price-setting equations for which solutions cannot be  achieved in practice, whereas empirical work on algorithms for  market-making has focused on sets of heuristics and rules that lack  theoretical justification. The algorithm presented in this thesis is  theoretically justified by results in finance, and at the same time  flexible enough to be easily extended by incorporating modules for  dealing with considerations like portfolio risk and competition from  other market-makers. We analyze the performance of our  algorithm experimentally in artificial markets with different  parameter settings and find that many reasonable real-world properties  emerge. For example, the spread increases in response to  uncertainty about the true value of a stock, average spreads tend to be higher  in more volatile markets, and market-makers with lower  average spreads perform better in environments with multiple competitive market-makers. In addition, the time series data generated by simple  markets populated with market-makers using our algorithm replicate  properties of real-world financial time series, such as volatility  clustering and the fat-tailed nature of return distributions, without the  need to specify explicit models for opinion propagation and  herd behavior in the trading crowd.",AITR-2003-005; CBCL-226,49 p.; 3910312 bytes; 827445 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,233,"intelligent market-making in artificial financial markets this thesis describes and evaluates a market-making  algorithm for setting prices in financial markets with asymmetric  information, and analyzes the properties of artificial markets in which the  algorithm is used. the core of our algorithm is a technique for  maintaining an online probability density estimate of the underlying  value of a stock. previous theoretical work on market-making has  led to price-setting equations for which solutions cannot be  achieved in practice, whereas empirical work on algorithms for  market-making has focused on sets of heuristics and rules that lack  theoretical justification. the algorithm presented in this thesis is  theoretically justified by results in finance, and at the same time  flexible enough to be easily extended by incorporating modules for  dealing with considerations like portfolio risk and competition from  other market-makers. we analyze the performance of our  algorithm experimentally in artificial markets with different  parameter settings and find that many reasonable real-world properties  emerge. for example, the spread increases in response to  uncertainty about the true value of a stock, average spreads tend to be higher  in more volatile markets, and market-makers with lower  average spreads perform better in environments with multiple competitive market-makers. in addition, the time series data generated by simple  markets populated with market-makers using our algorithm replicate  properties of real-world financial time series, such as volatility  clustering and the fat-tailed nature of return distributions, without the  need to specify explicit models for opinion propagation and  herd behavior in the trading crowd.",language models
,"Gajos, Krzysztof; Shrobe, Howard",2004-10-08T20:39:03Z,2004-10-08T20:39:03Z,2003-06-01,http://hdl.handle.net/1721.1/6721,AIM-2003-016,"Delegation, Arbitration and High-Level Service Discovery as Key Elements of a Software Infrastructure for Pervasive Computing","The dream of pervasive computing is slowly becoming  a reality. A number of projects around the world are constantly contributing ideas and solutions  that are bound to change the way we interact with our environments and with one another. An  essential component of the future is a software infrastructure that is capable of supporting interactions  on scales ranging from a single physical space to intercontinental collaborations. Such  infrastructure must help applications adapt to very diverse environments and must protect people's  privacy and respect their personal preferences. In this paper we indicate a number of limitations present  in the software infrastructures proposed so far (including our previous work). We then describe the  framework for building an infrastructure that satisfies the abovementioned criteria. This  framework hinges on the concepts of delegation, arbitration and high-level service discovery.  Components of our own implementation of such an infrastructure are presented.",AIM-2003-016,17 p.; 1039823 bytes; 325691 bytes,application/postscript; application/pdf,en_US,AI; pervasive computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,234,"delegation, arbitration and high-level service discovery as key elements of a software infrastructure for pervasive computing the dream of pervasive computing is slowly becoming  a reality. a number of projects around the world are constantly contributing ideas and solutions  that are bound to change the way we interact with our environments and with one another. an  essential component of the future is a software infrastructure that is capable of supporting interactions  on scales ranging from a single physical space to intercontinental collaborations. such  infrastructure must help applications adapt to very diverse environments and must protect people's  privacy and respect their personal preferences. in this paper we indicate a number of limitations present  in the software infrastructures proposed so far (including our previous work). we then describe the  framework for building an infrastructure that satisfies the abovementioned criteria. this  framework hinges on the concepts of delegation, arbitration and high-level service discovery.  components of our own implementation of such an infrastructure are presented.",privacy/ethics
,"Louie, Jennifer",2004-10-01T14:00:10Z,2004-10-01T14:00:10Z,2003-06-01,http://hdl.handle.net/1721.1/5571,AITR-2003-009; CBCL-227,A Biological Model of Object Recognition with Feature Learning,"Previous biological models of object recognition in  cortex have been evaluated using idealized scenes  and have hard-coded features, such as the HMAX  model by Riesenhuber and Poggio [10]. Because  HMAX uses the same set of features for all object  classes, it does not perform well in the task of detecting  a target object in clutter. This thesis presents a new  model that integrates learning of object-specific  features with the HMAX. The new model performs  better than the standard HMAX and comparably to a  computer vision system on face detection. Results from  experimenting with unsupervised learning of features  and the use of a biologically-plausible classifier are  presented.",AITR-2003-009; CBCL-227,4307593 bytes; 5073756 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,235,"a biological model of object recognition with feature learning previous biological models of object recognition in  cortex have been evaluated using idealized scenes  and have hard-coded features, such as the hmax  model by riesenhuber and poggio [10]. because  hmax uses the same set of features for all object  classes, it does not perform well in the task of detecting  a target object in clutter. this thesis presents a new  model that integrates learning of object-specific  features with the hmax. the new model performs  better than the standard hmax and comparably to a  computer vision system on face detection. results from  experimenting with unsupervised learning of features  and the use of a biologically-plausible classifier are  presented.",object recognition/detection
,"Rosen, Ezra",2004-10-01T14:00:11Z,2004-10-01T14:00:11Z,2003-06-05,http://hdl.handle.net/1721.1/5572,AITR-2003-010; CBCL-228,Face Representation in Cortex: Studies Using a Simple and Not So Special Model,"The face inversion effect has been widely documented  as an effect of the uniqueness of face processing. Using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. In simulations using HMAX, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. Using  many, sharply tuned units, an ``expert'' has a large inversion effect. On the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. As the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. Using the same representations as with  faces, we create experts and novices for cars. We also measure the  feasibility of a view-based model for recognition of rotated objects  using HMAX. Using faces, we show that transfer of learning to novel views is possible.  Given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. Although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",AITR-2003-010; CBCL-228,66 p.; 13121869 bytes; 3182779 bytes,application/postscript; application/pdf,en_US,AI; Face Recognition; Representation; Invariance; HMAX,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,236,"face representation in cortex: studies using a simple and not so special model the face inversion effect has been widely documented  as an effect of the uniqueness of face processing. using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. in simulations using hmax, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. using  many, sharply tuned units, an ``expert'' has a large inversion effect. on the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. as the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. using the same representations as with  faces, we create experts and novices for cars. we also measure the  feasibility of a view-based model for recognition of rotated objects  using hmax. using faces, we show that transfer of learning to novel views is possible.  given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",face detection
,"Koile, Kimberle; Tollmar, Konrad; Demirdjian, David; Shrobe, Howard; Darrell, Trevor",2004-10-08T20:39:02Z,2004-10-08T20:39:02Z,2003-06-10,http://hdl.handle.net/1721.1/6720,AIM-2003-015,Activity Zones for Context-Aware Computing,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",AIM-2003-015,12 p.; 17075202 bytes; 8771896 bytes,application/postscript; application/pdf,en_US,AI; context-aware; activity; intelligent environment; ubiquitous; 3d tracker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,237,"activity zones for context-aware computing location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or euclidean  distance various landmarks. a user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. we show  how to partition a space into such regions based on  patterns of observed user location and  motion. these regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. we suggest that context-aware applications can benefit from a  location representation learned from observing users.  we describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",language models
,"Monteleoni, Claire",2004-10-20T20:32:01Z,2004-10-20T20:32:01Z,2003-06-12,http://hdl.handle.net/1721.1/7107,AITR-2003-011,Online Learning of Non-stationary Sequences,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",AITR-2003-011,48 p.; 1815576 bytes; 911860 bytes,application/postscript; application/pdf,en_US,AI; online learning; relative loss bounds; switching dynamics; wireless; 802.11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,238,"online learning of non-stationary sequences we consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. the performance of each expert may change over time in a manner unknown to the learner. we formulate a class of universal learning algorithms for this problem by expressing them as simple bayesian algorithms operating on models analogous to hidden markov models (hmms). we derive a new performance bound for such algorithms which is considerably simpler than existing bounds. the bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. we find an analytic expression for the a priori resolution at which we need to learn the rate parameter. we extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous hmms. we apply and examine our algorithm in the context of the problem of energy management in wireless networks. we analyze the new results in the framework of information theory.",high performance computing
,"Marjanovic, Matthew J.",2004-10-20T20:32:04Z,2004-10-20T20:32:04Z,2003-06-20,http://hdl.handle.net/1721.1/7108,AITR-2003-013,Teaching an Old Robot New Tricks: Learning Novel Tasks via Interaction with People and Things,"As AI has begun to reach out beyond its symbolic, objectivist roots into the embodied, experientialist realm, many projects are exploring different aspects of creating machines which interact with and respond to the world as humans do. Techniques for visual processing, object recognition, emotional response, gesture production and recognition, etc., are necessary components of a complete humanoid robot. However, most projects invariably concentrate on developing a few of these individual components, neglecting the issue of how all of these pieces would eventually fit together.  The focus of the work in this dissertation is on creating a framework into which such specific competencies can be embedded, in a way that they can interact with each other and build layers of new functionality. To be of any practical value, such a framework must satisfy the real-world constraints of functioning in real-time with noisy sensors and actuators. The humanoid robot Cog provides an unapologetically adequate platform from which to take on such a challenge.  This work makes three contributions to embodied AI. First, it offers a general-purpose architecture for developing behavior-based systems distributed over networks of PC's. Second, it provides a motor-control system that simulates several biological features which impact the development of motor behavior. Third, it develops a framework for a system which enables a robot to learn new behaviors via interacting with itself and the outside world. A few basic functional modules are built into this framework, enough to demonstrate the robot learning some very simple behaviors taught by a human trainer.  A primary motivation for this project is the notion that it is practically impossible to build an ""intelligent"" machine unless it is designed partly to build itself. This work is a proof-of-concept of such an approach to integrating multiple perceptual and motor systems into a complete learning agent.",AITR-2003-013,181 p.; 13057317 bytes; 13082678 bytes,application/postscript; application/pdf,en_US,AI; cog humanoid robot embodied learning phd thesis metaphor pancake reaching vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,239,"teaching an old robot new tricks: learning novel tasks via interaction with people and things as ai has begun to reach out beyond its symbolic, objectivist roots into the embodied, experientialist realm, many projects are exploring different aspects of creating machines which interact with and respond to the world as humans do. techniques for visual processing, object recognition, emotional response, gesture production and recognition, etc., are necessary components of a complete humanoid robot. however, most projects invariably concentrate on developing a few of these individual components, neglecting the issue of how all of these pieces would eventually fit together.  the focus of the work in this dissertation is on creating a framework into which such specific competencies can be embedded, in a way that they can interact with each other and build layers of new functionality. to be of any practical value, such a framework must satisfy the real-world constraints of functioning in real-time with noisy sensors and actuators. the humanoid robot cog provides an unapologetically adequate platform from which to take on such a challenge.  this work makes three contributions to embodied ai. first, it offers a general-purpose architecture for developing behavior-based systems distributed over networks of pc's. second, it provides a motor-control system that simulates several biological features which impact the development of motor behavior. third, it develops a framework for a system which enables a robot to learn new behaviors via interacting with itself and the outside world. a few basic functional modules are built into this framework, enough to demonstrate the robot learning some very simple behaviors taught by a human trainer.  a primary motivation for this project is the notion that it is practically impossible to build an ""intelligent"" machine unless it is designed partly to build itself. this work is a proof-of-concept of such an approach to integrating multiple perceptual and motor systems into a complete learning agent.",robotics
,"Lee, Lily",2004-10-20T20:32:06Z,2004-10-20T20:32:06Z,2003-06-26,http://hdl.handle.net/1721.1/7109,AITR-2003-014,Gait Analysis for Classification,"This thesis describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. A suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. Despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. Each of the integration methods are investigated for their advantages and disadvantages. An improved gait representation is built based on our experiences with the initial set of gait representations. In addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.",AITR-2003-014,110 p.; 4040471 bytes; 994319 bytes,application/postscript; application/pdf,en_US,AI; gait recognition; gender classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,240,"gait analysis for classification this thesis describes a representation of gait appearance for the purpose of person identification and classification. this gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. a suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. we demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. each of the integration methods are investigated for their advantages and disadvantages. an improved gait representation is built based on our experiences with the initial set of gait representations. in addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.",image classification
,"Timoner, Samson",2004-10-20T20:32:09Z,2004-10-20T20:32:09Z,2003-07-04,http://hdl.handle.net/1721.1/7110,AITR-2003-015,Compact Representations for Fast Nonrigid Registration of Medical Images,"We develop efficient techniques for the non-rigid registration of medical images by using representations that adapt to the anatomy found in such images.   Images of anatomical structures typically have uniform intensity interiors and smooth boundaries. We create methods to represent such regions compactly using tetrahedra. Unlike voxel-based representations, tetrahedra can accurately describe the expected smooth surfaces of medical objects. Furthermore, the interior of such objects can be represented using a small number of tetrahedra. Rather than describing a medical object using tens of thousands of voxels, our representations generally contain only a few thousand elements.  Tetrahedra facilitate the creation of efficient non-rigid registration algorithms based on finite element methods (FEM). We create a fast, FEM-based method to non-rigidly register segmented anatomical structures from two subjects. Using our compact tetrahedral representations, this method generally requires less than one minute of processing time on a desktop PC.  We also create a novel method for the non-rigid registration of gray scale images. To facilitate a fast method, we create a tetrahedral representation of a displacement field that automatically adapts to both the anatomy in an image and to the displacement field. The resulting algorithm has a computational cost that is dominated by the number of nodes in the mesh (about 10,000), rather than the number of voxels in an image (nearly 10,000,000). For many non-rigid registration problems, we can find a transformation from one image to another in five minutes. This speed is important as it allows use of the algorithm during surgery.  We apply our algorithms to find correlations between the shape of anatomical structures and the presence of schizophrenia. We show that a study based on our representations outperforms studies based on other representations. We also use the results of our non-rigid registration algorithm as the basis of a segmentation algorithm. That algorithm also outperforms other methods in our tests, producing smoother segmentations and more accurately reproducing manual segmentations.",AITR-2003-015,183 p.; 19734335 bytes; 5610171 bytes,application/postscript; application/pdf,en_US,AI; non-rigid registration; medical image processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,241,"compact representations for fast nonrigid registration of medical images we develop efficient techniques for the non-rigid registration of medical images by using representations that adapt to the anatomy found in such images.   images of anatomical structures typically have uniform intensity interiors and smooth boundaries. we create methods to represent such regions compactly using tetrahedra. unlike voxel-based representations, tetrahedra can accurately describe the expected smooth surfaces of medical objects. furthermore, the interior of such objects can be represented using a small number of tetrahedra. rather than describing a medical object using tens of thousands of voxels, our representations generally contain only a few thousand elements.  tetrahedra facilitate the creation of efficient non-rigid registration algorithms based on finite element methods (fem). we create a fast, fem-based method to non-rigidly register segmented anatomical structures from two subjects. using our compact tetrahedral representations, this method generally requires less than one minute of processing time on a desktop pc.  we also create a novel method for the non-rigid registration of gray scale images. to facilitate a fast method, we create a tetrahedral representation of a displacement field that automatically adapts to both the anatomy in an image and to the displacement field. the resulting algorithm has a computational cost that is dominated by the number of nodes in the mesh (about 10,000), rather than the number of voxels in an image (nearly 10,000,000). for many non-rigid registration problems, we can find a transformation from one image to another in five minutes. this speed is important as it allows use of the algorithm during surgery.  we apply our algorithms to find correlations between the shape of anatomical structures and the presence of schizophrenia. we show that a study based on our representations outperforms studies based on other representations. we also use the results of our non-rigid registration algorithm as the basis of a segmentation algorithm. that algorithm also outperforms other methods in our tests, producing smoother segmentations and more accurately reproducing manual segmentations.",high performance computing
,"Timoner, Samson",2005-12-12T23:24:20Z,2005-12-12T23:24:20Z,2003-07-04,http://hdl.handle.net/1721.1/29830,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Compact Representations for Fast Nonrigid Registration of Medical Images,"We develop efficient techniques for the non-rigid registration of medical
images by using representations that adapt to the anatomy found in such
images.

 Images of anatomical structures typically have uniform intensity interiors
and smooth boundaries. We create methods to represent such regions
compactly using tetrahedra.  Unlike voxel-based representations, tetrahedra
can accurately describe the expected smooth surfaces of medical
objects. Furthermore, the interior of such objects can be represented using
a small number of tetrahedra. Rather than describing a medical object using
tens of thousands of voxels, our representations generally contain only a few
thousand elements.

Tetrahedra facilitate the creation of efficient non-rigid registration
algorithms based on finite element methods (FEM).  We create a fast,
FEM-based method to non-rigidly register segmented anatomical structures
from two subjects. Using our compact tetrahedral representations, this
method generally requires less than one minute of processing time on a desktop
PC.

We also create a novel method for the non-rigid registration of gray scale
images. To facilitate a fast method, we create a tetrahedral representation
of a displacement field that automatically adapts to both the anatomy in an
image and to the displacement field.  The resulting algorithm has a
computational cost that is dominated by the number of nodes in the mesh
(about 10,000), rather than the number of voxels in an image (nearly
10,000,000). For many non-rigid registration problems, we can find a
transformation from one image to another in five minutes. This speed is
important as it allows use of the algorithm during surgery.

We apply our algorithms to find correlations between the shape of
anatomical structures and the presence of schizophrenia. We show that a
study based on our representations outperforms studies based on other
representations. We also use the results of our non-rigid registration
algorithm as the basis of a segmentation algorithm. That algorithm also
outperforms other methods in our tests, producing smoother segmentations
and more accurately reproducing manual segmentations.",MIT-CSAIL-TR-2003-001; AITR-2003-015,183 p.; 160218641 bytes; 8166856 bytes,application/postscript; application/pdf,en_US,AI; non-rigid registration; medical image processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,242,"compact representations for fast nonrigid registration of medical images we develop efficient techniques for the non-rigid registration of medical
images by using representations that adapt to the anatomy found in such
images.

 images of anatomical structures typically have uniform intensity interiors
and smooth boundaries. we create methods to represent such regions
compactly using tetrahedra.  unlike voxel-based representations, tetrahedra
can accurately describe the expected smooth surfaces of medical
objects. furthermore, the interior of such objects can be represented using
a small number of tetrahedra. rather than describing a medical object using
tens of thousands of voxels, our representations generally contain only a few
thousand elements.

tetrahedra facilitate the creation of efficient non-rigid registration
algorithms based on finite element methods (fem).  we create a fast,
fem-based method to non-rigidly register segmented anatomical structures
from two subjects. using our compact tetrahedral representations, this
method generally requires less than one minute of processing time on a desktop
pc.

we also create a novel method for the non-rigid registration of gray scale
images. to facilitate a fast method, we create a tetrahedral representation
of a displacement field that automatically adapts to both the anatomy in an
image and to the displacement field.  the resulting algorithm has a
computational cost that is dominated by the number of nodes in the mesh
(about 10,000), rather than the number of voxels in an image (nearly
10,000,000). for many non-rigid registration problems, we can find a
transformation from one image to another in five minutes. this speed is
important as it allows use of the algorithm during surgery.

we apply our algorithms to find correlations between the shape of
anatomical structures and the presence of schizophrenia. we show that a
study based on our representations outperforms studies based on other
representations. we also use the results of our non-rigid registration
algorithm as the basis of a segmentation algorithm. that algorithm also
outperforms other methods in our tests, producing smoother segmentations
and more accurately reproducing manual segmentations.",high performance computing
,"Felzenszwalb, Pedro F.",2004-10-20T20:32:11Z,2004-10-20T20:32:11Z,2003-08-08,http://hdl.handle.net/1721.1/7111,AITR-2003-016,Representation and Detection of Shapes in Images,"We present a set of techniques that can be used to represent and detect shapes in images. Our methods revolve around a particular shape representation based on the description of objects using triangulated polygons. This representation is similar to the medial axis transform and has important properties from a computational perspective. The first problem we consider is the detection of non-rigid objects in images using deformable models. We present an efficient algorithm to solve this problem in a wide range of situations, and show examples in both natural and medical images. We also consider the problem of learning an accurate non-rigid shape model for a class of objects from examples. We show how to learn good models while constraining them to the form required by the detection algorithm. Finally, we consider the problem of low-level image segmentation and grouping. We describe a stochastic grammar that generates arbitrary triangulated polygons while capturing Gestalt principles of shape regularity. This grammar is used as a prior model over random shapes in a low level algorithm that detects objects in images.",AITR-2003-016,80 p.; 6877524 bytes; 3132998 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,249,"representation and detection of shapes in images we present a set of techniques that can be used to represent and detect shapes in images. our methods revolve around a particular shape representation based on the description of objects using triangulated polygons. this representation is similar to the medial axis transform and has important properties from a computational perspective. the first problem we consider is the detection of non-rigid objects in images using deformable models. we present an efficient algorithm to solve this problem in a wide range of situations, and show examples in both natural and medical images. we also consider the problem of learning an accurate non-rigid shape model for a class of objects from examples. we show how to learn good models while constraining them to the form required by the detection algorithm. finally, we consider the problem of low-level image segmentation and grouping. we describe a stochastic grammar that generates arbitrary triangulated polygons while capturing gestalt principles of shape regularity. this grammar is used as a prior model over random shapes in a low level algorithm that detects objects in images.",object recognition/detection
,"Felzenszwalb, Pedro F.",2005-12-19T22:44:55Z,2005-12-19T22:44:55Z,2003-08-08,http://hdl.handle.net/1721.1/30400,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Representation and Detection of Shapes in Images,"We present a set of techniques that can be used to represent anddetect shapes in images.  Our methods revolve around a particularshape representation based on the description of objects usingtriangulated polygons.  This representation is similar to the medialaxis transform and has important properties from a computationalperspective.  The first problem we consider is the detection ofnon-rigid objects in images using deformable models.  We present anefficient algorithm to solve this problem in a wide range ofsituations, and show examples in both natural and medical images.  Wealso consider the problem of learning an accurate non-rigid shapemodel for a class of objects from examples.  We show how to learn goodmodels while constraining them to the form required by the detectionalgorithm.  Finally, we consider the problem of low-level imagesegmentation and grouping.  We describe a stochastic grammar thatgenerates arbitrary triangulated polygons while capturing Gestaltprinciples of shape regularity.  This grammar is used as a prior modelover random shapes in a low level algorithm that detects objects inimages.",MIT-CSAIL-TR-2003-008; AITR-2003-016,80 p.; 38103057 bytes; 1889641 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,250,"representation and detection of shapes in images we present a set of techniques that can be used to represent anddetect shapes in images.  our methods revolve around a particularshape representation based on the description of objects usingtriangulated polygons.  this representation is similar to the medialaxis transform and has important properties from a computationalperspective.  the first problem we consider is the detection ofnon-rigid objects in images using deformable models.  we present anefficient algorithm to solve this problem in a wide range ofsituations, and show examples in both natural and medical images.  wealso consider the problem of learning an accurate non-rigid shapemodel for a class of objects from examples.  we show how to learn goodmodels while constraining them to the form required by the detectionalgorithm.  finally, we consider the problem of low-level imagesegmentation and grouping.  we describe a stochastic grammar thatgenerates arbitrary triangulated polygons while capturing gestaltprinciples of shape regularity.  this grammar is used as a prior modelover random shapes in a low level algorithm that detects objects inimages.",object recognition/detection
,"Beal, Jacob",2005-12-19T22:49:01Z,2005-12-19T22:49:01Z,2003-08-11,http://hdl.handle.net/1721.1/30404,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Near-Optimal Distributed Failure Circumscription,"Small failures should only disrupt a small part of a network.  One wayto do this is by marking the surrounding area as untrustworthy ---circumscribing the failure. This can be done with a distributedalgorithm using hierarchical clustering and neighbor relations, andthe resulting circumscription is near-optimal for convex failures.",MIT-CSAIL-TR-2003-009; AIM-2003-017,9 p.; 13236751 bytes; 840133 bytes,application/postscript; application/pdf,en_US,AI; amorphous distributed ad-hoc computing self-organizing stopping failure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,252,"near-optimal distributed failure circumscription small failures should only disrupt a small part of a network.  one wayto do this is by marking the surrounding area as untrustworthy ---circumscribing the failure. this can be done with a distributedalgorithm using hierarchical clustering and neighbor relations, andthe resulting circumscription is near-optimal for convex failures.",privacy/ethics
,"Beal, Jacob",2004-10-08T20:39:04Z,2004-10-08T20:39:04Z,2003-08-11,http://hdl.handle.net/1721.1/6722,AIM-2003-017,Near-Optimal Distributed Failure Circumscription,"Small failures should only disrupt a small part of a network. One way to do this is by marking the surrounding area as untrustworthy --- circumscribing the failure. This can be done with a distributed algorithm using hierarchical clustering and neighbor relations, and the resulting circumscription is near-optimal for convex failures.",AIM-2003-017,9 p.; 2144454 bytes; 705176 bytes,application/postscript; application/pdf,en_US,AI; amorphous distributed ad-hoc computing self-organizing stopping failure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,253,"near-optimal distributed failure circumscription small failures should only disrupt a small part of a network. one way to do this is by marking the surrounding area as untrustworthy --- circumscribing the failure. this can be done with a distributed algorithm using hierarchical clustering and neighbor relations, and the resulting circumscription is near-optimal for convex failures.",privacy/ethics
,"Balas, Benjamin J.; Sinha, Pawan",2004-10-20T21:05:11Z,2004-10-20T21:05:11Z,2003-08-13,http://hdl.handle.net/1721.1/7276,AIM-2003-018; CBCL-229,Dissociated Dipoles: Image representation via non-local comparisons,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . We introduce the ""Dissociated Dipole,"" or ""Sticks"" operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",AIM-2003-018; CBCL-229,15 p.; 691165 bytes; 974071 bytes,application/postscript; application/pdf,en_US,AI; image representation; recognition; non-local filtering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,254,"dissociated dipoles: image representation via non-local comparisons a fundamental question in visual neuroscience is how to represent image structure. the most common representational schemes rely on differential operators that compare adjacent image regions. while well-suited to encoding local relationships, such operators have significant drawbacks. specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. we find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . we introduce the ""dissociated dipole,"" or ""sticks"" operator, for encoding non-local image relationships. this operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. we report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. our results suggest that non-local encoding may be an effective scheme for representing image structure.",privacy/ethics
,"Balas, Benjamin J.; Sinha, Pawan",2005-12-19T22:36:12Z,2005-12-19T22:36:12Z,2003-08-13,http://hdl.handle.net/1721.1/30398,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Dissociated Dipoles: Image representation via non-local comparisons,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filters span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. .We introduce the Dissociated Dipole, or Sticks operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",MIT-CSAIL-TR-2003-011; AIM-2003-018; CBCL-229,15 p.; 18211324 bytes; 682491 bytes,application/postscript; application/pdf,en_US,AI; image representation; recognition; non-local filtering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,255,"dissociated dipoles: image representation via non-local comparisons a fundamental question in visual neuroscience is how to represent image structure. the most common representational schemes rely on differential operators that compare adjacent image regions. while well-suited to encoding local relationships, such operators have significant drawbacks. specifically, each filters span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. we find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. .we introduce the dissociated dipole, or sticks operator, for encoding non-local image relationships. this operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. we report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. our results suggest that non-local encoding may be an effective scheme for representing image structure.",privacy/ethics
,"Shimizu, Hiroaki; Poggio, Tomaso",2005-12-19T22:32:54Z,2005-12-19T22:32:54Z,2003-08-27,http://hdl.handle.net/1721.1/30397,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Direction Estimation of Pedestrian from Images,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",MIT-CSAIL-TR-2003-013; AIM-2003-020; CBCL-230,11 p.; 12026063 bytes; 489901 bytes,application/postscript; application/pdf,en_US,AI; pedestrian; walking direction; classification; SVM; recognition; human motion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,257,"direction estimation of pedestrian from images the capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.we introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using svms. we find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",image classification
,"Shimizu, Hiroaki; Poggio, Tomaso",2004-10-20T21:05:12Z,2004-10-20T21:05:12Z,2003-08-27,http://hdl.handle.net/1721.1/7277,AIM-2003-020; CBCL-230,Direction Estimation of Pedestrian from Images,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",AIM-2003-020; CBCL-230,11 p.; 784806 bytes; 664353 bytes,application/postscript; application/pdf,en_US,AI; pedestrian; walking direction; classification; SVM; recognition; human motion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,259,"direction estimation of pedestrian from images the capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  we introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using svms. we find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",image classification
,"Mukherjee, Sayan; Golland, Polina; Panchenko, Dmitry",2005-12-19T23:02:49Z,2005-12-19T23:02:49Z,2003-08-28,http://hdl.handle.net/1721.1/30408,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Permutation Tests for Classification,"We introduce and explore an approach to estimating statisticalsignificance of classification accuracy, which is particularly usefulin scientific applications of machine learning where highdimensionality of the data and the small number of training examplesrender most standard convergence bounds too loose to yield ameaningful guarantee of the generalization ability of theclassifier. Instead, we estimate statistical significance of theobserved classification accuracy, or the likelihood of observing suchaccuracy by chance due to spurious correlations of thehigh-dimensional data patterns with the class labels in the giventraining set. We adopt permutation testing, a non-parametric techniquepreviously developed in classical statistics for hypothesis testing inthe generative setting (i.e., comparing two probabilitydistributions). We demonstrate the method on real examples fromneuroimaging studies and DNA microarray analysis and suggest atheoretical analysis of the procedure that relates the asymptoticbehavior of the test to the existing convergence bounds.",MIT-CSAIL-TR-2003-016; AIM-2003-019,22 p.; 22876548 bytes; 882217 bytes,application/postscript; application/pdf,en_US,AI; Classification; Permutation testing; Statistical significance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,260,"permutation tests for classification we introduce and explore an approach to estimating statisticalsignificance of classification accuracy, which is particularly usefulin scientific applications of machine learning where highdimensionality of the data and the small number of training examplesrender most standard convergence bounds too loose to yield ameaningful guarantee of the generalization ability of theclassifier. instead, we estimate statistical significance of theobserved classification accuracy, or the likelihood of observing suchaccuracy by chance due to spurious correlations of thehigh-dimensional data patterns with the class labels in the giventraining set. we adopt permutation testing, a non-parametric techniquepreviously developed in classical statistics for hypothesis testing inthe generative setting (i.e., comparing two probabilitydistributions). we demonstrate the method on real examples fromneuroimaging studies and dna microarray analysis and suggest atheoretical analysis of the procedure that relates the asymptoticbehavior of the test to the existing convergence bounds.",high performance computing
,"Mukherjee, Sayan; Golland, Polina; Panchenko, Dmitry",2004-10-08T20:39:06Z,2004-10-08T20:39:06Z,2003-08-28,http://hdl.handle.net/1721.1/6723,AIM-2003-019,Permutation Tests for Classification,"We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",AIM-2003-019,22 p.; 1135156 bytes; 662639 bytes,application/postscript; application/pdf,en_US,AI; Classification; Permutation testing; Statistical significance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,261,"permutation tests for classification we introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. we adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). we demonstrate the method on real examples from neuroimaging studies and dna microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",high performance computing
,"Che, Austin",2005-12-19T22:50:24Z,2005-12-19T22:50:24Z,2003-08-31,http://hdl.handle.net/1721.1/30406,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Fluorescence Assay for Polymerase Arrival Rates,"To engineer complex synthetic biological systems will require modulardesign, assembly, and characterization strategies. The RNApolymerase arrival rate (PAR) is defined to be the rate that RNApolymerases arrive at a specified location on the DNA. Designing andcharacterizing biological modules in terms of RNA polymerase arrivalrates provides for many advantages in the construction and modeling ofbiological systems.PARMESAN is an in vitro method for measuring polymerase arrival ratesusing pyrrolo-dC, a fluorescent DNA base that can substitute forcytosine. Pyrrolo-dC shows a detectable fluorescence difference whenin single-stranded versus double-stranded DNA. During transcription,RNA polymerase separates the two strands of DNA, leading to a changein the fluorescence of pyrrolo-dC. By incorporating pyrrolo-dC atspecific locations in the DNA, fluorescence changes can be taken as adirect measurement of the polymerase arrival rate.",MIT-CSAIL-TR-2003-017; AITR-2003-017,112 p.; 113486485 bytes; 7023595 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,262,"fluorescence assay for polymerase arrival rates to engineer complex synthetic biological systems will require modulardesign, assembly, and characterization strategies. the rnapolymerase arrival rate (par) is defined to be the rate that rnapolymerases arrive at a specified location on the dna. designing andcharacterizing biological modules in terms of rna polymerase arrivalrates provides for many advantages in the construction and modeling ofbiological systems.parmesan is an in vitro method for measuring polymerase arrival ratesusing pyrrolo-dc, a fluorescent dna base that can substitute forcytosine. pyrrolo-dc shows a detectable fluorescence difference whenin single-stranded versus double-stranded dna. during transcription,rna polymerase separates the two strands of dna, leading to a changein the fluorescence of pyrrolo-dc. by incorporating pyrrolo-dc atspecific locations in the dna, fluorescence changes can be taken as adirect measurement of the polymerase arrival rate.",image classification
,"Che, Austin",2004-10-20T20:32:21Z,2004-10-20T20:32:21Z,2003-08-31,http://hdl.handle.net/1721.1/7112,AITR-2003-017,Fluorescence Assay for Polymerase Arrival Rates,"To engineer complex synthetic biological systems will require modular design, assembly, and characterization strategies. The RNA polymerase arrival rate (PAR) is defined to be the rate that RNA polymerases arrive at a specified location on the DNA. Designing and characterizing biological modules in terms of RNA polymerase arrival rates provides for many advantages in the construction and modeling of biological systems.  PARMESAN is an in vitro method for measuring polymerase arrival rates using pyrrolo-dC, a fluorescent DNA base that can substitute for cytosine. Pyrrolo-dC shows a detectable fluorescence difference when in single-stranded versus double-stranded DNA. During transcription, RNA polymerase separates the two strands of DNA, leading to a change in the fluorescence of pyrrolo-dC. By incorporating pyrrolo-dC at specific locations in the DNA, fluorescence changes can be taken as a direct measurement of the polymerase arrival rate.",AITR-2003-017,112 p.; 92476964 bytes; 3362118 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,263,"fluorescence assay for polymerase arrival rates to engineer complex synthetic biological systems will require modular design, assembly, and characterization strategies. the rna polymerase arrival rate (par) is defined to be the rate that rna polymerases arrive at a specified location on the dna. designing and characterizing biological modules in terms of rna polymerase arrival rates provides for many advantages in the construction and modeling of biological systems.  parmesan is an in vitro method for measuring polymerase arrival rates using pyrrolo-dc, a fluorescent dna base that can substitute for cytosine. pyrrolo-dc shows a detectable fluorescence difference when in single-stranded versus double-stranded dna. during transcription, rna polymerase separates the two strands of dna, leading to a change in the fluorescence of pyrrolo-dc. by incorporating pyrrolo-dc at specific locations in the dna, fluorescence changes can be taken as a direct measurement of the polymerase arrival rate.",image classification
,"Ross, Michael G.; Kaelbling, Leslie Pack",2004-10-08T20:43:02Z,2004-10-08T20:43:02Z,2003-09-08,http://hdl.handle.net/1721.1/6730,AIM-2003-022,Learning object segmentation from video data,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",AIM-2003-022,15 p.; 2769288 bytes; 1654353 bytes,application/postscript; application/pdf,en_US,AI; learning; image segmentation; motion; Markov random field; belief propagation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,264,"learning object segmentation from video data this memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. a video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. the purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  this work was funded in part by the office of naval research contract #n00014-00-1-0298, in part by the singapore-mit alliance agreement of 11/6/98, and in part by a national science foundation graduate student fellowship.",object recognition/detection
,"Kouh, Minjoon; Riesenhuber, Maximilian",2004-10-20T21:05:14Z,2004-10-20T21:05:14Z,2003-09-08,http://hdl.handle.net/1721.1/7278,AIM-2003-021; CBCL-231,Investigating shape representation in area V4 with HMAX: Orientation and Grating selectivities,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",AIM-2003-021; CBCL-231,14 p.; 2802887 bytes; 1234306 bytes,application/postscript; application/pdf,en_US,AI; Shape Tuning; Shape Representation; Features; HMAX; Visual Cortex; Gratings; V4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,265,"investigating shape representation in area v4 with hmax: orientation and grating selectivities the question of how shape is represented is of central interest to understanding visual processing in cortex. while tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as v4. we used the model of object recognition in cortex presented by riesenhuber and poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of v4 neurons from the literature. in particular, we investigated the issue of shape representation in visual area v1 and v4 using oriented bars and various types of gratings (polar, hyperbolic, and cartesian), as used in several physiology experiments. our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-cartesian stimuli. interestingly, the simulation results suggest that some v4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. however, the simulations also show that the stimulus set of cartesian and non-cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",high performance computing
,"Ross, Michael G.; Kaelbling, Leslie Pack",2005-12-19T22:46:46Z,2005-12-19T22:46:46Z,2003-09-08,http://hdl.handle.net/1721.1/30401,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning object segmentation from video data,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",MIT-CSAIL-TR-2003-018; AIM-2003-022,15 p.; 23365488 bytes; 1821447 bytes,application/postscript; application/pdf,en_US,AI; learning; image segmentation; motion; Markov random field; belief propagation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,266,"learning object segmentation from video data this memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. a video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. the purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.this work was funded in part by the office of naval research contract#n00014-00-1-0298, in part by the singapore-mit alliance agreement of11/6/98, and in part by a national science foundation graduate studentfellowship.",object recognition/detection
,"Kouh, Minjoon; Riesenhuber, Maximilian",2005-12-20T22:01:32Z,2005-12-20T22:01:32Z,2003-09-08,http://hdl.handle.net/1721.1/30424,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Investigating shape representation in area V4 with HMAX: Orientation and Grating selectivities,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",MIT-CSAIL-TR-2003-019; AIM-2003-021; CBCL-231,14 p.; 20150764 bytes; 1258167 bytes,application/postscript; application/pdf,en_US,AI; Shape Tuning; Shape Representation; Features; HMAX; Visual Cortex; Gratings; V4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,267,"investigating shape representation in area v4 with hmax: orientation and grating selectivities the question of how shape is represented is of central interest to understanding visual processing in cortex. while tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as v4.  we used the model of object recognition in cortex presented by riesenhuber and poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of v4 neurons from the literature.  in particular, we investigated the issue of shape representation in visual area v1 and v4 using oriented bars and various types of gratings (polar, hyperbolic, and cartesian), as used in several physiology experiments.  our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-cartesian stimuli.  interestingly, the simulation results suggest that some v4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  however, the simulations also show that the stimulus set of cartesian and non-cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",high performance computing
,"Eisenstein, Jacob",2005-12-22T01:12:20Z,2005-12-22T01:12:20Z,2003-10-28,http://hdl.handle.net/1721.1/30431,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Evolving Robocode Tank Fighters,"In this paper, I describe the application of genetic programming to evolve a controller for a robotic tank in a simulated environment.The purpose is to explore how genetic techniques can best be applied to produce controllers based on subsumption and behavior oriented languages such as REX.  As part of my implementation, I developed TableRex, a modification of REX that can be expressed on a fixed-lengthgenome.  Using a fixed subsumption architecture of TableRex modules, I evolved robots that beat some of the most competitive hand-coded adversaries.",MIT-CSAIL-TR-2003-026; AIM-2003-023,24 p.; 21707085 bytes; 716288 bytes,application/postscript; application/pdf,en_US,AI; genetic programming; robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,274,"evolving robocode tank fighters in this paper, i describe the application of genetic programming to evolve a controller for a robotic tank in a simulated environment.the purpose is to explore how genetic techniques can best be applied to produce controllers based on subsumption and behavior oriented languages such as rex.  as part of my implementation, i developed tablerex, a modification of rex that can be expressed on a fixed-lengthgenome.  using a fixed subsumption architecture of tablerex modules, i evolved robots that beat some of the most competitive hand-coded adversaries.",robotics
,"Eisenstein, Jacob",2004-10-08T20:43:03Z,2004-10-08T20:43:03Z,2003-10-28,http://hdl.handle.net/1721.1/6731,AIM-2003-023,Evolving Robocode Tank Fighters,"In this paper, I describe the application of genetic programming  to evolve a controller for a robotic tank in a simulated environment. The purpose is to explore how genetic techniques can best be applied  to produce controllers based on subsumption and behavior oriented  languages such as REX. As part of my implementation, I developed  TableRex, a modification of REX that can be expressed on a fixed-length genome. Using a fixed subsumption architecture of TableRex modules,  I evolved robots that beat some of the most competitive hand-coded  adversaries.",AIM-2003-023,24 p.; 963754 bytes; 653698 bytes,application/postscript; application/pdf,en_US,AI; genetic programming; robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,275,"evolving robocode tank fighters in this paper, i describe the application of genetic programming  to evolve a controller for a robotic tank in a simulated environment. the purpose is to explore how genetic techniques can best be applied  to produce controllers based on subsumption and behavior oriented  languages such as rex. as part of my implementation, i developed  tablerex, a modification of rex that can be expressed on a fixed-length genome. using a fixed subsumption architecture of tablerex modules,  i evolved robots that beat some of the most competitive hand-coded  adversaries.",robotics
,"Morgenstern, Christian; Heisele, Bernd",2004-10-20T21:05:16Z,2004-10-20T21:05:16Z,2003-11-28,http://hdl.handle.net/1721.1/7279,AIM-2003-024; CBCL-232,Component based recognition of objects in an office environment,We present a component-based approach for recognizing objects under large pose changes. From a set of training images of a given object we extract a large number of components which are clustered based on the similarity of their image features and their locations within the object image. The cluster centers build an initial set of component templates from which we select a subset for the final recognizer. In experiments we evaluate different sizes and types of components and three standard techniques for component selection. The component classifiers are finally compared to global classifiers on a database of four objects.,AIM-2003-024; CBCL-232,12 p.; 3572823 bytes; 962401 bytes,application/postscript; application/pdf,en_US,AI; computer vision; object recognition; component object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,280,component based recognition of objects in an office environment we present a component-based approach for recognizing objects under large pose changes. from a set of training images of a given object we extract a large number of components which are clustered based on the similarity of their image features and their locations within the object image. the cluster centers build an initial set of component templates from which we select a subset for the final recognizer. in experiments we evaluate different sizes and types of components and three standard techniques for component selection. the component classifiers are finally compared to global classifiers on a database of four objects.,object recognition/detection
,"Morgenstern, Christian; Heisele, Bernd",2005-12-22T01:15:11Z,2005-12-22T01:15:11Z,2003-11-28,http://hdl.handle.net/1721.1/30436,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Component based recognition of objects in an office environment,We present a component-based approach for recognizing objectsunder large pose changes. From a set of training images of a givenobject we extract a large number of components which are clusteredbased on the similarity of their image features and their locations withinthe object image. The cluster centers build an initial set of componenttemplates from which we select a subset for the final recognizer.In experiments we evaluate different sizes and types of components andthree standard techniques for component selection. The component classifiersare finally compared to global classifiers on a database of fourobjects.,MIT-CSAIL-TR-2003-031; AIM-2003-024; CBCL-232,12 p.; 20676042 bytes; 965767 bytes,application/postscript; application/pdf,en_US,AI; computer vision; object recognition; component object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,281,component based recognition of objects in an office environment we present a component-based approach for recognizing objectsunder large pose changes. from a set of training images of a givenobject we extract a large number of components which are clusteredbased on the similarity of their image features and their locations withinthe object image. the cluster centers build an initial set of componenttemplates from which we select a subset for the final recognizer.in experiments we evaluate different sizes and types of components andthree standard techniques for component selection. the component classifiersare finally compared to global classifiers on a database of fourobjects.,object recognition/detection
,"Chang, Yu-Han; Ho, Tracey; Kaelbling, Leslie Pack",2005-12-22T01:15:17Z,2005-12-22T01:15:17Z,2003-12-04,http://hdl.handle.net/1721.1/30437,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Mobilized ad-hoc networks:  A reinforcement learning approach,"Research in mobile ad-hoc networks has focused on situations in whichnodes have no control over their movements.  We investigate animportant but overlooked domain in which nodes do have controlover their movements.  Reinforcement learning methods can be used tocontrol both packet routing decisions and node mobility, dramaticallyimproving the connectivity of the network.  We first motivate theproblem by presenting theoretical bounds for the connectivityimprovement of partially mobile networks and then present superiorempirical results under a variety of different scenarios in which themobile nodes in our ad-hoc network are embedded with adaptive routingpolicies and learned movement policies.",MIT-CSAIL-TR-2003-032; AIM-2003-025,9 p.; 15523730 bytes; 577014 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; multi-agent learning; ad-hoc networking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,282,"mobilized ad-hoc networks:  a reinforcement learning approach research in mobile ad-hoc networks has focused on situations in whichnodes have no control over their movements.  we investigate animportant but overlooked domain in which nodes do have controlover their movements.  reinforcement learning methods can be used tocontrol both packet routing decisions and node mobility, dramaticallyimproving the connectivity of the network.  we first motivate theproblem by presenting theoretical bounds for the connectivityimprovement of partially mobile networks and then present superiorempirical results under a variety of different scenarios in which themobile nodes in our ad-hoc network are embedded with adaptive routingpolicies and learned movement policies.",language models
,"Chang, Yu-Han; Ho, Tracey; Kaelbling, Leslie Pack",2004-10-08T20:43:04Z,2004-10-08T20:43:04Z,2003-12-04,http://hdl.handle.net/1721.1/6732,AIM-2003-025,Mobilized ad-hoc networks: A reinforcement learning approach,"Research in mobile ad-hoc networks has focused on situations in which nodes have no control over their movements. We investigate an important but overlooked domain in which nodes do have control over their movements. Reinforcement learning methods can be used to control both packet routing decisions and node mobility, dramatically improving the connectivity of the network. We first motivate the problem by presenting theoretical bounds for the connectivity improvement of partially mobile networks and then present superior empirical results under a variety of different scenarios in which the mobile nodes in our ad-hoc network are embedded with adaptive routing policies and learned movement policies.",AIM-2003-025,9 p.; 771382 bytes; 1199447 bytes,application/postscript; application/pdf,en_US,AI; reinforcement learning; multi-agent learning; ad-hoc networking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,283,"mobilized ad-hoc networks: a reinforcement learning approach research in mobile ad-hoc networks has focused on situations in which nodes have no control over their movements. we investigate an important but overlooked domain in which nodes do have control over their movements. reinforcement learning methods can be used to control both packet routing decisions and node mobility, dramatically improving the connectivity of the network. we first motivate the problem by presenting theoretical bounds for the connectivity improvement of partially mobile networks and then present superior empirical results under a variety of different scenarios in which the mobile nodes in our ad-hoc network are embedded with adaptive routing policies and learned movement policies.",language models
,"Grauman, Kristen; Darrell, Trevor",2005-12-22T01:15:24Z,2005-12-22T01:15:24Z,2003-12-05,http://hdl.handle.net/1721.1/30438,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Fast Contour Matching Using Approximate Earth Mover's Distance,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",MIT-CSAIL-TR-2003-033; AIM-2003-026,16 p.; 18655633 bytes; 2291372 bytes,application/postscript; application/pdf,en_US,AI; contour matching; shape matching; EMD; image retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,284,"fast contour matching using approximate earth mover's distance weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  however, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  we present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the earth mover's distance (emd) into anormed space.  given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  we demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",high performance computing
,"Grauman, Kristen; Darrell, Trevor",2004-10-08T20:43:06Z,2004-10-08T20:43:06Z,2003-12-05,http://hdl.handle.net/1721.1/6733,AIM-2003-026,Fast Contour Matching Using Approximate Earth Mover's Distance,"Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",AIM-2003-026,16 p.; 7561935 bytes; 7530316 bytes,application/postscript; application/pdf,en_US,AI; contour matching; shape matching; EMD; image retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,285,"fast contour matching using approximate earth mover's distance weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. however, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. we present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the earth mover's distance (emd) into a normed space. given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. we demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",high performance computing
,"Beal, Jacob; Gilbert, Seth",2004-10-08T20:43:07Z,2004-10-08T20:43:07Z,2003-12-17,http://hdl.handle.net/1721.1/6734,AIM-2003-027,RamboNodes for the Metropolitan Ad Hoc Network,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",AIM-2003-027,22 p.; 1312502 bytes; 499111 bytes,application/postscript; application/pdf,en_US,AI; ad-hoc networks distributed algorithms atomic distributed shared memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,287,"rambonodes for the metropolitan ad hoc network we present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. for example, data might migrate away from failures or toward regions of high demand. the persistentnode algorithm provides this service robustly, but with limited safety guarantees. we use the rambo framework to transform persistentnode into rambonode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. in addition, a half-life analysis of rambonode shows that it is robust against continuous low-rate failures. finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",language models
,"Beal, Jacob; Gilbert, Seth",2005-12-22T01:15:30Z,2005-12-22T01:15:30Z,2003-12-17,http://hdl.handle.net/1721.1/30439,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,RamboNodes for the Metropolitan Ad Hoc Network,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",MIT-CSAIL-TR-2003-034; AIM-2003-027,22 p.; 23886105 bytes; 803571 bytes,application/postscript; application/pdf,en_US,AI; ad-hoc networks distributed algorithms atomic distributed shared memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,288,"rambonodes for the metropolitan ad hoc network we present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. for example, data might migrate away from failures or toward regions of high demand. the persistentnode algorithm provides this service robustly, but with limited safety guarantees. we use the rambo framework to transform persistentnode into rambonode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. in addition, a half-life analysis of rambonode shows that it is robust against continuous low-rate failures. finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",language models
,"Schneider, Robert; Riesenhuber, Maximilian",2004-10-20T21:05:18Z,2004-10-20T21:05:18Z,2004-01-14,http://hdl.handle.net/1721.1/7280,AIM-2004-004; CBCL-235,On the difficulty of feature-based attentional modulations in visual object recognition: A modeling study.,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45]. We modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",AIM-2004-004; CBCL-235,38 p.; 4871469 bytes; 1392271 bytes,application/postscript; application/pdf,en_US,AI; object recognition; attention; vision; modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,290,"on the difficulty of feature-based attentional modulations in visual object recognition: a modeling study. numerous psychophysical experiments have shown an important role for attentional modulations in vision. behaviorally, allocation of attention can improve performance in object detection and recognition tasks. at the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. however, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. to answer this question, we performed simulations with the hmax model of object recognition in cortex [45]. we modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. it turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. these observations lead us to postulate a new role for the observed attention-related neural response modulations.",object recognition/detection
,"Schneider, Robert; Riesenhuber, Maximilian",2005-12-22T01:18:52Z,2005-12-22T01:18:52Z,2004-01-14,http://hdl.handle.net/1721.1/30442,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On the difficulty of feature-based attentional modulations in visual object recognition: A modeling study.,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45].We modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",MIT-CSAIL-TR-2004-001; AIM-2004-004; CBCL-235,38 p.; 65171868 bytes; 2503743 bytes,application/postscript; application/pdf,en_US,AI; object recognition; attention; vision; modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,291,"on the difficulty of feature-based attentional modulations in visual object recognition: a modeling study. numerous psychophysical experiments have shown an important role for attentional modulations in vision. behaviorally, allocation of attention can improve performance in object detection and recognition tasks. at the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. however, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. to answer this question, we performed simulations with the hmax model of object recognition in cortex [45].we modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. it turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. these observations lead us to postulate a new role for the observed attention-related neural response modulations.",object recognition/detection
,"Rakhlin, Alexander; Panchenko, Dmitry; Mukherjee, Sayan",2005-12-22T01:18:58Z,2005-12-22T01:18:58Z,2004-01-27,http://hdl.handle.net/1721.1/30443,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Risk Bounds for Mixture Density Estimation,"In this paper we focus on the problem of estimating a boundeddensity using a finite combination of densities from a givenclass. We consider the Maximum Likelihood Procedure (MLE) and the greedy procedure described by Li and Barron. Approximation and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\frac{1}{\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",MIT-CSAIL-TR-2004-002; AIM-2004-001; CBCL-233,11 p.; 9297095 bytes; 498791 bytes,application/postscript; application/pdf,en_US,AI; density estimation; MLE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,292,"risk bounds for mixture density estimation in this paper we focus on the problem of estimating a boundeddensity using a finite combination of densities from a givenclass. we consider the maximum likelihood procedure (mle) and the greedy procedure described by li and barron. approximation and estimation bounds are given for the above methods. we extend and improve upon the estimation results of li and barron, and in particular prove an $o(\frac{1}{\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",language models
,"Wolf, Lior; Amnon Shashua,; Mukherjee, Sayan",2004-10-20T21:05:21Z,2004-10-20T21:05:21Z,2004-01-27,http://hdl.handle.net/1721.1/7282,AIM-2004-002; CBCL-234,Selecting Relevant Genes with a Spectral Approach,"Array technologies have made it possible to record simultaneously the expression pattern of thousands of genes. A fundamental problem in the analysis of gene expression data is the identification of highly relevant genes that either discriminate between phenotypic labels or are important with respect to the cellular process studied in the experiment: for example cell cycle or heat shock in yeast experiments, chemical or genetic perturbations of mammalian cell lines, and genes involved in class discovery for human tumors. In this paper we focus on the task of unsupervised gene selection. The problem of selecting a small subset of genes is particularly challenging as the datasets involved are typically characterized by a very small sample size ?? the order of few tens of tissue samples ??d by a very large feature space as the number of genes tend to be in the high thousands. We propose a model independent approach which scores candidate gene selections using spectral properties of the candidate affinity matrix. The algorithm is very straightforward to implement yet contains a number of remarkable properties which guarantee consistent sparse selections. To illustrate the value of our approach we applied our algorithm on five different datasets. The first consists of time course data from four well studied Hematopoietic cell lines (HL-60, Jurkat, NB4, and U937). The other four datasets include three well studied treatment outcomes (large cell lymphoma, childhood medulloblastomas, breast tumors) and one unpublished dataset (lymph status). We compared our approach both with other unsupervised methods (SOM,PCA,GS) and with supervised methods (SNR,RMB,RFE). The results clearly show that our approach considerably outperforms all the other unsupervised approaches in our study, is competitive with supervised methods and in some case even outperforms supervised approaches.",AIM-2004-002; CBCL-234,2062939 bytes; 836436 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,293,"selecting relevant genes with a spectral approach array technologies have made it possible to record simultaneously the expression pattern of thousands of genes. a fundamental problem in the analysis of gene expression data is the identification of highly relevant genes that either discriminate between phenotypic labels or are important with respect to the cellular process studied in the experiment: for example cell cycle or heat shock in yeast experiments, chemical or genetic perturbations of mammalian cell lines, and genes involved in class discovery for human tumors. in this paper we focus on the task of unsupervised gene selection. the problem of selecting a small subset of genes is particularly challenging as the datasets involved are typically characterized by a very small sample size ?? the order of few tens of tissue samples ??d by a very large feature space as the number of genes tend to be in the high thousands. we propose a model independent approach which scores candidate gene selections using spectral properties of the candidate affinity matrix. the algorithm is very straightforward to implement yet contains a number of remarkable properties which guarantee consistent sparse selections. to illustrate the value of our approach we applied our algorithm on five different datasets. the first consists of time course data from four well studied hematopoietic cell lines (hl-60, jurkat, nb4, and u937). the other four datasets include three well studied treatment outcomes (large cell lymphoma, childhood medulloblastomas, breast tumors) and one unpublished dataset (lymph status). we compared our approach both with other unsupervised methods (som,pca,gs) and with supervised methods (snr,rmb,rfe). the results clearly show that our approach considerably outperforms all the other unsupervised approaches in our study, is competitive with supervised methods and in some case even outperforms supervised approaches.",language models
,"Wolf, Lior; Shashua, Amnon; Mukherjee, Sayan",2005-12-22T01:19:04Z,2005-12-22T01:19:04Z,2004-01-27,http://hdl.handle.net/1721.1/30444,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Selecting Relevant Genes with a Spectral Approach,"Array technologies have made it possible to record simultaneouslythe expression pattern of thousands of genes. A fundamental problemin the analysis of gene expression data is the identification ofhighly relevant genes that either discriminate between phenotypiclabels or are important with respect to the cellular process studied inthe experiment: for example cell cycle or heat shock in yeast experiments,chemical or genetic perturbations of mammalian cell lines,and genes involved in class discovery for human tumors. In this paperwe focus on the task of unsupervised gene selection. The problemof selecting a small subset of genes is particularly challengingas the datasets involved are typically characterized by a very smallsample size  in the order of few tens of tissue samples  andby a very large feature space as the number of genes tend to bein the high thousands. We propose a model independent approachwhich scores candidate gene selections using spectral properties ofthe candidate affinity matrix. The algorithm is very straightforwardto implement yet contains a number of remarkable properties whichguarantee consistent sparse selections. To illustrate the value of ourapproach we applied our algorithm on five different datasets. Thefirst consists of time course data from four well studied Hematopoieticcell lines (HL-60, Jurkat, NB4, and U937). The other fourdatasets include three well studied treatment outcomes (large celllymphoma, childhood medulloblastomas, breast tumors) and oneunpublished dataset (lymph status). We compared our approachboth with other unsupervised methods (SOM,PCA,GS) and withsupervised methods (SNR,RMB,RFE). The results clearly showthat our approach considerably outperforms all the other unsupervisedapproaches in our study, is competitive with supervised methodsand in some case even outperforms supervised approaches.",MIT-CSAIL-TR-2004-003; AIM-2004-002; CBCL-234,0 p.; 12089662 bytes; 629163 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,294,"selecting relevant genes with a spectral approach array technologies have made it possible to record simultaneouslythe expression pattern of thousands of genes. a fundamental problemin the analysis of gene expression data is the identification ofhighly relevant genes that either discriminate between phenotypiclabels or are important with respect to the cellular process studied inthe experiment: for example cell cycle or heat shock in yeast experiments,chemical or genetic perturbations of mammalian cell lines,and genes involved in class discovery for human tumors. in this paperwe focus on the task of unsupervised gene selection. the problemof selecting a small subset of genes is particularly challengingas the datasets involved are typically characterized by a very smallsample size  in the order of few tens of tissue samples  andby a very large feature space as the number of genes tend to bein the high thousands. we propose a model independent approachwhich scores candidate gene selections using spectral properties ofthe candidate affinity matrix. the algorithm is very straightforwardto implement yet contains a number of remarkable properties whichguarantee consistent sparse selections. to illustrate the value of ourapproach we applied our algorithm on five different datasets. thefirst consists of time course data from four well studied hematopoieticcell lines (hl-60, jurkat, nb4, and u937). the other fourdatasets include three well studied treatment outcomes (large celllymphoma, childhood medulloblastomas, breast tumors) and oneunpublished dataset (lymph status). we compared our approachboth with other unsupervised methods (som,pca,gs) and withsupervised methods (snr,rmb,rfe). the results clearly showthat our approach considerably outperforms all the other unsupervisedapproaches in our study, is competitive with supervised methodsand in some case even outperforms supervised approaches.",language models
,"Rakhlin, Alexander; Panchenko, Dmitry; Mukherjee, Sayan",2004-10-20T21:05:19Z,2004-10-20T21:05:19Z,2004-01-27,http://hdl.handle.net/1721.1/7281,AIM-2004-001; CBCL-233,Risk Bounds for Mixture Density Estimation,"In this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class. We consider the Maximum Likelihood Procedure (MLE) and  the greedy procedure described by Li and Barron. Approximation  and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\\frac{1}{\\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",AIM-2004-001; CBCL-233,11 p.; 1656004 bytes; 658609 bytes,application/postscript; application/pdf,en_US,AI; density estimation; MLE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,295,"risk bounds for mixture density estimation in this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class. we consider the maximum likelihood procedure (mle) and  the greedy procedure described by li and barron. approximation  and estimation bounds are given for the above methods. we extend and improve upon the estimation results of li and barron, and in particular prove an $o(\\frac{1}{\\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",language models
,"Grauman, Kristen; Shakhnarovich, Gregory; Darrell, Trevor",2004-10-08T20:43:09Z,2004-10-08T20:43:09Z,2004-01-28,http://hdl.handle.net/1721.1/6735,AIM-2004-003,Virtual Visual Hulls: Example-Based 3D Shape Estimation from a Single Silhouette,"Recovering a volumetric model of a person, car, or other object of interest from a single snapshot would be useful for many computer graphics applications. 3D model estimation in general is hard, and currently requires active sensors, multiple views, or integration over time. For a known object class, however, 3D shape can be successfully inferred from a single snapshot. We present a method for generating a ``virtual visual hull''-- an estimate of the 3D shape of an object from a known class, given a single silhouette observed from an unknown viewpoint. For a given class, a large database of multi-view silhouette examples from calibrated, though possibly varied, camera rigs are collected. To infer a novel single view input silhouette's virtual visual hull, we search for 3D shapes in the database which are most consistent with the observed contour. The input is matched to component single views of the multi-view training examples. A set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. The 3D shape estimate for the input is then found by interpolating between the contours of these aligned views. When the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. We show results of our algorithm on real and synthetic images of people.",AIM-2004-003,25 p.; 7098694 bytes; 2050007 bytes,application/postscript; application/pdf,en_US,AI; visual hulls; silhouettes; nearest neighbors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,296,"virtual visual hulls: example-based 3d shape estimation from a single silhouette recovering a volumetric model of a person, car, or other object of interest from a single snapshot would be useful for many computer graphics applications. 3d model estimation in general is hard, and currently requires active sensors, multiple views, or integration over time. for a known object class, however, 3d shape can be successfully inferred from a single snapshot. we present a method for generating a ``virtual visual hull''-- an estimate of the 3d shape of an object from a known class, given a single silhouette observed from an unknown viewpoint. for a given class, a large database of multi-view silhouette examples from calibrated, though possibly varied, camera rigs are collected. to infer a novel single view input silhouette's virtual visual hull, we search for 3d shapes in the database which are most consistent with the observed contour. the input is matched to component single views of the multi-view training examples. a set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. the 3d shape estimate for the input is then found by interpolating between the contours of these aligned views. when the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. we show results of our algorithm on real and synthetic images of people.",language models
,"Grauman, Kristen; Shakhnarovich, Gregory; Darrell, Trevor",2005-12-22T01:19:14Z,2005-12-22T01:19:14Z,2004-01-28,http://hdl.handle.net/1721.1/30445,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Virtual Visual Hulls: Example-Based 3D Shape Estimation from a Single Silhouette,"Recovering a volumetric model of a person, car, or other objectof interest from a single snapshot would be useful for many computergraphics applications.  3D model estimation in general is hard, andcurrently requires active sensors, multiple views, or integration overtime.  For a known object class, however, 3D shape can be successfullyinferred from a single snapshot.  We present a method for generating a``virtual visual hull''-- an estimate of the 3D shape of an objectfrom a known class, given a single silhouette observed from an unknownviewpoint.  For a given class, a large database of multi-viewsilhouette examples from calibrated, though possibly varied, camerarigs are collected.  To infer a novel single view input silhouette'svirtual visual hull, we search for 3D shapes in the database which aremost consistent with the observed contour.  The input is matched tocomponent single views of the multi-view training examples.  A set ofviewpoint-aligned virtual views are generated from the visual hullscorresponding to these examples.  The 3D shape estimate for the inputis then found by interpolating between the contours of these alignedviews.  When the underlying shape is ambiguous given a single viewsilhouette, we produce multiple visual hull hypotheses; if a sequenceof input images is available, a dynamic programming approach isapplied to find the maximum likelihood path through the feasiblehypotheses over time.  We show results of our algorithm on real andsynthetic images of people.",MIT-CSAIL-TR-2004-004; AIM-2004-003,25 p.; 47215444 bytes; 7506667 bytes,application/postscript; application/pdf,en_US,AI; visual hulls; silhouettes; nearest neighbors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,297,"virtual visual hulls: example-based 3d shape estimation from a single silhouette recovering a volumetric model of a person, car, or other objectof interest from a single snapshot would be useful for many computergraphics applications.  3d model estimation in general is hard, andcurrently requires active sensors, multiple views, or integration overtime.  for a known object class, however, 3d shape can be successfullyinferred from a single snapshot.  we present a method for generating a``virtual visual hull''-- an estimate of the 3d shape of an objectfrom a known class, given a single silhouette observed from an unknownviewpoint.  for a given class, a large database of multi-viewsilhouette examples from calibrated, though possibly varied, camerarigs are collected.  to infer a novel single view input silhouette'svirtual visual hull, we search for 3d shapes in the database which aremost consistent with the observed contour.  the input is matched tocomponent single views of the multi-view training examples.  a set ofviewpoint-aligned virtual views are generated from the visual hullscorresponding to these examples.  the 3d shape estimate for the inputis then found by interpolating between the contours of these alignedviews.  when the underlying shape is ambiguous given a single viewsilhouette, we produce multiple visual hull hypotheses; if a sequenceof input images is available, a dynamic programming approach isapplied to find the maximum likelihood path through the feasiblehypotheses over time.  we show results of our algorithm on real andsynthetic images of people.",language models
,"Shrobe, Howard; Laddaga, Robert",2005-12-22T01:19:31Z,2005-12-22T01:19:31Z,2004-02-09,http://hdl.handle.net/1721.1/30447,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New Architectural Models for Visibly Controllable Computing: The Relevance of Dynamic Object Oriented Architecturesand Plan Based Computing Models,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or perhaps on how to ease the system's continued evolution.  But if we look at life cycle costs, then we must conclude that the important question is how to make a system easy to operate.  To do this we need to make it easy for the operators to see what's going on and to then manipulate the system so that it does what it is supposed to.  This is a radically different criterion for success.What makes a computer system visible and controllable?  This is a difficult question, but it's clear that today's modern operating systems with nearly 50 million source lines of code are neither.  Strikingly, the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's mainstream sytsems, but with only 1 Million lines of code.  This paper is a retrospective examination of the features of the Lisp Machine hardware and software system.  Our key claim is that by building the Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained.It is our hope that this is a lesson that can impact tomorrow's designs.  We also speculate on how the spirit of the Lisp Machine could be extended to include a comprehensive access control model and how new layers of abstraction could further enrich this model.",MIT-CSAIL-TR-2004-006; AIM-2004-005,52 p.; 54496871 bytes; 1580494 bytes,application/postscript; application/pdf,en_US,AI; Software Environments; Computer Archicture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,299,"new architectural models for visibly controllable computing: the relevance of dynamic object oriented architecturesand plan based computing models traditionally, we've focussed on the question of how to make a system easy to code the first time, or perhaps on how to ease the system's continued evolution.  but if we look at life cycle costs, then we must conclude that the important question is how to make a system easy to operate.  to do this we need to make it easy for the operators to see what's going on and to then manipulate the system so that it does what it is supposed to.  this is a radically different criterion for success.what makes a computer system visible and controllable?  this is a difficult question, but it's clear that today's modern operating systems with nearly 50 million source lines of code are neither.  strikingly, the mit lisp machine and its commercial successors provided almost the same functionality as today's mainstream sytsems, but with only 1 million lines of code.  this paper is a retrospective examination of the features of the lisp machine hardware and software system.  our key claim is that by building the object abstraction into the lowest tiers of the system, great synergy and clarity were obtained.it is our hope that this is a lesson that can impact tomorrow's designs.  we also speculate on how the spirit of the lisp machine could be extended to include a comprehensive access control model and how new layers of abstraction could further enrich this model.",language models
,"Shrobe, Howard; Laddaga, Robert",2004-10-22T20:14:43Z,2004-10-22T20:14:43Z,2004-02-09,http://hdl.handle.net/1721.1/7286,AIM-2004-005,New Architectural Models for Visibly Controllable Computing: The Relevance of Dynamic Object Oriented Architectures and Plan Based Computing Models,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or  perhaps on how to ease the system's continued evolution. But if we look at life cycle costs, then we  must conclude that the important question is how to make a system easy to operate. To do this we  need to make it easy for the operators to see what's going on and to then manipulate the system so  that it does what it is supposed to. This is a radically different criterion for success.  What makes a computer system visible and controllable? This is a difficult question, but it's clear that  today's modern operating systems with nearly 50 million source lines of code are neither. Strikingly,  the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's  mainstream sytsems, but with only 1 Million lines of code. This paper is a retrospective examination of  the features of the Lisp Machine hardware and software system. Our key claim is that by building the  Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained. It is our hope that this is a lesson that can impact tomorrow's designs. We also speculate on how the  spirit of the Lisp Machine could be extended to include a comprehensive access control model and how  new layers of abstraction could further enrich this model.",AIM-2004-005,52 p.; 2594625 bytes; 829436 bytes,application/postscript; application/pdf,en_US,AI; Software Environments; Computer Archicture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,300,"new architectural models for visibly controllable computing: the relevance of dynamic object oriented architectures and plan based computing models traditionally, we've focussed on the question of how to make a system easy to code the first time, or  perhaps on how to ease the system's continued evolution. but if we look at life cycle costs, then we  must conclude that the important question is how to make a system easy to operate. to do this we  need to make it easy for the operators to see what's going on and to then manipulate the system so  that it does what it is supposed to. this is a radically different criterion for success.  what makes a computer system visible and controllable? this is a difficult question, but it's clear that  today's modern operating systems with nearly 50 million source lines of code are neither. strikingly,  the mit lisp machine and its commercial successors provided almost the same functionality as today's  mainstream sytsems, but with only 1 million lines of code. this paper is a retrospective examination of  the features of the lisp machine hardware and software system. our key claim is that by building the  object abstraction into the lowest tiers of the system, great synergy and clarity were obtained. it is our hope that this is a lesson that can impact tomorrow's designs. we also speculate on how the  spirit of the lisp machine could be extended to include a comprehensive access control model and how  new layers of abstraction could further enrich this model.",language models
,Riesenhuber; Jarudi; Gilad; Sinha,2005-12-22T01:20:02Z,2005-12-22T01:20:02Z,2004-03-05,http://hdl.handle.net/1721.1/30451,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Face processing in humans is compatible with a simple shape-based model of vision,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ featurally are much easier to distinguish when inverted than those that differ configurally (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002)  a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects expectations, there is no difference between featurally and configurally transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",MIT-CSAIL-TR-2004-010; AIM-2004-006; CBCL-236,12 p.; 14255528 bytes; 840975 bytes,application/postscript; application/pdf,en_US,AI; object recognition; faces; psychophysics; inversion effect; neuroscience; comput,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,304,"face processing in humans is compatible with a simple shape-based model of vision understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. inspired by a large body of physiological evidence (felleman and van essen, 1991; hubel and wiesel, 1962; livingstone and hubel, 1988; tso et al., 2001; zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (hummel and biederman, 1992; riesenhuber and poggio, 1999; selfridge, 1959). however, these models appear to be incompatible with some well-known psychophysical results. prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. it has been reported that faces that differ featurally are much easier to distinguish when inverted than those that differ configurally (freire et al., 2000; le grand et al., 2001; mondloch et al., 2002)  a finding that is difficult to reconcile with the aforementioned models. here we show that after controlling for subjects expectations, there is no difference between featurally and configurally transformed faces in terms of inversion effect. this result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",object recognition/detection
,Riesenhuber; Jarudi; Gilad; Sinha,2004-10-20T21:05:22Z,2004-10-20T21:05:22Z,2004-03-05,http://hdl.handle.net/1721.1/7283,AIM-2004-006; CBCL-236,Face processing in humans is compatible with a simple shape-based model of vision,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",AIM-2004-006; CBCL-236,12 p.; 1595221 bytes; 690861 bytes,application/postscript; application/pdf,en_US,AI; object recognition; faces; psychophysics; inversion effect; neuroscience; comput,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,305,"face processing in humans is compatible with a simple shape-based model of vision understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. inspired by a large body of physiological evidence (felleman and van essen, 1991; hubel and wiesel, 1962; livingstone and hubel, 1988; tso et al., 2001; zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (hummel and biederman, 1992; riesenhuber and poggio, 1999; selfridge, 1959). however, these models appear to be incompatible with some well-known psychophysical results. prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. it has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (freire et al., 2000; le grand et al., 2001; mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. this result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",face detection
,"Yokono, Jerry Jun; Poggio, Tomaso",2005-12-22T01:25:52Z,2005-12-22T01:25:52Z,2004-03-24,http://hdl.handle.net/1721.1/30454,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Evaluation of sets of oriented and non-oriented receptive fields as local descriptors,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",MIT-CSAIL-TR-2004-013; AIM-2004-007; CBCL-237,20 p.; 86081886 bytes; 22425671 bytes,application/postscript; application/pdf,en_US,"AI; local descriptor; steerable filter; Gaussian derivatives; selectivity,invariance",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,308,"evaluation of sets of oriented and non-oriented receptive fields as local descriptors local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. we propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. in this paper, we evaluate several local descriptors with respect to selectivity and invariance. the descriptors that we evaluated are gaussian derivatives up to the third order, gray image patches, and laplacian-based descriptors with either three scales or one scale filters. we compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. comparisons have been made keeping the dimensionality of the descriptors roughly constant. the overall results indicate a good performance by the descriptor based on a set of oriented gaussian filters. it is interesting that oriented receptive fields similar to the gaussian derivatives as well as receptive fields similar to the laplacian are found in primate visual cortex.",object recognition/detection
,"Yokono, Jerry Jun; Poggio, Tomaso",2004-10-20T21:05:24Z,2004-10-20T21:05:24Z,2004-03-24,http://hdl.handle.net/1721.1/7284,AIM-2004-007; CBCL-237,Evaluation of sets of oriented and non-oriented receptive fields as local descriptors,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",AIM-2004-007; CBCL-237,20 p.; 3426196 bytes; 1925439 bytes,application/postscript; application/pdf,en_US,AI; local descriptor; steerable filter; Gaussian derivatives; selectivity; invariance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,309,"evaluation of sets of oriented and non-oriented receptive fields as local descriptors local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. we propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. in this paper, we evaluate several local descriptors with respect to selectivity and invariance. the descriptors that we evaluated are gaussian derivatives up to the third order, gray image patches, and laplacian-based descriptors with either three scales or one scale filters. we compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. comparisons have been made keeping the dimensionality of the descriptors roughly constant. the overall results indicate a good performance by the descriptor based on a set of oriented gaussian filters. it is interesting that oriented receptive fields similar to the gaussian derivatives as well as receptive fields similar to the laplacian are found in primate visual cortex.",object recognition/detection
,"Torralba, Antonio",2004-10-08T20:43:12Z,2004-10-08T20:43:12Z,2004-04-14,http://hdl.handle.net/1721.1/6737,AIM-2004-009,Contextual Influences on Saliency,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",AIM-2004-009,12 p.; 2980182 bytes; 1698158 bytes,application/postscript; application/pdf,en_US,AI; Attention; context; saliency; scene recognition; object detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,315,"contextual influences on saliency this article describes a model for including scene/context priors in attention guidance. in the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. the scene is represented by means of a low-dimensional global description obtained from low-level features. the global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",object recognition/detection
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2004-10-08T20:43:10Z,2004-10-08T20:43:10Z,2004-04-14,http://hdl.handle.net/1721.1/6736,AIM-2004-008,Sharing visual features for multiclass and multiview object detection,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",AIM-2004-008,17 p.; 4223512 bytes; 1537371 bytes,application/postscript; application/pdf,en_US,AI; Object detection; sharing features; feature selection; multiclass; Boosting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,317,"sharing visual features for multiclass and multiview object detection we consider the problem of detecting a large number of different classes of objects in cluttered scenes. traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. this can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. in particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. it seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  we present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). the detectors for each class are trained jointly, rather than independently. for a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. the features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",object recognition/detection
,"Torralba, Antonio",2005-12-22T01:26:54Z,2005-12-22T01:26:54Z,2004-04-14,http://hdl.handle.net/1721.1/30460,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Contextual Influences on Saliency,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",MIT-CSAIL-TR-2004-020; AIM-2004-009,12 p.; 18377938 bytes; 597436 bytes,application/postscript; application/pdf,en_US,AI; Attention; context; saliency; scene recognition; object detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,318,"contextual influences on saliency this article describes a model for including scene/context priors in attention guidance. in the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. the scene is represented by means of a low-dimensional global description obtained from low-level features. the global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",object recognition/detection
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2005-12-19T22:41:45Z,2005-12-19T22:41:45Z,2004-04-14,http://hdl.handle.net/1721.1/30399,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Sharing visual features for multiclass and multiview object detection,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",MIT-CSAIL-TR-2004-019; AIM-2004-008,17 p.; 24172096 bytes; 1434721 bytes,application/postscript; application/pdf,en_US,AI; Object detection; sharing features; feature selection; multiclass; Boosting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,320,"sharing visual features for multiclass and multiview object detection we consider the problem of detecting a large number of different classes of objects in cluttered scenes. traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. this can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. in particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. it seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.we present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). the detectors for each class are trained jointly, rather than independently. for a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. the features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",object recognition/detection
,"Perez-Breva, Luis",2005-12-22T01:27:18Z,2005-12-22T01:27:18Z,2004-04-21,http://hdl.handle.net/1721.1/30463,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Cascading Regularized Classifiers,"Among the various methods to combine classifiers, Boosting was originally thought as an stratagem to cascade pairs of classifiers through their disagreement. I recover the same idea from the work of Niyogi et al. to show how to loosen the requirement of weak learnability, central to Boosting, and introduce a new cascading stratagem. The paper concludes with an empirical study of an implementation of the cascade that, under assumptions that mirror the conditions imposed by Viola and Jones in [VJ01], has the property to preserve the generalization ability of boosting.",MIT-CSAIL-TR-2004-023; AIM-2004-028,8 p.; 8847621 bytes; 505102 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,321,"cascading regularized classifiers among the various methods to combine classifiers, boosting was originally thought as an stratagem to cascade pairs of classifiers through their disagreement. i recover the same idea from the work of niyogi et al. to show how to loosen the requirement of weak learnability, central to boosting, and introduce a new cascading stratagem. the paper concludes with an empirical study of an implementation of the cascade that, under assumptions that mirror the conditions imposed by viola and jones in [vj01], has the property to preserve the generalization ability of boosting.",language models
,"Yokono, Jerry Jun; Poggio, Tomaso",2004-10-20T21:05:26Z,2004-10-20T21:05:26Z,2004-04-27,http://hdl.handle.net/1721.1/7285,AIM-2004-010; CBCL-238,Rotation Invariant Object Recognition from One Training Example,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",AIM-2004-010; CBCL-238,15 p.; 5162833 bytes; 968095 bytes,application/postscript; application/pdf,en_US,AI; object recognition; local descriptor; rotation invariant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,323,"rotation invariant object recognition from one training example local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. such a descriptor--based on a set of oriented gaussian derivative filters-- is used in our recognition system. we report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. we also describe feature selection based on a single training image. virtual images are generated by rotating and rescaling the image and robust features are selected. the results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",object recognition/detection
,"Yokono, Jerry Jun; Poggio, Tomaso",2005-12-22T01:30:35Z,2005-12-22T01:30:35Z,2004-04-27,http://hdl.handle.net/1721.1/30465,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Rotation Invariant Object Recognition from One Training Example,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",MIT-CSAIL-TR-2004-025; AIM-2004-010; CBCL-238,15 p.; 38274547 bytes; 7811820 bytes,application/postscript; application/pdf,en_US,AI; object recognition; local descriptor; rotation invariant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,324,"rotation invariant object recognition from one training example local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. such a descriptor--based on a set of oriented gaussian derivative filters-- is used in our recognition system. we report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. we also describe feature selection based on a single training image. virtual images are generated by rotating and rescaling the image and robust features are selected. the results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",object recognition/detection
,"Zollei, Lilla; Fisher, John; Wells, William",2004-10-08T20:43:13Z,2004-10-08T20:43:13Z,2004-04-28,http://hdl.handle.net/1721.1/6738,AIM-2004-011,A Unified Statistical and Information Theoretic Framework for Multi-modal Image Registration,"We formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  A unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. Additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. We analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. Among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",AIM-2004-011,21 p.; 2760680 bytes; 531001 bytes,application/postscript; application/pdf,en_US,AI; registration; information theory; unified framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,325,"a unified statistical and information theoretic framework for multi-modal image registration we formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  a unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. we analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",language models
,"Zollei, Lilla; Fisher, John; Wells, William",2005-12-22T01:30:39Z,2005-12-22T01:30:39Z,2004-04-28,http://hdl.handle.net/1721.1/30466,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Unified Statistical and Information Theoretic Framework for Multi-modal Image Registration,"We formulate and interpret several multi-modal registration methods inthe context of a unified statistical and information theoretic framework. A unified interpretation clarifies the implicit assumptionsof each method yielding a better understanding of their relativestrengths and weaknesses. Additionally, we discuss a generativestatistical model from which we derive a novel analysis tool, the""auto-information function"", as a means of assessing and exploiting thecommon spatial dependencies inherent in multi-modal imagery. Weanalytically derive useful properties of the ""auto-information"" aswell as verify them empirically on multi-modal imagery. Among theuseful aspects of the ""auto-information function"" is that it canbe computed from imaging modalities independently and it allows one todecompose the search space of registration problems.",MIT-CSAIL-TR-2004-026; AIM-2004-011,21 p.; 17309765 bytes; 765629 bytes,application/postscript; application/pdf,en_US,AI; registration; information theory; unified framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,326,"a unified statistical and information theoretic framework for multi-modal image registration we formulate and interpret several multi-modal registration methods inthe context of a unified statistical and information theoretic framework. a unified interpretation clarifies the implicit assumptionsof each method yielding a better understanding of their relativestrengths and weaknesses. additionally, we discuss a generativestatistical model from which we derive a novel analysis tool, the""auto-information function"", as a means of assessing and exploiting thecommon spatial dependencies inherent in multi-modal imagery. weanalytically derive useful properties of the ""auto-information"" aswell as verify them empirically on multi-modal imagery. among theuseful aspects of the ""auto-information function"" is that it canbe computed from imaging modalities independently and it allows one todecompose the search space of registration problems.",language models
,"Kennell, Jonathan",2004-10-20T20:32:23Z,2004-10-20T20:32:23Z,2004-05-18,http://hdl.handle.net/1721.1/7113,AITR-2004-002,Generative Temporal Planning with Complex Processes,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems. This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander. To support such a system, this thesis presents the Spock generative planner. To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes. This is in contrast to traditional planners, whose operators represent simple atomic or durative actions. Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes. RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators. Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution. Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts. This thesis describes the Spock algorithm in detail, along with example problems and test results.",AITR-2004-002,90 p.; 15726143 bytes; 1269432 bytes,application/postscript; application/pdf,en_US,"AI; planning ""temporal planning""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,332,"generative temporal planning with complex processes autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems. this thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander. to support such a system, this thesis presents the spock generative planner. to generate plans, spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes. this is in contrast to traditional planners, whose operators represent simple atomic or durative actions. spock represents operators using the rmpl language, which describes behaviors using parallel and sequential compositions of state and activity episodes. rmpl is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators. spock also is significant in that it uniformly represents operators and plan-space processes in terms of temporal plan networks, which support temporal flexibility for robust plan execution. finally, spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts. this thesis describes the spock algorithm in detail, along with example problems and test results.",language models
,"Stamatoiu, Oana L.",2004-10-20T20:32:25Z,2004-10-20T20:32:25Z,2004-05-18,http://hdl.handle.net/1721.1/7114,AITR-2004-001,Learning Commonsense Categorical Knowledge in a Thread Memory System,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",AITR-2004-001,96 p.; 6550712 bytes; 1993377 bytes,application/postscript; application/pdf,en_US,AI; learning; context; categorization; similarity; Bridge; thread memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,333,"learning commonsense categorical knowledge in a thread memory system if we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. this endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. in this work, i contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. the architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. this thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``a blue bird flew to the tree'' and ``the small bird flew to the cage'' that birds can fly. one of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",language models
,"Kennell, Jonathan",2005-12-22T01:31:24Z,2005-12-22T01:31:24Z,2004-05-18,http://hdl.handle.net/1721.1/30472,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Generative Temporal Planning with Complex Processes,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems.  This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander.  To support such a system, this thesis presents the Spock generative planner.  To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes.  This is in contrast to traditional planners, whose operators represent simple atomic or durative actions.  Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes.  RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators.  Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution.  Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts.  This thesis describes the Spock algorithm in detail, along with example problems and test results.",MIT-CSAIL-TR-2004-032; AITR-2004-002,90 p.; 56421512 bytes; 2495752 bytes,application/postscript; application/pdf,en_US,"AI; planning ""temporal planning""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,334,"generative temporal planning with complex processes autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems.  this thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander.  to support such a system, this thesis presents the spock generative planner.  to generate plans, spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes.  this is in contrast to traditional planners, whose operators represent simple atomic or durative actions.  spock represents operators using the rmpl language, which describes behaviors using parallel and sequential compositions of state and activity episodes.  rmpl is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators.  spock also is significant in that it uniformly represents operators and plan-space processes in terms of temporal plan networks, which support temporal flexibility for robust plan execution.  finally, spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts.  this thesis describes the spock algorithm in detail, along with example problems and test results.",language models
,"Stamatoiu, Oana L.",2005-12-22T01:31:32Z,2005-12-22T01:31:32Z,2004-05-18,http://hdl.handle.net/1721.1/30473,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning Commonsense Categorical Knowledge in a Thread Memory System,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",MIT-CSAIL-TR-2004-033; AITR-2004-001,96 p.; 68735708 bytes; 2432875 bytes,application/postscript; application/pdf,en_US,AI; learning; context; categorization; similarity; Bridge; thread memory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,336,"learning commonsense categorical knowledge in a thread memory system if we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. this endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. in this work, i contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. this thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``ablue bird flew to the tree'' and ``the small bird flew to the cage''that birds can fly. one of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",language models
,"Goler, Jonathan A.",2004-10-20T20:32:27Z,2004-10-20T20:32:27Z,2004-05-28,http://hdl.handle.net/1721.1/7115,AITR-2004-003,BioJADE: A Design and Simulation Tool for Synthetic Biological Systems,"The next generations of both biological engineering and computer engineering demand that control be  exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems  may provide us with the ability to build cells that are capable of a plethora of activities, from  computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not  only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a  comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical  design tool built in Java, utilizing a database back end, and supports a range of simulations using an  XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can  compile designs into actual DNA, and then generate synthesis instructions to build the physical parts.  The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for  synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks  repository, which enables the sharing of BioBrick components between researchers, and vastly reduces  the barriers to entry for aspiring Synthetic Biologists.",AITR-2004-003,54 p.; 5905018 bytes; 1459615 bytes,application/postscript; application/pdf,en_US,AI; BioJADE; Synthetic Biology; DNA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,338,"biojade: a design and simulation tool for synthetic biological systems the next generations of both biological engineering and computer engineering demand that control be  exerted at the molecular level. creating, characterizing and controlling synthetic biological systems  may provide us with the ability to build cells that are capable of a plethora of activities, from  computation to synthesizing nanostructures. to develop these systems, we must have a set of tools not  only for synthesizing systems, but also designing and simulating them. the biojade project provides a  comprehensive, extensible design and simulation platform for synthetic biology. biojade is a graphical  design tool built in java, utilizing a database back end, and supports a range of simulations using an  xml communication protocol. biojade currently supports a library of over 100 parts with which it can  compile designs into actual dna, and then generate synthesis instructions to build the physical parts.  the biojade project contributes several tools to synthetic biology. biojade in itself is a powerful tool for  synthetic biology designers. additionally, we developed and now make use of a centralized biobricks  repository, which enables the sharing of biobrick components between researchers, and vastly reduces  the barriers to entry for aspiring synthetic biologists.",image classification
,"Goler, Jonathan A.",2005-12-22T01:31:51Z,2005-12-22T01:31:51Z,2004-05-28,http://hdl.handle.net/1721.1/30475,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,BioJADE: A Design and Simulation Tool for Synthetic Biological Systems,"The next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical design tool built in Java, utilizing a database back end, and supports a range of simulations using an XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can compile designs into actual DNA, and then generate synthesis instructions to build the physical parts. The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks repository, which enables the sharing of BioBrick components between researchers, and vastly reduces the barriers to entry for aspiring Synthetic Biologists.",MIT-CSAIL-TR-2004-036; AITR-2004-003,54 p.; 53265402 bytes; 4755231 bytes,application/postscript; application/pdf,en_US,AI; BioJADE; Synthetic Biology; DNA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,339,"biojade: a design and simulation tool for synthetic biological systems the next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. to develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. the biojade project provides a comprehensive, extensible design and simulation platform for synthetic biology. biojade is a graphical design tool built in java, utilizing a database back end, and supports a range of simulations using an xml communication protocol. biojade currently supports a library of over 100 parts with which it can compile designs into actual dna, and then generate synthesis instructions to build the physical parts. the biojade project contributes several tools to synthetic biology. biojade in itself is a powerful tool for synthetic biology designers. additionally, we developed and now make use of a centralized biobricks repository, which enables the sharing of biobrick components between researchers, and vastly reduces the barriers to entry for aspiring synthetic biologists.",image classification
,"Hearn, Robert A.",2005-12-22T01:34:55Z,2005-12-22T01:34:55Z,2004-06-16,http://hdl.handle.net/1721.1/30479,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Building Grounded Abstractions for Artificial Intelligence Programming,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",MIT-CSAIL-TR-2004-040; AITR-2004-004,58 p.; 45433655 bytes; 1795607 bytes,application/postscript; application/pdf,en_US,AI; Artificial Intelligence; Society of Mind; Multi-Agent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,343,"building grounded abstractions for artificial intelligence programming most artificial intelligence (ai) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). each approach suffers from particular drawbacks. high-level ai uses abstractions that often have no relation to the way real, biological brains work. low-level ai, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. i have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. at the ``ground level'', i define a primitive, perceptron-like computational unit. i then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. the new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. as a demonstration of these abstractions, i have implemented a simulator for ``creatures'' controlled by a network of abstract units. the creatures exist in a simple 2d world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. this program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",privacy/ethics
,"Hearn, Robert A.",2004-10-20T20:32:29Z,2004-10-20T20:32:29Z,2004-06-16,http://hdl.handle.net/1721.1/7116,AITR-2004-004,Building Grounded Abstractions for Artificial Intelligence Programming,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from  particular drawbacks. High-level AI uses abstractions that often have no relation to the way real,  biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. I have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit.  I then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. The new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. As a  demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a  network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior.",AITR-2004-004,58 p.; 330188 bytes; 26969 bytes,application/postscript; application/pdf,en_US,AI; Artificial Intelligence; Society of Mind; Multi-Agent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,344,"building grounded abstractions for artificial intelligence programming most artificial intelligence (ai) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). each approach suffers from  particular drawbacks. high-level ai uses abstractions that often have no relation to the way real,  biological brains work. low-level ai, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. i have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. at the ``ground level'', i define a primitive, perceptron-like computational unit.  i then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. the new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. as a  demonstration of these abstractions, i have implemented a simulator for ``creatures'' controlled by a  network of abstract units. the creatures exist in a simple 2d world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. this program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior.",privacy/ethics
,"Teevan, Jaime",2004-10-08T20:43:15Z,2004-10-08T20:43:15Z,2004-06-18,http://hdl.handle.net/1721.1/6739,AIM-2004-012,How People Re-find Information When the Web Changes,"This paper investigates how people return to information in a dynamic information environment. For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed. Changes can benefit users by providing new information, but they hinder returning to previously viewed information. The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment. A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not. While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution. The implications of these observations for systems that support re-finding in dynamic environments are discussed.",AIM-2004-012,9 p.; 1451699 bytes; 688288 bytes,application/postscript; application/pdf,en_US,AI; re-finding; information management; dynamic information,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,345,"how people re-find information when the web changes this paper investigates how people return to information in a dynamic information environment. for example, a person might want to return to web content via a link encountered earlier on a web page, only to learn that the link has since been removed. changes can benefit users by providing new information, but they hinder returning to previously viewed information. the observational study presented here analyzed instances, collected via a web search, where people expressed difficulty re-finding information because of changes to the information or its environment. a number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not. while people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution. the implications of these observations for systems that support re-finding in dynamic environments are discussed.",language models
,"Teevan, Jaime",2005-12-22T01:35:01Z,2005-12-22T01:35:01Z,2004-06-18,http://hdl.handle.net/1721.1/30480,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,How People Re-find Information When the Web Changes,"This paper investigates how people return to information in a dynamic information environment.  For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed.  Changes can benefit users by providing new information, but they hinder returning to previously viewed information.  The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  The implications of these observations for systems that support re-finding in dynamic environments are discussed.",MIT-CSAIL-TR-2004-041; AIM-2004-012,9 p.; 16783533 bytes; 654765 bytes,application/postscript; application/pdf,en_US,AI; re-finding; information management; dynamic information,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,346,"how people re-find information when the web changes this paper investigates how people return to information in a dynamic information environment.  for example, a person might want to return to web content via a link encountered earlier on a web page, only to learn that the link has since been removed.  changes can benefit users by providing new information, but they hinder returning to previously viewed information.  the observational study presented here analyzed instances, collected via a web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  a number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  while people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  the implications of these observations for systems that support re-finding in dynamic environments are discussed.",language models
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2005-12-22T01:35:14Z,2005-12-22T01:35:14Z,2004-06-25,http://hdl.handle.net/1721.1/30482,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Contextual models for object detection using boosted random fields,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",MIT-CSAIL-TR-2004-043; AIM-2004-013,10 p.; 11085755 bytes; 604755 bytes,application/postscript; application/pdf,en_US,AI; Object detection; context; boosting; BP; random fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,348,"contextual models for object detection using boosted random fields we seek to both detect and segment objects in images.  to exploit both local image data as well as contextual information, we introduce boosted random fields (brfs), which uses boosting to learn the graph structure and local evidence of a conditional random field (crf). the graph structure is learned by assembling graph fragments in an additive model. the connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. we show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. we apply our system to detect stuff and things in office and street scenes.",object recognition/detection
,"Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2004-10-08T20:43:16Z,2004-10-08T20:43:16Z,2004-06-25,http://hdl.handle.net/1721.1/6740,AIM-2004-013,Contextual models for object detection using boosted random fields,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",AIM-2004-013,10 p.; 2184856 bytes; 906515 bytes,application/postscript; application/pdf,en_US,AI; Object detection; context; boosting; BP; random fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,349,"contextual models for object detection using boosted random fields we seek to both detect and segment objects in images. to exploit both local image data as well as contextual information, we introduce boosted random fields (brfs), which uses boosting to learn the graph structure and local evidence of a conditional random field (crf). the graph structure is learned by assembling graph fragments in an additive model. the connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. we show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. we apply our system to detect stuff and things in office and street scenes.",object recognition/detection
,"Indyk, Piotr; Woodruff, David",2004-10-08T20:43:17Z,2004-10-08T20:43:17Z,2004-07-02,http://hdl.handle.net/1721.1/6741,AIM-2004-014,Optimal Approximations of the Frequency Moments,"We give a one-pass, O~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. Together with known lower bounds, this resolves the main problem left open by Alon, Matias, Szegedy, STOC'96. Our algorithm enables deletions as well as insertions of stream elements.",AIM-2004-014,18 p.; 3705201 bytes; 761567 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,351,"optimal approximations of the frequency moments we give a one-pass, o~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. together with known lower bounds, this resolves the main problem left open by alon, matias, szegedy, stoc'96. our algorithm enables deletions as well as insertions of stream elements.",high performance computing
,"Indyk, Piotr; Woodruff, David",2005-12-22T01:35:22Z,2005-12-22T01:35:22Z,2004-07-02,http://hdl.handle.net/1721.1/30483,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Optimal Approximations of the Frequency Moments,"We give a one-pass, O~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. Together with known lower bounds, this resolves the main problem left open by Alon, Matias, Szegedy, STOC'96. Our algorithm enables deletions as well as insertions of stream elements.",MIT-CSAIL-TR-2004-044; AIM-2004-014,18 p.; 18493060 bytes; 833343 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,352,"optimal approximations of the frequency moments we give a one-pass, o~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. together with known lower bounds, this resolves the main problem left open by alon, matias, szegedy, stoc'96. our algorithm enables deletions as well as insertions of stream elements.",high performance computing
,"Badoiu, Mihai; Indyk, Piotr; Sidiropoulos, Anastasios",2005-12-22T01:35:27Z,2005-12-22T01:35:27Z,2004-07-05,http://hdl.handle.net/1721.1/30484,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Constant-Factor Approximation Algorithm for Embedding Unweighted Graphs into Trees,"We present a constant-factor approximation algorithm for computing anembedding of the shortest path metric of an unweighted graph into atree, that minimizes the multiplicative distortion.",MIT-CSAIL-TR-2004-045; AIM-2004-015,8 p.; 6685848 bytes; 331083 bytes,application/postscript; application/pdf,en_US,AI; embeddings; approximation algorithms; trees,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,353,"a constant-factor approximation algorithm for embedding unweighted graphs into trees we present a constant-factor approximation algorithm for computing anembedding of the shortest path metric of an unweighted graph into atree, that minimizes the multiplicative distortion.",high performance computing
,"Badoiu, Mihai; Indyk, Piotr; Sidiropoulos, Anastasios",2004-10-08T20:43:19Z,2004-10-08T20:43:19Z,2004-07-05,http://hdl.handle.net/1721.1/6742,AIM-2004-015,A Constant-Factor Approximation Algorithm for Embedding Unweighted Graphs into Trees,"We present a constant-factor approximation algorithm for computing an embedding of the shortest path metric of an unweighted graph into a tree, that minimizes the multiplicative distortion.",AIM-2004-015,8 p.; 981451 bytes; 626039 bytes,application/postscript; application/pdf,en_US,AI; embeddings; approximation algorithms; trees,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,354,"a constant-factor approximation algorithm for embedding unweighted graphs into trees we present a constant-factor approximation algorithm for computing an embedding of the shortest path metric of an unweighted graph into a tree, that minimizes the multiplicative distortion.",high performance computing
,"Kemp, Charles; Griffiths, Thomas L.; Tenenbaum, Joshua B.",2005-12-22T01:36:09Z,2005-12-22T01:36:09Z,2004-07-22,http://hdl.handle.net/1721.1/30489,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Discovering Latent Classes in Relational Data,"We present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  Our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  This model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.We apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",MIT-CSAIL-TR-2004-050; AIM-2004-019,12 p.; 13382538 bytes; 572002 bytes,application/postscript; application/pdf,en_US,AI; learning; categorization; relations; kinship,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,359,"discovering latent classes in relational data we present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  this model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.we apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",high performance computing
,"Uzuner, Ozlem",2005-12-22T01:36:15Z,2005-12-22T01:36:15Z,2004-07-25,http://hdl.handle.net/1721.1/30490,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Distribution Volume Tracking on Privacy-Enhanced Wireless Grid,"In this paper, we discuss a wireless grid in which users are highly mobile, and form ad-hoc and sometimes short-lived connections with other devices.  As they roam through networks, the users may choose to employ privacy-enhancing technologies to address their privacy needs and benefit from the computational power of the grid for a variety of tasks, including sharing content.  The high rate of mobility of the users on the wireless grid, when combined with privacy enhancing mechanisms and ad-hoc connections, makes it difficult to conclusively link devices and/or individuals with network activities and to hold them liable for particular downloads.  Protecting intellectual property in this scenario requires a solution that can work in absence of knowledge about behavior of particular individuals.  Building on previous work, we argue for a solution that ensures proper compensation to content owners without inhibiting use and dissemination of works.  Our proposal is based on digital tracking for measuring distribution volume of content and compensation of authors based on this accounting information.  The emphasis is on obtaining good estimates of rate of popularity of works, without keeping track of activities of individuals or devices.  The contribution of this paper is a revenue protection mechanism, Distribution Volume Tracking, that does not invade the privacy of users in the wireless grid and works even in the presence of privacy-enhancing technologies they may employ.",MIT-CSAIL-TR-2004-051; AIM-2004-018,7 p.; 11942167 bytes; 409314 bytes,application/postscript; application/pdf,en_US,AI; Intellectual Property Protection; Privacy; Wireless Computational Grid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,360,"distribution volume tracking on privacy-enhanced wireless grid in this paper, we discuss a wireless grid in which users are highly mobile, and form ad-hoc and sometimes short-lived connections with other devices.  as they roam through networks, the users may choose to employ privacy-enhancing technologies to address their privacy needs and benefit from the computational power of the grid for a variety of tasks, including sharing content.  the high rate of mobility of the users on the wireless grid, when combined with privacy enhancing mechanisms and ad-hoc connections, makes it difficult to conclusively link devices and/or individuals with network activities and to hold them liable for particular downloads.  protecting intellectual property in this scenario requires a solution that can work in absence of knowledge about behavior of particular individuals.  building on previous work, we argue for a solution that ensures proper compensation to content owners without inhibiting use and dissemination of works.  our proposal is based on digital tracking for measuring distribution volume of content and compensation of authors based on this accounting information.  the emphasis is on obtaining good estimates of rate of popularity of works, without keeping track of activities of individuals or devices.  the contribution of this paper is a revenue protection mechanism, distribution volume tracking, that does not invade the privacy of users in the wireless grid and works even in the presence of privacy-enhancing technologies they may employ.",privacy/ethics
,"Serre, Thomas; Riesenhuber, Maximilian",2005-12-22T01:36:22Z,2005-12-22T01:36:22Z,2004-07-27,http://hdl.handle.net/1721.1/30491,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Realistic Modeling of Simple and Complex Cell Tuning in the HMAXModel, and Implications for Invariant Object Recognition in Cortex","Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",MIT-CSAIL-TR-2004-052; AIM-2004-017; CBCL-239,11 p.; 24089158 bytes; 2715073 bytes,application/postscript; application/pdf,en_US,AI; object recognition; simple cell; complex cell; hmax; V1; IT; view-tuned unit; in,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,361,"realistic modeling of simple and complex cell tuning in the hmaxmodel, and implications for invariant object recognition in cortex riesenhuber \& poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. in particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (vtus)found in inferotemporal cortex (it), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. there is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. however, in the original paper by riesenhuber \& poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, v1. we show thatunits in the early levels of hmax can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model vtus still hold in this morerealistic version of the model.",object recognition/detection
,"Sezgin, Tevfik Metin; Davis, Randall",2005-12-22T01:36:30Z,2005-12-22T01:36:30Z,2004-07-28,http://hdl.handle.net/1721.1/30492,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Early Sketch Processing with Application in HMM Based Sketch Recognition,"Freehand sketching is a natural and crucial part of everyday humaninteraction, yet is almost totally unsupported by current user interfaces. With the increasing availability of tablet notebooks and pen based PDAs, sketchbased interaction has gained attention as a natural interaction modality.We are working to combine the flexibility and ease of use of paper and pencilwith the processing power of a computer, to produce a user interface fordesign that feels as natural as paper, yet is considerably smarter. One of themost basic tasks in accomplishing this is converting the original digitizedpen strokes in a sketch into the intended geometric objects. In this paper wedescribe an implemented system that combines multiple sources of knowledge toprovide robust early processing for freehand sketching. We also show how thisearly processing system can be used as part of a fast sketch recognition system with polynomial time segmentation and recognition algorithms.",MIT-CSAIL-TR-2004-053; AIM-2004-016,16 p.; 29102772 bytes; 1401214 bytes,application/postscript; application/pdf,en_US,AI; Sketch Recognition; Early Sketch Processing; Shape Approximation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,362,"early sketch processing with application in hmm based sketch recognition freehand sketching is a natural and crucial part of everyday humaninteraction, yet is almost totally unsupported by current user interfaces. with the increasing availability of tablet notebooks and pen based pdas, sketchbased interaction has gained attention as a natural interaction modality.we are working to combine the flexibility and ease of use of paper and pencilwith the processing power of a computer, to produce a user interface fordesign that feels as natural as paper, yet is considerably smarter. one of themost basic tasks in accomplishing this is converting the original digitizedpen strokes in a sketch into the intended geometric objects. in this paper wedescribe an implemented system that combines multiple sources of knowledge toprovide robust early processing for freehand sketching. we also show how thisearly processing system can be used as part of a fast sketch recognition system with polynomial time segmentation and recognition algorithms.",object recognition/detection
,"Kreiman, Gabriel; Hung, Chou; Poggio, Tomaso; DiCarlo, James",2005-12-19T23:29:04Z,2005-12-19T23:29:04Z,2004-09-21,http://hdl.handle.net/1721.1/30417,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Selectivity of Local Field Potentials in Macaque Inferior Temporal Cortex,"While single neurons in inferior temporal (IT) cortex show differential responses to distinct complex stimuli, little is known about the responses of populations of neurons in IT.  We recorded single electrode data, including multi-unit activity (MUA) and local field potentials (LFP), from 618 sites in the inferior temporal cortex of macaque monkeys while the animals passively viewed 78 different pictures of complex stimuli. The LFPs were obtained by low-pass filtering the extracellular electrophysiological signal with a corner frequency of 300 Hz. As reported previously, we observed that spike counts from MUA showed selectivity for some of the pictures.  Strikingly, the LFP data, which is thought to constitute an average over large numbers of neurons, also showed significantly selective responses.  The LFP responses were less selective than the MUA responses both in terms of the proportion of selective sites as well as in the selectivity of each site. We observed that there was only little overlap between the selectivity of MUA and LFP recordings from the same electrode.  To assess the spatial organization of selective responses, we compared the selectivity of nearby sites recorded along the same penetration and sites recorded from different penetrations.  We observed that MUA selectivity was correlated on spatial scales up to 800 &#61549;m while the LFP selectivity was correlated over a larger spatial extent, with significant correlations between sites separated by several mm.  Our data support the idea that there is some topographical arrangement to the organization of selectivity in inferior temporal cortex and that this organization may be relevant for the representation of object identity in IT.",MIT-CSAIL-TR-2004-056; AIM-2004-020; CBCL-240,1 p.; 154663209 bytes; 29794050 bytes,application/postscript; application/pdf,en_US,AI; object recognition; inferior temporal cortex; local field potentials,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,366,"selectivity of local field potentials in macaque inferior temporal cortex while single neurons in inferior temporal (it) cortex show differential responses to distinct complex stimuli, little is known about the responses of populations of neurons in it.  we recorded single electrode data, including multi-unit activity (mua) and local field potentials (lfp), from 618 sites in the inferior temporal cortex of macaque monkeys while the animals passively viewed 78 different pictures of complex stimuli. the lfps were obtained by low-pass filtering the extracellular electrophysiological signal with a corner frequency of 300 hz. as reported previously, we observed that spike counts from mua showed selectivity for some of the pictures.  strikingly, the lfp data, which is thought to constitute an average over large numbers of neurons, also showed significantly selective responses.  the lfp responses were less selective than the mua responses both in terms of the proportion of selective sites as well as in the selectivity of each site. we observed that there was only little overlap between the selectivity of mua and lfp recordings from the same electrode.  to assess the spatial organization of selective responses, we compared the selectivity of nearby sites recorded along the same penetration and sites recorded from different penetrations.  we observed that mua selectivity was correlated on spatial scales up to 800 &#61549;m while the lfp selectivity was correlated over a larger spatial extent, with significant correlations between sites separated by several mm.  our data support the idea that there is some topographical arrangement to the organization of selectivity in inferior temporal cortex and that this organization may be relevant for the representation of object identity in it.",high performance computing
,"Arsenio, Artur Miguel",2005-12-22T14:51:02Z,2005-12-22T14:51:02Z,2004-09-26,http://hdl.handle.net/1721.1/30591,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Cognitive-Developmental Learning for a Humanoid Robot: A Caregiver's Gift,"The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties.This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes.Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",MIT-CSAIL-TR-2004-057; AITR-2004-006,388 p.; 610948893 bytes; 19073417 bytes,application/postscript; application/pdf,en_US,AI; Humanoid Robots; Developmental Learning; Perception; Human-robot Interactions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,367,"cognitive-developmental learning for a humanoid robot: a caregiver's gift the goal of this work is to build a cognitive system for the humanoid robot, cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. this thesis addresses a broad spectrum of machine learning problems across several categorization levels. actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot cog. this framework is effectively applied to a collection of ai, computer vision, and signal processing problems. cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. human caregivers provide a helping hand for communicating such information to the robot. this is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. self-exploration of the world extends the robot's knowledge concerning object properties.this thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. a human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. the humanoid robot therefore sees the world through the caregiver's eyes.building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",robotics
,"Benjamin, Michael R.",2005-12-19T23:28:02Z,2005-12-19T23:28:02Z,2004-09-27,http://hdl.handle.net/1721.1/30416,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,The Interval Programming Model for Multi-objective Decision Making,"The interval programming model (IvP) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  The central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. The piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.In this paper the model and solution algorithms are described, and theapplicability of IvP to certain applications arediscussed.",MIT-CSAIL-TR-2004-058; AIM-2004-021,32 p.; 42228177 bytes; 2845444 bytes,application/postscript; application/pdf,en_US,AI; multi-objective decision making; behavior-based control; action selection; MCDM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,368,"the interval programming model for multi-objective decision making the interval programming model (ivp) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  the central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. the piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.in this paper the model and solution algorithms are described, and theapplicability of ivp to certain applications arediscussed.",language models
,"Yeo, Gene; Van Nostrand, Eric; Holste, Dirk; Poggio, Tomaso; Burge, Christopher",2005-12-19T23:22:45Z,2005-12-19T23:22:45Z,2004-09-30,http://hdl.handle.net/1721.1/30411,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Predictive identification of alternative events conserved in human and mouse,"Alternative pre-messenger RNA splicing affects a majority of human genes and plays important roles in development and disease.  Alternative splicing (AS) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  Here we describe sequence features that distinguish exons subject to evolutionarily conserved AS, which we call 'alternative-conserved exons' (ACEs) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, ACEScan.  Genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted ACEs.  Alternative splicing was verified in both human and mouse tissues using an RT-PCR-sequencing protocol for 21 of 30 (70%) predicted ACEs tested, supporting the validity of a majority of ACEScan predictions.  By contrast, AS was observed in mouse tissues for only 2 of 15 (13%) tested exons which had EST or cDNA evidence of AS in human but were not predicted ACEs, and was never observed for eleven negative control exons in human or mouse tissues.  Predicted ACEs were much more likely to preserve reading frame, and less likely to disrupt protein domains than other AS events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, RNA processing and development.  Our results also imply that the vast majority of AS events represented in the human EST databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",MIT-CSAIL-TR-2004-059; AIM-2004-022,56 p.; 49600059 bytes; 2638503 bytes,application/postscript; application/pdf,en_US,AI; alternative splicing; comparative genomics; classification; regularization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,369,"predictive identification of alternative events conserved in human and mouse alternative pre-messenger rna splicing affects a majority of human genes and plays important roles in development and disease.  alternative splicing (as) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  here we describe sequence features that distinguish exons subject to evolutionarily conserved as, which we call 'alternative-conserved exons' (aces) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, acescan.  genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted aces.  alternative splicing was verified in both human and mouse tissues using an rt-pcr-sequencing protocol for 21 of 30 (70%) predicted aces tested, supporting the validity of a majority of acescan predictions.  by contrast, as was observed in mouse tissues for only 2 of 15 (13%) tested exons which had est or cdna evidence of as in human but were not predicted aces, and was never observed for eleven negative control exons in human or mouse tissues.  predicted aces were much more likely to preserve reading frame, and less likely to disrupt protein domains than other as events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, rna processing and development.  our results also imply that the vast majority of as events represented in the human est databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",object recognition/detection
,"Tucker-Kellogg, Lisa",2005-12-19T23:32:53Z,2005-12-19T23:32:53Z,2004-10-01,http://hdl.handle.net/1721.1/30419,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Systematic Conformational Search with Constraint Satisfaction,"Throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  This thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state NMR spectroscopy.  A separate portion of the thesis isabout protein-DNA binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.The search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromNMR experiments.  Systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  Our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) OmniMerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  Although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  Propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of OmniMerge.(3) The search may be run in two stages, once at low-resolutionusing a side-effect of OmniMerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) An A* function prioritizes each subchain based onestimated future search costs.  Subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.A common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.In addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.These novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",MIT-CSAIL-TR-2004-060; AITR-2004-007,177 p.; 127791565 bytes; 5501537 bytes,application/postscript; application/pdf,en_US,AI; Distance Geometry; Nuclear Magnetic Resonance (NMR); Molecular Modeling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,370,"systematic conformational search with constraint satisfaction throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  this thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state nmr spectroscopy.  a separate portion of the thesis isabout protein-dna binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.the search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromnmr experiments.  systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) omnimerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of omnimerge.(3) the search may be run in two stages, once at low-resolutionusing a side-effect of omnimerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) an a* function prioritizes each subchain based onestimated future search costs.  subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.a common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.in addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.these novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",high performance computing
,"Steinkraus, Kurt; Kaelbling, Leslie Pack",2005-12-22T01:41:41Z,2005-12-22T01:41:41Z,2004-10-21,http://hdl.handle.net/1721.1/30496,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining dynamic abstractions in large MDPs,"One of the reasons that it is difficult to plan and act in real-worlddomains is that they are very large.  Existing research generallydeals with the large domain size using a static representation andexploiting a single type of domain structure.  In this paper, wecreate a framework that encapsulates existing and new abstraction andapproximation methods into modules, and combines arbitrary modulesinto a system that allows for dynamic representation changes.  We showthat the dynamic changes of representation allow our framework tosolve larger and more interesting domains than were previouslypossible, and while there are no optimality guarantees, suitablemodule choices gain tractability at little cost to optimality.",MIT-CSAIL-TR-2004-065; AIM-2004-023,12 p.; 9975204 bytes; 424481 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,375,"combining dynamic abstractions in large mdps one of the reasons that it is difficult to plan and act in real-worlddomains is that they are very large.  existing research generallydeals with the large domain size using a static representation andexploiting a single type of domain structure.  in this paper, wecreate a framework that encapsulates existing and new abstraction andapproximation methods into modules, and combines arbitrary modulesinto a system that allows for dynamic representation changes.  we showthat the dynamic changes of representation allow our framework tosolve larger and more interesting domains than were previouslypossible, and while there are no optimality guarantees, suitablemodule choices gain tractability at little cost to optimality.",high performance computing
,"Werfel, Justin",2005-12-22T02:15:12Z,2005-12-22T02:15:12Z,2004-11-09,http://hdl.handle.net/1721.1/30500,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Neural Network Models for Zebra Finch Song Production and Reinforcement Learning,"The zebra finch is a standard experimental system for studying learning and generation of temporally extended motor patterns.  The first part of this project concerned the evaluation of simple models for the operation and structure of the network in the motor nucleus RA.  A directed excitatory chain with a global inhibitory network, for which experimental evidence exists, was found to produce waves of activity similar to those observed in RA; this similarity included one particularly important feature of the measured activity, synchrony between the onset of bursting in one neuron and the offset of bursting in another.  Other models, which were simpler and more analytically tractable, were also able to exhibit this feature, but not for parameter values quantitatively close to those observed.Another issue of interest concerns how these networks are initially learned by the bird during song acquisition.  The second part of the project concerned the analysis of exemplars of REINFORCE algorithms, a general class of algorithms for reinforcement learning in neural networks, which are on several counts more biologically plausible than standard prescriptions such as backpropagation.  The former compared favorably with backpropagation on tasks involving single input-output pairs, though a noise analysis suggested it should not perform so well.  On tasks involving trajectory learning, REINFORCE algorithms meet with some success, though the analysis that predicts their success on input-output-pair tasks fails to explain it for trajectories.",MIT-CSAIL-TR-2004-070; AITR-2004-008,61 p.; 385088 bytes; 121048 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,380,"neural network models for zebra finch song production and reinforcement learning the zebra finch is a standard experimental system for studying learning and generation of temporally extended motor patterns.  the first part of this project concerned the evaluation of simple models for the operation and structure of the network in the motor nucleus ra.  a directed excitatory chain with a global inhibitory network, for which experimental evidence exists, was found to produce waves of activity similar to those observed in ra; this similarity included one particularly important feature of the measured activity, synchrony between the onset of bursting in one neuron and the offset of bursting in another.  other models, which were simpler and more analytically tractable, were also able to exhibit this feature, but not for parameter values quantitatively close to those observed.another issue of interest concerns how these networks are initially learned by the bird during song acquisition.  the second part of the project concerned the analysis of exemplars of reinforce algorithms, a general class of algorithms for reinforcement learning in neural networks, which are on several counts more biologically plausible than standard prescriptions such as backpropagation.  the former compared favorably with backpropagation on tasks involving single input-output pairs, though a noise analysis suggested it should not perform so well.  on tasks involving trajectory learning, reinforce algorithms meet with some success, though the analysis that predicts their success on input-output-pair tasks fails to explain it for trajectories.",privacy/ethics
,"Cadieu, Charles; Kouh, Minjoon; Riesenhuber, Maximilian; Poggio, Tomaso",2005-12-22T02:15:22Z,2005-12-22T02:15:22Z,2004-11-12,http://hdl.handle.net/1721.1/30501,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Shape Representation in V4: Investigating Position-Specific Tuning for Boundary Conformation with the Standard Model of Object Recognition,"The computational processes in the intermediate stages of the ventral pathway responsible for visual object recognition are not well understood. A recent physiological study by A. Pasupathy and C. Connor in intermediate area V4 using contour stimuli, proposes that a population of V4 neurons display bjectcentered,position-specific curvature tuning [18]. The standard model of object recognition, a recently developed model [23] to account for recognition properties of IT cells (extending classical suggestions by Hubel, Wiesel and others [9, 10, 19]), is used here to model the response of the V4 cells described in [18]. Our results show that a feedforward, network level mechanism can exhibit selectivity and invariance properties that correspond to the responses of the V4 cells described in [18]. These results suggest howobject-centered, position-specific curvature tuning of V4 cells may arise from combinations of complex V1 cell responses. Furthermore, the model makes predictions about the responses of the same V4 cells studied by Pasupathy and Connor to novel gray level patterns, such as gratings and natural images. Thesepredictions suggest specific experiments to further explore shape representation in V4.",MIT-CSAIL-TR-2004-071; AIM-2004-024; CBCL-241,12 p.; 38801213 bytes; 5292186 bytes,application/postscript; application/pdf,en_US,AI; V4 Visual Cortex Object Recognition Standard Model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,382,"shape representation in v4: investigating position-specific tuning for boundary conformation with the standard model of object recognition the computational processes in the intermediate stages of the ventral pathway responsible for visual object recognition are not well understood. a recent physiological study by a. pasupathy and c. connor in intermediate area v4 using contour stimuli, proposes that a population of v4 neurons display bjectcentered,position-specific curvature tuning [18]. the standard model of object recognition, a recently developed model [23] to account for recognition properties of it cells (extending classical suggestions by hubel, wiesel and others [9, 10, 19]), is used here to model the response of the v4 cells described in [18]. our results show that a feedforward, network level mechanism can exhibit selectivity and invariance properties that correspond to the responses of the v4 cells described in [18]. these results suggest howobject-centered, position-specific curvature tuning of v4 cells may arise from combinations of complex v1 cell responses. furthermore, the model makes predictions about the responses of the same v4 cells studied by pasupathy and connor to novel gray level patterns, such as gratings and natural images. thesepredictions suggest specific experiments to further explore shape representation in v4.",object recognition/detection
,"Wolf, Lior; Martin, Ian",2005-12-22T02:15:29Z,2005-12-22T02:15:29Z,2004-11-12,http://hdl.handle.net/1721.1/30502,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Regularization Through Feature Knock Out,"In this paper, we present and analyze a novel regularization technique based on enhancing our dataset with corrupted copies of the original data. The motivation is that since the learning algorithm lacks information about which parts of thedata are reliable, it has to produce more robust classification functions. We then demonstrate how this regularization leads to redundancy in the resulting  classifiers, which is somewhat in contrast to the common interpretations of the Occams razor principle. Using this framework, we propose a simple addition to the gentle boosting algorithm which enables it to work with only a few examples. We test this new algorithm on a variety of datasets and show convincing results.",MIT-CSAIL-TR-2004-072; AIM-2004-025; CBCL-242,0 p.; 16224097 bytes; 656543 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,383,"regularization through feature knock out in this paper, we present and analyze a novel regularization technique based on enhancing our dataset with corrupted copies of the original data. the motivation is that since the learning algorithm lacks information about which parts of thedata are reliable, it has to produce more robust classification functions. we then demonstrate how this regularization leads to redundancy in the resulting  classifiers, which is somewhat in contrast to the common interpretations of the occams razor principle. using this framework, we propose a simple addition to the gentle boosting algorithm which enables it to work with only a few examples. we test this new algorithm on a variety of datasets and show convincing results.",high performance computing
,"Serre, Thomas; Wolf, Lior; Poggio, Tomaso",2005-12-22T02:15:45Z,2005-12-22T02:15:45Z,2004-11-14,http://hdl.handle.net/1721.1/30504,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A new biologically motivated framework for robust object recognition,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",MIT-CSAIL-TR-2004-074; AIM-2004-026; CBCL-243,10 p.; 17638397 bytes; 793841 bytes,application/postscript; application/pdf,en_US,AI; visual cortex; object recognition; face detection; hierarchy; feature learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,384,"a new biologically motivated framework for robust object recognition in this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. we also show that our system is ableto learn from very few examples with no prior category knowledge.  thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",object recognition/detection
,"Srebro, Nathan",2005-12-22T02:16:24Z,2005-12-22T02:16:24Z,2004-11-22,http://hdl.handle.net/1721.1/30507,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning with Matrix Factorizations,"Matrices that can be factored into a product of two simpler matricescan serve as a useful and often natural model in the analysis oftabulated or high-dimensional data.  Models based on matrixfactorization (Factor Analysis, PCA) have been extensively used instatistical analysis and machine learning for over a century, withmany new formulations and models suggested in recent years (LatentSemantic Indexing, Aspect Models, Probabilistic PCA, Exponential PCA,Non-Negative Matrix Factorization and others).  In this thesis weaddress several issues related to learning with matrix factorizations:we study the asymptotic behavior and generalization ability ofexisting methods, suggest new optimization methods, and present anovel maximum-margin high-dimensional matrix factorizationformulation.",MIT-CSAIL-TR-2004-076; AITR-2004-009,132 p.; 96239481 bytes; 5561927 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,386,"learning with matrix factorizations matrices that can be factored into a product of two simpler matricescan serve as a useful and often natural model in the analysis oftabulated or high-dimensional data.  models based on matrixfactorization (factor analysis, pca) have been extensively used instatistical analysis and machine learning for over a century, withmany new formulations and models suggested in recent years (latentsemantic indexing, aspect models, probabilistic pca, exponential pca,non-negative matrix factorization and others).  in this thesis weaddress several issues related to learning with matrix factorizations:we study the asymptotic behavior and generalization ability ofexisting methods, suggest new optimization methods, and present anovel maximum-margin high-dimensional matrix factorizationformulation.",language models
,"Grauman, Kristen; Darrell, Trevor",2005-12-22T02:16:07Z,2005-12-22T02:16:07Z,2004-11-22,http://hdl.handle.net/1721.1/30505,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Efficient Image Matching with Distributions of Local Invariant Features,"Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes the minimal-cost correspondence between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.We evaluate our technique with scene recognition and texture classification tasks.",MIT-CSAIL-TR-2004-075; AIM-2004-027,18 p.; 78764149 bytes; 16484168 bytes,application/postscript; application/pdf,en_US,AI; image matching; object recognition; content-based image retrieval; texture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,387,"efficient image matching with distributions of local invariant features sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  we present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  similarity between images is measured with an approximation of the earth mover's distance (emd), which quickly computes the minimal-cost correspondence between two bags of features.  each image's feature distribution is mapped into a normed space with a low-distortion embedding of emd.  examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  we also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.we evaluate our technique with scene recognition and texture classification tasks.",image classification
,"Liang, Percy; Srebro, Nathan",2005-12-22T02:19:52Z,2005-12-22T02:19:52Z,2004-12-30,http://hdl.handle.net/1721.1/30511,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Methods and Experiments With Bounded Tree-width Markov Networks,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",MIT-CSAIL-TR-2004-081; AIM-2004-030,10 p.; 10714507 bytes; 473643 bytes,application/postscript; application/pdf,en_US,AI; tree-width; hypertrees; Markov Networks; maximum likelihood; MDL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,391,"methods and experiments with bounded tree-width markov networks markov trees generalize naturally to bounded tree-width markov networks, onwhich exact computations can still be done efficiently.  however, learning themaximum likelihood markov network with tree-width greater than 1 is np-hard, sowe discuss a few algorithms for approximating the optimal markov network.  wepresent a set of methods for training a density estimator.  each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  on thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",high performance computing
,"Kouh, Minjoon; Poggio, Tomaso",2005-12-22T02:19:58Z,2005-12-22T02:19:58Z,2004-12-31,http://hdl.handle.net/1721.1/30512,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A general mechanism for tuning: Gain control circuits and synapses underlie tuning of cortical neurons,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,MIT-CSAIL-TR-2004-082; AIM-2004-031; CBCL-245,9 p.; 11464000 bytes; 738607 bytes,application/postscript; application/pdf,en_US,AI; tuning; gain control; normalization; Gaussian; neuron; cortex; biophysics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,392,a general mechanism for tuning: gain control circuits and synapses underlie tuning of cortical neurons tuning to an optimal stimulus is a widespread property of neurons in cortex. we propose that such tuning is a consequence of normalization or gain control circuits. we also present a biologically plausible neural circuitry of tuning.,object recognition/detection
,"Richards, Whitman; Seung, H. Sebastian",2005-12-22T02:20:05Z,2005-12-22T02:20:05Z,2004-12-31,http://hdl.handle.net/1721.1/30513,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Neural Voting Machines,"Winner-take-all networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",MIT-CSAIL-TR-2004-083; AIM-2004-029,12 p.; 13714512 bytes; 523751 bytes,application/postscript; application/pdf,en_US,AI; WTA; Borda machine; Condorcet procedure; neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,393,"neural voting machines winner-take-all networks typically pick as winners that alternative with the largest excitatory input. this choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. in the social choice community, many other procedures will yield more robust winners. the borda count and the pair-wise condorcet tally are among the most favored. their implementations are simple modifications of classical recurrent networks.",language models
,"Beal, Jacob; Sussman, Gerald",2005-12-22T02:20:32Z,2005-12-22T02:20:32Z,2005-01-18,http://hdl.handle.net/1721.1/30516,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Biologically-Inspired Robust Spatial Programming,"Inspired by the robustness and flexibility of biological systems, we are developing linguistic and programming tools to allow us to program spatial systems populated by vast numbers of unreliable components interconnected in unknown, irregular, and time-varying ways. We organize our computations around geometry, making the fact that our system is made up of discrete individuals implicit. Geometry allows us to specify requirements in terms of the behavior of the space occupied by the aggregate rather than the behavior of individuals, thereby decreasing complexity. So we describe the behavior of space explicitly, abstracting away the discrete nature of the components. As an example, we present the Amorphous Medium Language, which describes behavior in terms of homeostatic maintenance of constraints on nested regions of space.",MIT-CSAIL-TR-2005-003; AIM-2005-001,17 p.; 42144634 bytes; 6024067 bytes,application/postscript; application/pdf,en_US,AI; amorphous robust biological spatial sensor networks language programming medium,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,398,"biologically-inspired robust spatial programming inspired by the robustness and flexibility of biological systems, we are developing linguistic and programming tools to allow us to program spatial systems populated by vast numbers of unreliable components interconnected in unknown, irregular, and time-varying ways. we organize our computations around geometry, making the fact that our system is made up of discrete individuals implicit. geometry allows us to specify requirements in terms of the behavior of the space occupied by the aggregate rather than the behavior of individuals, thereby decreasing complexity. so we describe the behavior of space explicitly, abstracting away the discrete nature of the components. as an example, we present the amorphous medium language, which describes behavior in terms of homeostatic maintenance of constraints on nested regions of space.",language models
,"Kondacs, Attila",2005-12-22T02:20:51Z,2005-12-22T02:20:51Z,2005-01-28,http://hdl.handle.net/1721.1/30518,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Determining articulator configuration in voiced stop consonants by matching time-domain patterns in pitch periods,"In this thesis I will be concerned with linking the observed speechsignal to the configuration of articulators.Due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. The typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.I argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. The patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.My patterns are an alternative to frequency. I use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. These features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. No linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.The measurements I use are neurophysiologically plausible: theauditory system could be using similar methods.I have demonstrated this approach by constructing a robust techniquefor categorizing the English voiced stops as the consonants B, D, or Gbased on the vocalic portions of their releases. The classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",MIT-CSAIL-TR-2005-005; AITR-2005-001,96 p.; 85678480 bytes; 3087600 bytes,application/postscript; application/pdf,en_US,AI; speech processing; stop consonants; pitch period; spatio-temporal patterns,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,400,"determining articulator configuration in voiced stop consonants by matching time-domain patterns in pitch periods in this thesis i will be concerned with linking the observed speechsignal to the configuration of articulators.due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. the typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.i argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. the patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.my patterns are an alternative to frequency. i use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. these features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. no linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.the measurements i use are neurophysiologically plausible: theauditory system could be using similar methods.i have demonstrated this approach by constructing a robust techniquefor categorizing the english voiced stops as the consonants b, d, or gbased on the vocalic portions of their releases. the classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",speech/audio recognition
,"Sussman, Gerald Jay; Wisdom, Jack",2005-12-22T02:21:07Z,2005-12-22T02:21:07Z,2005-02-02,http://hdl.handle.net/1721.1/30520,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Functional Differential Geometry,"Differential geometry is deceptively simple.  It is surprisingly easyto get the right answer with unclear and informal symbol manipulation.To address this problem we use computer programs to communicate aprecise understanding of the computations in differential geometry.Expressing the methods of differential geometry in a computer languageforces them to be unambiguous and computationally effective.  The taskof formulating a method as a computer-executable program and debuggingthat program is a powerful exercise in the learning process.  Also,once formalized procedurally, a mathematical idea becomes a tool thatcan be used directly to compute results.",MIT-CSAIL-TR-2005-007; AIM-2005-003,77 p.; 38556269 bytes; 1665777 bytes,application/postscript; application/pdf,en_US,AI; Scheme  differential geometry calculus manifolds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,402,"functional differential geometry differential geometry is deceptively simple.  it is surprisingly easyto get the right answer with unclear and informal symbol manipulation.to address this problem we use computer programs to communicate aprecise understanding of the computations in differential geometry.expressing the methods of differential geometry in a computer languageforces them to be unambiguous and computationally effective.  the taskof formulating a method as a computer-executable program and debuggingthat program is a powerful exercise in the learning process.  also,once formalized procedurally, a mathematical idea becomes a tool thatcan be used directly to compute results.",language models
,"Balas, Benjamin",2005-12-22T02:21:13Z,2005-12-22T02:21:13Z,2005-02-07,http://hdl.handle.net/1721.1/30521,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using computational models to study texture representations in the human visual system.,"Traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. At the same time, computer algorithms for the synthesis of natural textures have improved dramatically. The current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. We employ Portilla and Simoncellis 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). We find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",MIT-CSAIL-TR-2005-008; AIM-2005-002; CBCL-244,11 p.; 33342937 bytes; 4614631 bytes,application/postscript; application/pdf,en_US,AI; texture perception; texture synthesis; psychophysics; natural images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,403,"using computational models to study texture representations in the human visual system. traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. at the same time, computer algorithms for the synthesis of natural textures have improved dramatically. the current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. we employ portilla and simoncellis 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). we find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",privacy/ethics
,"Riemann, Reina; Winstein, Keith",2005-12-22T02:24:37Z,2005-12-22T02:24:37Z,2005-02-24,http://hdl.handle.net/1721.1/30524,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Improving 802.11 Range with Forward Error Correction,"The ISO/IEC 8802-11:1999(E) specification uses a 32-bit CRC for error detection and whole-packet retransmissions for recovery. In long-distance orhigh-interference links where the probability of a bit error is high,this strategy results in excessive losses, because any erroneous bitcauses an entire packet to be discarded. By ignoring the CRC andadding redundancy to 802.11 payloads in software, we achievedsubstantially reduced loss rates on indoor and outdoor long-distancelinks and extended line-of-sight range outdoors by 70 percent.",MIT-CSAIL-TR-2005-011; AIM-2005-004,10 p.; 8245944 bytes; 467111 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,406,"improving 802.11 range with forward error correction the iso/iec 8802-11:1999(e) specification uses a 32-bit crc for error detection and whole-packet retransmissions for recovery. in long-distance orhigh-interference links where the probability of a bit error is high,this strategy results in excessive losses, because any erroneous bitcauses an entire packet to be discarded. by ignoring the crc andadding redundancy to 802.11 payloads in software, we achievedsubstantially reduced loss rates on indoor and outdoor long-distancelinks and extended line-of-sight range outdoors by 70 percent.",face detection
,"Sivic, Josef; Russell, Bryan C.; Efros, Alexei A.; Zisserman, Andrew; Freeman, William T.",2005-12-22T02:24:48Z,2005-12-22T02:24:48Z,2005-02-25,http://hdl.handle.net/1721.1/30525,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Discovering object categories in image collections,"Given a set of images containing multiple object categories,we seek to discover those categories and their image locations withoutsupervision.  We achieve this using generative modelsfrom the statistical text literature: probabilistic Latent SemanticAnalysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysisthese are used to discover topics in a corpus using the bag-of-wordsdocument representation. Here we discover topics as object categories, sothat an image containing instances of several categories is modelled as amixture of topics.The models are applied to images by using avisual analogue of a word, formed by vector quantizing SIFT like regiondescriptors.  We investigate a set of increasingly demanding scenarios,starting with image sets containing only two object categories through tosets containing multiple categories (including airplanes, cars, faces,motorbikes, spotted cats) and background clutter. The object categoriessample both intra-class and scale variation, and both the categories andtheir approximate spatial layout are found without supervision.We also demonstrate classification of unseen images and images containingmultiple objects. Performance of the proposed unsupervised method is compared tothe semi-supervised approach of Fergus et al.",MIT-CSAIL-TR-2005-012; AIM-2005-005,0 p.; 29582654 bytes; 2849946 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,408,"discovering object categories in image collections given a set of images containing multiple object categories,we seek to discover those categories and their image locations withoutsupervision.  we achieve this using generative modelsfrom the statistical text literature: probabilistic latent semanticanalysis (plsa), and latent dirichlet allocation (lda). in text analysisthese are used to discover topics in a corpus using the bag-of-wordsdocument representation. here we discover topics as object categories, sothat an image containing instances of several categories is modelled as amixture of topics.the models are applied to images by using avisual analogue of a word, formed by vector quantizing sift like regiondescriptors.  we investigate a set of increasingly demanding scenarios,starting with image sets containing only two object categories through tosets containing multiple categories (including airplanes, cars, faces,motorbikes, spotted cats) and background clutter. the object categoriessample both intra-class and scale variation, and both the categories andtheir approximate spatial layout are found without supervision.we also demonstrate classification of unseen images and images containingmultiple objects. performance of the proposed unsupervised method is compared tothe semi-supervised approach of fergus et al.",image classification
,"Balas, Benjamin; Sinha, Pawan",2005-12-22T02:25:04Z,2005-12-22T02:25:04Z,2005-03-01,http://hdl.handle.net/1721.1/30528,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Receptive field structures for recognition,"Localized operators, like Gabor wavelets and difference-of-Gaussian filters, are considered to be useful tools for image representation. This is due to their ability to form a sparse code that can serve as a basis set for high-fidelity reconstruction of natural images. However, for many visual tasks, the more appropriate criterion of representational efficacy is recognition, rather than reconstruction. It is unclear whether simple local features provide the stability necessary to subserve robust recognition of complex objects. In this paper, we search the space of two-lobed differential operators for those that constitute a good representational code under recognition/discrimination criteria. We find that a novel operator, which we call the dissociated dipole displays useful properties in this regard. We describe simple computational experiments to assess the merits of such dipoles relative to the more traditional local operators. The results suggest that non-local operators constitute a vocabulary that is stable across a range of image transformations.",MIT-CSAIL-TR-2005-015; AIM-2005-006; CBCL-246,17 p.; 34808229 bytes; 3368874 bytes,application/postscript; application/pdf,en_US,AI; object recognition; face recognition; sparse coding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,410,"receptive field structures for recognition localized operators, like gabor wavelets and difference-of-gaussian filters, are considered to be useful tools for image representation. this is due to their ability to form a sparse code that can serve as a basis set for high-fidelity reconstruction of natural images. however, for many visual tasks, the more appropriate criterion of representational efficacy is recognition, rather than reconstruction. it is unclear whether simple local features provide the stability necessary to subserve robust recognition of complex objects. in this paper, we search the space of two-lobed differential operators for those that constitute a good representational code under recognition/discrimination criteria. we find that a novel operator, which we call the dissociated dipole displays useful properties in this regard. we describe simple computational experiments to assess the merits of such dipoles relative to the more traditional local operators. the results suggest that non-local operators constitute a vocabulary that is stable across a range of image transformations.",object recognition/detection
,"Taycher, Leonid; Fisher III, John W.; Darrell, Trevor",2005-12-22T02:25:14Z,2005-12-22T02:25:14Z,2005-03-02,http://hdl.handle.net/1721.1/30529,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining Object and Feature Dynamics in Probabilistic Tracking,"Objects can exhibit different dynamics at different scales, a property that isoftenexploited by visual tracking algorithms. A local dynamicmodel is typically used to extract image features that are then used as inputsto a system for tracking the entire object using a global dynamic model.Approximate local dynamicsmay be brittle---point trackers drift due to image noise and adaptivebackground models adapt to foreground objects that becomestationary---but constraints from the global model can make them more robust.We propose a probabilistic framework for incorporating globaldynamics knowledge into the local feature extraction processes.A global tracking algorithm can beformulated as a generative model and used to predict feature values thatinfluence the observation process of thefeature extractor. We combine such models in a multichain graphicalmodel framework.We show the utility of our framework for improving feature tracking and thusshapeand motion estimates in a batch factorization algorithm.We also propose an approximate filtering algorithm appropriate for onlineapplications, and demonstrate its application to problems such as backgroundsubtraction, structure from motion and articulated body tracking.",MIT-CSAIL-TR-2005-016; AIM-2005-008,0 p.; 44997544 bytes; 4278776 bytes,application/postscript; application/pdf,en_US,AI; graphical models; feature extraction; tracking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,412,"combining object and feature dynamics in probabilistic tracking objects can exhibit different dynamics at different scales, a property that isoftenexploited by visual tracking algorithms. a local dynamicmodel is typically used to extract image features that are then used as inputsto a system for tracking the entire object using a global dynamic model.approximate local dynamicsmay be brittle---point trackers drift due to image noise and adaptivebackground models adapt to foreground objects that becomestationary---but constraints from the global model can make them more robust.we propose a probabilistic framework for incorporating globaldynamics knowledge into the local feature extraction processes.a global tracking algorithm can beformulated as a generative model and used to predict feature values thatinfluence the observation process of thefeature extractor. we combine such models in a multichain graphicalmodel framework.we show the utility of our framework for improving feature tracking and thusshapeand motion estimates in a batch factorization algorithm.we also propose an approximate filtering algorithm appropriate for onlineapplications, and demonstrate its application to problems such as backgroundsubtraction, structure from motion and articulated body tracking.",language models
,"Grauman, Kristen; Darrell, Trevor",2005-12-19T23:36:26Z,2005-12-19T23:36:26Z,2005-03-17,http://hdl.handle.net/1721.1/30420,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pyramid Match Kernels: Discriminative Classification with Sets of Image Features,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",MIT-CSAIL-TR-2005-017; AIM-2005-007,12 p.; 68553886 bytes; 13808123 bytes,application/postscript; application/pdf,en_US,AI; kernel; unordered sets; correspondence; object recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,413,"pyramid match kernels: discriminative classification with sets of image features discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  we present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  this ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. since the kernel does not penalize thepresence of extra features, it is robust to clutter.  we show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only formercer kernels.  we demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",high performance computing
,"Wolf, Lior; Bileschi, Stanley",2005-12-22T02:25:27Z,2005-12-22T02:25:27Z,2005-03-30,http://hdl.handle.net/1721.1/30531,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining Variable Selection with Dimensionality Reduction,"This paper bridges the gap between variable selection methods (e.g., Pearson coefficients, KS test) and dimensionality reductionalgorithms (e.g., PCA, LDA). Variable selection algorithms encounter difficulties dealing with highly correlated data,since many features are similar in quality. Dimensionality reduction algorithms tend to combine all variables and cannotselect a subset of significant variables.Our approach combines both methodologies by applying variable selection followed by dimensionality reduction. Thiscombination makes sense only when using the same utility function in both stages, which we do. The resulting algorithmbenefits from complex features as variable selection algorithms do, and at the same time enjoys the benefits of dimensionalityreduction.1",MIT-CSAIL-TR-2005-019; AIM-2005-009; CBCL-247,10 p.; 14957523 bytes; 722450 bytes,application/postscript; application/pdf,en_US,AI; Computer Vision; Statistical Learning; Variable Selection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,415,"combining variable selection with dimensionality reduction this paper bridges the gap between variable selection methods (e.g., pearson coefficients, ks test) and dimensionality reductionalgorithms (e.g., pca, lda). variable selection algorithms encounter difficulties dealing with highly correlated data,since many features are similar in quality. dimensionality reduction algorithms tend to combine all variables and cannotselect a subset of significant variables.our approach combines both methodologies by applying variable selection followed by dimensionality reduction. thiscombination makes sense only when using the same utility function in both stages, which we do. the resulting algorithmbenefits from complex features as variable selection algorithms do, and at the same time enjoys the benefits of dimensionalityreduction.1",high performance computing
,"Pohl, Kilian M.; Fisher, John; Grimson, W. Eric L.; Wells, William M.",2005-12-22T02:25:35Z,2005-12-22T02:25:35Z,2005-04-01,http://hdl.handle.net/1721.1/30532,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"An Expectation Maximization Approach for Integrated Registration, Segmentation, and Intensity Correction","This paper presents a statistical framework which combines the registration of an atlas with the segmentation of MR images. We use an Expectation Maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. An example of the approach is given for a brain structure-dependent affine mapping approach. The algorithm produces high quality segmentations for brain tissues as well as their substructures. We demonstrate the approach on a set of 30 brain MR images. In addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",MIT-CSAIL-TR-2005-020; AIM-2005-010,13 p.; 18741928 bytes; 826219 bytes,application/postscript; application/pdf,en_US,AI; Expectation Maximization; Segmentation; Registration; Medical Image Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,416,"an expectation maximization approach for integrated registration, segmentation, and intensity correction this paper presents a statistical framework which combines the registration of an atlas with the segmentation of mr images. we use an expectation maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. an example of the approach is given for a brain structure-dependent affine mapping approach. the algorithm produces high quality segmentations for brain tissues as well as their substructures. we demonstrate the approach on a set of 30 brain mr images. in addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",language models
,"Werfel, Justin; Bar-Yam, Yaneer; Nagpal, Radhika",2005-12-22T02:28:10Z,2005-12-22T02:28:10Z,2005-04-08,http://hdl.handle.net/1721.1/30536,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Construction by robot swarms using extended stigmergy,"We describe a system in which simple, identical, autonomous robots assemble two-dimensional structures out of identical building blocks.  We show that, in a system divided in this way into mobile units and structural units, giving the blocks limited communication abilities enables robots to have sufficient global structural knowledge to rapidly build elaborate pre-designed structures.  In this way we extend the principle of stigmergy (storing information in the environment) used by social insects, by increasing the capabilities of the blocks that represent that environmental information.  As a result, arbitrary solid structures can be built using a few fixed, local behaviors, without requiring construction to be planned out in detail.",MIT-CSAIL-TR-2005-024; AIM-2005-011,19 p.; 16033902 bytes; 615762 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,420,"construction by robot swarms using extended stigmergy we describe a system in which simple, identical, autonomous robots assemble two-dimensional structures out of identical building blocks.  we show that, in a system divided in this way into mobile units and structural units, giving the blocks limited communication abilities enables robots to have sufficient global structural knowledge to rapidly build elaborate pre-designed structures.  in this way we extend the principle of stigmergy (storing information in the environment) used by social insects, by increasing the capabilities of the blocks that represent that environmental information.  as a result, arbitrary solid structures can be built using a few fixed, local behaviors, without requiring construction to be planned out in detail.",robotics
,"Beal, Jacob",2005-12-22T02:28:22Z,2005-12-22T02:28:22Z,2005-04-13,http://hdl.handle.net/1721.1/30538,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning From Snapshot Examples,"Examples are a powerful tool for teaching both humans and computers.In order to learn from examples, however, a student must first extractthe examples from its stream of perception. Snapshot learning is ageneral approach to this problem, in which relevant samples ofperception are used as examples.  Learning from these examples can inturn improve the judgement of the snapshot mechanism, improving thequality of future examples.  One way to implement snapshot learning isthe Top-Cliff heuristic, which identifies relevant samples using ageneralized notion of peaks. I apply snapshot learning with theTop-Cliff heuristic to solve a distributed learning problem and showthat the resulting system learns rapidly and robustly, and canhallucinate useful examples in a perceptual stream from a teacherlesssystem.",MIT-CSAIL-TR-2005-026; AIM-2005-012,22 p.; 16733589 bytes; 735336 bytes,application/postscript; application/pdf,en_US,AI; unsupervised supervised learning examples,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,422,"learning from snapshot examples examples are a powerful tool for teaching both humans and computers.in order to learn from examples, however, a student must first extractthe examples from its stream of perception. snapshot learning is ageneral approach to this problem, in which relevant samples ofperception are used as examples.  learning from these examples can inturn improve the judgement of the snapshot mechanism, improving thequality of future examples.  one way to implement snapshot learning isthe top-cliff heuristic, which identifies relevant samples using ageneralized notion of peaks. i apply snapshot learning with thetop-cliff heuristic to solve a distributed learning problem and showthat the resulting system learns rapidly and robustly, and canhallucinate useful examples in a perceptual stream from a teacherlesssystem.",image classification
,"Caponnetto, Andrea; Vito, Ernesto De",2005-12-22T02:28:27Z,2005-12-22T02:28:27Z,2005-04-14,http://hdl.handle.net/1721.1/30539,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Fast Rates for Regularized Least-squares Algorithm,"We develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel Hilbert spaces for supervised learning.  We show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  In fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",MIT-CSAIL-TR-2005-027; AIM-2005-013; CBCL-248,25 p.; 16130108 bytes; 833989 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; regularized least-squares; reproducing kernel Hilbert space; effe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,423,"fast rates for regularized least-squares algorithm we develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel hilbert spaces for supervised learning.  we show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  in fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",high performance computing
,"Eisenstein, Jacob; Davis, Randall",2005-12-22T02:28:32Z,2005-12-22T02:28:32Z,2005-04-19,http://hdl.handle.net/1721.1/30540,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Gestural Cues for Sentence Segmentation,"In human-human dialogues, face-to-face meetings are often preferred over phone conversations.One explanation is that non-verbal modalities such as gesture provide additionalinformation, making communication more efficient and accurate. If so, computerprocessing of natural language could improve by attending to non-verbal modalitiesas well. We consider the problem of sentence segmentation, using hand-annotatedgesture features to improve recognition. We find that gesture features correlate wellwith sentence boundaries, but that these features improve the overall performance of alanguage-only system only marginally. This finding is in line with previous research onthis topic. We provide a regression analysis, revealing that for sentence boundarydetection, the gestural features are largely redundant with the language model andpause features. This suggests that gestural features can still be useful when speech recognition is inaccurate.",MIT-CSAIL-TR-2005-028; AIM-2005-014,13 p.; 13772256 bytes; 521371 bytes,application/postscript; application/pdf,en_US,AI; gesture; natural language processing; multimodal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,424,"gestural cues for sentence segmentation in human-human dialogues, face-to-face meetings are often preferred over phone conversations.one explanation is that non-verbal modalities such as gesture provide additionalinformation, making communication more efficient and accurate. if so, computerprocessing of natural language could improve by attending to non-verbal modalitiesas well. we consider the problem of sentence segmentation, using hand-annotatedgesture features to improve recognition. we find that gesture features correlate wellwith sentence boundaries, but that these features improve the overall performance of alanguage-only system only marginally. this finding is in line with previous research onthis topic. we provide a regression analysis, revealing that for sentence boundarydetection, the gestural features are largely redundant with the language model andpause features. this suggests that gestural features can still be useful when speech recognition is inaccurate.",language models
,"Taylor, Christopher; Rahimi, Ali; Bachrach, Jonathan; Shrobe, Howard",2005-12-22T02:28:41Z,2005-12-22T02:28:41Z,2005-04-26,http://hdl.handle.net/1721.1/30541,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Simultaneous Localization, Calibration, and Tracking in an ad Hoc Sensor Network","We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",MIT-CSAIL-TR-2005-029; AIM-2005-016,18 p.; 40655574 bytes; 2128443 bytes,application/postscript; application/pdf,en_US,AI; sensor network; localization; bayesian filter; extended kalman filter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,425,"simultaneous localization, calibration, and tracking in an ad hoc sensor network we introduce simultaneous localization and tracking (slat), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  our proposed solution, laslat, is a bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. it does not require globally accessible beacon signals or  accurate ranging between the nodes.  when applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",high performance computing
,"Vito, Ernesto De; Caponnetto, Andrea",2005-12-22T02:28:54Z,2005-12-22T02:28:54Z,2005-05-16,http://hdl.handle.net/1721.1/30543,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Risk Bounds for Regularized Least-squares Algorithm with Operator-valued kernels,We show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel Hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  We first briefly introduce central concepts on operator-valued kernels.  Then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,MIT-CSAIL-TR-2005-031; AIM-2005-015; CBCL-249,17 p.; 12090406 bytes; 642646 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; reproducing kernel Hilbert space; effective dimension,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,427,risk bounds for regularized least-squares algorithm with operator-valued kernels we show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  we first briefly introduce central concepts on operator-valued kernels.  then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,privacy/ethics
,"Caponnetto, Andrea; Rakhlin, Alexander",2005-12-22T02:29:32Z,2005-12-22T02:29:32Z,2005-05-17,http://hdl.handle.net/1721.1/30545,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Some Properties of Empirical Risk Minimization over Donsker Classes,"We study properties of algorithms which minimize (or almost minimize) empirical error over a Donsker class of functions. We show that the L2-diameter of the set of almost-minimizers is converging to zero in probability. Therefore, as the number of samples grows, it is becoming unlikely that adding a point (or a number of points) to the training set will result in a large jump (in L2 distance) to a new hypothesis. We also show that under some conditions the expected errors of the almost-minimizers are becoming close with a rate faster than n^{-1/2}.",MIT-CSAIL-TR-2005-033; AIM-2005-018; CBCL-250,9 p.; 7033622 bytes; 434782 bytes,application/postscript; application/pdf,en_US,AI; empirical risk minimization; stability; empirical processes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,429,"some properties of empirical risk minimization over donsker classes we study properties of algorithms which minimize (or almost minimize) empirical error over a donsker class of functions. we show that the l2-diameter of the set of almost-minimizers is converging to zero in probability. therefore, as the number of samples grows, it is becoming unlikely that adding a point (or a number of points) to the training set will result in a large jump (in l2 distance) to a new hypothesis. we also show that under some conditions the expected errors of the almost-minimizers are becoming close with a rate faster than n^{-1/2}.",object recognition/detection
,"Nahnsen, Thade; Uzuner, Ozlem; Katz, Boris",2005-12-22T02:29:37Z,2005-12-22T02:29:37Z,2005-05-19,http://hdl.handle.net/1721.1/30546,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Lexical Chains and Sliding Locality Windows in Content-based Text Similarity Detection,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",MIT-CSAIL-TR-2005-034; AIM-2005-017,9 p.; 17827888 bytes; 7011726 bytes,application/postscript; application/pdf,en_US,AI; Natural Language Processing; N-grams; Text Similarity; Lexical Chains,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,430,"lexical chains and sliding locality windows in content-based text similarity detection we present a system to determine content similarity of documents. more specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. we experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",language models
,"Wu, Jia Jane",2005-12-22T02:29:44Z,2005-12-22T02:29:44Z,2005-05-25,http://hdl.handle.net/1721.1/30547,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Comparing Visual Features for Morphing Based Recognition,"This thesis presents a method of object classification using the idea of deformable shape matching.  Three types of visual features, geometric blur, C1 and SIFT, are used to generate feature descriptors.  These feature descriptors are then used to find point correspondences between pairs of images.  Various morphable models are created by small subsets of these correspondences using thin-plate spline.  Given these morphs, a simple algorithm, least median of squares (LMEDS), is used to find the best morph.  A scoring metric, using both LMEDS and distance transform, is used to classify test images based on a nearest neighbor algorithm.  We perform the experiments on the Caltech 101 dataset [5].  To ease computation, for each test image, a shortlist is created containing 10 of the most likely candidates.  We were unable to duplicate the performance of [1] in the shortlist stage because we did not use hand-segmentation to extract objects for our training images.  However, our gain from the shortlist to correspondence stage is comparable to theirs.  In our experiments, we improved from 21% to 28% (gain of 33%), while [1] improved from 41% to 48% (gain of 17%).  We find that using a non-shape based approach, C2 [14], the overall classification rate of 33.61% is higher than all of the shaped based methods tested in our experiments.",MIT-CSAIL-TR-2005-035; AITR-2005-002; CBCL-251,42 p.; 39773758 bytes; 1459526 bytes,application/postscript; application/pdf,en_US,AI; object recognition; shape-based,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,432,"comparing visual features for morphing based recognition this thesis presents a method of object classification using the idea of deformable shape matching.  three types of visual features, geometric blur, c1 and sift, are used to generate feature descriptors.  these feature descriptors are then used to find point correspondences between pairs of images.  various morphable models are created by small subsets of these correspondences using thin-plate spline.  given these morphs, a simple algorithm, least median of squares (lmeds), is used to find the best morph.  a scoring metric, using both lmeds and distance transform, is used to classify test images based on a nearest neighbor algorithm.  we perform the experiments on the caltech 101 dataset [5].  to ease computation, for each test image, a shortlist is created containing 10 of the most likely candidates.  we were unable to duplicate the performance of [1] in the shortlist stage because we did not use hand-segmentation to extract objects for our training images.  however, our gain from the shortlist to correspondence stage is comparable to theirs.  in our experiments, we improved from 21% to 28% (gain of 33%), while [1] improved from 41% to 48% (gain of 17%).  we find that using a non-shape based approach, c2 [14], the overall classification rate of 33.61% is higher than all of the shaped based methods tested in our experiments.",object recognition/detection
,"Caponnetto, Andrea; Rosasco, Lorenzo; Vito, Ernesto De; Verri, Alessandro",2005-12-22T02:29:53Z,2005-12-22T02:29:53Z,2005-05-27,http://hdl.handle.net/1721.1/30548,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Empirical Effective Dimension and Optimal Rates for Regularized Least Squares Algorithm,"This paper presents an approach to model selection for regularized least-squares on reproducing kernel Hilbert spaces in the semi-supervised setting.  The role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  The main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  The empirical effective dimension can be computed from independent unlabelled samples.  This makes the approach particularly appealing in the semi-supervised setting.",MIT-CSAIL-TR-2005-036; AIM-2005-019; CBCL-252,14 p.; 11158573 bytes; 526018 bytes,application/postscript; application/pdf,en_US,AI; optimal rates; effective dimension; semi-supervised learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,433,"empirical effective dimension and optimal rates for regularized least squares algorithm this paper presents an approach to model selection for regularized least-squares on reproducing kernel hilbert spaces in the semi-supervised setting.  the role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  the main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  the empirical effective dimension can be computed from independent unlabelled samples.  this makes the approach particularly appealing in the semi-supervised setting.",language models
,"Taylor, Christopher J.",2005-12-22T02:30:00Z,2005-12-22T02:30:00Z,2005-05-31,http://hdl.handle.net/1721.1/30549,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Simultaneous Localization and Tracking in Wireless Ad-hoc Sensor Networks,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",MIT-CSAIL-TR-2005-037; AITR-2005-003,69 p.; 81859537 bytes; 3510560 bytes,application/postscript; application/pdf,en_US,AI; Localization; Target Tracking; Sensor Network; Calibration,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,434,"simultaneous localization and tracking in wireless ad-hoc sensor networks in this thesis we present laslat, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. laslat is based on a bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. the algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. laslat is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.we also present results from a centralized java implementation oflaslat on both two- and three-dimensional sensor networks in whichranges are obtained using the cricket ranging system. laslat is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",language models
,"Segonne, Florent; Pons, Jean-Philippe; Fischl, Bruce; Grimson, Eric",2005-12-22T02:32:21Z,2005-12-22T02:32:21Z,2005-06-01,http://hdl.handle.net/1721.1/30550,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Novel Active Contour Framework. Multi-component Level Set  Evolution under Topology Control,"We present a novel framework to exert a topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. A method, based on the concept of simple point borrowed from digital topology, was recently proposed to achieve a strict topology preservation during a level set evolution. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. We demonstrate the strength of our method on a wide range of numerical experiments.",MIT-CSAIL-TR-2005-038; AIM-2005-020,16 p.; 60440380 bytes; 1311817 bytes,application/postscript; application/pdf,en_US,AI; digital topology; level set; active contour,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,435,"a novel active contour framework. multi-component level set  evolution under topology control we present a novel framework to exert a topology control over a level set evolution. level set methods offer several advantages over parametric active contours, in particular automated topological changes. in some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. a method, based on the concept of simple point borrowed from digital topology, was recently proposed to achieve a strict topology preservation during a level set evolution. however, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. we introduce a topologically controlled level set framework that greatly alleviates this problem. unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. we demonstrate the strength of our method on a wide range of numerical experiments.",high performance computing
,"rahimi, ali; recht, ben; darrell, trevor",2005-12-22T02:32:35Z,2005-12-22T02:32:35Z,2005-06-06,http://hdl.handle.net/1721.1/30552,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Nonlinear Latent Variable Models for Video Sequences,"Many high-dimensional time-varying signals can be modeled as a  sequence of noisy nonlinear observations of a low-dimensional  dynamical process.  Given high-dimensional observations and a  distribution describing the dynamical process, we present a  computationally inexpensive approximate algorithm for estimating the  inverse of this mapping. Once this mapping is learned, we can invert  it to construct a generative model for the signals. Our algorithm  can be thought of as learning a manifold of images by taking into  account the dynamics underlying the low-dimensional representation  of these images. It also serves as a nonlinear system identification  procedure that estimates the inverse of the observation function in  nonlinear dynamic system.  Our algorithm reduces to a generalized  eigenvalue problem, so it does not suffer from the computational or  local minimum issues traditionally associated with nonlinear system  identification, allowing us to apply it to the problem of learning  generative models for video sequences.",MIT-CSAIL-TR-2005-041; AIM-2005-021,11 p.; 13801637 bytes; 2196348 bytes,application/postscript; application/pdf,en_US,"AI; Manifold learning,nonlinear system identification; unsupervised learning",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,438,"nonlinear latent variable models for video sequences many high-dimensional time-varying signals can be modeled as a  sequence of noisy nonlinear observations of a low-dimensional  dynamical process.  given high-dimensional observations and a  distribution describing the dynamical process, we present a  computationally inexpensive approximate algorithm for estimating the  inverse of this mapping. once this mapping is learned, we can invert  it to construct a generative model for the signals. our algorithm  can be thought of as learning a manifold of images by taking into  account the dynamics underlying the low-dimensional representation  of these images. it also serves as a nonlinear system identification  procedure that estimates the inverse of the observation function in  nonlinear dynamic system.  our algorithm reduces to a generalized  eigenvalue problem, so it does not suffer from the computational or  local minimum issues traditionally associated with nonlinear system  identification, allowing us to apply it to the problem of learning  generative models for video sequences.",high performance computing
,"Hung, Chou; Kreiman, Gabriel; Poggio, Tomaso; DiCarlo, James J.",2005-12-22T02:33:06Z,2005-12-22T02:33:06Z,2005-07-06,http://hdl.handle.net/1721.1/30556,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Ultra-fast Object Recognition from Few Spikes,"Understanding the complex brain computations leading to object recognition requires quantitatively characterizing the information represented in inferior temporal cortex (IT), the highest stage of the primate visual stream. A read-out technique based on a trainable classifier is used to characterize the neural coding of selectivity and invariance at the population level. The activity of very small populations of independently recorded IT neurons (~100 randomly selected cells) over very short time intervals (as small as 12.5 ms) contains surprisingly accurate and robust information about both object identity and category, which is furthermore highly invariant to object position and scale. Significantly, selectivity and invariance are present even for novel objects, indicating that these properties arise from the intrinsic circuitry and do not require object-specific learning. Within the limits of the technique, there is no detectable difference in the latency or temporal resolution of the IT information supporting so-called categorization (a.k. basic level) and identification (a.k. subordinate level) tasks.  Furthermore, where information, in particular information about stimulus location and scale, can also be read-out from the same small population of IT neurons. These results show how it is possible to decode invariant object information rapidly, accurately and robustly from a small population in IT and provide insights into the nature of the neural code for different kinds of object-related information.",MIT-CSAIL-TR-2005-045; AIM-2005-022; CBCL-253,30 p.; 77109103 bytes; 12556007 bytes,application/postscript; application/pdf,en_US,AI; object recognition; neural coding; inferior temporal cortex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,443,"ultra-fast object recognition from few spikes understanding the complex brain computations leading to object recognition requires quantitatively characterizing the information represented in inferior temporal cortex (it), the highest stage of the primate visual stream. a read-out technique based on a trainable classifier is used to characterize the neural coding of selectivity and invariance at the population level. the activity of very small populations of independently recorded it neurons (~100 randomly selected cells) over very short time intervals (as small as 12.5 ms) contains surprisingly accurate and robust information about both object identity and category, which is furthermore highly invariant to object position and scale. significantly, selectivity and invariance are present even for novel objects, indicating that these properties arise from the intrinsic circuitry and do not require object-specific learning. within the limits of the technique, there is no detectable difference in the latency or temporal resolution of the it information supporting so-called categorization (a.k. basic level) and identification (a.k. subordinate level) tasks.  furthermore, where information, in particular information about stimulus location and scale, can also be read-out from the same small population of it neurons. these results show how it is possible to decode invariant object information rapidly, accurately and robustly from a small population in it and provide insights into the nature of the neural code for different kinds of object-related information.",object recognition/detection
,"Yokono, Jerry Jun; Poggio, Tomaso",2005-12-22T02:33:20Z,2005-12-22T02:33:20Z,2005-07-07,http://hdl.handle.net/1721.1/30557,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Boosting a Biologically Inspired Local Descriptor for Geometry-free Face and Full Multi-view 3D Object Recognition,"Object recognition systems relying on local descriptors are increasingly used because of their perceived robustness with respect to occlusions and to global geometrical deformations.  Descriptors of this type -- based on a set of oriented Gaussian derivative filters -- are used in our recognition system.  In this paper, we explore a multi-view 3D object recognition system that does not use explicit geometrical information. The basic idea is to find discriminant features to describe an object across different views.  A boosting procedure is used to select features out of a large feature pool of local features collected from the positive training examples.  We describe experiments on face images with excellent recognition rate.",MIT-CSAIL-TR-2005-046; AIM-2005-023; CBCL-254,22 p.; 49560015 bytes; 7562398 bytes,application/postscript; application/pdf,en_US,AI; 3D multiview; object recognition; SVM and boosting classifiers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,444,"boosting a biologically inspired local descriptor for geometry-free face and full multi-view 3d object recognition object recognition systems relying on local descriptors are increasingly used because of their perceived robustness with respect to occlusions and to global geometrical deformations.  descriptors of this type -- based on a set of oriented gaussian derivative filters -- are used in our recognition system.  in this paper, we explore a multi-view 3d object recognition system that does not use explicit geometrical information. the basic idea is to find discriminant features to describe an object across different views.  a boosting procedure is used to select features out of a large feature pool of local features collected from the positive training examples.  we describe experiments on face images with excellent recognition rate.",object recognition/detection
,"Richards, Whitman",2005-12-22T02:36:19Z,2005-12-22T02:36:19Z,2005-08-16,http://hdl.handle.net/1721.1/30565,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Collective Choice with Uncertain Domain Moldels,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a top-cycle.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voters view of how alternatives are related. As more and more voters preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",MIT-CSAIL-TR-2005-054; AIM-2005-024,18 p.; 17793797 bytes; 614937 bytes,application/postscript; application/pdf,en_US,AI; collective choice; uncertainty; voting; top-cycles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,454,"collective choice with uncertain domain moldels when groups of individuals make choices among several alternatives, the most compelling social outcome is the condorcet winner, namely the alternative beating all others in a pair-wise contest. obviously the condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  however, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a top-cycle.)  we explore the sensitivity of condorcet winners to various perturbations in the voting process that lead to top-cycles. surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voters view of how alternatives are related. as more and more voters preference orderings on alternatives depart from a shared model of the domain, then unique condorcet outcomes become increasingly unlikely.",language models
,"Russell, Bryan C.; Torralba, Antonio; Murphy, Kevin P.; Freeman, William T.",2005-12-22T02:36:40Z,2005-12-22T02:36:40Z,2005-09-08,http://hdl.handle.net/1721.1/30567,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,LabelMe: a database and web-based tool for image annotation,"Research in object detection and recognition in cluttered scenes requires large image collections with ground truth labels.  The labels should provide information about the object classes present in each image, as well as their shape and locations, and possibly other attributes such as pose.  Such data is useful for testing, as well as for supervised learning.  This project provides a web-based annotation tool that makes it easy to annotate images, and to instantly sharesuch annotations with the community.  This tool, plus an initial set of 10,000 images (3000 of which have been labeled), can be found at http://www.csail.mit.edu/$\sim$brussell/research/LabelMe/intro.html",MIT-CSAIL-TR-2005-056; AIM-2005-025,11 p.; 12518984 bytes; 559659 bytes,application/postscript; application/pdf,en_US,AI; object recognition detection database annotation tool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,456,"labelme: a database and web-based tool for image annotation research in object detection and recognition in cluttered scenes requires large image collections with ground truth labels.  the labels should provide information about the object classes present in each image, as well as their shape and locations, and possibly other attributes such as pose.  such data is useful for testing, as well as for supervised learning.  this project provides a web-based annotation tool that makes it easy to annotate images, and to instantly sharesuch annotations with the community.  this tool, plus an initial set of 10,000 images (3000 of which have been labeled), can be found at http://www.csail.mit.edu/$\sim$brussell/research/labelme/intro.html",object recognition/detection
,"Stauffer, Chris",2005-12-22T02:36:45Z,2005-12-22T02:36:45Z,2005-09-20,http://hdl.handle.net/1721.1/30568,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automated Audio-visual Activity Analysis,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",MIT-CSAIL-TR-2005-057; AIM-2005-026,9 p.; 32903979 bytes; 1153580 bytes,application/postscript; application/pdf,en_US,AI; Unsupervised; activity analysis; scene modeling; tracking; event detection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,457,"automated audio-visual activity analysis current computer vision techniques can effectively monitor gross activities in sparse environments.  unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  in many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  this paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  this results in a compact description of the different types of compound audio-visual events in an environment.",high performance computing
,"Theocharous, Georgios; Mahadevan, Sridhar; Kaelbling, Leslie Pack",2005-12-22T02:36:53Z,2005-12-22T02:36:53Z,2005-09-27,http://hdl.handle.net/1721.1/30569,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Spatial and Temporal Abstractions in POMDPs Applied to Robot Navigation,"Partially observable Markov decision processes (POMDPs) are a well studied paradigm for programming autonomous robots, where the robot sequentially chooses actions to achieve long term goals efficiently.  Unfortunately, for real world robots and other similar domains, the uncertain outcomes of the actions and the fact that the true world state may not be completely observable make learning of models of the world extremely difficult, and using them algorithmically infeasible.  In this paper we show that learning POMDP models and planning with them can become significantly easier when we incorporate into our algorithms the notions of spatial and tempral abstraction.  We demonstrate the superiority of our algorithms by comparing them with previous flat approaches for large scale robot navigation.",MIT-CSAIL-TR-2005-058; AIM-2005-027,72 p.; 73465696 bytes; 2744720 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,458,"spatial and temporal abstractions in pomdps applied to robot navigation partially observable markov decision processes (pomdps) are a well studied paradigm for programming autonomous robots, where the robot sequentially chooses actions to achieve long term goals efficiently.  unfortunately, for real world robots and other similar domains, the uncertain outcomes of the actions and the fact that the true world state may not be completely observable make learning of models of the world extremely difficult, and using them algorithmically infeasible.  in this paper we show that learning pomdp models and planning with them can become significantly easier when we incorporate into our algorithms the notions of spatial and tempral abstraction.  we demonstrate the superiority of our algorithms by comparing them with previous flat approaches for large scale robot navigation.",robotics
,"Das, Sanmay",2005-12-22T02:37:21Z,2005-12-22T02:37:21Z,2005-10-07,http://hdl.handle.net/1721.1/30573,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Learning to Trade with Insider Information,"This paper introduces algorithms for learning how to trade usinginsider (superior) information in Kyle's model of financial markets.Prior results in finance theory relied on the insider having perfectknowledge of the structure and parameters of the market. I show herethat it is possible to learn the equilibrium trading strategy whenits form is known even without knowledge of the parameters governingtrading in the model. However, the rate of convergence toequilibrium is slow, and an approximate algorithm that does notconverge to the equilibrium strategy achieves better utility whenthe horizon is limited. I analyze this approximate algorithm fromthe perspective of reinforcement learning and discuss the importanceof domain knowledge in designing a successful learning algorithm.",MIT-CSAIL-TR-2005-063; AIM-2005-028; CBCL-255,15 p.; 16523384 bytes; 613212 bytes,application/postscript; application/pdf,en_US,AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,463,"learning to trade with insider information this paper introduces algorithms for learning how to trade usinginsider (superior) information in kyle's model of financial markets.prior results in finance theory relied on the insider having perfectknowledge of the structure and parameters of the market. i show herethat it is possible to learn the equilibrium trading strategy whenits form is known even without knowledge of the parameters governingtrading in the model. however, the rate of convergence toequilibrium is slow, and an approximate algorithm that does notconverge to the equilibrium strategy achieves better utility whenthe horizon is limited. i analyze this approximate algorithm fromthe perspective of reinforcement learning and discuss the importanceof domain knowledge in designing a successful learning algorithm.",privacy/ethics
,"Geiger, Gadi; Amara, Domenic G",2005-12-22T02:37:36Z,2005-12-22T02:37:36Z,2005-10-18,http://hdl.handle.net/1721.1/30575,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Towards the Prevention of Dyslexia,"Previous studies have shown that dyslexic individuals who supplement windowed reading practice with intensive small-scale hand-eye coordination tasks exhibit marked improvement in their reading skills. Here we examine whether similar hand-eye coordination activities, in the form of artwork performed by children in kindergarten, first and second grades, could reduce the number of students at-risk for reading problems. Our results suggest that daily hand-eye coordination activities significantly reduce the number of students at-risk. We believe that the effectiveness of these activities derives from their ability to prepare the students perceptually for reading.",MIT-CSAIL-TR-2005-065; AIM-2005-029; CBCL-256,0 p.; 14865502 bytes; 6633149 bytes,application/postscript; application/pdf,en_US,AI; dyslexia; prevention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,465,"towards the prevention of dyslexia previous studies have shown that dyslexic individuals who supplement windowed reading practice with intensive small-scale hand-eye coordination tasks exhibit marked improvement in their reading skills. here we examine whether similar hand-eye coordination activities, in the form of artwork performed by children in kindergarten, first and second grades, could reduce the number of students at-risk for reading problems. our results suggest that daily hand-eye coordination activities significantly reduce the number of students at-risk. we believe that the effectiveness of these activities derives from their ability to prepare the students perceptually for reading.",image classification
,"Lippert, Ross; Rifkin, Ryan",2005-12-22T02:40:10Z,2005-12-22T02:40:10Z,2005-10-20,http://hdl.handle.net/1721.1/30577,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Asymptotics of Gaussian Regularized Least-Squares,"We consider regularized least-squares (RLS) with a Gaussian kernel. Weprove that if we let the Gaussian bandwidth $\sigma \rightarrow\infty$ while letting the regularization parameter $\lambda\rightarrow 0$, the RLS solution tends to a polynomial whose order iscontrolled by the relative rates of decay of $\frac{1}{\sigma^2}$ and$\lambda$: if $\lambda = \sigma^{-(2k+1)}$, then, as $\sigma \rightarrow\infty$, the RLS solution tends to the $k$th order polynomial withminimal empirical error.  We illustrate the result with an example.",MIT-CSAIL-TR-2005-067; AIM-2005-030; CBCL-257,1 p.; 7286963 bytes; 527607 bytes,application/postscript; application/pdf,en_US,AI; machine learning; regularization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,467,"asymptotics of gaussian regularized least-squares we consider regularized least-squares (rls) with a gaussian kernel. weprove that if we let the gaussian bandwidth $\sigma \rightarrow\infty$ while letting the regularization parameter $\lambda\rightarrow 0$, the rls solution tends to a polynomial whose order iscontrolled by the relative rates of decay of $\frac{1}{\sigma^2}$ and$\lambda$: if $\lambda = \sigma^{-(2k+1)}$, then, as $\sigma \rightarrow\infty$, the rls solution tends to the $k$th order polynomial withminimal empirical error.  we illustrate the result with an example.",image classification
,"Andoni, Alexandr; Indyk, Piotr",2005-12-22T02:40:39Z,2005-12-22T02:40:39Z,2005-11-04,http://hdl.handle.net/1721.1/30583,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,New LSH-based Algorithm for Approximate Nearest Neighbor,"We present an algorithm for c-approximate nearest neighbor problem in a d-dimensional Euclidean space,  achieving query time ofO(dn^{1/c^2+o(1)}) and space O(dn + n^{1+1/c^2+o(1)}).",MIT-CSAIL-TR-2005-073; AIM-2005-031,12 p.; 11656417 bytes; 559939 bytes,application/postscript; application/pdf,en_US,AI; locality sensitive hashing; nearest neighbor; high dimensions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,473,"new lsh-based algorithm for approximate nearest neighbor we present an algorithm for c-approximate nearest neighbor problem in a d-dimensional euclidean space,  achieving query time ofo(dn^{1/c^2+o(1)}) and space o(dn + n^{1+1/c^2+o(1)}).",language models
,"Monteleoni, Claire; Jaakkola, Tommi",2005-12-22T02:40:44Z,2005-12-22T02:40:44Z,2005-11-17,http://hdl.handle.net/1721.1/30584,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Online Learning of Non-stationary Sequences,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,MIT-CSAIL-TR-2005-074; AIM-2005-032,8 p.; 10189026 bytes; 760649 bytes,application/postscript; application/pdf,en_US,AI; online learning; regret bounds; non-stationarity; HMM; wireless networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,475,online learning of non-stationary sequences we consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  we derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  on the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. we demonstrate the algorithm in the context of wireless networks.,high performance computing
,"Dasgupta, Sanjoy; Kalai, Adam Tauman; Monteleoni, Claire",2005-12-22T02:40:49Z,2005-12-22T02:40:49Z,2005-11-17,http://hdl.handle.net/1721.1/30585,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Analysis of Perceptron-Based Active Learning,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",MIT-CSAIL-TR-2005-075; AIM-2005-033,15 p.; 11491832 bytes; 599624 bytes,application/postscript; application/pdf,en_US,AI; active learning; perceptron; label-complexity; mistake bound; selective sampling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,476,"analysis of perceptron-based active learning we start by showing that in an active learning setting, the perceptron algorithm needs $\omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  we then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. for data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{o}(d \log \frac{1}{\epsilon})$ labels. this exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",high performance computing
,"Uzuner, Ozlem",2005-12-22T02:41:36Z,2005-12-22T02:41:36Z,2005-11-18,http://hdl.handle.net/1721.1/30587,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Identifying Expression Fingerprints using Linguistic Information,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",MIT-CSAIL-TR-2005-077; AITR-2005-004,216 p.; 179019584 bytes; 5410679 bytes,application/postscript; application/pdf,en_US,AI; natural language processing; syntactic information; content; expression,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,477,"identifying expression fingerprints using linguistic information this thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. copyright law protects expression of content.  recognizing literaryworks for copyright protection requires identification of theexpression of their content.  the expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  these fingerprints consist of bothsyntactic and semantic elements of language.  examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  the semantic elements of expression consist ofhigh-level, broad semantic categories.  syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  the performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.in the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing digital rightsmanagement (drm) systems.  however, drm systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  expression fingerprints facilitatedigital tracking even when literary works are drm- and watermark-free,and even when they are paraphrased.  as such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",language models
,"Taycher, Leonid; Shakhnarovich, Gregory; Demirdjian, David; Darrell, Trevor",2005-12-22T02:41:53Z,2005-12-22T02:41:53Z,2005-12-01,http://hdl.handle.net/1721.1/30588,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Conditional Random People: Tracking Humans with CRFs and Grid Filters,"We describe a state-space tracking approach based on a Conditional Random Field(CRF) model, where the observation potentials are \emph{learned} from data. Wefind functions that embed both state and observation into a space wheresimilarity corresponds to $L_1$ distance, and define an observation potentialbased on distance in this space. This potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. We show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse HMM, resulting in real-time system performance. The resulting system isused for human pose tracking in video sequences.",MIT-CSAIL-TR-2005-079; AIM-2005-034,9 p.; 21558399 bytes; 932744 bytes,application/postscript; application/pdf,en_US,AI; articulated tracking; grid filter; conditional random field,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,481,"conditional random people: tracking humans with crfs and grid filters we describe a state-space tracking approach based on a conditional random field(crf) model, where the observation potentials are \emph{learned} from data. wefind functions that embed both state and observation into a space wheresimilarity corresponds to $l_1$ distance, and define an observation potentialbased on distance in this space. this potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. we show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse hmm, resulting in real-time system performance. the resulting system isused for human pose tracking in video sequences.",high performance computing
,"Ivanov, Yuri; Serre, Thomas; Bouvrie, Jacob",2005-12-22T02:44:17Z,2005-12-22T02:44:17Z,2005-12-14,http://hdl.handle.net/1721.1/30590,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Error weighted classifier combination for multi-modal human identification,In this paper we describe a technique of classifier combination used in a human identification system. The system integrates all available features from multi-modal sources within a Bayesian framework. The framework allows representinga class of popular classifier combination rules and methods within a single formalism. It relies on a per-class measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. The method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. We show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,MIT-CSAIL-TR-2005-081; AIM-2005-035; CBCL-258,7 p.; 22108540 bytes; 952178 bytes,application/postscript; application/pdf,en_US,AI; classifier combination; face recognition; identification; multi-modal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,482,error weighted classifier combination for multi-modal human identification in this paper we describe a technique of classifier combination used in a human identification system. the system integrates all available features from multi-modal sources within a bayesian framework. the framework allows representinga class of popular classifier combination rules and methods within a single formalism. it relies on a per-class measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. the method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. we show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,high performance computing
,"Serre, T.; Kouh, M.; Cadieu, C.; Knoblich, U.; Kreiman, G.; Poggio, T.",2007-03-12T16:41:47Z,2007-03-12T16:41:47Z,2005-12-19,http://hdl.handle.net/1721.1/36407,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Theory of Object Recognition: Computations and Circuits in the Feedforward Path of the Ventral Stream in Primate Visual Cortex,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",MIT-CSAIL-TR-2005-082; AIM-2005-36; CBCL-259,130 p.,,,AI; object recognition; standard model; theory; visual cortex; ventral stream; hmax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,483,"a theory of object recognition: computations and circuits in the feedforward path of the ventral stream in primate visual cortex we describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. we show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. we also show that the theory is consistent with (and in some case has predicted) several properties of neurons in v1, v4, it and pfc. the theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. the theory suggests a number of open questions for visual physiology and psychophysics.",object recognition/detection
John Leonard,"Benjamin, Michael R.; Leonard, John J.; Schmidt, Henrik; Newman, Paul M.",2009-06-18T17:45:13Z,2009-06-18T17:45:13Z,2009-06-18,http://hdl.handle.net/1721.1/45569,MIT-CSAIL-TR-2009-028,An Overview of MOOS-IvP and a Brief Users Guide to the IvP Helm Autonomy Software,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",,168 p.,,,UUV; AUV; Behavior Based Architecture; Unmanned Surface Vehicles; Behavior Based Control; Unmanned Marine Vehicles; Action Selection; Collision Avoidance; Multi-Objective Optimization; Unmanned Vehicles; MOOS; USV; Autonomous Decision Making; Autonomous Marine Vehicles; Autonomous Helm; Arbitration; Autonomous Vehicles; Underwater Vehicles; AI; MOOSDB; Behaviors,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,733,"an overview of moos-ivp and a brief users guide to the ivp helm autonomy software this document describes the ivp helm - an open source behavior-based autonomy application for unmanned vehicles. ivp is short for interval programming - a technique for representing and solving multi-objective optimizations problems. behaviors in the ivp helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. the ivp helm is written as a moos application where moos is a set of open source publish-subscribe autonomy middleware tools. this document describes the configuration and use of the ivp helm, provides examples of simple missions and information on how to download and build the software from the moos-ivp server at www.moosivp.org.",robotics
Brian Williams,"Li, Hui X.",2010-04-16T16:15:05Z,2010-04-16T16:15:05Z,2010-04-09,http://hdl.handle.net/1721.1/53720,MIT-CSAIL-TR-2010-018,Kongming: A Generative Planner for Hybrid Systems with Temporally Extended Goals,"Most unmanned missions in space and undersea are commanded by a ""script"" that specifies a sequence of discrete commands and continuous actions. Currently such scripts are mostly hand-generated by human operators. This introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. For discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals to be accomplished, and the goal-directed systems automatically output the correct, executable command sequences. Increasingly, the control of autonomous systems involves performing actions with a mix of discrete and continuous effects. For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and take sample, and continuous actions, like descend and ascend, which are influenced by the dynamical model of the vehicle. A hybrid planner generates a sequence of discrete and continuous actions that achieve the mission goals. In this thesis, I present a novel approach to solve the generative planning problem for temporally extended goals for hybrid systems, involving both continuous and discrete actions. The planner, Kongming, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for continuous actions. Second, it engages novel reformulation schemes to handle temporally flexible actions and temporally extended goals. I have successfully demonstrated controlling an AUV in the Atlantic ocean using mission scripts solely generated by Kongming. I have also empirically evaluated Kongming on various real-world scenarios in the underwater domain and the air vehicle domain, and found it successfully and efficiently generates valid and optimal plans.",,237 p.,,,combinatorial optimization; AI planning; autonomous systems,Model-based Embedded and Robotic Systems,Funded by the Boeing Company under contract MIT-BA-GTA-1,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2010,783,"kongming: a generative planner for hybrid systems with temporally extended goals most unmanned missions in space and undersea are commanded by a ""script"" that specifies a sequence of discrete commands and continuous actions. currently such scripts are mostly hand-generated by human operators. this introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. for discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals to be accomplished, and the goal-directed systems automatically output the correct, executable command sequences. increasingly, the control of autonomous systems involves performing actions with a mix of discrete and continuous effects. for example, a typical autonomous underwater vehicle (auv) mission involves discrete actions, like get gps and take sample, and continuous actions, like descend and ascend, which are influenced by the dynamical model of the vehicle. a hybrid planner generates a sequence of discrete and continuous actions that achieve the mission goals. in this thesis, i present a novel approach to solve the generative planning problem for temporally extended goals for hybrid systems, involving both continuous and discrete actions. the planner, kongming, incorporates two innovations. first, it employs a compact representation of all hybrid plans, called a hybrid flow graph, which combines the strengths of a planning graph for discrete actions and flow tubes for continuous actions. second, it engages novel reformulation schemes to handle temporally flexible actions and temporally extended goals. i have successfully demonstrated controlling an auv in the atlantic ocean using mission scripts solely generated by kongming. i have also empirically evaluated kongming on various real-world scenarios in the underwater domain and the air vehicle domain, and found it successfully and efficiently generates valid and optimal plans.",language models
Leslie Kaelbling,"Kawaguchi, Kenji",2016-06-01T22:00:14Z,2016-06-01T22:00:14Z,2016-05-26,http://hdl.handle.net/1721.1/102796,MIT-CSAIL-TR-2016-006,Towards Practical Theory: Bayesian Optimization and Optimal Exploration,"This thesis discusses novel principles to improve the theoretical analyses of a class of methods, aiming to provide theoretically driven yet practically useful methods. The thesis focuses on a class of methods, called bound-based search, which includes several planning algorithms (e.g., the A* algorithm and the UCT algorithm), several optimization methods (e.g., Bayesian optimization and Lipschitz optimization), and some learning algorithms (e.g., PAC-MDP algorithms). For Bayesian optimization, this work solves an open problem and achieves an exponential convergence rate. For learning algorithms, this thesis proposes a new analysis framework, called PAC-RMDP, and improves the previous theoretical bounds. The PAC-RMDP framework also provides a unifying view of some previous near-Bayes optimal and PAC-MDP algorithms. All proposed algorithms derived on the basis of the new principles produced competitive results in our numerical experiments with standard benchmark tests.",,87 p.,,,PAC-MDP; AI planning; Global optimization,Learning and Intelligent Systems,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,SM thesis,,,,,,,,,2016-06-01T22:00:15Z,,,,,,,,,,,,,,,,2016,1023,"towards practical theory: bayesian optimization and optimal exploration this thesis discusses novel principles to improve the theoretical analyses of a class of methods, aiming to provide theoretically driven yet practically useful methods. the thesis focuses on a class of methods, called bound-based search, which includes several planning algorithms (e.g., the a* algorithm and the uct algorithm), several optimization methods (e.g., bayesian optimization and lipschitz optimization), and some learning algorithms (e.g., pac-mdp algorithms). for bayesian optimization, this work solves an open problem and achieves an exponential convergence rate. for learning algorithms, this thesis proposes a new analysis framework, called pac-rmdp, and improves the previous theoretical bounds. the pac-rmdp framework also provides a unifying view of some previous near-bayes optimal and pac-mdp algorithms. all proposed algorithms derived on the basis of the new principles produced competitive results in our numerical experiments with standard benchmark tests.",high performance computing
Julie A Shah,"Unhelkar, Vaibhav V.; Shah, Julie A.",2018-05-17T19:19:11Z,2018-05-17T19:19:11Z,2018-05-17,http://hdl.handle.net/1721.1/115482,MIT-CSAIL-TR-2018-015,Learning Models of Sequential Decision-Making without Complete State Specification using Bayesian Nonparametric Inference and Active Querying,"Learning models of decision-making behavior during sequential tasks is useful across a variety of applications, including human-machine interaction. In this paper, we present an approach to learning such models within Markovian domains based on observing and querying a decision-making agent. In contrast to classical approaches to behavior learning, we do not assume complete knowledge of the state features that impact an agent's decisions. Using tools from Bayesian nonparametric inference and time series of agents  decisions, we first provide an inference algorithm to identify the presence of any unmodeled state features that impact decision making, as well as likely candidate models. In order to identify the best model among these candidates, we next provide an active querying approach that resolves model ambiguity by querying the decision maker. Results from our evaluations demonstrate that, using the proposed algorithms, an observer can identify the presence of latent state features, recover their dynamics, and estimate their impact on decisions during sequential tasks.",,11 p.,,,"Decision Making, Graphical Models, Human-AI Collaboration",Interactive Robotics Group,,,,,,,,,,,,,2018-05-17T19:19:11Z,,,,,,,,,,,,,,,,2018,1055,"learning models of sequential decision-making without complete state specification using bayesian nonparametric inference and active querying learning models of decision-making behavior during sequential tasks is useful across a variety of applications, including human-machine interaction. in this paper, we present an approach to learning such models within markovian domains based on observing and querying a decision-making agent. in contrast to classical approaches to behavior learning, we do not assume complete knowledge of the state features that impact an agent's decisions. using tools from bayesian nonparametric inference and time series of agents  decisions, we first provide an inference algorithm to identify the presence of any unmodeled state features that impact decision making, as well as likely candidate models. in order to identify the best model among these candidates, we next provide an active querying approach that resolves model ambiguity by querying the decision maker. results from our evaluations demonstrate that, using the proposed algorithms, an observer can identify the presence of latent state features, recover their dynamics, and estimate their impact on decisions during sequential tasks.",high performance computing
Dina Katabi,"Perli, Samuel David; Gollakota, Shyamnath; Katabi, Dina",2009-02-19T19:15:22Z,2009-02-19T19:15:22Z,2009-02-18,http://hdl.handle.net/1721.1/44616,MIT-CSAIL-TR-2009-007,Overcoming the Antennas-Per-Node Throughput Limit in MIMO LANs,"Today, the number of concurrent packets in a MIMO LAN is limited by the number of antennas on the AP. This paper shows how to overcome this limit. It presents a new design where multiple client-AP pairs can communicate concurrently, on the same 802.11 channel. We demonstrate both analytically and experimentally that our design almost doubles the throughput of a MIMO LAN.  The key idea underlying our approach is Interference Alignment and Cancellation (IAC), a novel technique for decoding concurrent sender-receiver pairs in MIMO LANs. It exploits two basic properties of MIMO LANs. First, MIMO transmitters can control the alignment of their signals at a receiver. Second, APs are typically connected to a backend Ethernet, which they can use for coordination. Hence, in IAC, transmitters align their signals such that the first AP can decode at least one of the concurrent packets. Once a packet is decoded, it is sent over the Ethernet to the second AP, which subtracts it from its received signal to decode a second packet, which it sends to the third AP to decode the next packet, and so on. We implement our technique in 2x2 MIMO GNU Radios, and demonstrate via wireless experiments that IAC increases the average throughput of a MIMO LAN by 1.5x on the downlink and 2x on the uplink.",,15 p.,,,Interference alignment; Interference cancellation; Wireless networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,711,"overcoming the antennas-per-node throughput limit in mimo lans today, the number of concurrent packets in a mimo lan is limited by the number of antennas on the ap. this paper shows how to overcome this limit. it presents a new design where multiple client-ap pairs can communicate concurrently, on the same 802.11 channel. we demonstrate both analytically and experimentally that our design almost doubles the throughput of a mimo lan.  the key idea underlying our approach is interference alignment and cancellation (iac), a novel technique for decoding concurrent sender-receiver pairs in mimo lans. it exploits two basic properties of mimo lans. first, mimo transmitters can control the alignment of their signals at a receiver. second, aps are typically connected to a backend ethernet, which they can use for coordination. hence, in iac, transmitters align their signals such that the first ap can decode at least one of the concurrent packets. once a packet is decoded, it is sent over the ethernet to the second ap, which subtracts it from its received signal to decode a second packet, which it sends to the third ap to decode the next packet, and so on. we implement our technique in 2x2 mimo gnu radios, and demonstrate via wireless experiments that iac increases the average throughput of a mimo lan by 1.5x on the downlink and 2x on the uplink.",high performance computing
Fredo Durand,"Igarashi, Takeo; Durand, Fredo; Rivers, Alec",2010-05-06T17:45:04Z,2010-05-06T17:45:04Z,2010-05-05,http://hdl.handle.net/1721.1/54731,MIT-CSAIL-TR-2010-023,A User Study Comparing 3D Modeling with Silhouettes and Google SketchUp,"We describe a user study comparing 3D Modeling with Silhouettes and Google SketchUp. In the user study, ten users were asked to create 3D models of three different objects, using either 3D Modeling with Silhouettes or Google SketchUp. Ten different users were then asked to rank images of the models produced by the first group. We show that the models made with 3D Modeling with Silhouettes were ranked significantly higher on average than those made with Google SketchUp.",,4 p.,,,sketch-based modeling,Computer Graphics,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2010,786,"a user study comparing 3d modeling with silhouettes and google sketchup we describe a user study comparing 3d modeling with silhouettes and google sketchup. in the user study, ten users were asked to create 3d models of three different objects, using either 3d modeling with silhouettes or google sketchup. ten different users were then asked to rank images of the models produced by the first group. we show that the models made with 3d modeling with silhouettes were ranked significantly higher on average than those made with google sketchup.",image classification
,"Taylor, Michael Bedford; Lee, Walter; Amarasinghe, Saman; Agarwal, Anant",2005-12-22T01:32:06Z,2005-12-22T01:32:06Z,2004-06-08,http://hdl.handle.net/1721.1/30477,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"Scalar Operand Networks: Design, Implementation, and Analysis","The bypass paths and multiported register files in microprocessors serve as an implicit interconnect tocommunicate operand values among pipeline stages and multiple ALUs. Previous superscalar designs implementedthis interconnect using centralized structures that do not scale with increasing ILP demands. Insearch of scalability, recent microprocessor designs in industry and academia exhibit a trend toward distributedresources such as partitioned register files, banked caches, multiple independent compute pipelines,and even multiple program counters. Some of these partitioned microprocessor designs have begun to implementbypassing and operand transport using point-to-point interconnects. We call interconnects optimizedfor scalar data transport, whether centralized or distributed, scalar operand networks. Although thesenetworks share many of the challenges of multiprocessor networks such as scalability and deadlock avoidance,they have many unique requirements, including ultra-low latencies (a few cycles versus tens of cycles)and ultra-fast operation-operand matching. This paper discusses the unique properties of scalar operandnetworks (SONs), examines alternative ways of implementing them, and introduces the AsTrO taxonomy todistinguish between them. It discusses the design of two alternative networks in the context of the Raw microprocessor,and presents detailed timing, area and energy statistics for a real implementation. The paperalso presents a 5-tuple performance model for SONs and analyzes their performance sensitivity to networkproperties for ILP workloads.",MIT-CSAIL-TR-2004-038; MIT-LCS-TM-645,27 p.; 60061887 bytes; 2516865 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,341,"scalar operand networks: design, implementation, and analysis the bypass paths and multiported register files in microprocessors serve as an implicit interconnect tocommunicate operand values among pipeline stages and multiple alus. previous superscalar designs implementedthis interconnect using centralized structures that do not scale with increasing ilp demands. insearch of scalability, recent microprocessor designs in industry and academia exhibit a trend toward distributedresources such as partitioned register files, banked caches, multiple independent compute pipelines,and even multiple program counters. some of these partitioned microprocessor designs have begun to implementbypassing and operand transport using point-to-point interconnects. we call interconnects optimizedfor scalar data transport, whether centralized or distributed, scalar operand networks. although thesenetworks share many of the challenges of multiprocessor networks such as scalability and deadlock avoidance,they have many unique requirements, including ultra-low latencies (a few cycles versus tens of cycles)and ultra-fast operation-operand matching. this paper discusses the unique properties of scalar operandnetworks (sons), examines alternative ways of implementing them, and introduces the astro taxonomy todistinguish between them. it discusses the design of two alternative networks in the context of the raw microprocessor,and presents detailed timing, area and energy statistics for a real implementation. the paperalso presents a 5-tuple performance model for sons and analyzes their performance sensitivity to networkproperties for ilp workloads.",high performance computing
Patrick Winston,"Eisenberg, Joshua D.; Finlayson, Mark A.",2016-11-10T20:45:06Z,2016-11-10T20:45:06Z,2016-11-09,http://hdl.handle.net/1721.1/105279,,"Data and Code for ""Automatic Identification of Narrative Diegesis and Point of View""","This archive contains the code and data for the workshop article ""Automatic Identification of Narrative Diegesis and Point of View,"" published in 2016 in the 2nd Workshop for Computing News Storylines (CNewsStory 2016), co-located with EMNLP 2016 in Austin, TX. The root of the archive contains a README file which explains the archive contents. Furthermore, the archive can be imported directly into the Eclipse IDE as a project encapsulating the executable code required to reproduce the results of the paper; the code compiles with Java 1.8. The archive also contains a copy of the final version of the paper for reference.",,224 MiB,,,,Genesis,,,,,,,,,,,,,2016-11-10T20:45:06Z,,,,,,,,,,,,,,,,2016,1033,"data and code for ""automatic identification of narrative diegesis and point of view"" this archive contains the code and data for the workshop article ""automatic identification of narrative diegesis and point of view,"" published in 2016 in the 2nd workshop for computing news storylines (cnewsstory 2016), co-located with emnlp 2016 in austin, tx. the root of the archive contains a readme file which explains the archive contents. furthermore, the archive can be imported directly into the eclipse ide as a project encapsulating the executable code required to reproduce the results of the paper; the code compiles with java 1.8. the archive also contains a copy of the final version of the paper for reference.",language models
Barbara Liskov,"Zhou, You",2009-04-23T17:15:06Z,2009-04-23T17:15:06Z,2009-04-16,http://hdl.handle.net/1721.1/45141,MIT-CSAIL-TR-2009-015,Computing Network Coordinates in the Presence of Byzantine Faults,"Network coordinate systems allow for efficient construction of large-scale distributed systems on the Internet. Coordinates provide locality information in a compact way, without requiring each node to contact every potential neighbor; distances between two nodes' coordinates represent estimates of the network latency between them. Past work on network coordinates has assumed that all nodes in the system behave correctly. The techniques in these systems do not behave well when nodes are Byzantine. These Byzantine failures, wherein a faulty node can behave arbitrarily, can make the coordinate-based distance estimates meaningless. For example, a Byzantine node can delay responding to some other node, thus distorting that node's computation of its own location. We present a network coordinate system based on landmarks, reference nodes that are used for measurements, some of which may be Byzantine faulty. It scales linearly in the number of clients computing their coordinates and does not require excessive network traffic to allow clients to do so. Our results show that our system is able to compute accurate coordinates even when some landmarks are exhibiting Byzantine faults.",,60 p.,,,,Programming Methodology,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,M.Eng.,,,,,,,,,,,,,,,,,,,,,2009,718,"computing network coordinates in the presence of byzantine faults network coordinate systems allow for efficient construction of large-scale distributed systems on the internet. coordinates provide locality information in a compact way, without requiring each node to contact every potential neighbor; distances between two nodes' coordinates represent estimates of the network latency between them. past work on network coordinates has assumed that all nodes in the system behave correctly. the techniques in these systems do not behave well when nodes are byzantine. these byzantine failures, wherein a faulty node can behave arbitrarily, can make the coordinate-based distance estimates meaningless. for example, a byzantine node can delay responding to some other node, thus distorting that node's computation of its own location. we present a network coordinate system based on landmarks, reference nodes that are used for measurements, some of which may be byzantine faulty. it scales linearly in the number of clients computing their coordinates and does not require excessive network traffic to allow clients to do so. our results show that our system is able to compute accurate coordinates even when some landmarks are exhibiting byzantine faults.",language models
Eric Grimson,"Dalley, Gerald; Izo, Tomas",2006-06-12T18:36:58Z,2006-06-12T18:36:58Z,2006-06-12,http://hdl.handle.net/1721.1/32999,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Schematic Querying of Large Tracking Databases,"In dealing with long-term tracking databases withwide-area coverage, an important problem is in formulating anintuitive and fast query system for analysis. In such a querysystem, a user who is not a computer vision research should beable to readily specify a novel query to the system and obtainthe desired results. Furthermore, these queries should be able tonot only search out individual actors (e.g. ""find all white cars"")but also find interactions amongst multiple actors (e.g. ""find alldrag racing activities in the city""). Informally, we have foundthat people often use sketches when describing activities andinteractions. In this paper, we demonstrate a preliminary systemthat automatically interprets schematic drawings of activities.The system transforms the schematics into executable code thatsearches a tracking database. Through our query optimization,these queries tend to take orders of magnitude less time to executethan equivalent queries running on a partially-optimized SQLdatabase.",MIT-CSAIL-TR-2006-043,7 p.; 391836 bytes; 7529339 bytes,application/pdf; application/postscript,en_US,,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,523,"schematic querying of large tracking databases in dealing with long-term tracking databases withwide-area coverage, an important problem is in formulating anintuitive and fast query system for analysis. in such a querysystem, a user who is not a computer vision research should beable to readily specify a novel query to the system and obtainthe desired results. furthermore, these queries should be able tonot only search out individual actors (e.g. ""find all white cars"")but also find interactions amongst multiple actors (e.g. ""find alldrag racing activities in the city""). informally, we have foundthat people often use sketches when describing activities andinteractions. in this paper, we demonstrate a preliminary systemthat automatically interprets schematic drawings of activities.the system transforms the schematics into executable code thatsearches a tracking database. through our query optimization,these queries tend to take orders of magnitude less time to executethan equivalent queries running on a partially-optimized sqldatabase.",high performance computing
,"Wang, Yanwei; Shah, Julie",2022-06-15T14:42:32Z,2022-06-15T14:42:32Z,2022-06-15,https://hdl.handle.net/1721.1/143430,,Universal Motion Generator: Trajectory Autocompletion by Motion Prompts,"Foundation models, which are large neural networks trained on massive datasets, have shown
impressive generalization in both the language and the vision domain. While fine-tuning foundation
models for new tasks at test-time is impractical due to billions of parameters in those models, prompts
have been employed to re-purpose models for test-time tasks on the fly. In this report, we ideate the equivalent foundation model for motion generation and the corresponding formats of prompt that can condition such a model. The central goal is to learn a behavior prior for motion generation that can be re-used in a novel scene.",,,,en_US,"Robot Learning, Large Language Models, Motion Generation",,CSAIL NSF MI project  6939398,Attribution-NonCommercial-NoDerivs 3.0 United States,http://creativecommons.org/licenses/by-nc-nd/3.0/us/,,,,,,,,,,,,,,Working Paper,,,,,,,,,,,,2022,1070,"universal motion generator: trajectory autocompletion by motion prompts foundation models, which are large neural networks trained on massive datasets, have shown
impressive generalization in both the language and the vision domain. while fine-tuning foundation
models for new tasks at test-time is impractical due to billions of parameters in those models, prompts
have been employed to re-purpose models for test-time tasks on the fly. in this report, we ideate the equivalent foundation model for motion generation and the corresponding formats of prompt that can condition such a model. the central goal is to learn a behavior prior for motion generation that can be re-used in a novel scene.",high performance computing
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-05-29T20:45:03Z,2015-05-29T20:45:03Z,2015-05-29,http://hdl.handle.net/1721.1/97130,MIT-CSAIL-TR-2015-021,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems,"We analyze reported patches for three existing generate-and-validate patch generation systems (GenProg, RSRepair, and AE). The basic principle behind generate-and-validate systems is to accept only plausible patches that produce correct outputs for all inputs in the test suite used to validate the patches. Because of errors in the patch evaluation infrastructure, the majority of the reported patches are not plausible -- they do not produce correct outputs even for the inputs in the validation test suite. The overwhelming majority of the reported patches are not correct and are equivalent to a single modification that simply deletes functionality. Observed negative effects include the introduction of security vulnerabilities and the elimination of desirable standard functionality. We also present Kali, a generate-and-validate patch generation system that only deletes functionality. Working with a simpler and more effectively focused search space, Kali generates at least as many correct patches as prior GenProg, RSRepair, and AE systems. Kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. We also discuss the patches produced by ClearView, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. Our analysis indicates that ClearView successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. At least 4 of these patches are correct.",,24 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2015-05-29T20:45:03Z,,,,,,,,,,,,,,,,2015,1003,"an analysis of patch plausibility and correctness for generate-and-validate patch generation systems we analyze reported patches for three existing generate-and-validate patch generation systems (genprog, rsrepair, and ae). the basic principle behind generate-and-validate systems is to accept only plausible patches that produce correct outputs for all inputs in the test suite used to validate the patches. because of errors in the patch evaluation infrastructure, the majority of the reported patches are not plausible -- they do not produce correct outputs even for the inputs in the validation test suite. the overwhelming majority of the reported patches are not correct and are equivalent to a single modification that simply deletes functionality. observed negative effects include the introduction of security vulnerabilities and the elimination of desirable standard functionality. we also present kali, a generate-and-validate patch generation system that only deletes functionality. working with a simpler and more effectively focused search space, kali generates at least as many correct patches as prior genprog, rsrepair, and ae systems. kali also generates at least as many patches that produce correct outputs for the inputs in the validation test suite as the three prior systems. we also discuss the patches produced by clearview, a generate-and-validate binary hot patching system that leverages learned invariants to produce patches that enable systems to survive otherwise fatal defects and security attacks. our analysis indicates that clearview successfully patches 9 of the 10 security vulnerabilities used to evaluate the system. at least 4 of these patches are correct.",language models
Patrick Winston,"Finlayson, Mark Alan",2014-12-30T21:45:10Z,2014-12-30T21:45:10Z,2014-12-30,http://hdl.handle.net/1721.1/92563,,"Supplementary Materials for ""A Survey of Corpora in Computational and Cognitive Narrative Science""","This archive contains supplementary materials for the article titled ""A Survey of Corpora in Computational and Cognitive Narrative Science"" by Mark A. Finlayson, published in the journal *Sprache und Datenverarbeitung*. The archive contains two files. The first file is the raw bibliographic data of the survey, containing 2600+ citations. The second file is a spreadsheet with the coded features of each corpus, plus the analyses that underlie sections 3 & 4 of the paper.",,1172839 bytes,,,,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2014-12-30T21:45:10Z,,,,,,,,,,,,,,,,2014,979,"supplementary materials for ""a survey of corpora in computational and cognitive narrative science"" this archive contains supplementary materials for the article titled ""a survey of corpora in computational and cognitive narrative science"" by mark a. finlayson, published in the journal *sprache und datenverarbeitung*. the archive contains two files. the first file is the raw bibliographic data of the survey, containing 2600+ citations. the second file is a spreadsheet with the coded features of each corpus, plus the analyses that underlie sections 3 & 4 of the paper.",privacy/ethics
Martin Rinard,"Qi, Zichao; Long, Fan; Achour, Sara; Rinard, Martin",2015-02-02T22:00:04Z,2015-02-02T22:00:04Z,2015-02-02,http://hdl.handle.net/1721.1/93255,,An Analysis of Patch Plausibility and Correctness for Generate-And-Validate Patch Generation Systems (Supplementary Material),"We analyze reported patches for three prior generate-and-validate patch generation systems (GenProg, RSRepair, and AE). Because of experimental error, the majority of the reported patches violate the basic principle behind the design of these systems -- they do not produce correct outputs even for the inputs in the test suite used to validate the patches. We also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. We also present Kali, a generate-and-validate patch generation system that simply deletes functionality. Working with a simpler and more effectively focused search space, Kali produces more correct patches and at least as many patches that produce correct outputs for the inputs in the validation test suite as prior GenProg, RSRepair, and AE systems.",,,,,,Computer Architecture,,,,,,,,,,http://hdl.handle.net/1721.1/94337,,,2015-02-02T22:00:04Z,,Main paper: http://hdl.handle.net/1721.1/94337,,,,,,,,,,,,,,2015,982,"an analysis of patch plausibility and correctness for generate-and-validate patch generation systems (supplementary material) we analyze reported patches for three prior generate-and-validate patch generation systems (genprog, rsrepair, and ae). because of experimental error, the majority of the reported patches violate the basic principle behind the design of these systems -- they do not produce correct outputs even for the inputs in the test suite used to validate the patches. we also show that the overwhelming majority of the accepted patches are not correct and are equivalent to a single modification that simply deletes functionality. we also present kali, a generate-and-validate patch generation system that simply deletes functionality. working with a simpler and more effectively focused search space, kali produces more correct patches and at least as many patches that produce correct outputs for the inputs in the validation test suite as prior genprog, rsrepair, and ae systems.",language models
,"Bruening, Derek; Chapin, John",2023-03-29T14:41:53Z,2023-03-29T14:41:53Z,2000-05,https://hdl.handle.net/1721.1/149297,MIT-LCS-TM-607,Systematic Testing of Multithreaded Programs,"We present a practical testing algorithm called ExitBlock that systematically and deterministically finds program errors resulting from unintended timing dependencies.  ExitBlock executes a program or a portion of a program on a given input multiple times, enumerating meaningful schedules in order to cover all program behaviors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,9,"systematic testing of multithreaded programs we present a practical testing algorithm called exitblock that systematically and deterministically finds program errors resulting from unintended timing dependencies.  exitblock executes a program or a portion of a program on a given input multiple times, enumerating meaningful schedules in order to cover all program behaviors.",high performance computing
Martin Rinard,"Shen, Jiasi; Rinard, Martin",2017-08-29T22:00:05Z,2017-08-29T22:00:05Z,2017-08-29,http://hdl.handle.net/1721.1/111067,MIT-CSAIL-TR-2017-012,Inference and Regeneration of Programs that Manipulate Relational Databases,"We present a new technique that infers models of programs that manipulate relational databases. This technique generates test databases and input commands, runs the program, then observes the resulting outputs and updated databases to infer the model. Because the technique works only with the externally observable inputs, outputs, and databases, it can infer the behavior of programs written in arbitrary languages using arbitrary coding styles and patterns. We also present a technique for automatically regenerating an implementation of the program based on the inferred model. The regenerator can produce a translated implementation in a different language and systematically include relevant security and error checks. We present results that illustrate the use of the technique to eliminate SQL injection vulnerabilities and the translation of applications from Java and Ruby on Rails to Python.",,14 p.,,,,Program Analysis and Compilation,,,,,,,,,,,,,2017-08-29T22:00:05Z,,,,,,,,,,,,,,,,2017,1046,"inference and regeneration of programs that manipulate relational databases we present a new technique that infers models of programs that manipulate relational databases. this technique generates test databases and input commands, runs the program, then observes the resulting outputs and updated databases to infer the model. because the technique works only with the externally observable inputs, outputs, and databases, it can infer the behavior of programs written in arbitrary languages using arbitrary coding styles and patterns. we also present a technique for automatically regenerating an implementation of the program based on the inferred model. the regenerator can produce a translated implementation in a different language and systematically include relevant security and error checks. we present results that illustrate the use of the technique to eliminate sql injection vulnerabilities and the translation of applications from java and ruby on rails to python.",language models
Brian Williams,"Leaute, Thomas",2006-04-28T18:22:21Z,2006-04-28T18:22:21Z,2006-04-28,http://hdl.handle.net/1721.1/32537,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Coordinating Agile Systems through the Model-based Execution of Temporal Plans,"Agile autonomous systems are emerging, such as unmanned aerial vehicles (UAVs), that must robustly perform tightly coordinated time-critical missions; for example, military surveillance or search-and-rescue scenarios. In the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness, in the context of space probes and planetary rovers, modeled as discrete systems. We address the challenge of extending plan execution to systems with continuous dynamics, such as air vehicles and robot manipulators, and that are controlled indirectly through the setting of continuous state variables.Systems with continuous dynamics are more challenging than discrete systems, because they require continuous, low-level control, and cannot be controlled by issuing simple sequences of discrete commands. Hence, manually controlling these systems (or plants) at a low level can become very costly, in terms of the number of human operators necessary to operate the plant. For example, in the case of a fleet of UAVs performing a search-and-rescue scenario, the traditional approach to controlling the UAVs involves providing series of close waypoints for each aircraft, which incurs a high workload for the human operators, when the fleet consists of a large number of vehicles.Our solution is a novel, model-based executive, called Sulu, that takes as input a qualitative state plan, specifying the desired evolution of the state of the system. This approach elevates the interaction between the human operator and the plant, to a more abstract level where the operator is able to coach the plant by qualitatively specifying the tasks, or activities, the plant must perform. These activities are described in a qualitative manner, because they specify regions in the plants state space in which the plant must be at a certain point in time. Time constraints are also described qualitatively, in the form of flexible temporal constraints between activities in the state plan. The design of low-level control inputs in order to meet this abstract goal specification is then delegated to the autonomous controller, hence decreasing the workload per human operator. This approach also provides robustness to the executive, by giving it room to adapt to disturbances and unforeseen events, while satisfying the qualitative constraints on the plant state, specified in the qualitative state plan.Sulu reasons on a model of the plant in order to dynamically generate near-optimal control sequences to fulfill the qualitative state plan. To achieve optimality and safety, Sulu plans into the future, framing the problem as a disjunctive linear programming problem. To achieve robustness to disturbances and maintain tractability, planning is folded within a receding horizon, continuous planning and execution framework. The key to performance is a problem reduction method based on constraint pruning. We benchmark performance using multi-UAV firefighting scenarios on a real-time, hardware-in-the-loop testbed.",MIT-CSAIL-TR-2006-029,155 p.; 41524644 bytes; 17696521 bytes,application/postscript; application/pdf,en_US,Model-based Programming; Qualitative Reasoning; Temporal Reasoning; Continuous Scheduling; Path Planning; Model Predictive Control,Model-based Embedded and Robotic Systems,,,,SM thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,511,"coordinating agile systems through the model-based execution of temporal plans agile autonomous systems are emerging, such as unmanned aerial vehicles (uavs), that must robustly perform tightly coordinated time-critical missions; for example, military surveillance or search-and-rescue scenarios. in the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness, in the context of space probes and planetary rovers, modeled as discrete systems. we address the challenge of extending plan execution to systems with continuous dynamics, such as air vehicles and robot manipulators, and that are controlled indirectly through the setting of continuous state variables.systems with continuous dynamics are more challenging than discrete systems, because they require continuous, low-level control, and cannot be controlled by issuing simple sequences of discrete commands. hence, manually controlling these systems (or plants) at a low level can become very costly, in terms of the number of human operators necessary to operate the plant. for example, in the case of a fleet of uavs performing a search-and-rescue scenario, the traditional approach to controlling the uavs involves providing series of close waypoints for each aircraft, which incurs a high workload for the human operators, when the fleet consists of a large number of vehicles.our solution is a novel, model-based executive, called sulu, that takes as input a qualitative state plan, specifying the desired evolution of the state of the system. this approach elevates the interaction between the human operator and the plant, to a more abstract level where the operator is able to coach the plant by qualitatively specifying the tasks, or activities, the plant must perform. these activities are described in a qualitative manner, because they specify regions in the plants state space in which the plant must be at a certain point in time. time constraints are also described qualitatively, in the form of flexible temporal constraints between activities in the state plan. the design of low-level control inputs in order to meet this abstract goal specification is then delegated to the autonomous controller, hence decreasing the workload per human operator. this approach also provides robustness to the executive, by giving it room to adapt to disturbances and unforeseen events, while satisfying the qualitative constraints on the plant state, specified in the qualitative state plan.sulu reasons on a model of the plant in order to dynamically generate near-optimal control sequences to fulfill the qualitative state plan. to achieve optimality and safety, sulu plans into the future, framing the problem as a disjunctive linear programming problem. to achieve robustness to disturbances and maintain tractability, planning is folded within a receding horizon, continuous planning and execution framework. the key to performance is a problem reduction method based on constraint pruning. we benchmark performance using multi-uav firefighting scenarios on a real-time, hardware-in-the-loop testbed.",language models
Saman Amarasinghe,"Kjolstad, Fredrik; Kamil, Shoaib; Ragan-Kelley, Jonathan; Levin, David I.W.; Sueda, Shinjiro; Chen, Desai; Vouga, Etienne; Kaufman, Danny M.; Kanwar, Gurtej; Matusik, Wojciech; Amarasinghe, Saman",2015-05-26T19:15:03Z,2015-05-26T19:15:03Z,2015-05-26,http://hdl.handle.net/1721.1/97075,MIT-CSAIL-TR-2015-017,Simit: A Language for Physical Simulation,"Using existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. The alternative is to prototype simulations in a high-level language like Matlab, thereby sacrificing performance. The Matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. However, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. Translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and the machine. High-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude. In this paper, we present Simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph, and as a set of global vectors, matrices and tensors depending on what is convenient at any given time. Simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. Using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. We demonstrate that Simit is easy to use: a Simit program is typically shorter than a Matlab program; that it is high-performance: a Simit program running sequentially on a CPU performs comparably to hand-optimized simulations; and that it is portable: Simit programs can be compiled for GPUs with no change to the program, delivering 5-25x speedups over our optimized CPU code.",,17 p.,,,"Graphs, Matrices, Tensors, Simulation",Computer Architecture,,,,,,,,,,,,,2015-05-26T19:15:04Z,,,,,,,,,,,,,,,,2015,1002,"simit: a language for physical simulation using existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. the alternative is to prototype simulations in a high-level language like matlab, thereby sacrificing performance. the matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. however, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and the machine. high-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude. in this paper, we present simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph, and as a set of global vectors, matrices and tensors depending on what is convenient at any given time. simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. we demonstrate that simit is easy to use: a simit program is typically shorter than a matlab program; that it is high-performance: a simit program running sequentially on a cpu performs comparably to hand-optimized simulations; and that it is portable: simit programs can be compiled for gpus with no change to the program, delivering 5-25x speedups over our optimized cpu code.",language models
,"Bhargava, Nikhil; Williams, Brian C.",2019-08-15T17:30:01Z,2019-08-15T17:30:01Z,2019-08,https://hdl.handle.net/1721.1/121993,,Faster Dynamic Controllability Checking in Temporal Networks with Integer Bounds,"Simple Temporal Networks with Uncertainty (STNUs) provide a useful formalism with which to reason about events and the temporal constraints that apply to them. STNUs are in particular notable because they facilitate reasoning over stochastic, or uncontrollable, actions and their corresponding durations. To evaluate the feasibility of a set of constraints associated with an STNU, one checks the network's \textit{dynamic controllability}, which determines whether an adaptive schedule can be constructed on-the-fly. Our work provides a dynamic controllability checker that is able to quickly refute the controllability of an STNU with integer bounds, such as those found in planning problems. Our work is faster than the existing best runtime for networks with integer bounds and executes in O(min(mn, m\sqrt{n}\log{N}) + km + k^2n + kn\log{n}). Our approach pre-processes the STNU using an existing O(n^3) dynamic controllability checking algorithm and provides tighter bounds on its runtime. This makes our work easily adaptable to other algorithms that rely on checking variants of dynamic controllability.",,,,en_US,temporal reasoning; scheduling,,,,,,,,,,,,,,,,,,Article,,,,,,,,,International Joint Conference in Artificial Intelligence,,,2019,1063,"faster dynamic controllability checking in temporal networks with integer bounds simple temporal networks with uncertainty (stnus) provide a useful formalism with which to reason about events and the temporal constraints that apply to them. stnus are in particular notable because they facilitate reasoning over stochastic, or uncontrollable, actions and their corresponding durations. to evaluate the feasibility of a set of constraints associated with an stnu, one checks the network's \textit{dynamic controllability}, which determines whether an adaptive schedule can be constructed on-the-fly. our work provides a dynamic controllability checker that is able to quickly refute the controllability of an stnu with integer bounds, such as those found in planning problems. our work is faster than the existing best runtime for networks with integer bounds and executes in o(min(mn, m\sqrt{n}\log{n}) + km + k^2n + kn\log{n}). our approach pre-processes the stnu using an existing o(n^3) dynamic controllability checking algorithm and provides tighter bounds on its runtime. this makes our work easily adaptable to other algorithms that rely on checking variants of dynamic controllability.",high performance computing
,"Makatura, Liane; Foshey, Michael; Wang, Bohan; Hhnlein, Felix; Ma, Pingchuan; Deng, Bolei; Tjandrasuwita, Megan; Spielberg, Andrew; Owens, Crystal Elaine; Chen, Peter Yichen; Zhao, Allan; Zhu, Amy; Norton, Wil J; Gu, Edward; Jacob, Joshua; Li, Yifei; Schulz, Adriana; Matusik, Wojciech",2023-07-27T17:28:58Z,2023-07-27T17:28:58Z,2023-07-27,https://hdl.handle.net/1721.1/151174,,How Can Large Language Models Help Humans in Design And Manufacturing?,,,,,en_US,"Large Language Models, GPT-4, computational design, computational fabrication, CAD, CAM, design for manufacturing, simulation, inverse design",,,Attribution-NonCommercial-ShareAlike 3.0 United States,http://creativecommons.org/licenses/by-nc-sa/3.0/us/,,,,,,,,,,,,,,Article,,,,,,,,,,,,2023,1074,how can large language models help humans in design and manufacturing? ,language models
Li-Shiuan Peh,"Sun, Chen; Chen, Chia-Hsin Owen; Kurian, George; Wei, Lan; Miller, Jason; Agarwal, Anant; Peh, Li-Shiuan; Stojanovic, Vladimir",2012-02-08T20:15:04Z,2012-02-08T20:15:04Z,2012-02-08,http://hdl.handle.net/1721.1/69050,MIT-CSAIL-TR-2012-004,DSENT - A Tool Connecting Emerging Photonics with Electronics for Opto-Electronic Networks-on-Chip Modeling,"With the advent of many-core chips that place substantial demand on the NoC, photonics has been investigated as a promising alternative to electrical NoCs. While numerous opto-electronic NoCs have been proposed, their evaluations tend to be based on fixed numbers for both photonic and electrical components, making it difficult to co-optimize. Through our own forays into opto-electronic NoC design, we observe that photonics and electronics are very much intertwined, reflecting a strong need for a NoC modeling tool that accurately models parameterized electronic and photonic components within a unified framework, capturing their interactions faithfully. In this paper, we present a tool, DSENT, for design space exploration of electrical and opto-electrical networks. We form a framework that constructs basic NoC building blocks from electrical and photonic technology parameters. To demonstrate potential use cases, we perform a network case study illustrating data-rate tradeoffs, a comparison with scaled electrical technology, and sensitivity to photonics parameters.",,8 p.,,,,Computer Architecture,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported,http://creativecommons.org/licenses/by-nc-sa/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2012,886,"dsent - a tool connecting emerging photonics with electronics for opto-electronic networks-on-chip modeling with the advent of many-core chips that place substantial demand on the noc, photonics has been investigated as a promising alternative to electrical nocs. while numerous opto-electronic nocs have been proposed, their evaluations tend to be based on fixed numbers for both photonic and electrical components, making it difficult to co-optimize. through our own forays into opto-electronic noc design, we observe that photonics and electronics are very much intertwined, reflecting a strong need for a noc modeling tool that accurately models parameterized electronic and photonic components within a unified framework, capturing their interactions faithfully. in this paper, we present a tool, dsent, for design space exploration of electrical and opto-electrical networks. we form a framework that constructs basic noc building blocks from electrical and photonic technology parameters. to demonstrate potential use cases, we perform a network case study illustrating data-rate tradeoffs, a comparison with scaled electrical technology, and sensitivity to photonics parameters.",language models
,"Feamster, Nick; Johari, Ramesh; Balakrishnan, Hari",2005-12-22T02:24:22Z,2005-12-22T02:24:22Z,2005-02-08,http://hdl.handle.net/1721.1/30522,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Stable Policy Routing with Provider Independence,"Thousands of competing autonomous systems (ASes) mustcooperate with each other to provide global Internet connectivity.These ASes encode various economic, business,and performance decisions in their routing policies. The currentinterdomain routing system enables ASes to express policyusing rankings that determine how each router in an ASorders the different routes to a destination, and filters thatdetermine which routes are hidden from each neighboringAS. Since the Internet is composed of many independent,competing networks, the interdomain routing system shouldallow providers to set their rankings independently, and tohave no constraints on allowed filters. This paper studiesrouting protocol stability under these constraints. We firstdemonstrate that certain rankings that are commonly usedin practice may not ensure routing stability. We then provethat, with ranking independence and unrestricted filtering,guaranteeing that the routing system will converge to a stablepath assignment essentially requires ASes to rank routesbased on AS-path lengths. Finally, we discuss the implicationsof these results for the future of interdomain routing.",MIT-CSAIL-TR-2005-009; MIT-LCS-TR-981,14 p.; 28950106 bytes; 1209509 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,405,"stable policy routing with provider independence thousands of competing autonomous systems (ases) mustcooperate with each other to provide global internet connectivity.these ases encode various economic, business,and performance decisions in their routing policies. the currentinterdomain routing system enables ases to express policyusing rankings that determine how each router in an asorders the different routes to a destination, and filters thatdetermine which routes are hidden from each neighboringas. since the internet is composed of many independent,competing networks, the interdomain routing system shouldallow providers to set their rankings independently, and tohave no constraints on allowed filters. this paper studiesrouting protocol stability under these constraints. we firstdemonstrate that certain rankings that are commonly usedin practice may not ensure routing stability. we then provethat, with ranking independence and unrestricted filtering,guaranteeing that the routing system will converge to a stablepath assignment essentially requires ases to rank routesbased on as-path lengths. finally, we discuss the implicationsof these results for the future of interdomain routing.",language models
Dina Katabi,"Katabi, Dina; Gollakota, Shyamnath",2008-04-08T19:15:18Z,2008-04-08T19:15:18Z,2008-04-08,http://hdl.handle.net/1721.1/41084,,ZigZag Decoding: Combating Hidden Terminals in Wireless Networks,"This paper presents ZigZag, an 802.11 receiver that combats hidden terminals. ZigZag exploits 802.11 retransmissions which, in the case of hidden terminals, cause successive collisions. Due to asynchrony, these collisions have different interference-free stretches at their start, which ZigZag uses to bootstrap its decoding.  ZigZag makes no changes to the 802.11 MAC and introduces no overhead when there are no collisions. But, when senders collide, ZigZag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. We build a prototype of ZigZag in GNU Radio. In a testbed of 14 USRP nodes, ZigZag reduces the average packet loss rate at hidden terminals from 82.3% to about 0.7%.",MIT-CSAIL-TR-2008-018,14 p.,,,Hidden Terminals; Software Radios; Wireless Networks,Networks & Mobile Systems,,,,,,,,,http://hdl.handle.net/1721.1/42842,http://hdl.handle.net/1721.1/42842,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,651,"zigzag decoding: combating hidden terminals in wireless networks this paper presents zigzag, an 802.11 receiver that combats hidden terminals. zigzag exploits 802.11 retransmissions which, in the case of hidden terminals, cause successive collisions. due to asynchrony, these collisions have different interference-free stretches at their start, which zigzag uses to bootstrap its decoding.  zigzag makes no changes to the 802.11 mac and introduces no overhead when there are no collisions. but, when senders collide, zigzag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. we build a prototype of zigzag in gnu radio. in a testbed of 14 usrp nodes, zigzag reduces the average packet loss rate at hidden terminals from 82.3% to about 0.7%.",high performance computing
,"Liskov, Barbara H.; Moh, Chuang-Hue; Richman, Steven; Shrira, Liuba; Chueng, Yin; Boyapati, Chandrasekhar",2023-03-29T15:35:50Z,2023-03-29T15:35:50Z,2002-06,https://hdl.handle.net/1721.1/149953,MIT-LCS-TR-851,Safe Lazy Software Upgrades in Object-Oriented Databases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,132,safe lazy software upgrades in object-oriented databases ,object recognition/detection
Tomaso Poggio,"Isik, Leyla; Meyers, Ethan M.; Leibo, Joel Z.; Poggio, Tomaso",2012-04-26T18:15:04Z,2012-04-26T18:15:04Z,2012-04-20,http://hdl.handle.net/1721.1/70170,MIT-CSAIL-TR-2012-010; CBCL-307,Preliminary MEG decoding results,"Decoding analysis has been applied to electrophysiology and fMRI data to study the visual system, however, this method has only been applied to MEG visual data in a few instances. Here we use the Neural Decoding Toolbox for Matlab to show that it is possible to decode visual stimuli based on MEG data.",,5 p.,,,Vision; Decoding; MEG,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2012,890,"preliminary meg decoding results decoding analysis has been applied to electrophysiology and fmri data to study the visual system, however, this method has only been applied to meg visual data in a few instances. here we use the neural decoding toolbox for matlab to show that it is possible to decode visual stimuli based on meg data.",high performance computing
Nancy Lynch,"Fan, Rui; Lynch, Nancy",2008-07-28T13:30:11Z,2008-07-28T13:30:11Z,2006-07-23,http://hdl.handle.net/1721.1/41890,,An $\Omega(n \log n)$ Lower Bound on the Cost of Mutual Exclusion,"We prove an $\Omega(n \log n)$ lower bound on the number ofnon-busywaiting memory accesses by any deterministic algorithm solving$n$ process mutual exclusion that communicates via shared registers.The cost of the algorithm is measured in the \emph{state change} costmodel, a variation of the cache coherent model. Our bound is tight inthis model. We introduce a novel information theoretic prooftechnique. We first establish a lower bound on the information neededby processes to solve mutual exclusion. Then we relate the amount ofinformation processes can acquire through shared memory accesses tothe cost they incur. We believe our proof technique is flexible andintuitive, and may be applied to a variety of other problems andsystem models.",MIT-CSAIL-TR-2008-047,14 p.,,,Mutual exclusion; Time complexity; Lower bound techniques,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2006,532,"an $\omega(n \log n)$ lower bound on the cost of mutual exclusion we prove an $\omega(n \log n)$ lower bound on the number ofnon-busywaiting memory accesses by any deterministic algorithm solving$n$ process mutual exclusion that communicates via shared registers.the cost of the algorithm is measured in the \emph{state change} costmodel, a variation of the cache coherent model. our bound is tight inthis model. we introduce a novel information theoretic prooftechnique. we first establish a lower bound on the information neededby processes to solve mutual exclusion. then we relate the amount ofinformation processes can acquire through shared memory accesses tothe cost they incur. we believe our proof technique is flexible andintuitive, and may be applied to a variety of other problems andsystem models.",high performance computing
Barbara Liskov,"Liskov, Barbara; Cowling, James",2012-07-23T19:15:02Z,2012-07-23T19:15:02Z,2012-07-23,http://hdl.handle.net/1721.1/71763,MIT-CSAIL-TR-2012-021,Viewstamped Replication Revisited,"This paper presents an updated version of Viewstamped Replication, a replication technique that handles failures in which nodes crash. It describes how client requests are handled, how the group reorganizes when a replica fails, and how a failed replica is able to rejoin the group. The paper also describes a number of important optimizations and presents a protocol for handling reconfigurations that can change both the group membership and the number of failures the group is able to handle.",,14 p.,,,VR; state machine replication; consensus; fault-tolerance; agreement,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,900,"viewstamped replication revisited this paper presents an updated version of viewstamped replication, a replication technique that handles failures in which nodes crash. it describes how client requests are handled, how the group reorganizes when a replica fails, and how a failed replica is able to rejoin the group. the paper also describes a number of important optimizations and presents a protocol for handling reconfigurations that can change both the group membership and the number of failures the group is able to handle.",language models
Dina Katabi,"Katabi, Dina; Rahul, Hariharan; Jakubczak, Szymon",2009-02-09T16:30:14Z,2009-02-09T16:30:14Z,2009-02-07,http://hdl.handle.net/1721.1/44585,MIT-CSAIL-TR-2009-005,SoftCast: One Video to Serve All Wireless Receivers,"The main challenge in wireless video multicast is to scalably serve multiple receivers who have different channel characteristics. Current wireless transmission schemes, however, cannot support smooth degradation. Specifically, each packet is transmitted at a particular bitrate and is decodable only by receivers that support the chosen bitrate. Broadcasting a video stream to all receivers requires transmitting at the lowest bitrate, and hence reduces everyone to the performance of the worst receiver in the multicast group.This paper introduces SoftCast, an alternative design for wireless video multicast, in which a sender broadcasts a single stream and each receiver watches a video quality that matches its channel quality. SoftCast achieves this by making the magnitude of the transmitted signal proportional to the pixel value. Hence, channel noise directly translates to a small perturbation in pixel values, allowing graceful degradation with increasing noise. SoftCast introduces a novel power allocation scheme that allows the transmission of real-valued video signals in a compact and resilient manner. We implement SoftCast in the WARP radio platform. Our results show that SoftCast improves the average video quality across multicast receivers by 3-7dB over the current approach. Further, it stays competitive with the current approach even for regular unicast.",,14 p.,,,Wireless networks; Multicast; Wireless video; Video coding,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,708,"softcast: one video to serve all wireless receivers the main challenge in wireless video multicast is to scalably serve multiple receivers who have different channel characteristics. current wireless transmission schemes, however, cannot support smooth degradation. specifically, each packet is transmitted at a particular bitrate and is decodable only by receivers that support the chosen bitrate. broadcasting a video stream to all receivers requires transmitting at the lowest bitrate, and hence reduces everyone to the performance of the worst receiver in the multicast group.this paper introduces softcast, an alternative design for wireless video multicast, in which a sender broadcasts a single stream and each receiver watches a video quality that matches its channel quality. softcast achieves this by making the magnitude of the transmitted signal proportional to the pixel value. hence, channel noise directly translates to a small perturbation in pixel values, allowing graceful degradation with increasing noise. softcast introduces a novel power allocation scheme that allows the transmission of real-valued video signals in a compact and resilient manner. we implement softcast in the warp radio platform. our results show that softcast improves the average video quality across multicast receivers by 3-7db over the current approach. further, it stays competitive with the current approach even for regular unicast.",language models
,"Shen, Jiasi; Rinard, Martin",2021-11-15T22:40:41Z,2021-11-15T22:40:41Z,2021-11-15,https://hdl.handle.net/1721.1/138144,,Active Loop Detection for Applications that Access Databases,"We present Shear, a new system that observes and manipulates the interaction between an application and its surrounding environment to learn a model of the behavior of the application. Shear implements active loop detection to infer the loop structures in the application. This technique repeatedly presents the application with the same input, altering the program's interaction with the environment at precisely chosen execution points to elicit different program behaviors depending on the loop structure in the application. The ability to alter interactions between the application and the environment enables Shear to infer a broader range of loop structures otherwise undetectable given only the ability to observe application behavior. Active loop detection therefore enables Shear to infer a broader range of loop structures than previous approaches.",,,,en_US,,,,,,,,,,,,,,,,,,,Technical Report,,,,,,,,,,,,2021,1069,"active loop detection for applications that access databases we present shear, a new system that observes and manipulates the interaction between an application and its surrounding environment to learn a model of the behavior of the application. shear implements active loop detection to infer the loop structures in the application. this technique repeatedly presents the application with the same input, altering the program's interaction with the environment at precisely chosen execution points to elicit different program behaviors depending on the loop structure in the application. the ability to alter interactions between the application and the environment enables shear to infer a broader range of loop structures otherwise undetectable given only the ability to observe application behavior. active loop detection therefore enables shear to infer a broader range of loop structures than previous approaches.",high performance computing
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2015-12-21T19:00:09Z,2015-12-21T19:00:09Z,2015-12-19,http://hdl.handle.net/1721.1/100464,MIT-CSAIL-TR-2015-033,Cache Calculus: Modeling Caches through Differential Equations,"Caches are critical to performance, yet their behavior is hard to understand and model. In particular, prior work does not provide closed-form solutions of cache performance, i.e. simple expressions for the miss rate of a specific access pattern. Existing cache models instead use numerical methods that, unlike closed-form solutions, are computationally expensive and yield limited insight. We present cache calculus, a technique that models cache behavior as a system of ordinary differential equations, letting standard calculus techniques find simple and accurate solutions of cache performance for common access patterns.",,4 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:10Z,,,,,,,,,,,,,,,,2015,1015,"cache calculus: modeling caches through differential equations caches are critical to performance, yet their behavior is hard to understand and model. in particular, prior work does not provide closed-form solutions of cache performance, i.e. simple expressions for the miss rate of a specific access pattern. existing cache models instead use numerical methods that, unlike closed-form solutions, are computationally expensive and yield limited insight. we present cache calculus, a technique that models cache behavior as a system of ordinary differential equations, letting standard calculus techniques find simple and accurate solutions of cache performance for common access patterns.",high performance computing
Dina Katabi,"Woo, Grace Rusi; Kheradpour, Pouya; Katabi, Dina",2007-05-29T18:21:50Z,2007-05-29T18:21:50Z,2007-05-29,http://hdl.handle.net/1721.1/37587,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Beyond the Bits: Cooperative Packet Recovery Using Physical Layer Information,"Wireless networks can suffer from high packet loss rates.  This paper shows that the loss rate can be significantly reduced by exposing information readily available at the physical layer. We make the physical layer convey an estimate of its confidence that a particular bit is ``0'' or ``1'' to the higher layers.  When used with cooperative design, this information dramatically improves the throughput of the wireless network. Access points that hear the same transmission combine their information to correct bits in a packet with minimal overhead. Similarly, a receiver may combine multiple erroneous transmissions to recover a correct packet.  We analytically prove that our approach minimizes the errors in packet recovery.  We also experimentally demonstrate its benefits using a testbed of GNU software radios. The results show that our approach can reduce loss rate by up to 10x in comparison with the current approach, and significantly outperforms prior cooperation proposals.",MIT-CSAIL-TR-2007-027,12 p.,,,,Networks & Mobile Systems,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2007,596,"beyond the bits: cooperative packet recovery using physical layer information wireless networks can suffer from high packet loss rates.  this paper shows that the loss rate can be significantly reduced by exposing information readily available at the physical layer. we make the physical layer convey an estimate of its confidence that a particular bit is ``0'' or ``1'' to the higher layers.  when used with cooperative design, this information dramatically improves the throughput of the wireless network. access points that hear the same transmission combine their information to correct bits in a packet with minimal overhead. similarly, a receiver may combine multiple erroneous transmissions to recover a correct packet.  we analytically prove that our approach minimizes the errors in packet recovery.  we also experimentally demonstrate its benefits using a testbed of gnu software radios. the results show that our approach can reduce loss rate by up to 10x in comparison with the current approach, and significantly outperforms prior cooperation proposals.",high performance computing
,"Marnette, Bruno; Kuncak, Viktor; Rinard, Martin",2005-12-22T02:33:49Z,2005-12-22T02:33:49Z,2005-08-03,http://hdl.handle.net/1721.1/30561,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Algorithms and Complexity for Sets with Cardinality Constraints,"Typestate systems ensure many desirable properties of imperativeprograms, including initialization of object fields and correct use ofstateful library interfaces.  Abstract sets with cardinalityconstraints naturally generalize typestate properties: relationshipsbetween the typestates of objects can be expressed as subset anddisjointness relations on sets, and elements of sets can berepresented as sets of cardinality one.  In addition, sets withcardinality constraints provide a natural language for specifyingoperations and invariants of data structures.Motivated by these program analysis applications, thispaper presents new algorithms and new complexity results forconstraints on sets and their cardinalities.  We studyseveral classes of constraints and demonstrate a trade-offbetween their expressive power and their complexity.Our first result concerns a quantifier-free fragment of BooleanAlgebra with Presburger Arithmetic.  We give a nondeterministicpolynomial-time algorithm for reducing the satisfiability of sets withsymbolic cardinalities to constraints on constant cardinalities, andgive a polynomial-space algorithm for the resulting problem.  The bestpreviously existing algorithm runs in exponential space andnondeterministic exponential time.In a quest for more efficient fragments, we identify severalsubclasses of sets with cardinality constraints whose satisfiabilityis NP-hard.  Finally, we identify a class of constraints that haspolynomial-time satisfiability and entailment problems and can serveas a foundation for efficient program analysis.  We give a system ofrewriting rules for enforcing certain consistency properties of theseconstraints and show how to extract complete information fromconstraints in normal form.  This result implies the soundness andcompleteness of our algorithms.",MIT-CSAIL-TR-2005-050; MIT-LCS-TR-997,19 p.; 33171425 bytes; 1663929 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,449,"on algorithms and complexity for sets with cardinality constraints typestate systems ensure many desirable properties of imperativeprograms, including initialization of object fields and correct use ofstateful library interfaces.  abstract sets with cardinalityconstraints naturally generalize typestate properties: relationshipsbetween the typestates of objects can be expressed as subset anddisjointness relations on sets, and elements of sets can berepresented as sets of cardinality one.  in addition, sets withcardinality constraints provide a natural language for specifyingoperations and invariants of data structures.motivated by these program analysis applications, thispaper presents new algorithms and new complexity results forconstraints on sets and their cardinalities.  we studyseveral classes of constraints and demonstrate a trade-offbetween their expressive power and their complexity.our first result concerns a quantifier-free fragment of booleanalgebra with presburger arithmetic.  we give a nondeterministicpolynomial-time algorithm for reducing the satisfiability of sets withsymbolic cardinalities to constraints on constant cardinalities, andgive a polynomial-space algorithm for the resulting problem.  the bestpreviously existing algorithm runs in exponential space andnondeterministic exponential time.in a quest for more efficient fragments, we identify severalsubclasses of sets with cardinality constraints whose satisfiabilityis np-hard.  finally, we identify a class of constraints that haspolynomial-time satisfiability and entailment problems and can serveas a foundation for efficient program analysis.  we give a system ofrewriting rules for enforcing certain consistency properties of theseconstraints and show how to extract complete information fromconstraints in normal form.  this result implies the soundness andcompleteness of our algorithms.",language models
Silvio Micali,"Chiesa, Alessandro; Micali, Silvio; Zhu, Zeyuan Allen",2013-12-03T20:45:05Z,2013-12-03T20:45:05Z,2013-12-03,http://hdl.handle.net/1721.1/82632,MIT-CSAIL-TR-2013-029,Bridging Utility Maximization and Regret Minimization,"We relate the strategies obtained by (1) utility maximizers who use regret to refine their set of undominated strategies, and (2) regret minimizers who use weak domination to refine their sets of regret-minimizing strategies.",,9 p.,,,,Theory of Computation,,,,,,,,,,,,,2013-12-03T20:45:05Z,,,,,,,,,,,,,,,,2013,946,"bridging utility maximization and regret minimization we relate the strategies obtained by (1) utility maximizers who use regret to refine their set of undominated strategies, and (2) regret minimizers who use weak domination to refine their sets of regret-minimizing strategies.",language models
Hari Balakrishnan,"Chen, Tiffany Yu-Han; Sivaraman, Anirudh; Das, Somak; Ravindranath, Lenin; Balakrishnan, Hari",2015-09-25T15:45:09Z,2015-09-25T15:45:09Z,2015-09-24,http://hdl.handle.net/1721.1/98905,MIT-CSAIL-TR-2015-029,Designing a Context-Sensitive Context Detection Service for Mobile Devices,"This paper describes the design, implementation, and evaluation of Amoeba, a context-sensitive context detection service for mobile devices. Amoeba exports an API that allows a client to express interest in one or more context types (activity, indoor/outdoor, and entry/exit to/from named regions), subscribe to specific modes within each context (e.g., ""walking"" or ""running"", but no other activity), and specify a response latency (i.e., how often the client is notified). Each context has a detector that returns its estimate of the mode. The detectors take both the desired subscriptions and the current context detection into account, adjusting both the types of sensors and the sampling rates to achieve high accuracy and low energy consumption. We have implemented Amoeba on Android. Experiments with Amoeba on 45+ hours of data show that our activity detector achieves an accuracy between 92% and 99%, outperforming previous proposals like UCLA* (59%), EEMSS (82%) and SociableSense (72%), while consuming 4 to 6 less energy.",,12 p.,,,context detection; context sensing; activity recognition; indoor detection; geofence; sensors; mobile sensing; energy,Networks & Mobile Systems,,,,,,,,,,,,,2015-09-25T15:45:09Z,,,,,,,,,,,,,,,,2015,1009,"designing a context-sensitive context detection service for mobile devices this paper describes the design, implementation, and evaluation of amoeba, a context-sensitive context detection service for mobile devices. amoeba exports an api that allows a client to express interest in one or more context types (activity, indoor/outdoor, and entry/exit to/from named regions), subscribe to specific modes within each context (e.g., ""walking"" or ""running"", but no other activity), and specify a response latency (i.e., how often the client is notified). each context has a detector that returns its estimate of the mode. the detectors take both the desired subscriptions and the current context detection into account, adjusting both the types of sensors and the sampling rates to achieve high accuracy and low energy consumption. we have implemented amoeba on android. experiments with amoeba on 45+ hours of data show that our activity detector achieves an accuracy between 92% and 99%, outperforming previous proposals like ucla* (59%), eemss (82%) and sociablesense (72%), while consuming 4 to 6 less energy.",high performance computing
,"Salcianu, Alexandru; Rinard, Martin",2005-12-22T01:31:05Z,2005-12-22T01:31:05Z,2004-05-17,http://hdl.handle.net/1721.1/30470,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Combined Pointer and Purity Analysis for Java Programs,"We present a new method purity analysis for Java programs.A method is pure if it does not mutate any location that exists in the program state right before method invocation.Our analysis is built on top of a combined pointer and escape analysis for Java programs and is capable of determining that methods are pure even when the methods do heap mutation, provided that the mutation affects only objects created after the beginning of the method. Because our analysis extracts a precise representation of the region of the heap that each method may access, it is able to provide useful information even for methods with externally visible side effects. In particular, it can recognize read-only parameters (a parameter is read-only if the method does not mutate any objects transitively reachable from the parameter) and safe parameters (a parameter is safe if it is read-only and the method does not create any new externally visible paths in the heap to objects transitively reachable from the parameter). The analysis can also generate regular expressions that characterize the externally visible heap locations that the method mutates.We have implemented our analysis and used it to analyze several data structure implementations. Our results show that our analysis effectively recognize a variety of pure methods, including pure methods that allocate and mutate complex auxiliary data structures. Even if the methods are not pure, our analysis can provide information which may enable developers to usefully bound the potential side effects of the method.",MIT-CSAIL-TR-2004-030; MIT-LCS-TR-949,16 p.; 28889089 bytes; 1207932 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,331,"a combined pointer and purity analysis for java programs we present a new method purity analysis for java programs.a method is pure if it does not mutate any location that exists in the program state right before method invocation.our analysis is built on top of a combined pointer and escape analysis for java programs and is capable of determining that methods are pure even when the methods do heap mutation, provided that the mutation affects only objects created after the beginning of the method. because our analysis extracts a precise representation of the region of the heap that each method may access, it is able to provide useful information even for methods with externally visible side effects. in particular, it can recognize read-only parameters (a parameter is read-only if the method does not mutate any objects transitively reachable from the parameter) and safe parameters (a parameter is safe if it is read-only and the method does not create any new externally visible paths in the heap to objects transitively reachable from the parameter). the analysis can also generate regular expressions that characterize the externally visible heap locations that the method mutates.we have implemented our analysis and used it to analyze several data structure implementations. our results show that our analysis effectively recognize a variety of pure methods, including pure methods that allocate and mutate complex auxiliary data structures. even if the methods are not pure, our analysis can provide information which may enable developers to usefully bound the potential side effects of the method.",object recognition/detection
,"Ajmani, Sameer",2005-12-19T23:30:00Z,2005-12-19T23:30:00Z,2005-10-06,http://hdl.handle.net/1721.1/30418,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems (PhD thesis),"Upgrading the software of long-lived, highly-available distributedsystems is difficult.  It is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  Instead, upgrades may happengradually, and there may be long periods of time when different nodesare running different software versions and need to communicate usingincompatible protocols.  We present a methodology and infrastructurethat address these challenges and make it possible to upgradedistributed systems automatically while limiting service disruption.Our methodology defines how to enable nodes to interoperate acrossversions, how to preserve the state of a system across upgrades, and howto schedule an upgrade so as to limit service disruption.  The approachis modular: defining an upgrade requires understanding only the newsoftware and the version it replaces.The upgrade infrastructure is a generic platform for distributing andinstalling software while enabling nodes to interoperate acrossversions.  The infrastructure requires no access to the system sourcecode and is transparent: node software is unaware that differentversions even exist.  We have implemented a prototype of theinfrastructure called Upstart that intercepts socket communication usinga dynamically-linked C++ library.  Experiments show that Upstart has lowoverhead and works well for both local-area and Internet systems.",MIT-CSAIL-TR-2005-061; MIT-LCS-TR-1004,164 p.; 135378118 bytes; 5540633 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,460,"automatic software upgrades for distributed systems (phd thesis) upgrading the software of long-lived, highly-available distributedsystems is difficult.  it is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  instead, upgrades may happengradually, and there may be long periods of time when different nodesare running different software versions and need to communicate usingincompatible protocols.  we present a methodology and infrastructurethat address these challenges and make it possible to upgradedistributed systems automatically while limiting service disruption.our methodology defines how to enable nodes to interoperate acrossversions, how to preserve the state of a system across upgrades, and howto schedule an upgrade so as to limit service disruption.  the approachis modular: defining an upgrade requires understanding only the newsoftware and the version it replaces.the upgrade infrastructure is a generic platform for distributing andinstalling software while enabling nodes to interoperate acrossversions.  the infrastructure requires no access to the system sourcecode and is transparent: node software is unaware that differentversions even exist.  we have implemented a prototype of theinfrastructure called upstart that intercepts socket communication usinga dynamically-linked c++ library.  experiments show that upstart has lowoverhead and works well for both local-area and internet systems.",language models
Silvio Micali,"Chen, Jing; Micali, Silvio; Pass, Rafael",2014-06-10T21:00:02Z,2014-06-10T21:00:02Z,2014-06-09,http://hdl.handle.net/1721.1/87727,MIT-CSAIL-TR-2014-013,Possibilistic Beliefs and Higher-Level Rationality,"We consider rationality and rationalizability for normal-form games of incomplete information in which the players have possibilistic beliefs about their opponents. In this setting, we prove that the strategies compatible with the players being level-k rational coincide with the strategies surviving a natural k-step iterated elimination procedure. We view the latter strategies as the (level-k) rationalizable ones in our possibilistic setting.",,10 p.,,,,Theory of Computation,,,,,,,,,,,,,2014-06-10T21:00:03Z,,,,,,,,,,,,,,,,2014,962,"possibilistic beliefs and higher-level rationality we consider rationality and rationalizability for normal-form games of incomplete information in which the players have possibilistic beliefs about their opponents. in this setting, we prove that the strategies compatible with the players being level-k rational coincide with the strategies surviving a natural k-step iterated elimination procedure. we view the latter strategies as the (level-k) rationalizable ones in our possibilistic setting.",language models
,"Su, Sara L.; Durand, Fredo; Agrawala, Maneesh",2005-12-22T02:28:16Z,2005-12-22T02:28:16Z,2005-04-12,http://hdl.handle.net/1721.1/30537,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,De-Emphasis of Distracting Image Regions Using Texture Power Maps,"A major obstacle in photography is the presence of distracting elements that pull attention away from the main subject and clutter the composition. In this article, we present a new image-processing technique that reduces the salience of distracting regions. It is motivated by computational models of attention that predict that texture variation influences bottom-up attention mechanisms. Our method reduces the spatial variation of texture using power maps, high-order features describing local frequency content in an image. We show how modification of power maps results in  powerful image de-emphasis. We validate our results using a user search experiment and eye tracking data.",MIT-CSAIL-TR-2005-025; MIT-LCS-TR-987,12 p.; 24844567 bytes; 1742248 bytes,application/postscript; application/pdf,en_US,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,421,"de-emphasis of distracting image regions using texture power maps a major obstacle in photography is the presence of distracting elements that pull attention away from the main subject and clutter the composition. in this article, we present a new image-processing technique that reduces the salience of distracting regions. it is motivated by computational models of attention that predict that texture variation influences bottom-up attention mechanisms. our method reduces the spatial variation of texture using power maps, high-order features describing local frequency content in an image. we show how modification of power maps results in  powerful image de-emphasis. we validate our results using a user search experiment and eye tracking data.",image classification
Saman Amarasinghe,"Thies, William; Urbanski, John Paul; Thorsen, Todd; Amarasinghe, Saman",2006-05-05T19:42:11Z,2006-05-05T19:42:11Z,2006-05-05,http://hdl.handle.net/1721.1/32543,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Abstraction Layers for Scalable Microfluidic Biocomputers (Extended Version),"Microfluidic devices are emerging as an attractive technology for automatically orchestrating the reactions needed in a biological computer.  Thousands of microfluidic primitives have already been integrated on a single chip, and recent trends indicate that the hardware complexity is increasing at rates comparable to Moore's Law.  As in the case of silicon, it will be critical to develop abstraction layers--such as programming languages and Instruction Set Architectures (ISAs)--that decouple software development from changes in the underlying device technology.Towards this end, this paper presents BioStream, a portable language for describing biology protocols, and the Fluidic ISA, a stable interface for microfluidic chip designers.  A novel algorithm translates microfluidic mixing operations from the BioStream layer to the Fluidic ISA.  To demonstrate the benefits of these abstraction layers, we build two microfluidic chips that can both execute BioStream code despite significant differences at the device level.  We consider this to be an important step towards building scalable biocomputers.",MIT-CSAIL-TR-2006-034,14 p.; 6101702 bytes; 453129 bytes,application/postscript; application/pdf,en_US,biological computing; DNA computing; microfluidics,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,516,"abstraction layers for scalable microfluidic biocomputers (extended version) microfluidic devices are emerging as an attractive technology for automatically orchestrating the reactions needed in a biological computer.  thousands of microfluidic primitives have already been integrated on a single chip, and recent trends indicate that the hardware complexity is increasing at rates comparable to moore's law.  as in the case of silicon, it will be critical to develop abstraction layers--such as programming languages and instruction set architectures (isas)--that decouple software development from changes in the underlying device technology.towards this end, this paper presents biostream, a portable language for describing biology protocols, and the fluidic isa, a stable interface for microfluidic chip designers.  a novel algorithm translates microfluidic mixing operations from the biostream layer to the fluidic isa.  to demonstrate the benefits of these abstraction layers, we build two microfluidic chips that can both execute biostream code despite significant differences at the device level.  we consider this to be an important step towards building scalable biocomputers.",language models
Brian Williams,"Effinger, Robert",2018-01-30T23:46:14Z,2018-01-30T23:46:14Z,2012-02-02,http://hdl.handle.net/1721.1/113366,MIT-CSAIL-TR-2018-006,Risk-minimizing program execution in robotic domains,"In this thesis, we argue that autonomous robots operating in hostile and uncertain environments can improve robustness by computing and reasoning explicitly about risk. Autonomous robots with a keen sensitivity to risk can be trusted with critical missions, such as exploring deep space and assisting on the battlefield. We introduce a novel, risk-minimizing approach to program execution that utilizes program flexibility and estimation of risk in order to make runtime decisions that minimize the probability of program failure. Our risk-minimizing executive, called Murphy, utilizes two forms of program flexibility, 1) flexible scheduling of activity timing, and 2) redundant choice between subprocedures, in order to minimize two forms of program risk, 1) exceptions arising from activity failures, and 2) exceptions arising from timing constraint violations in a program. Murphy takes two inputs, a program written in a nondeterministic variant of the Reactive Model-based Programming Language (RMPL) and a set of stochastic activity failure models, one for each activity in a program, and computes two outputs, a risk-minimizing decision policy and value function. The decision policy informs Murphy which decisions to make at runtime in order to minimize risk, while the value function quantifies risk. In order to execute with low latency, Murphy computes the decision policy and value function offline, as a compilation step prior to program execution. In this thesis, we develop three approaches to RMPL program execution. First, we develop an approach that is guaranteed to minimize risk. For this approach, we reason probabilistically about risk by framing program execution as a Markov Decision Process (MDP). Next, we develop an approach that avoids risk altogether. For this approach, we frame program execution as a novel form of constraint-based temporal reasoning. Finally, we develop an execution approach that trades optimality in risk avoidance for tractability. For this approach, we leverage prior work in hierarchical decomposition of MDPs in order to mitigate complexity. We benchmark the tractability of each approach on a set of representative RMPL programs, and we demonstrate the applicability of the approach on a humanoid robot simulator.",,161 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,Brian Williams; Model-based Embedded and Robotic Systems,,,,,,,2018-01-30T23:46:14Z,,,,,,,,,,,,,,,,2012,884,"risk-minimizing program execution in robotic domains in this thesis, we argue that autonomous robots operating in hostile and uncertain environments can improve robustness by computing and reasoning explicitly about risk. autonomous robots with a keen sensitivity to risk can be trusted with critical missions, such as exploring deep space and assisting on the battlefield. we introduce a novel, risk-minimizing approach to program execution that utilizes program flexibility and estimation of risk in order to make runtime decisions that minimize the probability of program failure. our risk-minimizing executive, called murphy, utilizes two forms of program flexibility, 1) flexible scheduling of activity timing, and 2) redundant choice between subprocedures, in order to minimize two forms of program risk, 1) exceptions arising from activity failures, and 2) exceptions arising from timing constraint violations in a program. murphy takes two inputs, a program written in a nondeterministic variant of the reactive model-based programming language (rmpl) and a set of stochastic activity failure models, one for each activity in a program, and computes two outputs, a risk-minimizing decision policy and value function. the decision policy informs murphy which decisions to make at runtime in order to minimize risk, while the value function quantifies risk. in order to execute with low latency, murphy computes the decision policy and value function offline, as a compilation step prior to program execution. in this thesis, we develop three approaches to rmpl program execution. first, we develop an approach that is guaranteed to minimize risk. for this approach, we reason probabilistically about risk by framing program execution as a markov decision process (mdp). next, we develop an approach that avoids risk altogether. for this approach, we frame program execution as a novel form of constraint-based temporal reasoning. finally, we develop an execution approach that trades optimality in risk avoidance for tractability. for this approach, we leverage prior work in hierarchical decomposition of mdps in order to mitigate complexity. we benchmark the tractability of each approach on a set of representative rmpl programs, and we demonstrate the applicability of the approach on a humanoid robot simulator.",robotics
,"Riesenhuber, Maximilian; Poggio, Tomaso",2004-10-20T21:03:32Z,2004-10-20T21:03:32Z,2000-08-07,http://hdl.handle.net/1721.1/7231,AIM-1695; CBCL-190,Computational Models of Object Recognition in Cortex: A Review,"Understanding how biological visual systems perform object recognition is one of the ultimate goals in computational neuroscience. Among the biological models of recognition the main distinctions are between feedforward and feedback and between object-centered and view-centered. From a computational viewpoint the different recognition tasks - for instance categorization and identification - are very similar, representing different trade-offs between specificity and invariance. Thus the different tasks do not strictly require different classes of models. The focus of the review is on feedforward, view-based models that are supported by psychophysical and physiological data.",AIM-1695; CBCL-190,683319 bytes; 124521 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,20,"computational models of object recognition in cortex: a review understanding how biological visual systems perform object recognition is one of the ultimate goals in computational neuroscience. among the biological models of recognition the main distinctions are between feedforward and feedback and between object-centered and view-centered. from a computational viewpoint the different recognition tasks - for instance categorization and identification - are very similar, representing different trade-offs between specificity and invariance. thus the different tasks do not strictly require different classes of models. the focus of the review is on feedforward, view-based models that are supported by psychophysical and physiological data.",object recognition/detection
Manolis Kellis,"Feizi, Soheil; Makhdoumi, Ali; Duffy, Ken; Kellis, Manolis; Medard, Muriel",2015-09-23T19:00:07Z,2015-09-23T19:00:07Z,2015-09-21,http://hdl.handle.net/1721.1/98878,MIT-CSAIL-TR-2015-028,Network Maximal Correlation,"Identifying nonlinear relationships in large datasets is a daunting task particularly when the form of the nonlinearity is unknown. Here, we introduce Network Maximal Correlation (NMC) as a fundamental measure to capture nonlinear associations in networks without the knowledge of underlying nonlinearity shapes. NMC infers, possibly nonlinear, transformations of variables with zero means and unit variances by maximizing total nonlinear correlation over the underlying network. For the case of having two variables, NMC is equivalent to the standard Maximal Correlation. We characterize a solution of the NMC optimization using geometric properties of Hilbert spaces for both discrete and jointly Gaussian variables. For discrete random variables, we show that the NMC optimization is an instance of the Maximum Correlation Problem and provide necessary conditions for its global optimal solution. Moreover, we propose an efficient algorithm based on Alternating Conditional Expectation (ACE) which converges to a local NMC optimum. For this algorithm, we provide guidelines for choosing appropriate starting points to jump out of local maximizers. We also propose a distributed algorithm to compute a 1-$\epsilon$ approximation of the NMC value for large and dense graphs using graph partitioning. For jointly Gaussian variables, under some conditions, we show that the NMC optimization can be simplified to a Max-Cut problem, where we provide conditions under which an NMC solution can be computed exactly. Under some general conditions, we show that NMC can infer the underlying graphical model for functions of latent jointly Gaussian variables. These functions are unknown, bijective, and can be nonlinear. This result broadens the family of continuous distributions whose graphical models can be characterized efficiently. We illustrate the robustness of NMC in real world applications by showing its continuity with respect to small perturbations of joint distributions. We also show that sample NMC (NMC computed using empirical distributions) converges exponentially fast to the true NMC value. Finally, we apply NMC to different cancer datasets including breast, kidney and liver cancers, and show that NMC infers gene modules that are significantly associated with survival times of individuals while they are not detected using linear association measures.",,48 p.,,,,Computational Biology (Kellis),,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-09-23T19:00:07Z,,,,,,,,,,,,,,,,2015,1008,"network maximal correlation identifying nonlinear relationships in large datasets is a daunting task particularly when the form of the nonlinearity is unknown. here, we introduce network maximal correlation (nmc) as a fundamental measure to capture nonlinear associations in networks without the knowledge of underlying nonlinearity shapes. nmc infers, possibly nonlinear, transformations of variables with zero means and unit variances by maximizing total nonlinear correlation over the underlying network. for the case of having two variables, nmc is equivalent to the standard maximal correlation. we characterize a solution of the nmc optimization using geometric properties of hilbert spaces for both discrete and jointly gaussian variables. for discrete random variables, we show that the nmc optimization is an instance of the maximum correlation problem and provide necessary conditions for its global optimal solution. moreover, we propose an efficient algorithm based on alternating conditional expectation (ace) which converges to a local nmc optimum. for this algorithm, we provide guidelines for choosing appropriate starting points to jump out of local maximizers. we also propose a distributed algorithm to compute a 1-$\epsilon$ approximation of the nmc value for large and dense graphs using graph partitioning. for jointly gaussian variables, under some conditions, we show that the nmc optimization can be simplified to a max-cut problem, where we provide conditions under which an nmc solution can be computed exactly. under some general conditions, we show that nmc can infer the underlying graphical model for functions of latent jointly gaussian variables. these functions are unknown, bijective, and can be nonlinear. this result broadens the family of continuous distributions whose graphical models can be characterized efficiently. we illustrate the robustness of nmc in real world applications by showing its continuity with respect to small perturbations of joint distributions. we also show that sample nmc (nmc computed using empirical distributions) converges exponentially fast to the true nmc value. finally, we apply nmc to different cancer datasets including breast, kidney and liver cancers, and show that nmc infers gene modules that are significantly associated with survival times of individuals while they are not detected using linear association measures.",language models
,"Ne Win, Toh; Ernst, Michael D.",2023-03-29T15:35:26Z,2023-03-29T15:35:26Z,2002-05,https://hdl.handle.net/1721.1/149943,MIT-LCS-TR-841,Verifying Distributed Algorithms via Dynamic Analysis and Theorem Proving,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,127,verifying distributed algorithms via dynamic analysis and theorem proving ,high performance computing
,"Weinstein, Eugene; Steele, Kenneth; Agarwal, Anant; Glass, James",2005-12-22T01:27:05Z,2005-12-22T01:27:05Z,2004-04-14,http://hdl.handle.net/1721.1/30461,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A 1020-Node Modular Microphone Array and Beamformer for Intelligent Computing Spaces,"Ubiquitous computing environments are characterized by an unboundedamount of noise and crosstalk. In these environments, traditionalmethods of sound capture are insufficient, and array microphones areneeded in order to obtain a clean recording of desired speech. In thiswork, we have designed, implemented, and tested LOUD, a novel 1020-nodemicrophone array utilizing the Raw tile parallel processorarchitecture for computation. To the best of our knowledge,this is currently the largest microphone array in the world. We haveexplored the uses of the array within ubiquitous computing scenarios byimplementing an acoustic beamforming algorithm for sound sourceamplification in a noisy environment, and have obtained preliminaryresults demonstrating the efficacy of the array. From one to 1020microphones, we have shown a 13.7dB increase in peak SNR on arepresentative utterance, an 87.2% drop in word error rate withinterferer present, and an 89.6% drop in WER without an interferer.",MIT-CSAIL-TR-2004-021; MIT-LCS-TM-642,18 p.; 26946578 bytes; 901671 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,319,"a 1020-node modular microphone array and beamformer for intelligent computing spaces ubiquitous computing environments are characterized by an unboundedamount of noise and crosstalk. in these environments, traditionalmethods of sound capture are insufficient, and array microphones areneeded in order to obtain a clean recording of desired speech. in thiswork, we have designed, implemented, and tested loud, a novel 1020-nodemicrophone array utilizing the raw tile parallel processorarchitecture for computation. to the best of our knowledge,this is currently the largest microphone array in the world. we haveexplored the uses of the array within ubiquitous computing scenarios byimplementing an acoustic beamforming algorithm for sound sourceamplification in a noisy environment, and have obtained preliminaryresults demonstrating the efficacy of the array. from one to 1020microphones, we have shown a 13.7db increase in peak snr on arepresentative utterance, an 87.2% drop in word error rate withinterferer present, and an 89.6% drop in wer without an interferer.",speech/audio recognition
Karen Sollins,"Guo, Nina X.",2011-06-07T21:15:04Z,2011-06-07T21:15:04Z,2011-06-07,http://hdl.handle.net/1721.1/63260,MIT-CSAIL-TR-2011-030,Scalable Information-Sharing Network Management,"This thesis analyzes scalable information-sharing network management. It looks into one of the large problems in network management today: finding information across different network domains. Information-sharing network management is a method to solving the problem, though it is important to make it scalable. The solution proposed uses the Publish-Subscribe Internet Routing Paradigm (PSIRP) inter-domain design as the base structure. The design borrows from Border Gateway Protocol ideas and uses the Chord protocol as one of the key methods of finding information. The conclusion after analyzing the scalability of PSIRP is that its use of Chord gives it an advantage that allows a O(log^2 N) tradeoff between performance and distribution.",,56 p.,,,"network management, scalability, information-centric networking, inter-domain routing",Advanced Network Architecture,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,858,"scalable information-sharing network management this thesis analyzes scalable information-sharing network management. it looks into one of the large problems in network management today: finding information across different network domains. information-sharing network management is a method to solving the problem, though it is important to make it scalable. the solution proposed uses the publish-subscribe internet routing paradigm (psirp) inter-domain design as the base structure. the design borrows from border gateway protocol ideas and uses the chord protocol as one of the key methods of finding information. the conclusion after analyzing the scalability of psirp is that its use of chord gives it an advantage that allows a o(log^2 n) tradeoff between performance and distribution.",language models
,"Balazinska, Magdalena; Balakrishnan, Hari; Madden, Samuel; Stonebraker, Mike",2005-12-22T02:16:16Z,2005-12-22T02:16:16Z,2004-11-22,http://hdl.handle.net/1721.1/30506,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Availability-Consistency Trade-Offs in a Fault-Tolerant Stream Processing System,"processing. In contrast to previous techniques that handlenode failures, our approach also tolerates network failuresand network partitions. The approach is based on a principledtrade-off between consistency and availability in theface of failure, that (1) ensures that all data on an inputstream is processed within a specified time threshold, but(2) reduces the impact of failures by limiting if possible thenumber of results produced based on partially available inputdata, and (3) corrects these results when failures heal.Our approach is well-suited for applications such as environmentmonitoring, where high availability and real-timeresponse is preferable to perfect answers.Our approach uses replication and guarantees that all processingreplicas achieve state consistency, both in the absenceof failures and after a failure heals. We achieve consistencyin the former case by defining a data-serializing operatorthat ensures that the order of tuples to a downstreamoperator is the same at all the replicas. To achieve consistencyafter a failure heals, we develop approaches based oncheckpoint/redo and undo/redo techniques.We have implemented these schemes in a prototype distributedstream processing system, and present experimentalresults that show that the system meets the desiredavailability-consistency trade-offs.",MIT-CSAIL-TR-2004-077; MIT-LCS-TR-974,12 p.; 25764097 bytes; 1231086 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,385,"availability-consistency trade-offs in a fault-tolerant stream processing system processing. in contrast to previous techniques that handlenode failures, our approach also tolerates network failuresand network partitions. the approach is based on a principledtrade-off between consistency and availability in theface of failure, that (1) ensures that all data on an inputstream is processed within a specified time threshold, but(2) reduces the impact of failures by limiting if possible thenumber of results produced based on partially available inputdata, and (3) corrects these results when failures heal.our approach is well-suited for applications such as environmentmonitoring, where high availability and real-timeresponse is preferable to perfect answers.our approach uses replication and guarantees that all processingreplicas achieve state consistency, both in the absenceof failures and after a failure heals. we achieve consistencyin the former case by defining a data-serializing operatorthat ensures that the order of tuples to a downstreamoperator is the same at all the replicas. to achieve consistencyafter a failure heals, we develop approaches based oncheckpoint/redo and undo/redo techniques.we have implemented these schemes in a prototype distributedstream processing system, and present experimentalresults that show that the system meets the desiredavailability-consistency trade-offs.",language models
Martin Rinard,"Sidiroglou-Douskos, Stelios; Lahtinen, Eric; Long, Fan; Piselli, Paolo; Rinard, Martin",2014-10-22T21:30:11Z,2014-10-22T21:30:11Z,2014-09-30,http://hdl.handle.net/1721.1/91149,MIT-CSAIL-TR-2014-025,Automatic Error Elimination by Multi-Application Code Transfer,"We present pDNA, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. Experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pDNA to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. Because pDNA works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, pDNA is the first system to eliminate software errors via the successful transfer of correct code across applications.",,15 p.,,,automatic patching; software self-healing,Program Analysis,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,MIT-CSAIL-TR-2014-026,http://hdl.handle.net/1721.1/91150,,,2014-10-22T21:30:12Z,,,,,,,,,,,,,,,,2014,971,"automatic error elimination by multi-application code transfer we present pdna, a system for automatically transfer- ring correct code from donor applications into recipient applications to successfully eliminate errors in the recipient. experimental results using six donor applications to eliminate nine errors in six recipient applications highlight the ability of pdna to transfer code across applications to eliminate otherwise fatal integer and buffer overflow errors. because pdna works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. to the best of our knowledge, pdna is the first system to eliminate software errors via the successful transfer of correct code across applications.",high performance computing
,"Kuncak, Viktor; Rinard, Martin",2023-03-29T15:36:43Z,2023-03-29T15:36:43Z,2003-01,https://hdl.handle.net/1721.1/149974,MIT-LCS-TR-879,On the Theory of Structural Subtyping,"We show that the first-order theory of structural subtyping of non-recursive types is decidable.   Let Sigma be a language consisting of function symbols (representing type constructors) and C a decidable structure in the relational language L containing a binary relation <. C represents primitive types; < represents a subtype ordering.  We introduce the notion of Sigma-term-power of C, which generalizes the structure arising in structural subtyping.  The domain of the Sigma-term-power of C is the set of Sigma-terms over the set of elements of C.   We show that the decidability of the first-order theory of C implies the decidability of the first-order theory of the Sigma-term-power of C.  This result implies the decidability of the first-order theory of structural subtyping of non-recursive types.   Our decision procedure is based on quantifier elimination and makes use of quantifier elimination for term algebras and Feferman-Vaught construction for products of decidable structures.   We also explore connections between the theory of structural subtyping of recursive types and monadic second-order theory of tree-like structures.  In particular, we give an embedding of the monadic second-order theory of infinite binary tree into the first-order theory of structural subtyping of recursive types.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,188,"on the theory of structural subtyping we show that the first-order theory of structural subtyping of non-recursive types is decidable.   let sigma be a language consisting of function symbols (representing type constructors) and c a decidable structure in the relational language l containing a binary relation <. c represents primitive types; < represents a subtype ordering.  we introduce the notion of sigma-term-power of c, which generalizes the structure arising in structural subtyping.  the domain of the sigma-term-power of c is the set of sigma-terms over the set of elements of c.   we show that the decidability of the first-order theory of c implies the decidability of the first-order theory of the sigma-term-power of c.  this result implies the decidability of the first-order theory of structural subtyping of non-recursive types.   our decision procedure is based on quantifier elimination and makes use of quantifier elimination for term algebras and feferman-vaught construction for products of decidable structures.   we also explore connections between the theory of structural subtyping of recursive types and monadic second-order theory of tree-like structures.  in particular, we give an embedding of the monadic second-order theory of infinite binary tree into the first-order theory of structural subtyping of recursive types.",language models
,"Kaelbling, Leslie Pack; Lozano-Perez, Tomas",2009-09-17T18:30:10Z,2009-09-17T18:30:10Z,2009-09-12,http://hdl.handle.net/1721.1/46722,MIT-CSAIL-TR-2009-043,Finding aircraft collision-avoidance strategies using policy search methods,A progress report describing the application of policy gradient and policy search by dynamic programming methods to an aircraft collision avoidance problem inspired by the requirements of next-generation TCAS.,,11 p.,,,,Learning and Intelligent Systems,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2009,748,finding aircraft collision-avoidance strategies using policy search methods a progress report describing the application of policy gradient and policy search by dynamic programming methods to an aircraft collision avoidance problem inspired by the requirements of next-generation tcas.,high performance computing
Nancy Lynch,"Lynch, Nancy; Newport, Calvin",2009-06-05T15:30:06Z,2009-06-05T15:30:06Z,2009-06-04,http://hdl.handle.net/1721.1/45553,MIT-CSAIL-TR-2009-023,Modeling Radio Networks,"We describe a modeling framework and collection of foundational composition results for the study of probabilistic distributed algorithms in synchronous radio networks. Existing results in this setting rely on informal descriptions of the channel behavior and therefore lack easy comparability and are prone to error caused by definition subtleties. Our framework rectifies these issues by providing: (1) a method to precisely describe a radio channel as a probabilistic automaton; (2) a mathematical notion of implementing one channel using another channel, allowing for direct comparisons of channel strengths and a natural decomposition of problems into implementing a more powerful channel and solving the problem on the powerful channel; (3) a mathematical definition of a problem and solving a problem; (4) a pair of composition results that simplify the tasks of proving properties about channel implementation algorithms and combining problems with channel implementations. Our goal is to produce a model streamlined for the needs of the radio network algorithms community.",,21 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,727,"modeling radio networks we describe a modeling framework and collection of foundational composition results for the study of probabilistic distributed algorithms in synchronous radio networks. existing results in this setting rely on informal descriptions of the channel behavior and therefore lack easy comparability and are prone to error caused by definition subtleties. our framework rectifies these issues by providing: (1) a method to precisely describe a radio channel as a probabilistic automaton; (2) a mathematical notion of implementing one channel using another channel, allowing for direct comparisons of channel strengths and a natural decomposition of problems into implementing a more powerful channel and solving the problem on the powerful channel; (3) a mathematical definition of a problem and solving a problem; (4) a pair of composition results that simplify the tasks of proving properties about channel implementation algorithms and combining problems with channel implementations. our goal is to produce a model streamlined for the needs of the radio network algorithms community.",language models
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2006-03-10T01:22:47Z,2006-03-10T01:22:47Z,2006-03-08,http://hdl.handle.net/1721.1/31310,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Task-Structured Probabilistic I/O Automata to Analyze an Oblivious Transfer Protocol,"AbstractThe Probabilistic I/O Automata framework of Lynch, Segala and Vaandrager provides tools for precisely specifying protocols and reasoning about their correctness using multiple levels of abstraction, based on implementation relationships between these levels. We enhance this framework to allow analyzing protocols that use cryptographic primitives. This requires resolving and reconciling issues such as nondeterministic behavior and scheduling, randomness, resource-bounded computation, and computational hardness assumptions. The enhanced framework allows for more rigorous and systematic analysis of cryptographic protocols. To demonstrate the use of this framework, wepresent an example analysis that we have done for an Oblivious Transfer protocol.",MIT-CSAIL-TR-2006-019,98 p.; 104102681 bytes; 4467324 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,http://hdl.handle.net/1721.1/33217,http://hdl.handle.net/1721.1/33217,,,,,,,,,,,,,,,,,,,2006,502,"using task-structured probabilistic i/o automata to analyze an oblivious transfer protocol abstractthe probabilistic i/o automata framework of lynch, segala and vaandrager provides tools for precisely specifying protocols and reasoning about their correctness using multiple levels of abstraction, based on implementation relationships between these levels. we enhance this framework to allow analyzing protocols that use cryptographic primitives. this requires resolving and reconciling issues such as nondeterministic behavior and scheduling, randomness, resource-bounded computation, and computational hardness assumptions. the enhanced framework allows for more rigorous and systematic analysis of cryptographic protocols. to demonstrate the use of this framework, wepresent an example analysis that we have done for an oblivious transfer protocol.",language models
,"Boyapati, Chandrasekhar; Lee, Robert; Rinard, Martin",2023-03-29T15:35:19Z,2023-03-29T15:35:19Z,2002-03,https://hdl.handle.net/1721.1/149941,MIT-LCS-TR-839,A Type System for Preventing Data Races and Deadlocks in Java Programs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,114,a type system for preventing data races and deadlocks in java programs ,language models
Nicholas Roy,"Roy, Nicholas; He, Ruijie",2009-09-28T21:00:15Z,2009-09-28T21:00:15Z,2009-09-23,http://hdl.handle.net/1721.1/46820,MIT-CSAIL-TR-2009-044,Efficient POMDP Forward Search by Predicting the Posterior Belief Distribution,"Online, forward-search techniques have demonstrated promising results for solving problems in partially observable environments. These techniques depend on the ability to efficiently search and evaluate the set of beliefs reachable from the current belief. However, enumerating or sampling action-observation sequences to compute the reachable beliefs is computationally demanding; coupled with the need to satisfy real-time constraints, existing online solvers can only search to a limited depth. In this paper, we propose that policies can be generated directly from the distribution of the agent's posterior belief. When the underlying state distribution is Gaussian, and the observation function is an exponential family distribution, we can calculate this distribution of beliefs without enumerating the possible observations. This property not only enables us to plan in problems with large observation spaces, but also allows us to search deeper by considering policies composed of multi-step action sequences. We present the Posterior Belief Distribution (PBD) algorithm, an efficient forward-search POMDP planner for continuous domains, demonstrating that better policies are generated when we can perform deeper forward search.",,12 p.,,,,"Robotics, Vision & Sensor Networks",,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2009,749,"efficient pomdp forward search by predicting the posterior belief distribution online, forward-search techniques have demonstrated promising results for solving problems in partially observable environments. these techniques depend on the ability to efficiently search and evaluate the set of beliefs reachable from the current belief. however, enumerating or sampling action-observation sequences to compute the reachable beliefs is computationally demanding; coupled with the need to satisfy real-time constraints, existing online solvers can only search to a limited depth. in this paper, we propose that policies can be generated directly from the distribution of the agent's posterior belief. when the underlying state distribution is gaussian, and the observation function is an exponential family distribution, we can calculate this distribution of beliefs without enumerating the possible observations. this property not only enables us to plan in problems with large observation spaces, but also allows us to search deeper by considering policies composed of multi-step action sequences. we present the posterior belief distribution (pbd) algorithm, an efficient forward-search pomdp planner for continuous domains, demonstrating that better policies are generated when we can perform deeper forward search.",high performance computing
Dina Katabi,"Chachulski, Szymon; Jennings, Michael; Katti, Sachin; Katabi, Dina",2007-02-23T23:23:30Z,2007-02-23T23:23:30Z,2007-02-23,http://hdl.handle.net/1721.1/36345,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Trading Structure for Randomness in Wireless Opportunistic Routing,"Opportunistic routing is a recent technique that achieves high throughput in the face of lossy wireless links. The current opportunistic routing protocol, ExOR, ties the MAC with routing, imposing a strict schedule on routers' access to the medium. Although the scheduler delivers opportunistic gains, it misses some of the inherent features of the 802.11 MAC. For example, it prevents spatial reuse and thus may underutilize the wireless medium.  It also eliminates the layering abstraction, making the protocol less amenable to extensions of alternate traffic type such as multicast.This paper presents MORE, a MAC-independent opportunistic routing protocol. MORE randomly mixes packets before forwarding them. This randomness ensures that routers that hear the same transmission do not forward the same packets. Thus, MORE needs no special scheduler to coordinate routers and can run directly on top of 802.11. Experimental results from a 20-node wireless testbed show that MORE's average unicast throughput is 20% higher than ExOR, and the gains rise to 50% over ExOR when there is a chance of spatial reuse. For multicast, MORE's gains increase with the number of destinations, and are 35-200% greater than ExOR.",MIT-CSAIL-TR-2007-014,14 p.,,,Network Coding; Opportunistic Routing; Wireless Networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,580,"trading structure for randomness in wireless opportunistic routing opportunistic routing is a recent technique that achieves high throughput in the face of lossy wireless links. the current opportunistic routing protocol, exor, ties the mac with routing, imposing a strict schedule on routers' access to the medium. although the scheduler delivers opportunistic gains, it misses some of the inherent features of the 802.11 mac. for example, it prevents spatial reuse and thus may underutilize the wireless medium.  it also eliminates the layering abstraction, making the protocol less amenable to extensions of alternate traffic type such as multicast.this paper presents more, a mac-independent opportunistic routing protocol. more randomly mixes packets before forwarding them. this randomness ensures that routers that hear the same transmission do not forward the same packets. thus, more needs no special scheduler to coordinate routers and can run directly on top of 802.11. experimental results from a 20-node wireless testbed show that more's average unicast throughput is 20% higher than exor, and the gains rise to 50% over exor when there is a chance of spatial reuse. for multicast, more's gains increase with the number of destinations, and are 35-200% greater than exor.",language models
,"Thies, William F.; Karczmarek, Michael; Amarasinghe, Saman",2023-03-29T14:42:24Z,2023-03-29T14:42:24Z,2001-08,https://hdl.handle.net/1721.1/149309,MIT-LCS-TM-620,StreaMIT: A Language for Streaming Applications,"We characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly important. The StreaMIT language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreaMIT compiler aims to improve the performance of streaming applications via stream-specific analyses and optimizations. In this paper, we motivate, describe and justify the language features of StreaMIT, which include: a structured model of streams, a messaging system for control, a re-initialization mechanism, and a natural textual syntax. We also present a means of reasoning about time in terms of ""information flow"": a concept that we believe is fundamental to the streaming domain. Using this concept, we give a formal semantics for StreaMIT's messaging system, as well as a simple algorithm for detecting deadlock and buffer overlow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,68,"streamit: a language for streaming applications we characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly important. the streamit language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. at the same time, the streamit compiler aims to improve the performance of streaming applications via stream-specific analyses and optimizations. in this paper, we motivate, describe and justify the language features of streamit, which include: a structured model of streams, a messaging system for control, a re-initialization mechanism, and a natural textual syntax. we also present a means of reasoning about time in terms of ""information flow"": a concept that we believe is fundamental to the streaming domain. using this concept, we give a formal semantics for streamit's messaging system, as well as a simple algorithm for detecting deadlock and buffer overlow.",language models
Brian Williams,"Dong, Shuonan",2018-01-30T23:46:40Z,2018-01-30T23:46:40Z,2012-08-23,http://hdl.handle.net/1721.1/113367,MIT-CSAIL-TR-2018-007,Learning and recognition of hybrid manipulation tasks in variable environments using probabilistic flow tubes,"Robots can act as proxies for human operators in environments where a human operator is not present or cannot directly perform a task, such as in dangerous or remote situations. Teleoperation is a common interface for controlling robots that are designed to be human proxies. Unfortunately, teleoperation may fail to preserve the natural fluidity of human motions due to interface limitations such as communication delays, non-immersive sensing, and controller uncertainty. I envision a robot that can learn a set of motions that a teleoperator commonly performs, so that it can autonomously execute routine tasks or recognize a user's motion in real time. Tasks can be either primitive activities or compound plans. During online operation, the robot can recognize a user's teleoperated motions on the fly and offer real-time assistance, for example, by autonomously executing the remainder of the task. I realize this vision by addressing three main problems: (1) learning primitive activities by identifying significant features of the example motions and generalizing the behaviors from user demonstration trajectories; (2) recognizing activities in real time by determining the likelihood that a user is currently executing one of several learned activities; and (3) learning complex plans by generalizing a sequence of activities, through auto-segmentation and incremental learning of previously unknown activities. To solve these problems, I first present an approach to learning activities from human demonstration that (1) provides flexibility and robustness when encoding a user's demonstrated motions by using a novel representation called a probabilistic flow tube, and (2) automatically determines the relevant features of a motion so that they can be preserved during autonomous execution in new situations. I next introduce an approach to real-time motion recognition that (1) uses temporal information to successfully model motions that may be non-Markovian, (2) provides fast real-time recognition of motions in progress by using an incremental temporal alignment approach, and (3) leverages the probabilistic flow tube representation to ensure robustness during recognition against varying environment states. Finally, I develop an approach to learn combinations of activities that (1) automatically determines where activities should be segmented in a sequence and (2) learns previously unknown activities on the fly. I demonstrate the results of autonomously executing motions learned by my approach on two different robotic platforms supporting user-teleoperated manipulation tasks in a variety of environments. I also present the results of real-time recognition in different scenarios, including a robotic hardware platform. Systematic testing in a two-dimensional environment shows up to a 27% improvement in activity recognition rates over prior art, while maintaining average computing times for incremental recognition of less than half of human reaction time.",,144 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,PhD thesis,,,,,,,,,2018-01-30T23:46:40Z,,,,,,,,,,,,,,,,2012,903,"learning and recognition of hybrid manipulation tasks in variable environments using probabilistic flow tubes robots can act as proxies for human operators in environments where a human operator is not present or cannot directly perform a task, such as in dangerous or remote situations. teleoperation is a common interface for controlling robots that are designed to be human proxies. unfortunately, teleoperation may fail to preserve the natural fluidity of human motions due to interface limitations such as communication delays, non-immersive sensing, and controller uncertainty. i envision a robot that can learn a set of motions that a teleoperator commonly performs, so that it can autonomously execute routine tasks or recognize a user's motion in real time. tasks can be either primitive activities or compound plans. during online operation, the robot can recognize a user's teleoperated motions on the fly and offer real-time assistance, for example, by autonomously executing the remainder of the task. i realize this vision by addressing three main problems: (1) learning primitive activities by identifying significant features of the example motions and generalizing the behaviors from user demonstration trajectories; (2) recognizing activities in real time by determining the likelihood that a user is currently executing one of several learned activities; and (3) learning complex plans by generalizing a sequence of activities, through auto-segmentation and incremental learning of previously unknown activities. to solve these problems, i first present an approach to learning activities from human demonstration that (1) provides flexibility and robustness when encoding a user's demonstrated motions by using a novel representation called a probabilistic flow tube, and (2) automatically determines the relevant features of a motion so that they can be preserved during autonomous execution in new situations. i next introduce an approach to real-time motion recognition that (1) uses temporal information to successfully model motions that may be non-markovian, (2) provides fast real-time recognition of motions in progress by using an incremental temporal alignment approach, and (3) leverages the probabilistic flow tube representation to ensure robustness during recognition against varying environment states. finally, i develop an approach to learn combinations of activities that (1) automatically determines where activities should be segmented in a sequence and (2) learns previously unknown activities on the fly. i demonstrate the results of autonomously executing motions learned by my approach on two different robotic platforms supporting user-teleoperated manipulation tasks in a variety of environments. i also present the results of real-time recognition in different scenarios, including a robotic hardware platform. systematic testing in a two-dimensional environment shows up to a 27% improvement in activity recognition rates over prior art, while maintaining average computing times for incremental recognition of less than half of human reaction time.",robotics
,"Rivest, Ronald; Schiefelbein, M. Curran; Zissman, Marc A.; Bay, Jason; Bugnion, Edouard; Finnerty, Jill; Liccardi, Ilaria; Nelson, Brad; Norige, Adam S.; Shen, Emily H.; Wanger, Jenny; Yahalom, Raphael; Alekseyev, Jesslyn D.; Brubaker, Chad; Ferretti, Luca; Ishikawa, Charlie; Raykova, Mariana; Schlaman, Brendan; Schwartz, Robert X.; Sudduth, Emma; Tessaro, Stefano",2023-02-22T17:36:33Z,2023-02-22T17:36:33Z,2023-02-22,https://hdl.handle.net/1721.1/148149,Lincoln Laboratory Technical Report;TR-1288,Automated Exposure Notification for COVID-19,"Private Automated Contact Tracing (PACT) was a collaborative team and effort formed during the beginning of the Coronavirus Disease 2019 (COVID-19) pandemic. PACTs mission was to enhance contact tracing in pandemic response by designing exposure-detection functions in personal digital communication devices that have maximal public health utility while preserving privacy. PACT had four major lines of effort: proximity detection efficacy, privacy, public health integration, and public health efficacy. In support of these lines of effort, PACT executed several cross-layer activities that helped demonstrate public health efficacy. These included prototype development and  demonstrations; system analysis; data collection and experimentation; and large-scale deployment support. PACT convened two scientific workshops relating to privacy-preserving AEN: one virtual workshop in April 2020 and a second hybrid workshop in October 2021. This report is an outcome of the second workshop and serves as PACTs final report. It seeks to explain and discuss the use of automated exposure notification during the COVID-19 pandemic and to provide some recommendations for those who may try to design and deploy similar technologies in future pandemics.",,,,en_US,COVID-19; exposure notification; contact tracing; digital health; public health; GAEN; privacy; secure multiparty computation; differential privacy; Bluetooth; BLE; SARS-CoV-2; pandemic; governance; adoption; trust; PACT; proximity; TCFTL; smartphone; AEN; impact,,"IBM Research, the U.S. Defense Advanced Research Projects Agency (DARPA), and the U.S. Centers for Disease Control and Prevention (CDC).",,,The authors were among the 70+ in-person and virtual participants in the October 2021 ImPACT 2021 workshop. This final report has been heavily influenced by the discussion at that workshop.,,,,,,,,,,,,,Technical Report,,,,,,,,,,,,2023,1072,"automated exposure notification for covid-19 private automated contact tracing (pact) was a collaborative team and effort formed during the beginning of the coronavirus disease 2019 (covid-19) pandemic. pacts mission was to enhance contact tracing in pandemic response by designing exposure-detection functions in personal digital communication devices that have maximal public health utility while preserving privacy. pact had four major lines of effort: proximity detection efficacy, privacy, public health integration, and public health efficacy. in support of these lines of effort, pact executed several cross-layer activities that helped demonstrate public health efficacy. these included prototype development and  demonstrations; system analysis; data collection and experimentation; and large-scale deployment support. pact convened two scientific workshops relating to privacy-preserving aen: one virtual workshop in april 2020 and a second hybrid workshop in october 2021. this report is an outcome of the second workshop and serves as pacts final report. it seeks to explain and discuss the use of automated exposure notification during the covid-19 pandemic and to provide some recommendations for those who may try to design and deploy similar technologies in future pandemics.",privacy/ethics
Silvio Micali,"Azar, Pablo; Chen, Jing; Micali, Silvio",2010-12-30T09:15:11Z,2010-12-30T09:15:11Z,2010-12-20,http://hdl.handle.net/1721.1/60370,MIT-CSAIL-TR-2010-059,Conservative-Bayesian Mechanism Design,"Classical Bayesian mechanism design is ""centralized,"" that is, the designer is assumed to know the distribution D from which the players' type profile has been drawn. We instead investigate a very ""decentralized"" Bayesian model, where the designer has no knowledge at all, and each player only has some probabilistic information about D. For this decentralized model and many contexts of interest, where the goal is to maximize revenue, we show that, for arbitrary type distributions D (in particular, correlated ones), it is possible to design mechanisms matching to a significant extent the performance of the optimal centralized mechanisms. Our results are ""existential"" for a broad class of contexts (including combinatorial auctions) and ""constructive"" for auctions of a single good.",,18 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,825,"conservative-bayesian mechanism design classical bayesian mechanism design is ""centralized,"" that is, the designer is assumed to know the distribution d from which the players' type profile has been drawn. we instead investigate a very ""decentralized"" bayesian model, where the designer has no knowledge at all, and each player only has some probabilistic information about d. for this decentralized model and many contexts of interest, where the goal is to maximize revenue, we show that, for arbitrary type distributions d (in particular, correlated ones), it is possible to design mechanisms matching to a significant extent the performance of the optimal centralized mechanisms. our results are ""existential"" for a broad class of contexts (including combinatorial auctions) and ""constructive"" for auctions of a single good.",language models
Tomaso Poggio,"Kim, Heejung; Wohlwend, Jeremy; Leibo, Joel Z.; Poggio, Tomaso",2013-06-20T17:00:04Z,2013-06-20T17:00:04Z,2013-06-18,http://hdl.handle.net/1721.1/79354,MIT-CSAIL-TR-2013-013; CBCL-312,Body-form and body-pose recognition with a hierarchical model of the ventral stream,"When learning to recognize a novel body shape, e.g., a panda bear, we are not misled by changes in its pose. A ""jumping panda bear"" is readily recognized, despite having no prior visual experience with the conjunction of these concepts. Likewise, a novel pose can be estimated in an invariant way, with respect to the actor's body shape. These body and pose recognition tasks require invariance to non-generic transformations that previous models of the ventral stream do not have. We show that the addition of biologically plausible, class-specific mechanisms associating previously-viewed actors in a range of poses enables a hierarchical model of object recognition to account for this human capability. These associations could be acquired in an unsupervised manner from past experience.",,10 p.,,,Ventral stream; Modularity; Computational neuroscience; HMAX,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,2013-06-20T17:00:05Z,,,,,,,,,,,,,,,,2013,928,"body-form and body-pose recognition with a hierarchical model of the ventral stream when learning to recognize a novel body shape, e.g., a panda bear, we are not misled by changes in its pose. a ""jumping panda bear"" is readily recognized, despite having no prior visual experience with the conjunction of these concepts. likewise, a novel pose can be estimated in an invariant way, with respect to the actor's body shape. these body and pose recognition tasks require invariance to non-generic transformations that previous models of the ventral stream do not have. we show that the addition of biologically plausible, class-specific mechanisms associating previously-viewed actors in a range of poses enables a hierarchical model of object recognition to account for this human capability. these associations could be acquired in an unsupervised manner from past experience.",object recognition/detection
Fredo Durand,"Levin, Anat; Glasner, Daniel; Xiong, Ying; Durand, Fredo; Freeman, William; Matusik, Wojciech; Zickler, Todd",2013-04-24T18:00:04Z,2013-04-24T18:00:04Z,2013-04-24,http://hdl.handle.net/1721.1/78590,MIT-CSAIL-TR-2013-008,High Spatial Resolution BRDFs with Metallic powders Using Wave Optics Analysis,"This manuscript completes the analysis of our SIGGRAPH 2013 paper ""Fabricating BRDFs at High Spatial Resolution Using Wave Optics"" in which photolithography fabrication was used for manipulating reflectance effects. While photolithography allows for precise reflectance control, it is costly to fabricate. Here we explore an inexpensive alternative to micro-fabrication, in the form of metallic powders. Such powders are readily available at a variety of particle sizes and morphologies. Using an analysis similar to the micro-fabrication paper, we provide guidelines for the relation between the particles' shape and size and the reflectance functions they can produce.",,4 p.,,,,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2013,923,"high spatial resolution brdfs with metallic powders using wave optics analysis this manuscript completes the analysis of our siggraph 2013 paper ""fabricating brdfs at high spatial resolution using wave optics"" in which photolithography fabrication was used for manipulating reflectance effects. while photolithography allows for precise reflectance control, it is costly to fabricate. here we explore an inexpensive alternative to micro-fabrication, in the form of metallic powders. such powders are readily available at a variety of particle sizes and morphologies. using an analysis similar to the micro-fabrication paper, we provide guidelines for the relation between the particles' shape and size and the reflectance functions they can produce.",language models
,"Fu, Kevin; Sit, Emil; Smith, Kendra; Feamster, Nick",2023-03-29T15:34:08Z,2023-03-29T15:34:08Z,2001-03,https://hdl.handle.net/1721.1/149921,MIT-LCS-TR-818,Client Authentication on the Web,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,37,client authentication on the web ,language models
Karen Sollins,"Rock, Colleen T.",2016-09-26T22:15:03Z,2016-09-26T22:15:03Z,2016-09-26,http://hdl.handle.net/1721.1/104385,MIT-CSAIL-TR-2016-012,Examining Key Mobility Resources through Denial of Service Attacks on proposed Global Name Resolution Services,"The problem we address in this thesis is to uncover the design elements in a network architecture design that may open it up to denial of service (DoS) attacks and to expose the tradeoffs in mitigating those DoS opportunities. We take as our candidate network architecture design the Future Internet Architecture project MobilityFirst. MobilityFirst's overarching goal, driven by increasingly available wireless communication, is the support of mobility in an Internet architecture. At its core, MobilityFirst separates identification from location, as distinct from the current Internet architecture, and postulates the existence of globally unique, flat identifiers. In order to support mobility in this context, it also postulates a global name resolution service (GNRS). In this thesis we examine three alternative designs for the GNRS and the opportunities they expose for DoS attacks. We consider each one in depth analytically. As an example, we then study one particular attack in depth and are forced to conclude that approaches to mitigating this attack would have significant negative impact on the support of mobility thus exposing the dilemma in such system design tradeoffs.",,68 p.,,,DMap; GMap; Auspice,Advanced Network Architecture,,,,MEng thesis,,,,,,,,,2016-09-26T22:15:03Z,,,,,,,,,,,,,,,,2016,1029,"examining key mobility resources through denial of service attacks on proposed global name resolution services the problem we address in this thesis is to uncover the design elements in a network architecture design that may open it up to denial of service (dos) attacks and to expose the tradeoffs in mitigating those dos opportunities. we take as our candidate network architecture design the future internet architecture project mobilityfirst. mobilityfirst's overarching goal, driven by increasingly available wireless communication, is the support of mobility in an internet architecture. at its core, mobilityfirst separates identification from location, as distinct from the current internet architecture, and postulates the existence of globally unique, flat identifiers. in order to support mobility in this context, it also postulates a global name resolution service (gnrs). in this thesis we examine three alternative designs for the gnrs and the opportunities they expose for dos attacks. we consider each one in depth analytically. as an example, we then study one particular attack in depth and are forced to conclude that approaches to mitigating this attack would have significant negative impact on the support of mobility thus exposing the dilemma in such system design tradeoffs.",language models
Karen Sollins,"Beckler, Kendra K.",2015-02-02T16:30:05Z,2015-02-02T16:30:05Z,2015-01-31,http://hdl.handle.net/1721.1/93253,MIT-CSAIL-TR-2015-002,Improved Caching Strategies for Publish/Subscribe Internet Networking,"The systemic structure of TCP/IP is outdated; a new scheme for data transportation is needed in order to make the internet more adaptive to modern demands of mobility, information-driven demand, ever-increasing quantity of users and data, and performance requirements. While an information centric networking system addresses these issues, one required component for publish subscribe or content-addressed internet networking systems to work properly is an improved caching system. This allows the publish subscribe internet networking to dynamically route packets to mobile users, as an improvement over pure hierarchical or pure distributed caching systems. To this end, I proposed, implemented, and analyzed the workings of a superdomain caching system. The superdomain caching system is a hybrid of hierarchical and dynamic caching systems designed to continue reaping the benefits of the caching system for mobile users (who may move between neighboring domains in the midst of a network transaction) while minimizing the latency inherent in any distributed caching system to improve upon the content-addressed system.",,82 p.,,,Network caching; information centric networking,Advanced Network Architecture,,,,MEng thesis,,,,,,,,,2015-02-02T16:30:05Z,,,,,,,,,,,,,,,,2015,981,"improved caching strategies for publish/subscribe internet networking the systemic structure of tcp/ip is outdated; a new scheme for data transportation is needed in order to make the internet more adaptive to modern demands of mobility, information-driven demand, ever-increasing quantity of users and data, and performance requirements. while an information centric networking system addresses these issues, one required component for publish subscribe or content-addressed internet networking systems to work properly is an improved caching system. this allows the publish subscribe internet networking to dynamically route packets to mobile users, as an improvement over pure hierarchical or pure distributed caching systems. to this end, i proposed, implemented, and analyzed the workings of a superdomain caching system. the superdomain caching system is a hybrid of hierarchical and dynamic caching systems designed to continue reaping the benefits of the caching system for mobile users (who may move between neighboring domains in the midst of a network transaction) while minimizing the latency inherent in any distributed caching system to improve upon the content-addressed system.",language models
,"Clarke, Dwaine; Gassend, Blaise; Suh, G. Edward; van Dijk, Marten; Devadas, Srinivas",2023-03-29T14:42:58Z,2023-03-29T14:42:58Z,2002-08,https://hdl.handle.net/1721.1/149320,MIT-LCS-TM-631,Offline Authentication of Untrusted Storage,"We extend the offline memory correctness checking scheme presented by Blum et. al [BEG+91], by using incremental cryptography, to detect attacks by an active adversary. We also introduce a hybrid o_ine-online checking scheme designed for untrusted storages in file systems and databases. Previous work [GSC+02] [FKM00] [MVS00] describe systems in which Merkle trees are used to verify the authenticity of data stored on untrusted storage. The Merkle trees [Mer79] are used to check, after each operation, whether the storage performed correctly. The offline and hybrid checkers are designed for checking sequences of operations on an untrusted storage and, in the common case, require only a constant overhead on the number of accesses to the storage, as compared to the logarithmic overhead incurred by online Merkle tree schemes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,147,"offline authentication of untrusted storage we extend the offline memory correctness checking scheme presented by blum et. al [beg+91], by using incremental cryptography, to detect attacks by an active adversary. we also introduce a hybrid o_ine-online checking scheme designed for untrusted storages in file systems and databases. previous work [gsc+02] [fkm00] [mvs00] describe systems in which merkle trees are used to verify the authenticity of data stored on untrusted storage. the merkle trees [mer79] are used to check, after each operation, whether the storage performed correctly. the offline and hybrid checkers are designed for checking sequences of operations on an untrusted storage and, in the common case, require only a constant overhead on the number of accesses to the storage, as compared to the logarithmic overhead incurred by online merkle tree schemes",language models
,"Jayaraman, Karthick; Ganesh, Vijay; Tripunitara, Mahesh; Rinard, Martin C.; Chapin, Steve J.",2011-04-28T20:30:11Z,2011-04-28T20:30:11Z,2011-04-27,http://hdl.handle.net/1721.1/62562,MIT-CSAIL-TR-2011-026,ARBAC Policy for a Large Multi-National Bank,"Administrative role-based access control (ARBAC) is the first comprehensive administrative model proposed for role-based access control (RBAC). ARBAC has several features for designing highly expressive policies, but current work has not highlighted the utility of these expressive policies. In this report, we present a case study of designing an ARBAC policy for a bank comprising 18 branches. Using this case study we provide an assessment about the features of ARBAC that are likely to be used in realistic policies.",,6 p.,,,ARBAC policy; computer security; access control,Program Analysis,,Creative Commons Attribution-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,852,"arbac policy for a large multi-national bank administrative role-based access control (arbac) is the first comprehensive administrative model proposed for role-based access control (rbac). arbac has several features for designing highly expressive policies, but current work has not highlighted the utility of these expressive policies. in this report, we present a case study of designing an arbac policy for a bank comprising 18 branches. using this case study we provide an assessment about the features of arbac that are likely to be used in realistic policies.",language models
Martin Rinard,"Long, Fan; Sidiroglou-Douskos, Stelios; Kim, Deokhwan; Rinard, Martin",2013-08-12T02:30:08Z,2013-08-12T02:30:08Z,2013-08-06,http://hdl.handle.net/1721.1/79827,MIT-CSAIL-TR-2013-018,Sound Input Filter Generation for Integer Overflow Errors,"We present a system, SIFT, for generating input filters that nullify integer overflow errors associated with critical program sites such as memory allocation or block copy sites. SIFT uses a static program analysis to generate filters that discard inputs that may trigger integer overflow errors in the computations of the sizes of allocated memory blocks or the number of copied bytes in block copy operations. The generated filters are sound   if an input passes the filter, it will not trigger an integer overflow error for any analyzed site. Our results show that SIFT successfully analyzes (and therefore generates sound input filters for) 52 out of 58 memory allocation and block memory copy sites in analyzed input processing modules from five applications (VLC, Dillo, Swfdec, Swftools, and GIMP). These nullified errors include six known integer overflow vulnerabilities. Our results also show that applying these filters to 62895 real-world inputs produces no false positives. The analysis and filter generation times are all less than a second.",,20 p.,,,Static Analysis; Integer Overflow; Filter Generation,Computer Architecture,,,,,,,,,,,,,2013-08-12T02:30:08Z,,,,,,,,,,,,,,,,2013,933,"sound input filter generation for integer overflow errors we present a system, sift, for generating input filters that nullify integer overflow errors associated with critical program sites such as memory allocation or block copy sites. sift uses a static program analysis to generate filters that discard inputs that may trigger integer overflow errors in the computations of the sizes of allocated memory blocks or the number of copied bytes in block copy operations. the generated filters are sound   if an input passes the filter, it will not trigger an integer overflow error for any analyzed site. our results show that sift successfully analyzes (and therefore generates sound input filters for) 52 out of 58 memory allocation and block memory copy sites in analyzed input processing modules from five applications (vlc, dillo, swfdec, swftools, and gimp). these nullified errors include six known integer overflow vulnerabilities. our results also show that applying these filters to 62895 real-world inputs produces no false positives. the analysis and filter generation times are all less than a second.",privacy/ethics
Trevor Darrell,"Morency, Louis-Philippe",2006-11-17T11:12:55Z,2006-11-17T11:12:55Z,2006-11-15,http://hdl.handle.net/1721.1/34893,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Context-based Visual Feedback Recognition,"During face-to-face conversation, people use visual feedback (e.g.,head and eye gesture) to communicate relevant information and tosynchronize rhythm between participants. When recognizing visualfeedback, people often rely on more than their visual perception.For instance, knowledge about the current topic and from previousutterances help guide the recognition of nonverbal cues. The goal ofthis thesis is to augment computer interfaces with the ability toperceive visual feedback gestures and to enable the exploitation ofcontextual information from the current interaction state to improvevisual feedback recognition.We introduce the concept of visual feedback anticipationwhere contextual knowledge from an interactive system (e.g. lastspoken utterance from the robot or system events from the GUIinterface) is analyzed online to anticipate visual feedback from ahuman participant and improve visual feedback recognition. Ourmulti-modal framework for context-based visual feedback recognitionwas successfully tested on conversational and non-embodiedinterfaces for head and eye gesture recognition.We also introduce Frame-based Hidden-state Conditional RandomField model, a new discriminative model for visual gesturerecognition which can model the sub-structure of a gesture sequence,learn the dynamics between gesture labels, and can be directlyapplied to label unsegmented sequences. The FHCRF model outperformsprevious approaches (i.e. HMM, SVM and CRF) for visual gesturerecognition and can efficiently learn relevant contextualinformation necessary for visual feedback anticipation.A real-time visual feedback recognition library for interactiveinterfaces (called Watson) was developed to recognize head gaze,head gestures, and eye gaze using the images from a monocular orstereo camera and the context information from the interactivesystem. Watson was downloaded by more then 70 researchers around theworld and was successfully used by MERL, USC, NTT, MIT Media Lab andmany other research groups.",MIT-CSAIL-TR-2006-075,195 p.; 5912220 bytes; 20231190 bytes,application/pdf; application/postscript,en_US,,Vision,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,556,"context-based visual feedback recognition during face-to-face conversation, people use visual feedback (e.g.,head and eye gesture) to communicate relevant information and tosynchronize rhythm between participants. when recognizing visualfeedback, people often rely on more than their visual perception.for instance, knowledge about the current topic and from previousutterances help guide the recognition of nonverbal cues. the goal ofthis thesis is to augment computer interfaces with the ability toperceive visual feedback gestures and to enable the exploitation ofcontextual information from the current interaction state to improvevisual feedback recognition.we introduce the concept of visual feedback anticipationwhere contextual knowledge from an interactive system (e.g. lastspoken utterance from the robot or system events from the guiinterface) is analyzed online to anticipate visual feedback from ahuman participant and improve visual feedback recognition. ourmulti-modal framework for context-based visual feedback recognitionwas successfully tested on conversational and non-embodiedinterfaces for head and eye gesture recognition.we also introduce frame-based hidden-state conditional randomfield model, a new discriminative model for visual gesturerecognition which can model the sub-structure of a gesture sequence,learn the dynamics between gesture labels, and can be directlyapplied to label unsegmented sequences. the fhcrf model outperformsprevious approaches (i.e. hmm, svm and crf) for visual gesturerecognition and can efficiently learn relevant contextualinformation necessary for visual feedback anticipation.a real-time visual feedback recognition library for interactiveinterfaces (called watson) was developed to recognize head gaze,head gestures, and eye gaze using the images from a monocular orstereo camera and the context information from the interactivesystem. watson was downloaded by more then 70 researchers around theworld and was successfully used by merl, usc, ntt, mit media lab andmany other research groups.",language models
John Leonard,"Johannsson, Hordur; Kaess, Michael; Fallon, Maurice; Leonard, John J.",2012-05-25T19:15:05Z,2012-05-25T19:15:05Z,2012-05-25,http://hdl.handle.net/1721.1/70952,MIT-CSAIL-TR-2012-013,Temporally Scalable Visual SLAM using a Reduced Pose Graph,"In this paper, we demonstrate a system for temporally scalable visual SLAM using a reduced pose graph representation. Unlike previous visual SLAM approaches that use keyframes, our approach continually uses new measurements to improve the map, yet achieves efficiency by avoiding adding redundant frames and not using marginalization to reduce the graph. To evaluate our approach, we present results using an online binocular visual SLAM system that uses place recognition for both robustness and multi-session operation. To allow large-scale indoor mapping, our system automatically handles elevator rides based on accelerometer data. We demonstrate long-term mapping in a large multi-floor building, using approximately nine hours of data collected over the course of six months. Our results illustrate the capability of our visual SLAM system to scale in size with the area of exploration instead of the time of exploration.",,9 p.,,,robotics navigation,Marine Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,893,"temporally scalable visual slam using a reduced pose graph in this paper, we demonstrate a system for temporally scalable visual slam using a reduced pose graph representation. unlike previous visual slam approaches that use keyframes, our approach continually uses new measurements to improve the map, yet achieves efficiency by avoiding adding redundant frames and not using marginalization to reduce the graph. to evaluate our approach, we present results using an online binocular visual slam system that uses place recognition for both robustness and multi-session operation. to allow large-scale indoor mapping, our system automatically handles elevator rides based on accelerometer data. we demonstrate long-term mapping in a large multi-floor building, using approximately nine hours of data collected over the course of six months. our results illustrate the capability of our visual slam system to scale in size with the area of exploration instead of the time of exploration.",object recognition/detection
Michael Ernst,"Papi, Matthew M.; Ali, Mahmood; Correa Jr., Telmo Luis; Perkins, Jeff H.; Ernst, Michael D.",2007-09-20T19:21:48Z,2007-09-20T19:21:48Z,2007-09-17,http://hdl.handle.net/1721.1/38878,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Pluggable type-checking for custom type qualifiers in Java,"We have created a framework for adding custom type qualifiers to the Javalanguage in a backward-compatible way.  The type system designer definesthe qualifiers and creates a compiler plug-in that enforces theirsemantics.  Programmers can write the type qualifiers in their programs andbe informed of errors or assured that the program is free of those errors.The system builds on existing Java tools and APIs.In order to evaluate our framework, we have written four type-checkersusing the framework:  for a non-null type system that can detect andprevent null pointer errors; for an interned type system that can detectand prevent equality-checking errors; for a reference immutability typesystem, Javari, that can detect and prevent mutation errors; and for areference and object immutability type system, IGJ, that can detect andprevent even more mutation errors.  We have conducted case studies usingeach checker to find real errors in existing software.  These case studiesdemonstrate that the checkers and the framework are practical and useful.",MIT-CSAIL-TR-2007-047,10 p.,,,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,617,"pluggable type-checking for custom type qualifiers in java we have created a framework for adding custom type qualifiers to the javalanguage in a backward-compatible way.  the type system designer definesthe qualifiers and creates a compiler plug-in that enforces theirsemantics.  programmers can write the type qualifiers in their programs andbe informed of errors or assured that the program is free of those errors.the system builds on existing java tools and apis.in order to evaluate our framework, we have written four type-checkersusing the framework:  for a non-null type system that can detect andprevent null pointer errors; for an interned type system that can detectand prevent equality-checking errors; for a reference immutability typesystem, javari, that can detect and prevent mutation errors; and for areference and object immutability type system, igj, that can detect andprevent even more mutation errors.  we have conducted case studies usingeach checker to find real errors in existing software.  these case studiesdemonstrate that the checkers and the framework are practical and useful.",language models
Martin Rinard,"Kim, Deokhwan; Misailovic, Sasa; Rinard, Martin",2010-02-10T18:15:03Z,2010-02-10T18:15:03Z,2010-02-10,http://hdl.handle.net/1721.1/51680,MIT-CSAIL-TR-2010-007,Automatic Parallelization With Statistical Accuracy Bounds,"Traditional parallelizing compilers are designed to generate parallel programs that produce identical outputs as the original sequential program. The difficulty of performing the program analysis required to satisfy this goal and the restricted space of possible target parallel programs have both posed significant obstacles to the development of effective parallelizing compilers. The QuickStep compiler is instead designed to generate parallel programs that satisfy statistical accuracy guarantees. The freedom to generate parallel programs whose output may differ (within statistical accuracy bounds) from the output of the sequential program enables a dramatic simplification of the compiler and a significant expansion in the range of parallel programs that it can legally generate. QuickStep exploits this flexibility to take a fundamentally different approach from traditional parallelizing compilers. It applies a collection of transformations (loop parallelization, loop scheduling, synchronization introduction, and replication introduction) to generate a search space of parallel versions of the original sequential program. It then searches this space (prioritizing the parallelization of the most time-consuming loops in the application) to find a final parallelization that exhibits good parallel performance and satisfies the statistical accuracy guarantee. At each step in the search it performs a sequence of trial runs on representative inputs to examine the performance, accuracy, and memory accessing characteristics of the current generated parallel program. An analysis of these characteristics guides the steps the compiler takes as it explores the search space of parallel programs. Results from our benchmark set of applications show that QuickStep can automatically generate parallel programs with good performance and statistically accurate outputs. For two of the applications, the parallelization introduces noise into the output, but the noise remains within acceptable statistical bounds. The simplicity of the compilation strategy and the performance and statistical acceptability of the generated parallel programs demonstrate the advantages of the QuickStep approach.",,12 p.,,,,Computer Architecture,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2010,775,"automatic parallelization with statistical accuracy bounds traditional parallelizing compilers are designed to generate parallel programs that produce identical outputs as the original sequential program. the difficulty of performing the program analysis required to satisfy this goal and the restricted space of possible target parallel programs have both posed significant obstacles to the development of effective parallelizing compilers. the quickstep compiler is instead designed to generate parallel programs that satisfy statistical accuracy guarantees. the freedom to generate parallel programs whose output may differ (within statistical accuracy bounds) from the output of the sequential program enables a dramatic simplification of the compiler and a significant expansion in the range of parallel programs that it can legally generate. quickstep exploits this flexibility to take a fundamentally different approach from traditional parallelizing compilers. it applies a collection of transformations (loop parallelization, loop scheduling, synchronization introduction, and replication introduction) to generate a search space of parallel versions of the original sequential program. it then searches this space (prioritizing the parallelization of the most time-consuming loops in the application) to find a final parallelization that exhibits good parallel performance and satisfies the statistical accuracy guarantee. at each step in the search it performs a sequence of trial runs on representative inputs to examine the performance, accuracy, and memory accessing characteristics of the current generated parallel program. an analysis of these characteristics guides the steps the compiler takes as it explores the search space of parallel programs. results from our benchmark set of applications show that quickstep can automatically generate parallel programs with good performance and statistically accurate outputs. for two of the applications, the parallelization introduces noise into the output, but the noise remains within acceptable statistical bounds. the simplicity of the compilation strategy and the performance and statistical acceptability of the generated parallel programs demonstrate the advantages of the quickstep approach.",language models
,"Dolev, Shlomi; Lahiani, Limor; Lynch, Nancy; Nolte, Tina",2005-12-22T02:36:07Z,2005-12-22T02:36:07Z,2005-08-11,http://hdl.handle.net/1721.1/30563,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Self-Stabilizing Mobile Node Location Management and Message,"We present simple algorithms for achieving self-stabilizing locationmanagement and routing in mobile ad-hoc networks. While mobile clients maybe susceptible to corruption and stopping failures, mobile networks areoften deployed with a reliable GPS oracle, supplying frequent updates ofaccurate real time and location information to mobile nodes. Informationfrom a GPS oracle provides an external, shared source of consistency formobile nodes, allowing them to label and timestamp messages, and henceaiding in identification of, and eventual recovery from, corruption andfailures. Our algorithms use a GPS oracle.Our algorithms also take advantage of the Virtual Stationary Automataprogramming abstraction, consisting of mobile clients, virtual timedmachines called virtual stationary automata (VSAs), and a local broadcastservice connecting VSAs and mobile clients. VSAs are distributed at knownlocations over the plane, and emulated in a self-stabilizing manner by themobile nodes in the system. They serve as fault-tolerant building blocksthat can interact with mobile clients and each other, and can simplifyimplementations of services in mobile networks.We implement three self-stabilizing, fault-tolerant services, each builton the prior services: (1) VSA-to-VSA geographic routing, (2) mobileclient location management, and (3) mobile client end-to-end routing. Weuse a greedy version of the classical depth-first search algorithm toroute messages between VSAs in different regions. The mobile clientlocation management service is based on home locations: Each clientidentifier hashes to a set of home locations, regions whose VSAs areperiodically updated with the client\'s location. VSAs maintain thisinformation and answer queries for client locations. Finally, theVSA-to-VSA routing and location management services are used to implementmobile client end-to-end routing.",MIT-CSAIL-TR-2005-052; MIT-LCS-TR-999,20 p.; 27793653 bytes; 1205701 bytes,application/postscript; application/pdf,en_US,,Theory of Distributed Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,452,"self-stabilizing mobile node location management and message we present simple algorithms for achieving self-stabilizing locationmanagement and routing in mobile ad-hoc networks. while mobile clients maybe susceptible to corruption and stopping failures, mobile networks areoften deployed with a reliable gps oracle, supplying frequent updates ofaccurate real time and location information to mobile nodes. informationfrom a gps oracle provides an external, shared source of consistency formobile nodes, allowing them to label and timestamp messages, and henceaiding in identification of, and eventual recovery from, corruption andfailures. our algorithms use a gps oracle.our algorithms also take advantage of the virtual stationary automataprogramming abstraction, consisting of mobile clients, virtual timedmachines called virtual stationary automata (vsas), and a local broadcastservice connecting vsas and mobile clients. vsas are distributed at knownlocations over the plane, and emulated in a self-stabilizing manner by themobile nodes in the system. they serve as fault-tolerant building blocksthat can interact with mobile clients and each other, and can simplifyimplementations of services in mobile networks.we implement three self-stabilizing, fault-tolerant services, each builton the prior services: (1) vsa-to-vsa geographic routing, (2) mobileclient location management, and (3) mobile client end-to-end routing. weuse a greedy version of the classical depth-first search algorithm toroute messages between vsas in different regions. the mobile clientlocation management service is based on home locations: each clientidentifier hashes to a set of home locations, regions whose vsas areperiodically updated with the client\'s location. vsas maintain thisinformation and answer queries for client locations. finally, thevsa-to-vsa routing and location management services are used to implementmobile client end-to-end routing.",language models
,"Gassend, Blaise; Suh, G. Edward; Clarke, Dwaine; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:36:00Z,2023-03-29T15:36:00Z,2002-07,https://hdl.handle.net/1721.1/149957,MIT-LCS-TR-857,Caches and Merkle Trees for Efficient Memory Authentication,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,142,caches and merkle trees for efficient memory authentication ,high performance computing
Erik Demaine,"Hajiaghayi, MohammadTaghi; Kortsarz, Guy; Salavatipour, Mohammad R.",2006-01-05T20:42:38Z,2006-01-05T20:42:38Z,2005-11-26,http://hdl.handle.net/1721.1/30602,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Polylogarithmic Approximation Algorithm for Non-Uniform Multicommodity Buy-at-Bulk,"We consider the non-uniform multicommodity buy-at-bulknetworkdesign problem. In this problem we are given a graph $G(V,E)$withtwo cost functions on the edges, a buy cost $b:E\longrightarrow \RR^+$and a rent cost$r:E\longrightarrow\RR^+$, and a set of source-sink pairs$s_i,t_i\in V$ ($1\leq i\leq \alpha$)with each pair $i$ having a positivedemand $\delta_i$. Our goal is to designa minimum cost network $G(V,E')$such that for every $1\leq i\leq\alpha$,  $s_i$ and $t_i$ are in thesameconnected component in $G(V,E')$. Thetotal cost of $G(V,E')$ is the sum ofbuy costs of the edges in $E'$plus sum of total demand going through everyedge in $E'$ times therent cost of that edge. Since the costs of differentedges can bedifferent, we say that the problem is non-uniform. Thefirstnon-trivial approximation algorithm for this problem is due toCharikarand Karagiozova (STOC' 05) whose algorithm has anapproximation guarantee of$\exp(O(\sqrt{\log n\log\log n}))$,when all $\delta_i=1$ and$\exp(O(\sqrt{\log N\log\log N}))$ for the generaldemand case where $N$ isthe sum of all demands. We improve upon this result, bypresenting the firstpolylogarithmic (specifically, $O(\log^4 n)$ for unit demandsand $O(\log^4N)$ for the general demands)approximation for this problem. The algorithmrelies on a recent result\cite{HKS1} for the buy-at-bulk $k$-Steiner treeproblem.",MIT-CSAIL-TR-2006-002,16 p.; 19386222 bytes; 818306 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,479,"polylogarithmic approximation algorithm for non-uniform multicommodity buy-at-bulk we consider the non-uniform multicommodity buy-at-bulknetworkdesign problem. in this problem we are given a graph $g(v,e)$withtwo cost functions on the edges, a buy cost $b:e\longrightarrow \rr^+$and a rent cost$r:e\longrightarrow\rr^+$, and a set of source-sink pairs$s_i,t_i\in v$ ($1\leq i\leq \alpha$)with each pair $i$ having a positivedemand $\delta_i$. our goal is to designa minimum cost network $g(v,e')$such that for every $1\leq i\leq\alpha$,  $s_i$ and $t_i$ are in thesameconnected component in $g(v,e')$. thetotal cost of $g(v,e')$ is the sum ofbuy costs of the edges in $e'$plus sum of total demand going through everyedge in $e'$ times therent cost of that edge. since the costs of differentedges can bedifferent, we say that the problem is non-uniform. thefirstnon-trivial approximation algorithm for this problem is due tocharikarand karagiozova (stoc' 05) whose algorithm has anapproximation guarantee of$\exp(o(\sqrt{\log n\log\log n}))$,when all $\delta_i=1$ and$\exp(o(\sqrt{\log n\log\log n}))$ for the generaldemand case where $n$ isthe sum of all demands. we improve upon this result, bypresenting the firstpolylogarithmic (specifically, $o(\log^4 n)$ for unit demandsand $o(\log^4n)$ for the general demands)approximation for this problem. the algorithmrelies on a recent result\cite{hks1} for the buy-at-bulk $k$-steiner treeproblem.",high performance computing
,"Ajmani, Sameer; Morris, Robert T.; Liskov, Barbara H.",2023-03-29T15:35:41Z,2023-03-29T15:35:41Z,2001-05,https://hdl.handle.net/1721.1/149949,MIT-LCS-TR-847,A Trusted Third-Party Computation Service,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,50,a trusted third-party computation service ,high performance computing
Martin Rinard,"Jayaraman, Karthick; Rinard, Martin C.; Tripunitara, Mahesh; Ganesh, Vijay; Chapin, Steve",2010-05-06T17:30:07Z,2010-05-06T17:30:07Z,2010-05-05,http://hdl.handle.net/1721.1/54730,MIT-CSAIL-TR-2010-022,Automatic Error Finding in Access-Control Policies,"Access-control policies are a key infrastructural technology for computer security. However, a significant problem is that system administrators need to be able to automatically verify whether their policies capture the intended security goals. To address this important problem, researchers have proposed many automated verification techniques. Despite considerable progress in verification techniques, scalability is still a significant issue. Hence, in this paper we propose that error finding complements verification, and is a fruitful way of checking whether or not access control policies implement the security intent of system administrators. Error finding is more scalable (at the cost of completeness), and allows for the use of a wider variety of techniques. In this paper, we describe an abstraction-refinement based technique and its implementation, the Mohawk tool, aimed at finding errors in ARBAC access-control policies. The key insight behind our abstraction-refinement technique is that it is more efficient to look for errors in an abstract policy (with successive refinements, if necessary) than its complete counterpart. Mohawk accepts as input an access-control policy and a safety question. If Mohawk finds an error in the input policy, it terminates with a sequence of actions that cause the error. We provide an extensive comparison of Mohawk with the current state-of-the-art analysis tools. We show that Mohawk scales very well as the size and complexity of the input policies increase, and is orders of magnitude faster than competing tools. The Mohawk tool is open source and available from the Google Code website: http://code.google.com/p/mohawk/",,12 p.,,,access-control policies; error finding; bounded model-checking; abstraction refinement,Computer Architecture,,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,http://code.google.com/p/mohawk/,,,,,,,,,,,,,,,,,,,2010,787,"automatic error finding in access-control policies access-control policies are a key infrastructural technology for computer security. however, a significant problem is that system administrators need to be able to automatically verify whether their policies capture the intended security goals. to address this important problem, researchers have proposed many automated verification techniques. despite considerable progress in verification techniques, scalability is still a significant issue. hence, in this paper we propose that error finding complements verification, and is a fruitful way of checking whether or not access control policies implement the security intent of system administrators. error finding is more scalable (at the cost of completeness), and allows for the use of a wider variety of techniques. in this paper, we describe an abstraction-refinement based technique and its implementation, the mohawk tool, aimed at finding errors in arbac access-control policies. the key insight behind our abstraction-refinement technique is that it is more efficient to look for errors in an abstract policy (with successive refinements, if necessary) than its complete counterpart. mohawk accepts as input an access-control policy and a safety question. if mohawk finds an error in the input policy, it terminates with a sequence of actions that cause the error. we provide an extensive comparison of mohawk with the current state-of-the-art analysis tools. we show that mohawk scales very well as the size and complexity of the input policies increase, and is orders of magnitude faster than competing tools. the mohawk tool is open source and available from the google code website: http://code.google.com/p/mohawk/",high performance computing
Trevor Darrell,"Christoudias, C. Mario; Urtasun, Raquel; Darrell, Trevor",2008-02-19T13:45:16Z,2008-02-19T13:45:16Z,2008-02-17,http://hdl.handle.net/1721.1/40286,,Unsupervised Distributed Feature Selection for Multi-view Object Recognition,"Object recognition accuracy can be improved when information frommultiple views is integrated, but information in each view can oftenbe highly redundant. We consider the problem of distributed objectrecognition or indexing from multiple cameras, where thecomputational power available at each camera sensor is limited andcommunication between sensors is prohibitively expensive. In thisscenario, it is desirable to avoid sending redundant visual featuresfrom multiple views, but traditional supervised feature selectionapproaches are inapplicable as the class label is unknown at thecamera. In this paper we propose an unsupervised multi-view featureselection algorithm based on a distributed compression approach.With our method, a Gaussian Process model of the joint viewstatistics is used at the receiver to obtain a joint encoding of theviews without directly sharing information across encoders. Wedemonstrate our approach on recognition and indexing tasks withmulti-view image databases and show that our method comparesfavorably to an independent encoding of the features from eachcamera.",MIT-CSAIL-TR-2008-009,10 p.,,,Distributed Compression; Gaussian Processes; Multi-view Object Recognition,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,640,"unsupervised distributed feature selection for multi-view object recognition object recognition accuracy can be improved when information frommultiple views is integrated, but information in each view can oftenbe highly redundant. we consider the problem of distributed objectrecognition or indexing from multiple cameras, where thecomputational power available at each camera sensor is limited andcommunication between sensors is prohibitively expensive. in thisscenario, it is desirable to avoid sending redundant visual featuresfrom multiple views, but traditional supervised feature selectionapproaches are inapplicable as the class label is unknown at thecamera. in this paper we propose an unsupervised multi-view featureselection algorithm based on a distributed compression approach.with our method, a gaussian process model of the joint viewstatistics is used at the receiver to obtain a joint encoding of theviews without directly sharing information across encoders. wedemonstrate our approach on recognition and indexing tasks withmulti-view image databases and show that our method comparesfavorably to an independent encoding of the features from eachcamera.",object recognition/detection
,"Kumar, Vinay; Poggio, Tomaso",2004-10-20T21:04:37Z,2004-10-20T21:04:37Z,2000-09-01,http://hdl.handle.net/1721.1/7264,AIM-1696; CBCL-191,Learning-Based Approach to Estimation of Morphable Model Parameters,"We describe the key role played by partial  evaluation in the Supercomputing Toolkit, a  parallel computing system for scientific  applications that effectively exploits the vast  amount of parallelism exposed by partial  evaluation. The Supercomputing Toolkit  parallel processor and its associated partial  evaluation-based compiler have been used  extensively by scientists at MIT, and have  made possible recent results in astrophysics  showing that the motion of the planets in our  solar system is chaotically unstable.",AIM-1696; CBCL-191,1037544 bytes; 218112 bytes,application/postscript; application/pdf,en_US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,22,"learning-based approach to estimation of morphable model parameters we describe the key role played by partial  evaluation in the supercomputing toolkit, a  parallel computing system for scientific  applications that effectively exploits the vast  amount of parallelism exposed by partial  evaluation. the supercomputing toolkit  parallel processor and its associated partial  evaluation-based compiler have been used  extensively by scientists at mit, and have  made possible recent results in astrophysics  showing that the motion of the planets in our  solar system is chaotically unstable.",high performance computing
,"Tan, Godfrey",2023-03-29T15:36:12Z,2023-03-29T15:36:12Z,2002-10,https://hdl.handle.net/1721.1/149962,MIT-LCS-TR-866,Blueware: Bluetooth Simulator for ns,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,161,blueware: bluetooth simulator for ns ,high performance computing
,"Katti, Sachin; Katabi, Dina; Puchala, Katarzyna",2005-12-22T02:36:13Z,2005-12-22T02:36:13Z,2005-08-15,http://hdl.handle.net/1721.1/30564,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Slicing the Onion: Anonymous Routing Without PKI,"Recent years have witnessed many proposals for anonymous routing in overlay peer-to-peer networks. The proposed protocols either expose the receiver and the message content, or require the overlay nodes to have public-private key pairs with the public keys known to everyone. In practice, however, key distribution and management are well-known difficultproblems and have crippled any widespread deployment of anonymous routing. This paper uses a combination of information slicing and source routing to provide anonymous communication in a way similar to Onion Routing but without a public key infrastructure (PKI).",MIT-CSAIL-TR-2005-053; MIT-LCS-TR-1000,7 p.; 13490568 bytes; 735059 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,453,"slicing the onion: anonymous routing without pki recent years have witnessed many proposals for anonymous routing in overlay peer-to-peer networks. the proposed protocols either expose the receiver and the message content, or require the overlay nodes to have public-private key pairs with the public keys known to everyone. in practice, however, key distribution and management are well-known difficultproblems and have crippled any widespread deployment of anonymous routing. this paper uses a combination of information slicing and source routing to provide anonymous communication in a way similar to onion routing but without a public key infrastructure (pki).",language models
,"Zee, Karen; Rinard, Martin",2023-03-29T15:35:09Z,2023-03-29T15:35:09Z,2002-02,https://hdl.handle.net/1721.1/149937,MIT-LCS-TR-834,Write Barrier Removal by Static Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,106,write barrier removal by static analysis ,object recognition/detection
,"Shen, Jiasi; Rinard, MArtin",2018-09-27T13:22:38Z,2018-09-27T13:22:38Z,2018-09-27,http://hdl.handle.net/1721.1/118184,,Using Dynamic Monitoring to Synthesize Models of Applications That  Access Databases,"We previously developed Konure, a tool that uses active learning to 
infer the functionality of database applications. An alternative 
approach is to observe the inputs, outputs, and database traffic from a 
running system in normal use and then synthesize a model of the 
application from this information.  To evaluate these two approaches, we 
present Etch, which uses information from typical usage scenarios to 
synthesize a model of the functionality of database applications whose 
computation can be expressed in the Konure DSL.",,,,en_US,,,,,,,,,,,,,,,,,,,Article,,,,,,,,,,,,2018,1059,"using dynamic monitoring to synthesize models of applications that  access databases we previously developed konure, a tool that uses active learning to 
infer the functionality of database applications. an alternative 
approach is to observe the inputs, outputs, and database traffic from a 
running system in normal use and then synthesize a model of the 
application from this information.  to evaluate these two approaches, we 
present etch, which uses information from typical usage scenarios to 
synthesize a model of the functionality of database applications whose 
computation can be expressed in the konure dsl.",language models
Nancy Lynch,"Canetti, Ran; Cheung, Ling; Kaynar, Dilsun; Liskov, Moses; Lynch, Nancy; Pereira, Olivier; Segala, Roberto",2006-06-19T18:52:04Z,2006-06-19T18:52:04Z,2006-06-19,http://hdl.handle.net/1721.1/33154,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Using Probabilistic I/O Automata to Analyze an Oblivious Transfer Protocol,"We demonstrate how to carry out cryptographic security analysis ofdistributed protocols within the Probabilistic I/O Automataframework of Lynch, Segala, and Vaandrager. This framework providestools for arguing rigorously about the concurrency and schedulingaspects of protocols, and about protocols presented at differentlevels of abstraction. Consequently, it can help in makingcryptographic analysis more precise and less susceptible to errors.We concentrate on a relatively simple two-party Oblivious Transferprotocol, in the presence of a semi-honest adversary (essentially,an eavesdropper). For the underlying cryptographic notion ofsecurity, we use a version of Canetti's Universally Composablesecurity.In spite of the relative simplicity of the example, the exercise isquite nontrivial. It requires taking many fundamental issues intoaccount, including nondeterministic behavior, scheduling,resource-bounded computation, and computational hardness assumptionsfor cryptographic primitives.",MIT-CSAIL-TR-2006-046,129 p.; 1111678 bytes; 7337435 bytes,application/pdf; application/postscript,en_US,,Theory of Computation,,,,,"January 10, 2006",,,,,http://hdl.handle.net/1721.1/30566,http://hdl.handle.net/1721.1/30566,,,,,,,,,,,,,,,,,,2006,526,"using probabilistic i/o automata to analyze an oblivious transfer protocol we demonstrate how to carry out cryptographic security analysis ofdistributed protocols within the probabilistic i/o automataframework of lynch, segala, and vaandrager. this framework providestools for arguing rigorously about the concurrency and schedulingaspects of protocols, and about protocols presented at differentlevels of abstraction. consequently, it can help in makingcryptographic analysis more precise and less susceptible to errors.we concentrate on a relatively simple two-party oblivious transferprotocol, in the presence of a semi-honest adversary (essentially,an eavesdropper). for the underlying cryptographic notion ofsecurity, we use a version of canetti's universally composablesecurity.in spite of the relative simplicity of the example, the exercise isquite nontrivial. it requires taking many fundamental issues intoaccount, including nondeterministic behavior, scheduling,resource-bounded computation, and computational hardness assumptionsfor cryptographic primitives.",language models
,"Liskov, Moses",2005-12-22T12:00:00Z,2005-12-22T12:00:00Z,2003-10-14,http://hdl.handle.net/1721.1/30427,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Electronic Cash with Blind Deposits: How to Have No Spare Change,"Electronic cash schemes in which the bank authenticates many coins at once suffer from the problem that coins that are authenticated together can be linked to one another. Unfortunately, unless a user spends coins in a closely prescribed manner, different batches of coins (""wallets"") will be linked together in these schemes. This is illustrated by the problem of what a customer does with the ""spare change"" - an unusable small amount of money left in a wallet. We propose a new protocol to be used in e-cash schemes: blind deposits. In a blind deposit, a customer returns a coin to the bank without revealing the coin. We present a secure and efficient e-cash scheme with this added feature based on that of Liskov-Micali [LM01].",MIT-CSAIL-TR-2003-022; MIT-LCS-TM-639,13 p.; 12754502 bytes; 526739 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,271,"electronic cash with blind deposits: how to have no spare change electronic cash schemes in which the bank authenticates many coins at once suffer from the problem that coins that are authenticated together can be linked to one another. unfortunately, unless a user spends coins in a closely prescribed manner, different batches of coins (""wallets"") will be linked together in these schemes. this is illustrated by the problem of what a customer does with the ""spare change"" - an unusable small amount of money left in a wallet. we propose a new protocol to be used in e-cash schemes: blind deposits. in a blind deposit, a customer returns a coin to the bank without revealing the coin. we present a secure and efficient e-cash scheme with this added feature based on that of liskov-micali [lm01].",language models
Joshua Tenenbaum,"Salakhutdinov, Ruslan; Hinton, Geoffrey",2010-08-04T15:15:39Z,2010-08-04T15:15:39Z,2010-08-04,http://hdl.handle.net/1721.1/57474,MIT-CSAIL-TR-2010-037,An Efficient Learning Procedure for Deep Boltzmann Machines,"We present a new learning algorithm for Boltzmann Machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann Machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer ""pre-training"" phase that initializes the weights sensibly. The pre-training also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB datasets showing that Deep Boltzmann Machines learn very good generative models of hand-written digits and 3-D objects. We also show that the features discovered by Deep Boltzmann Machines are a very effective way to initialize the hidden layers of feed-forward neural nets which are then discriminatively fine-tuned.",,32 p.,,,Deep learning; Graphical models; Boltzmann Machines,Computational Cognitive Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,803,"an efficient learning procedure for deep boltzmann machines we present a new learning algorithm for boltzmann machines that contain many layers of hidden variables. data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent markov chains. the use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn boltzmann machines with multiple hidden layers and millions of parameters. the learning can be made more efficient by using a layer-by-layer ""pre-training"" phase that initializes the weights sensibly. the pre-training also allows the variational inference to be initialized sensibly with a single bottom-up pass. we present results on the mnist and norb datasets showing that deep boltzmann machines learn very good generative models of hand-written digits and 3-d objects. we also show that the features discovered by deep boltzmann machines are a very effective way to initialize the hidden layers of feed-forward neural nets which are then discriminatively fine-tuned.",high performance computing
,"Keidar, Idit; Rajsbaum, Sergio",2023-03-29T15:34:18Z,2023-03-29T15:34:18Z,2001-05,https://hdl.handle.net/1721.1/149924,MIT-LCS-TR-821,On the Cost of Fault-Tolerant Consensus When There Are No Faults - A Tutorial,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,53,on the cost of fault-tolerant consensus when there are no faults - a tutorial ,language models
Tomaso Poggio,"Terashima, Yoshito",2009-05-11T17:30:10Z,2009-05-11T17:30:10Z,2009-05-10,http://hdl.handle.net/1721.1/45516,CBCL-277; MIT-CSAIL-TR-2009-020,Scene Classification with a Biologically Inspired Method,"We present a biologically motivated method for scene image classification. The core of the method is to use shape based image property that is provided by a hierarchical feedforward model of the visual cortex [18]. Edge based and color based image properties are additionally used to improve the accuracy. The method consists of two stages of image analysis. In the first stage, each of three paths of classification uses each image property (i.e. shape, edge or color based features) independently. In the second stage, a single classifier assigns the category of an image based on the probability distributions of the first stage classifier outputs. Experiments show that the method boosts the classification accuracy over the shape based model. We demonstrate that this method achieves a high accuracy comparable to other reported methods on publicly available color image dataset.",,8 p.,,,image classification; vision,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,723,"scene classification with a biologically inspired method we present a biologically motivated method for scene image classification. the core of the method is to use shape based image property that is provided by a hierarchical feedforward model of the visual cortex [18]. edge based and color based image properties are additionally used to improve the accuracy. the method consists of two stages of image analysis. in the first stage, each of three paths of classification uses each image property (i.e. shape, edge or color based features) independently. in the second stage, a single classifier assigns the category of an image based on the probability distributions of the first stage classifier outputs. experiments show that the method boosts the classification accuracy over the shape based model. we demonstrate that this method achieves a high accuracy comparable to other reported methods on publicly available color image dataset.",image classification
Silvio Micali,"Micali, Silvio; Chen, Jing",2009-12-09T21:15:08Z,2009-12-09T21:15:08Z,2009-12-04,http://hdl.handle.net/1721.1/49869,MIT-CSAIL-TR-2009-061,Perfect and General Virtual Implementation For Perfectly Informed Players,"We show that, when the players are perfectly informed about each other, essentially all social-choice functions can be rationally robustly implemented via an extensive-form public-action mechanism that (1) is perfectly robust against collusion, (2) requires only a linear number of computation steps and communication bits, and (3) preserves the privacy of the players' types to a very high extent.",,4 p.,,,rationally robust implementation; Virtual implementation; perfectly informed players,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,766,"perfect and general virtual implementation for perfectly informed players we show that, when the players are perfectly informed about each other, essentially all social-choice functions can be rationally robustly implemented via an extensive-form public-action mechanism that (1) is perfectly robust against collusion, (2) requires only a linear number of computation steps and communication bits, and (3) preserves the privacy of the players' types to a very high extent.",privacy/ethics
,"Larsen, Samuel; Witchel, Emmett; Amarasinghe, Saman",2023-03-29T14:42:27Z,2023-03-29T14:42:27Z,2001-11,https://hdl.handle.net/1721.1/149310,MIT-LCS-TM-621,Techniques for Increasing and Detecting Memory Alignment,"Memory alignment is an important property in memory system performance. Extraction of alignment information at compile-time enables the possibility for new classes of program optimization. In this paper, we present methods for increasing and detecting the alignment of memory references in a program. Our transformations and analyses do not require interprocedural analysis and introduce almost no overhead. As a result, they can be incorporated into real compilation systems. On average, our techniques are able to achieve a five-fold increase in the number of dynamically aligned memory references. We are then able to detect 94% of these operations. This success is invaluable in providing performance gains in a range of different areas. When alignment information is incorporated into a vectorizing compiler, we can increase the performance of a G4 AltiVec processor by more than a factor of two. Using the same methods, we are able to reduce energy consumption in a data cache by as much as 35%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,91,"techniques for increasing and detecting memory alignment memory alignment is an important property in memory system performance. extraction of alignment information at compile-time enables the possibility for new classes of program optimization. in this paper, we present methods for increasing and detecting the alignment of memory references in a program. our transformations and analyses do not require interprocedural analysis and introduce almost no overhead. as a result, they can be incorporated into real compilation systems. on average, our techniques are able to achieve a five-fold increase in the number of dynamically aligned memory references. we are then able to detect 94% of these operations. this success is invaluable in providing performance gains in a range of different areas. when alignment information is incorporated into a vectorizing compiler, we can increase the performance of a g4 altivec processor by more than a factor of two. using the same methods, we are able to reduce energy consumption in a data cache by as much as 35%.",high performance computing
Patrick Winston,"Finlayson, Mark Alan",2016-11-08T23:00:04Z,2016-11-08T23:00:04Z,2016-11-08,http://hdl.handle.net/1721.1/105270,MIT-CSAIL-TR-2016-014,Report on the 2015 NSF Workshop on Unified Annotation Tooling,"On March 30 & 31, 2015, an international group of twenty-three researchers with expertise in linguistic annotation convened in Sunny Isles Beach, Florida to discuss problems with and potential solutions for the state of linguistic annotation tooling. The participants comprised 14 researchers from the U.S. and 9 from outside the U.S., with 7 countries and 4 continents represented, and hailed from fields and specialties including computational linguistics, artificial intelligence, speech processing, multi-modal data processing, clinical & medical natural language processing, linguistics, documentary linguistics, sign-language linguistics, corpus linguistics, and the digital humanities. The motivating problem of the workshop was the balkanization of annotation tooling, namely, that even though linguistic annotation requires sophisticated tool support to efficiently generate high-quality data, the landscape of tools for the field is fractured, incompatible, inconsistent, and lacks key capabilities. The overall goal of the workshop was to chart the way forward, centering on five key questions: (1) What are the problems with current tool landscape? (2) What are the possible benefits of solving some or all of these problems? (3) What capabilities are most needed? (4) How should we go about implementing these capabilities? And, (5) How should we ensure longevity and sustainability of the solution? I surveyed the participants before their arrival, which provided significant raw material for ideas, and the workshop discussion itself resulted in identification of ten specific classes of problems, five sets of most-needed capabilities. Importantly, we identified annotation project managers in computational linguistics as the key recipients and users of any solution, thereby succinctly addressing questions about the scope and audience of potential solutions. We discussed management and sustainability of potential solutions at length. The participants agreed on sixteen recommendations for future work. This technical report contains a detailed discussion of all these topics, a point-by-point review of the discussion in the workshop as it unfolded, detailed information on the participants and their expertise, and the summarized data from the surveys.",,61 p.,,,Linguistic Annotation; Annotation Tools & Resources; Natural Language Processing,Genesis,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2016-11-08T23:00:04Z,,,,,,,,,,,,,,,,2016,1032,"report on the 2015 nsf workshop on unified annotation tooling on march 30 & 31, 2015, an international group of twenty-three researchers with expertise in linguistic annotation convened in sunny isles beach, florida to discuss problems with and potential solutions for the state of linguistic annotation tooling. the participants comprised 14 researchers from the u.s. and 9 from outside the u.s., with 7 countries and 4 continents represented, and hailed from fields and specialties including computational linguistics, artificial intelligence, speech processing, multi-modal data processing, clinical & medical natural language processing, linguistics, documentary linguistics, sign-language linguistics, corpus linguistics, and the digital humanities. the motivating problem of the workshop was the balkanization of annotation tooling, namely, that even though linguistic annotation requires sophisticated tool support to efficiently generate high-quality data, the landscape of tools for the field is fractured, incompatible, inconsistent, and lacks key capabilities. the overall goal of the workshop was to chart the way forward, centering on five key questions: (1) what are the problems with current tool landscape? (2) what are the possible benefits of solving some or all of these problems? (3) what capabilities are most needed? (4) how should we go about implementing these capabilities? and, (5) how should we ensure longevity and sustainability of the solution? i surveyed the participants before their arrival, which provided significant raw material for ideas, and the workshop discussion itself resulted in identification of ten specific classes of problems, five sets of most-needed capabilities. importantly, we identified annotation project managers in computational linguistics as the key recipients and users of any solution, thereby succinctly addressing questions about the scope and audience of potential solutions. we discussed management and sustainability of potential solutions at length. the participants agreed on sixteen recommendations for future work. this technical report contains a detailed discussion of all these topics, a point-by-point review of the discussion in the workshop as it unfolded, detailed information on the participants and their expertise, and the summarized data from the surveys.",speech/audio recognition
Martin Rinard,"Carbin, Michael; Misailovic, Sasa; Rinard, Martin",2013-06-20T17:00:08Z,2013-06-20T17:00:08Z,2013-06-19,http://hdl.handle.net/1721.1/79355,MIT-CSAIL-TR-2013-014,Verifying Quantitative Reliability of Programs That Execute on Unreliable Hardware,"Emerging high-performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. Full detection and recovery from soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors. In this paper we present Rely, a programming language that enables developers to reason about the quantitative reliability of an application -- namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces. We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification, that characterizes the reliability of the underlying hardware components, and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely.",,22 p.,,,"unreliable hardware, probabilistic semantics, quantitative reliability",Computer Architecture,"This research was supported in part by the National Science Foundation (Grants CCF-0905244, CCF-1036241, CCF-1138967, CCF-1138967, and IIS-0835652), the United States Department of Energy (Grant DE-SC0008923), and DARPA (Grants FA8650-11-C-7192, FA8750-12-2-0110).",,,,,,,,,,,,2013-06-20T17:00:08Z,,,,,,,,,,,,en,,,,2013,929,"verifying quantitative reliability of programs that execute on unreliable hardware emerging high-performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. full detection and recovery from soft errors is challenging, expensive, and, for some applications, unnecessary. for example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors. in this paper we present rely, a programming language that enables developers to reason about the quantitative reliability of an application -- namely, the probability that it produces the correct result when executed on unreliable hardware. rely allows developers to specify the reliability requirements for each value that a function produces. we present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. the analysis takes a rely program with a reliability specification and a hardware specification, that characterizes the reliability of the underlying hardware components, and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. we demonstrate the application of quantitative reliability analysis on six computations implemented in rely.",language models
Saman Amarasinghe,"Amarasinghe, Saman; Rabbah, Rodric; Larsen, Samuel",2009-12-18T19:30:12Z,2009-12-18T19:30:12Z,2009-12-18,http://hdl.handle.net/1721.1/50235,MIT-CSAIL-TR-2009-064,Selective Vectorization for Short-Vector Instructions,"Multimedia extensions are nearly ubiquitous in today's general-purpose processors. These extensions consist primarily of a set of short-vector instructions that apply the same opcode to a vector of operands. Vector instructions introduce a data-parallel component to processors that exploit instruction-level parallelism, and present an opportunity for increased performance. In fact, ignoring a processor's vector opcodes can leave a significant portion of the available resources unused. In order for software developers to find short-vector instructions generally useful, however, the compiler must target these extensions with complete transparency and consistent performance. This paper describes selective vectorization, a technique for balancing computation across a processor's scalar and vector units. Current approaches for targeting short-vector instructions directly adopt vectorizing technology first developed for supercomputers. Traditional vectorization, however, can lead to a performance degradation since it fails to account for a processor's scalar resources. We formulate selective vectorization in the context of software pipelining. Our approach creates software pipelines with shorter initiation intervals, and therefore, higher performance. A key aspect of selective vectorization is its ability to manage transfer of operands between vector and scalar instructions. Even when operand transfer is expensive, our technique is sufficiently sophisticated to achieve significant performance gains. We evaluate selective vectorization on a set of SPEC FP benchmarks. On a realistic VLIW processor model, the approach achieves whole-program speedups of up to 1.35x over existing approaches. For individual loops, it provides speedups of up to 1.75x.",,25 p.,,,SIMD; Vectorization; Compiler,Computer Architecture,,Creative Commons Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2009,769,"selective vectorization for short-vector instructions multimedia extensions are nearly ubiquitous in today's general-purpose processors. these extensions consist primarily of a set of short-vector instructions that apply the same opcode to a vector of operands. vector instructions introduce a data-parallel component to processors that exploit instruction-level parallelism, and present an opportunity for increased performance. in fact, ignoring a processor's vector opcodes can leave a significant portion of the available resources unused. in order for software developers to find short-vector instructions generally useful, however, the compiler must target these extensions with complete transparency and consistent performance. this paper describes selective vectorization, a technique for balancing computation across a processor's scalar and vector units. current approaches for targeting short-vector instructions directly adopt vectorizing technology first developed for supercomputers. traditional vectorization, however, can lead to a performance degradation since it fails to account for a processor's scalar resources. we formulate selective vectorization in the context of software pipelining. our approach creates software pipelines with shorter initiation intervals, and therefore, higher performance. a key aspect of selective vectorization is its ability to manage transfer of operands between vector and scalar instructions. even when operand transfer is expensive, our technique is sufficiently sophisticated to achieve significant performance gains. we evaluate selective vectorization on a set of spec fp benchmarks. on a realistic vliw processor model, the approach achieves whole-program speedups of up to 1.35x over existing approaches. for individual loops, it provides speedups of up to 1.75x.",high performance computing
Anant Agarwal,"Hoffmann, Henry; Maggio, Martina; Santambrogio, Marco D.; Leva, Alberto; Agarwal, Anant",2010-10-22T23:15:19Z,2010-10-22T23:15:19Z,2010-10-13,http://hdl.handle.net/1721.1/59519,MIT-CSAIL-TR-2010-049,SEEC: A Framework for Self-aware Computing,"As the complexity of computing systems increases, application programmers must be experts in their application domain and have the systems knowledge required to address the problems that arise from parallelism, power, energy, and reliability concerns. One approach to relieving this burden is to make use of self-aware computing systems, which automatically adjust their behavior to help applications achieve their goals. This paper presents the SEEC framework, a unified computational model designed to enable self-aware computing in both applications and system software. In the SEEC model, applications specify goals, system software specifies possible actions, and the SEEC framework is responsible for deciding how to use the available actions to meet the application-specified goals. The SEEC framework is built around a general and extensible control system which provides predictable behavior and allows SEEC to make decisions that achieve goals while optimizing resource utilization. To demonstrate the applicability of the SEEC framework, this paper presents fivedifferent self-aware systems built using SEEC. Case studies demonstrate how these systems can control the performance of the PARSEC benchmarks, optimize performance per Watt for a video encoder, and respond to unexpected changes in the underlying environment. In general these studies demonstrate that systems built using the SEEC framework are goal-oriented, predictable, adaptive, and extensible.",,13 p.,,,Autonomic Computing; Adaptive Computing; Multicore; Control Theory,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,816,"seec: a framework for self-aware computing as the complexity of computing systems increases, application programmers must be experts in their application domain and have the systems knowledge required to address the problems that arise from parallelism, power, energy, and reliability concerns. one approach to relieving this burden is to make use of self-aware computing systems, which automatically adjust their behavior to help applications achieve their goals. this paper presents the seec framework, a unified computational model designed to enable self-aware computing in both applications and system software. in the seec model, applications specify goals, system software specifies possible actions, and the seec framework is responsible for deciding how to use the available actions to meet the application-specified goals. the seec framework is built around a general and extensible control system which provides predictable behavior and allows seec to make decisions that achieve goals while optimizing resource utilization. to demonstrate the applicability of the seec framework, this paper presents fivedifferent self-aware systems built using seec. case studies demonstrate how these systems can control the performance of the parsec benchmarks, optimize performance per watt for a video encoder, and respond to unexpected changes in the underlying environment. in general these studies demonstrate that systems built using the seec framework are goal-oriented, predictable, adaptive, and extensible.",high performance computing
Tomaso Poggio,"Mroueh, Youssef; Poggio, Tomaso; Rosasco, Lorenzo; Slotine, Jean-Jacques E.",2011-09-27T20:30:07Z,2011-09-27T20:30:07Z,2011-09-27,http://hdl.handle.net/1721.1/66085,MIT-CSAIL-TR-2011-043; CBCL-305,Multi-Class Learning: Simplex Coding And Relaxation Error,"We study multi-category classification in the framework of computational learning theory. We show how a relaxation approach, which is commonly used in binary classification, can be generalized to the multi-class setting. We propose a vector coding, namely the simplex coding, that allows to introduce a new notion of multi-class margin and cast multi-category classification into a vector valued regression problem. The analysis of the relaxation error be quantified and the binary case is recovered as a special case of our theory. From a computational point of view we can show that using the simplex coding we can design regularized learning algorithms for multi-category classification that can be trained at a complexity which is independent to the number of classes.",,3 p.,,,computational learning; machine learning; convex relaxation,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,870,"multi-class learning: simplex coding and relaxation error we study multi-category classification in the framework of computational learning theory. we show how a relaxation approach, which is commonly used in binary classification, can be generalized to the multi-class setting. we propose a vector coding, namely the simplex coding, that allows to introduce a new notion of multi-class margin and cast multi-category classification into a vector valued regression problem. the analysis of the relaxation error be quantified and the binary case is recovered as a special case of our theory. from a computational point of view we can show that using the simplex coding we can design regularized learning algorithms for multi-category classification that can be trained at a complexity which is independent to the number of classes.",high performance computing
Daniel Weitzner,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",2015-07-07T02:15:02Z,2015-07-07T02:15:02Z,2015-07-06,http://hdl.handle.net/1721.1/97690,MIT-CSAIL-TR-2015-026,Keys Under Doormats: Mandating insecurity by requiring government access to all data and communications,"Twenty years ago, law enforcement organizations lobbied to require data and communication services to engineer their products to guarantee law enforcement access to all data. After lengthy debate and vigorous predictions of enforcement channels going dark, these attempts to regulate the emerging Internet were abandoned. In the intervening years, innovation on the Internet flourished, and law enforcement agencies found new and more effective means of accessing vastly larger quantities of data. Today we are again hearing calls for regulation to mandate the provision of exceptional access mechanisms. In this report, a group of computer scientists and security experts, many of whom participated in a 1997 study of these same topics, has convened to explore the likely effects of imposing extraordinary access mandates. We have found that the damage that could be caused by law enforcement exceptional access requirements would be even greater today than it would have been 20 years ago. In the wake of the growing economic and social cost of the fundamental insecurity of today's Internet environment, any proposals that alter the security dynamics online should be approached with caution. Exceptional access would force Internet system developers to reverse forward secrecy design practices that seek to minimize the impact on user privacy when systems are breached. The complexity of today's Internet environment, with millions of apps and globally connected services, means that new law enforcement requirements are likely to introduce unanticipated, hard to detect security flaws. Beyond these and other technical vulnerabilities, the prospect of globally deployed exceptional access systems raises difficult problems about how such an environment would be governed and how to ensure that such systems would respect human rights and the rule of law.",,34 p.,,,,Decentralized Information Group,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2015-07-07T16:15:15Z,,,,,"Abelson, Harold; Anderson, Ross; Bellovin, Steven M.; Benaloh, Josh; Blaze, Matt; Diffie, Whitfield; Gilmore, John; Green, Matthew; Landau, Susan; Neumann, Peter G.; Rivest, Ronald L.; Schiller, Jeffrey I.; Schneier, Bruce; Specter, Michael; Weitzner, Daniel J.",,,,,,,,,,,2015,1006,"keys under doormats: mandating insecurity by requiring government access to all data and communications twenty years ago, law enforcement organizations lobbied to require data and communication services to engineer their products to guarantee law enforcement access to all data. after lengthy debate and vigorous predictions of enforcement channels going dark, these attempts to regulate the emerging internet were abandoned. in the intervening years, innovation on the internet flourished, and law enforcement agencies found new and more effective means of accessing vastly larger quantities of data. today we are again hearing calls for regulation to mandate the provision of exceptional access mechanisms. in this report, a group of computer scientists and security experts, many of whom participated in a 1997 study of these same topics, has convened to explore the likely effects of imposing extraordinary access mandates. we have found that the damage that could be caused by law enforcement exceptional access requirements would be even greater today than it would have been 20 years ago. in the wake of the growing economic and social cost of the fundamental insecurity of today's internet environment, any proposals that alter the security dynamics online should be approached with caution. exceptional access would force internet system developers to reverse forward secrecy design practices that seek to minimize the impact on user privacy when systems are breached. the complexity of today's internet environment, with millions of apps and globally connected services, means that new law enforcement requirements are likely to introduce unanticipated, hard to detect security flaws. beyond these and other technical vulnerabilities, the prospect of globally deployed exceptional access systems raises difficult problems about how such an environment would be governed and how to ensure that such systems would respect human rights and the rule of law.",privacy/ethics
Daniel Sanchez,"Beckmann, Nathan; Sanchez, Daniel",2015-12-21T19:00:15Z,2015-12-21T19:00:15Z,2015-12-19,http://hdl.handle.net/1721.1/100465,MIT-CSAIL-TR-2015-034,Bridging Theory and Practice in Cache Replacement,"Much prior work has studied processor cache replacement policies, but a large gap remains between theory and practice. The optimal policy (MIN) requires unobtainable knowledge of the future, and prior theoretically-grounded policies use reference models that do not match real programs. Meanwhile, practical policies are designed empirically. Lacking a strong theoretical foundation, they do not make the best use of the information available to them. This paper bridges theory and practice. We propose that practical policies should replace lines based on their economic value added (EVA), the difference of their expected hits from the average. We use Markov decision processes to show that EVA is optimal under some reasonable simplifications. We present an inexpensive, practical implementation of EVA and evaluate it exhaustively over many cache sizes. EVA outperforms prior practical policies and saves area at iso-performance. These results show that formalizing cache replacement yields practical benefits.",,14 p.,,,,Computer Architecture,,Creative Commons Attribution 4.0 International,http://creativecommons.org/licenses/by/4.0/,,,,,,,,,,2015-12-21T19:00:15Z,,,,,,,,,,,,,,,,2015,1016,"bridging theory and practice in cache replacement much prior work has studied processor cache replacement policies, but a large gap remains between theory and practice. the optimal policy (min) requires unobtainable knowledge of the future, and prior theoretically-grounded policies use reference models that do not match real programs. meanwhile, practical policies are designed empirically. lacking a strong theoretical foundation, they do not make the best use of the information available to them. this paper bridges theory and practice. we propose that practical policies should replace lines based on their economic value added (eva), the difference of their expected hits from the average. we use markov decision processes to show that eva is optimal under some reasonable simplifications. we present an inexpensive, practical implementation of eva and evaluate it exhaustively over many cache sizes. eva outperforms prior practical policies and saves area at iso-performance. these results show that formalizing cache replacement yields practical benefits.",high performance computing
Fredo Durand,"Gharbi, Michael; Malisiewicz, Tomasz; Paris, Sylvain; Durand, Frdo",2012-10-09T16:45:04Z,2012-10-09T16:45:04Z,2012-10-01,http://hdl.handle.net/1721.1/73685,MIT-CSAIL-TR-2012-032,A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although exemplar-based SVM is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of- the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency.",,11 p.,,,"Image retrieval, object detection, computer vision, parametric model",Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,910,"a gaussian approximation of feature space for fast image similarity we introduce a fast technique for the robust computation of image similarity. it builds on a re-interpretation of the recent exemplar-based svm approach, where a linear svm is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. although exemplar-based svm is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of- the-art performance on the pascal voc 2007 detection task despite its simplicity. we re-interpret it by viewing the svm between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. we show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. we then use a simple gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. this allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. we further show that our approach is equivalent to feature-space whitening and has links to image saliency.",high performance computing
Hal Abelson,"Ehrmann, Stephen C.; Gilbert, Steven W.; McMartin, Flora",2007-08-23T14:41:46Z,2007-08-23T14:41:46Z,2007-08-20,http://hdl.handle.net/1721.1/38482,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Factors Affecting the Adoption of Faculty-Developed Academic Software: A Study of Five iCampus Projects,"Initiated in 1999, iCampus is a research collaboration between Microsoft Research and MIT whose goal is to create and demonstrate technologies with the potential for revolutionary change throughout the university curriculum. The program was made possible by a $25 million research grant from Microsoft to MIT, and involves extensive collaboration between MIT and Microsoft staff.<p />This assessment study by the TLT Group addresses the question: The TLT Group has been asked, In light of the experience of iCampus, especially those projects selected by MIT and Microsoft for close study, what can be learned about priorities for educational technology initiatives in the future and about how the spread of such innovations can be more effectively supported?<p />The major conclusions are that the five projects studied improved important elements of an MIT education by making learning more authentic, active, collaborative, and feedback-rich.  Nevertheless, wider adoption beyond MIT was extremely difficult to achieve, largely due to structure issues in universities that make it difficult for educational technology to spread beyond the initial innovators, even to other departments within the same institution.  The report includes recommendations for universities, external sponsors, and for MIT in particular, about steps to take to achieve more effective dissemination.",,149 p.,,,educational technology; educational assessment,iCampus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,611,"factors affecting the adoption of faculty-developed academic software: a study of five icampus projects initiated in 1999, icampus is a research collaboration between microsoft research and mit whose goal is to create and demonstrate technologies with the potential for revolutionary change throughout the university curriculum. the program was made possible by a $25 million research grant from microsoft to mit, and involves extensive collaboration between mit and microsoft staff.<p />this assessment study by the tlt group addresses the question: the tlt group has been asked, in light of the experience of icampus, especially those projects selected by mit and microsoft for close study, what can be learned about priorities for educational technology initiatives in the future and about how the spread of such innovations can be more effectively supported?<p />the major conclusions are that the five projects studied improved important elements of an mit education by making learning more authentic, active, collaborative, and feedback-rich.  nevertheless, wider adoption beyond mit was extremely difficult to achieve, largely due to structure issues in universities that make it difficult for educational technology to spread beyond the initial innovators, even to other departments within the same institution.  the report includes recommendations for universities, external sponsors, and for mit in particular, about steps to take to achieve more effective dissemination.",privacy/ethics
Li-Shiuan Peh,"Krishna, Tushar; Beckmann, Bradford M.; Peh, Li-Shiuan; Reinhardt, Steven K.",2011-03-14T19:45:24Z,2011-03-14T19:45:24Z,2011-03-14,http://hdl.handle.net/1721.1/61695,MIT-CSAIL-TR-2011-013,BOOM: Broadcast Optimizations for On-chip Meshes,"Future many-core chips will require an on-chip network that can support broadcasts and multicasts at good power-performance. A vanilla on-chip network would send multiple unicast packets for each broadcast packet, resulting in latency, throughput and power overheads. Recent research in on-chip multicast support has proposed forking of broadcast/multicast packets within the network at the router buffers, but these techniques are far from ideal, since they increase buffer occupancy which lowers throughput, and packets incur delay and power penalties at each router. In this work, we analyze an ideal broadcast mesh; show the substantial gaps between state-of-the-art multicast NoCs and the ideal; then propose BOOM, which comprises a WHIRL routing protocol that ideally load balances broadcast traffic, a mXbar multicast crossbar circuit that enables multicast traversal at similar energy-delay as unicasts, and speculative bypassing of buffering for multicast flits. Together, they enable broadcast packets to approach the delay, energy, and throughput of the ideal fabric. Our simulations show BOOM realizing an average network latency that is 5% off ideal, attaining 96% of ideal throughput, with energy consumption that is 9% above ideal. Evaluations using synthetic traffic show BOOM achieving a latency reduction of 61%, throughput improvement of 63%, and buffer power reduction of 80% as compared to a baseline broadcast. Simulations with PARSEC benchmarks show BOOM reducing average request and network latency by 40% and 15% respectively.",,12 p.,,,multicore,Computer Architecture,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2011,841,"boom: broadcast optimizations for on-chip meshes future many-core chips will require an on-chip network that can support broadcasts and multicasts at good power-performance. a vanilla on-chip network would send multiple unicast packets for each broadcast packet, resulting in latency, throughput and power overheads. recent research in on-chip multicast support has proposed forking of broadcast/multicast packets within the network at the router buffers, but these techniques are far from ideal, since they increase buffer occupancy which lowers throughput, and packets incur delay and power penalties at each router. in this work, we analyze an ideal broadcast mesh; show the substantial gaps between state-of-the-art multicast nocs and the ideal; then propose boom, which comprises a whirl routing protocol that ideally load balances broadcast traffic, a mxbar multicast crossbar circuit that enables multicast traversal at similar energy-delay as unicasts, and speculative bypassing of buffering for multicast flits. together, they enable broadcast packets to approach the delay, energy, and throughput of the ideal fabric. our simulations show boom realizing an average network latency that is 5% off ideal, attaining 96% of ideal throughput, with energy consumption that is 9% above ideal. evaluations using synthetic traffic show boom achieving a latency reduction of 61%, throughput improvement of 63%, and buffer power reduction of 80% as compared to a baseline broadcast. simulations with parsec benchmarks show boom reducing average request and network latency by 40% and 15% respectively.",high performance computing
Brian Williams,"Blackmore, Lars; Block, Steve",2006-02-28T19:47:07Z,2006-02-28T19:47:07Z,2006-02-28,http://hdl.handle.net/1721.1/31217,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Control and Estimation for Cooperative Manipulator Tasks,"The objective of this project is to achieve reliable transfer of an object from one robotic manipulator to another. This capability is useful for a number of applications, for instance robotic assembly, or robots with multiple manipulators, such as humanoid robots.Achieving reliable object transfer poses a number of challenges for both control and estimation. As with most manipulation problems, the inverse kinematics problem must be solved so that the desired endpoint location can be specified in Cartesian coordinates, rather than in the joint space of the manipulator. Anadditional challenge particular to the cooperative robotics problem is that more than one manipulator may have a grasp on the same object. Manipulators that are carrying out simple position control may encounter problems when grasping the same object. Minor errors in forward kinematics can lead to large controllerforces, or even unstable dynamics, as each controller tries to counteract the other to drive the perceived error to zero.On the estimation side, carrying out reliable transfer depends critically on determining the grasp state; in other words, does a particular robot have a grasp on the object, or do both have the object? The grasp state must be determined before the sequence of events in a transfer task can proceed. For example, the manipulator receiving the object cannot move away until it is certain that the manipulator passing the object has released. In many instances, having pressure sensors mounted in the hand is infeasible. For example, packaging reasons can mean that the necessary space is not available, as is the case with the JPL LEMUR hexapod. We therefore need to infer the grasp state from the available observations, which are usually supplied by position encoders at the joints.For this project we assume that each manipulator carries out estimation independently, without joint angle observations from the other robot, but with knowledge of its own joint angles and of the commands to be issued to both robots. This is typical of a multi-agent cooperative task, and the lack of observations makes the estimation task even more challenging.This report describes the approach we use to solve this problem, which is comprised of an impedance controller and a hybrid estimator.",MIT-CSAIL-TR-2006-011,19 p.; 21923202 bytes; 1004827 bytes,application/postscript; application/pdf,en_US,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,493,"control and estimation for cooperative manipulator tasks the objective of this project is to achieve reliable transfer of an object from one robotic manipulator to another. this capability is useful for a number of applications, for instance robotic assembly, or robots with multiple manipulators, such as humanoid robots.achieving reliable object transfer poses a number of challenges for both control and estimation. as with most manipulation problems, the inverse kinematics problem must be solved so that the desired endpoint location can be specified in cartesian coordinates, rather than in the joint space of the manipulator. anadditional challenge particular to the cooperative robotics problem is that more than one manipulator may have a grasp on the same object. manipulators that are carrying out simple position control may encounter problems when grasping the same object. minor errors in forward kinematics can lead to large controllerforces, or even unstable dynamics, as each controller tries to counteract the other to drive the perceived error to zero.on the estimation side, carrying out reliable transfer depends critically on determining the grasp state; in other words, does a particular robot have a grasp on the object, or do both have the object? the grasp state must be determined before the sequence of events in a transfer task can proceed. for example, the manipulator receiving the object cannot move away until it is certain that the manipulator passing the object has released. in many instances, having pressure sensors mounted in the hand is infeasible. for example, packaging reasons can mean that the necessary space is not available, as is the case with the jpl lemur hexapod. we therefore need to infer the grasp state from the available observations, which are usually supplied by position encoders at the joints.for this project we assume that each manipulator carries out estimation independently, without joint angle observations from the other robot, but with knowledge of its own joint angles and of the commands to be issued to both robots. this is typical of a multi-agent cooperative task, and the lack of observations makes the estimation task even more challenging.this report describes the approach we use to solve this problem, which is comprised of an impedance controller and a hybrid estimator.",robotics
,"Ostrovsky, Rafail; Rackoff, Charles; Smith, Adam",2023-03-29T15:36:58Z,2023-03-29T15:36:58Z,2003-02,https://hdl.handle.net/1721.1/149979,MIT-LCS-TR-887,Efficient Consistency Proofs on a Committed Database,"A consistent query protocol allows a database owner to publish a very short string c which commits her to a particular database D with special consistency property (i.e., given c, every allowable query has unique and well-defined answer with respect to D.)  Moreover, when a user makes a query, any server hosting the database can answer the query, and provide a very short proof P that the answer is well-defined, unique, and consistent with c (and hence with D).  One potential application of consistent query protocols is for guaranteeing the consistency of many replicated copies of D---the owner can publish c, and users can verify the consistency of a query to some copy of D by making sure P is consistent with c.  This strong guarantee holds even for owners who try to cheat, while creating c.  The task of consistent query protocols was originally proposed for membership queries by Micali and Rabin, and subsequently and independently, by Kilian. In this setting a server can prove to a client whether or not a given key is present or not in a database, based only on a short public commitment c.  We strengthen their results in several ways. For membership queries, we improve the communication complexity; more importantly, we provide protocols for more general types of queries and more general relational databases.  For example, we consider databases in which entries have several keys and where we allow range queries (e.g. we allow a client to ask for all entries within a certain age range and a certain salary range).   Towards this goal, we introduce query algorithms with certain inherent robustness properties---called data-robust algorithms---and show how this robustness can be achieved. In particular, we illustrate our general technique by constructing an efficient data-robust algorithm for proving consistency of orthogonal range queries (a particular case of a ``join''query).  The server's proof convinces the client not only that all the matching entries provided are in D, but also that no others are present.  Our guarantees hold even if the answer is the empty set.  In the case of one-dimensional range queries we also show a new data-hiding technique---called explicit hashing---which allows us to a execute consistent query protocol P and at the same time protect the privacy of all other information in the database efficiently. In particular, we avoid the NP reductions required in a generic zero-knowledge proof.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,193,"efficient consistency proofs on a committed database a consistent query protocol allows a database owner to publish a very short string c which commits her to a particular database d with special consistency property (i.e., given c, every allowable query has unique and well-defined answer with respect to d.)  moreover, when a user makes a query, any server hosting the database can answer the query, and provide a very short proof p that the answer is well-defined, unique, and consistent with c (and hence with d).  one potential application of consistent query protocols is for guaranteeing the consistency of many replicated copies of d---the owner can publish c, and users can verify the consistency of a query to some copy of d by making sure p is consistent with c.  this strong guarantee holds even for owners who try to cheat, while creating c.  the task of consistent query protocols was originally proposed for membership queries by micali and rabin, and subsequently and independently, by kilian. in this setting a server can prove to a client whether or not a given key is present or not in a database, based only on a short public commitment c.  we strengthen their results in several ways. for membership queries, we improve the communication complexity; more importantly, we provide protocols for more general types of queries and more general relational databases.  for example, we consider databases in which entries have several keys and where we allow range queries (e.g. we allow a client to ask for all entries within a certain age range and a certain salary range).   towards this goal, we introduce query algorithms with certain inherent robustness properties---called data-robust algorithms---and show how this robustness can be achieved. in particular, we illustrate our general technique by constructing an efficient data-robust algorithm for proving consistency of orthogonal range queries (a particular case of a ``join''query).  the server's proof convinces the client not only that all the matching entries provided are in d, but also that no others are present.  our guarantees hold even if the answer is the empty set.  in the case of one-dimensional range queries we also show a new data-hiding technique---called explicit hashing---which allows us to a execute consistent query protocol p and at the same time protect the privacy of all other information in the database efficiently. in particular, we avoid the np reductions required in a generic zero-knowledge proof.",language models
Jovan Popovic,"Silva, Marco da; Abe, Yeuhi; Popovic, Jovan",2008-01-16T13:45:10Z,2008-01-16T13:45:10Z,2008-01-15,http://hdl.handle.net/1721.1/40091,,Simulation of Human Motion Data using Short-Horizon Model-Predictive Control,"Many data-driven animation techniques are capable of producing high quality motions of human characters. Few techniques, however, are capable of generating motions that are consistent with physically simulated environments. Physically simulated characters, in contrast, are automatically consistent with the environment, but their motionsare often unnatural because they are difficult to control. We present a model-predictive controller that yields natural motions by guiding simulated humans toward real motion data. During simulation, the predictive component of the controller solves a quadratic program to compute the forces for a short window of time into the future. These forces are then applied by a low-gain proportional-derivative component, which makes minor adjustments until the next planning cycle. The controller is fast enough for interactive systems such as games and training simulations. It requires no precomputation and little manual tuning. The controller is resilient to mismatches between the character dynamics and the input motion, which allows it to track motion capture data even where the real dynamics are not known precisely. The same principled formulation can generate natural walks, runs, and jumps in a number of different physically simulated surroundings.",,,,,Computer Graphics; Three Dimensional Graphics and Realism; Animation,Computer Graphics,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,633,"simulation of human motion data using short-horizon model-predictive control many data-driven animation techniques are capable of producing high quality motions of human characters. few techniques, however, are capable of generating motions that are consistent with physically simulated environments. physically simulated characters, in contrast, are automatically consistent with the environment, but their motionsare often unnatural because they are difficult to control. we present a model-predictive controller that yields natural motions by guiding simulated humans toward real motion data. during simulation, the predictive component of the controller solves a quadratic program to compute the forces for a short window of time into the future. these forces are then applied by a low-gain proportional-derivative component, which makes minor adjustments until the next planning cycle. the controller is fast enough for interactive systems such as games and training simulations. it requires no precomputation and little manual tuning. the controller is resilient to mismatches between the character dynamics and the input motion, which allows it to track motion capture data even where the real dynamics are not known precisely. the same principled formulation can generate natural walks, runs, and jumps in a number of different physically simulated surroundings.",high performance computing
Gerald Sussman,"Beal, Jacob; Indurkhya, Sagar",2009-09-04T21:30:14Z,2009-09-04T21:30:14Z,2009-09-04,http://hdl.handle.net/1721.1/46710,,Code for LOLCAT Method (Variant of Gillespie Algorithm),"This code and data is publicly listed code for the LOLCAT Method developed by Sagar Indurkhya and Jacob Beal, in the paper: ""Reaction factoring and bipartite update graphs accelerate the Gillespie algorithm for large-scale biochemical systems.""",,,,,Computational Systems Biology; Gillespie Algorithm; Stochastic Simulation Algorithm; Bioinformatics,Mathematics and Computation,,Creative Commons Attrbution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2009,747,"code for lolcat method (variant of gillespie algorithm) this code and data is publicly listed code for the lolcat method developed by sagar indurkhya and jacob beal, in the paper: ""reaction factoring and bipartite update graphs accelerate the gillespie algorithm for large-scale biochemical systems.""",high performance computing
,"Boyapati, Chandrasekhar; Lee, Robert; Rinard, Martin",2023-03-29T15:35:55Z,2023-03-29T15:35:55Z,2002-06,https://hdl.handle.net/1721.1/149955,MIT-LCS-TR-853,Safe Runtime Downcasts With Ownership Types,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,133,safe runtime downcasts with ownership types ,language models
,"Zhang, MIchael; Asanovic, Krste",2005-12-22T02:37:29Z,2005-12-22T02:37:29Z,2005-10-10,http://hdl.handle.net/1721.1/30574,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Victim Migration: Dynamically Adapting Between Private and Shared CMP Caches,"Future CMPs will have more cores and greater onchip cache capacity. The on-chip cache can either be divided into separate private L2 caches for each core, or treated as a large shared L2 cache. Private caches provide low hit latency but low capacity, while shared caches have higher hit latencies but greater capacity. Victim replication was previously introduced as a way of reducing the average hit latency of a shared cache by allowing a processor to make a replica of a primary cache victim in its local slice of the global L2 cache. Although victim replication performs well on multithreaded and single-threaded codes, it performs worse than the private scheme for multiprogrammed workloads where there is little sharing between the different programs running at the same time. In this paper, we propose victim migration, which improves on victim replication by adding an additional set of migration tags on each node which are used to implement an exclusive cache policy for replicas. When a replica has been created on a remote node, it is not also cached on the home node, but only recorded in the migration tags. This frees up space on the home node to store shared global lines or replicas for the local processor. We show that victim migration performs better than private, shared, and victim replication schemes across a range of single threaded, multithreaded, and multiprogrammed workloads, while using less area than a private cache design. Victim migration provides a reduction in average memory access latency of up to 10% over victim replication.",MIT-CSAIL-TR-2005-064; MIT-LCS-TR-1006,17 p.; 18487877 bytes; 796263 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,464,"victim migration: dynamically adapting between private and shared cmp caches future cmps will have more cores and greater onchip cache capacity. the on-chip cache can either be divided into separate private l2 caches for each core, or treated as a large shared l2 cache. private caches provide low hit latency but low capacity, while shared caches have higher hit latencies but greater capacity. victim replication was previously introduced as a way of reducing the average hit latency of a shared cache by allowing a processor to make a replica of a primary cache victim in its local slice of the global l2 cache. although victim replication performs well on multithreaded and single-threaded codes, it performs worse than the private scheme for multiprogrammed workloads where there is little sharing between the different programs running at the same time. in this paper, we propose victim migration, which improves on victim replication by adding an additional set of migration tags on each node which are used to implement an exclusive cache policy for replicas. when a replica has been created on a remote node, it is not also cached on the home node, but only recorded in the migration tags. this frees up space on the home node to store shared global lines or replicas for the local processor. we show that victim migration performs better than private, shared, and victim replication schemes across a range of single threaded, multithreaded, and multiprogrammed workloads, while using less area than a private cache design. victim migration provides a reduction in average memory access latency of up to 10% over victim replication.",high performance computing
Brian Williams,"Bhargava, Nikhil; Muise, Christian; Vaquero, Tiago; Williams, Brian",2018-01-29T23:15:05Z,2018-01-29T23:15:05Z,2018-01-29,http://hdl.handle.net/1721.1/113340,MIT-CSAIL-TR-2018-002,Delay Controllability: Multi-Agent Coordination under Communication Delay,"Simple Temporal Networks with Uncertainty provide a useful framework for modeling temporal constraints and, importantly, for modeling actions with uncertain durations. To determine whether we can construct a schedule for a given network, we typically consider one of two types of controllability: dynamic or strong. These controllability checks have strict conditions on how uncertainty is resolved; uncertain outcomes are either recognized immediately or not at all. In this paper, we introduce delay controllability, a novel generalization of both strong and dynamic controllability that additionally exposes a large range of controllability classes in between. To do so, we use a delay function to parameterize our controllability checking. This delay function represents the difference between when an event happens and the time that it is observed. We also provide a single unified algorithm for checking delay controllability that runs in O(n^3) time, matching the best known runtime for dynamic controllability, which we use to motivate the decision to generalize dynamic and strong controllability. We conclude by providing an empirical evaluation of delay controllability, demonstrating its superior accuracy and practical efficiency as compared to other existing approximations.",,34 p.,,,Temporal Controllability; Scheduling; Temporal Uncertainty,Model-based Embedded and Robotic Systems,,,,"New version posted April 19, 2019 with slight tweaks to the algorithm and added clarity based on reviewer feedback.",,,,,,,,,2018-01-29T23:15:05Z,,,,,,,,,,,,,,,,2018,1052,"delay controllability: multi-agent coordination under communication delay simple temporal networks with uncertainty provide a useful framework for modeling temporal constraints and, importantly, for modeling actions with uncertain durations. to determine whether we can construct a schedule for a given network, we typically consider one of two types of controllability: dynamic or strong. these controllability checks have strict conditions on how uncertainty is resolved; uncertain outcomes are either recognized immediately or not at all. in this paper, we introduce delay controllability, a novel generalization of both strong and dynamic controllability that additionally exposes a large range of controllability classes in between. to do so, we use a delay function to parameterize our controllability checking. this delay function represents the difference between when an event happens and the time that it is observed. we also provide a single unified algorithm for checking delay controllability that runs in o(n^3) time, matching the best known runtime for dynamic controllability, which we use to motivate the decision to generalize dynamic and strong controllability. we conclude by providing an empirical evaluation of delay controllability, demonstrating its superior accuracy and practical efficiency as compared to other existing approximations.",high performance computing
Tommi Jaakkola,"Monteleoni, Claire; Kaariainen, Matti",2007-01-24T12:56:52Z,2007-01-24T12:56:52Z,2007-01-23,http://hdl.handle.net/1721.1/35784,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Online Active Learning in Practice,"We compare the practical performance of several recently proposed algorithms for active learning in the online setting.  We consider two algorithms (and their combined variants) that are strongly online, in that they do not store any previously labeled examples, and for which formal guarantees have recently been proven under various assumptions.  We perform an empirical evaluation on optical character recognition (OCR) data, an application that we argue to be appropriately served by online active learning.  We compare the performance between the algorithm variants and show significant reductions in label-complexity over random sampling.",MIT-CSAIL-TR-2007-005,9 p.; 2188813 bytes; 767442 bytes,application/postscript; application/pdf,en_US,online learning; active learning; selective sampling; optical character recognition; OCR,Tommi's Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,570,"online active learning in practice we compare the practical performance of several recently proposed algorithms for active learning in the online setting.  we consider two algorithms (and their combined variants) that are strongly online, in that they do not store any previously labeled examples, and for which formal guarantees have recently been proven under various assumptions.  we perform an empirical evaluation on optical character recognition (ocr) data, an application that we argue to be appropriately served by online active learning.  we compare the performance between the algorithm variants and show significant reductions in label-complexity over random sampling.",object recognition/detection
Leslie Kaelbling,"Milch, Brian; Koller, Daphne",2008-05-13T16:00:19Z,2008-05-13T16:00:19Z,2008-05-12,http://hdl.handle.net/1721.1/41530,,Ignorable Information in Multi-Agent Scenarios,"In some multi-agent scenarios, identifying observations that an agent can safely ignore reduces exponentially the size of the agent's strategy space and hence the time required to find a Nash equilibrium. We consider games represented using the multi-agent influence diagram (MAID) framework of Koller and Milch [2001], and analyze the extent to which information edges can be eliminated. We define a notion of a safe edge removal transformation, where all equilibria in the reduced model are also equilibria in the original model. We show that existing edge removal algorithms for influence diagrams are safe, but limited, in that they do not detect certain cases where edges can be removed safely. We describe an algorithm that produces the ""minimal"" safe reduction, which removes as many edges as possible while still preserving safety. Finally, we note that both the existing edge removal algorithms and our new one can eliminate equilibria where agents coordinate their actions by conditioning on irrelevant information. Surprisingly, in some games these ""lost"" equilibria can be preferred by all agents in the game.",MIT-CSAIL-TR-2008-029,16 p.,,,Multi-agent influence diagrams; Irrelevance,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,661,"ignorable information in multi-agent scenarios in some multi-agent scenarios, identifying observations that an agent can safely ignore reduces exponentially the size of the agent's strategy space and hence the time required to find a nash equilibrium. we consider games represented using the multi-agent influence diagram (maid) framework of koller and milch [2001], and analyze the extent to which information edges can be eliminated. we define a notion of a safe edge removal transformation, where all equilibria in the reduced model are also equilibria in the original model. we show that existing edge removal algorithms for influence diagrams are safe, but limited, in that they do not detect certain cases where edges can be removed safely. we describe an algorithm that produces the ""minimal"" safe reduction, which removes as many edges as possible while still preserving safety. finally, we note that both the existing edge removal algorithms and our new one can eliminate equilibria where agents coordinate their actions by conditioning on irrelevant information. surprisingly, in some games these ""lost"" equilibria can be preferred by all agents in the game.",high performance computing
Silvio Micali,"Chen, Jing; Micali, Silvio",2008-07-09T22:15:19Z,2008-07-09T22:15:19Z,2008-06,http://hdl.handle.net/1721.1/41877,,Knowledge Benchmarks in Adversarial Mechanism Design and Implementation in Surviving Strategies (Part I),"We put forward new benchmarks and solution concepts for Adversarial Mechanism Design, as defined by [MV07.a], and we exemplify them in the case of truly combinatorial auctions.We benchmark the combined performance (the sum of the auction's efficiency and revenue) of a truly combinatorial auction against a very relevant but private knowledge of the players: essentially, the maximum revenue that the best informed player could guarantee if he were the seller. (I.e., by offering each other player a subset of the goods for a take-it-or-leave-it price.)We achieve this natural benchmark within a factor of 2, by means of a new and probabilistic auction mechanism, in surviving strategies. That is, the above performance of our mechanism is guaranteed in any rational play, independent of any possible beliefs of the players. Indeed, our performance guarantee holds for any possible choice of strategies, so long as each player chooses a strategy among those surviving iterated elimination of dominated strategies.Our mechanism is extremely robust. Namely, its performance guarantees hold even if all but one of the players collude (together or in separate groups) in any possible but reasonable way. Essentially, the only restriction for the collective utility function of a collusive subset S of the players is the following: the collective utility increases when one member of S is allocated a ubset of the goods ""individually better"" for him and/or his ""individual price"" is smaller, while the allocations and prices of all other members of S stay the same.Our results improve on the yet unpublished ones of [MV07.b]. The second part of this paper, dealing with a more aggressive benchmark (essentially, the maximum welfare privately known to the players) is forthcoming.",MIT-CSAIL-TR-2008-041,17 p.,,,,Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,662,"knowledge benchmarks in adversarial mechanism design and implementation in surviving strategies (part i) we put forward new benchmarks and solution concepts for adversarial mechanism design, as defined by [mv07.a], and we exemplify them in the case of truly combinatorial auctions.we benchmark the combined performance (the sum of the auction's efficiency and revenue) of a truly combinatorial auction against a very relevant but private knowledge of the players: essentially, the maximum revenue that the best informed player could guarantee if he were the seller. (i.e., by offering each other player a subset of the goods for a take-it-or-leave-it price.)we achieve this natural benchmark within a factor of 2, by means of a new and probabilistic auction mechanism, in surviving strategies. that is, the above performance of our mechanism is guaranteed in any rational play, independent of any possible beliefs of the players. indeed, our performance guarantee holds for any possible choice of strategies, so long as each player chooses a strategy among those surviving iterated elimination of dominated strategies.our mechanism is extremely robust. namely, its performance guarantees hold even if all but one of the players collude (together or in separate groups) in any possible but reasonable way. essentially, the only restriction for the collective utility function of a collusive subset s of the players is the following: the collective utility increases when one member of s is allocated a ubset of the goods ""individually better"" for him and/or his ""individual price"" is smaller, while the allocations and prices of all other members of s stay the same.our results improve on the yet unpublished ones of [mv07.b]. the second part of this paper, dealing with a more aggressive benchmark (essentially, the maximum welfare privately known to the players) is forthcoming.",language models
,"Mirrokni, Vahab S.; Lee, Walter; Karger, David; Amarasinghe, Saman",2023-03-29T14:43:03Z,2023-03-29T14:43:03Z,2002-12,https://hdl.handle.net/1721.1/149322,MIT-LCS-TM-635,A Theoretical and Practical Approach to Instruction Scheduling on Spatial Architectures,"This paper studies the problem of instruction assignment and scheduling on spatial architectures. Spatial architectures are architectures whose resources are organized in clusters, with non-zero communication delays between the clusters. On these architectures, instruction scheduling include both space scheduling, where instructions are mapped to clusters, and the traditional time scheduling. This paper considers the problem from both the theoretical and practical perspectives. It presents two integer linear program formulations with known performance bounds. We also present an 8-approximation algorithm for constant m and constant communication delays. Then, we introduce three heuristic algorithms based on list scheduling. Then we study a layer partitioning method. Our final algorithm is a combination of layer partitioning and the third heuristic. Two of the better algorithms are evaluated on the Raw machine. Results show that they are competitive with previously published results; for scientfici codes, our heuristics can perform an average of 25% better.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,175,"a theoretical and practical approach to instruction scheduling on spatial architectures this paper studies the problem of instruction assignment and scheduling on spatial architectures. spatial architectures are architectures whose resources are organized in clusters, with non-zero communication delays between the clusters. on these architectures, instruction scheduling include both space scheduling, where instructions are mapped to clusters, and the traditional time scheduling. this paper considers the problem from both the theoretical and practical perspectives. it presents two integer linear program formulations with known performance bounds. we also present an 8-approximation algorithm for constant m and constant communication delays. then, we introduce three heuristic algorithms based on list scheduling. then we study a layer partitioning method. our final algorithm is a combination of layer partitioning and the third heuristic. two of the better algorithms are evaluated on the raw machine. results show that they are competitive with previously published results; for scientfici codes, our heuristics can perform an average of 25% better.",high performance computing
Tomaso Poggio,"Shakhnarovich, Greg; Bouvrie, Jake; Rosasco, Lorenzo; Smale, Steve",2009-10-13T12:31:06Z,2009-10-13T12:31:06Z,2009-10-09,http://hdl.handle.net/1721.1/49425,CBCL-281; MIT-CSAIL-TR-2009-049,Notes on the Shannon Entropy of the Neural Response,"In these notes we focus on the concept of Shannon entropy in an attempt to provide a systematic way of assessing the discrimination properties of the neural response, and quantifying the role played by the number of layers and the number of templates.",,6 p.,,,computer vision; artificial intelligence; neuroscience; computation,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,754,"notes on the shannon entropy of the neural response in these notes we focus on the concept of shannon entropy in an attempt to provide a systematic way of assessing the discrimination properties of the neural response, and quantifying the role played by the number of layers and the number of templates.",object recognition/detection
Tomaso Poggio,"Bileschi, Stanley M",2008-04-09T20:15:10Z,2008-04-09T20:15:10Z,2008-04-09,http://hdl.handle.net/1721.1/41093,,A Multi-Scale Generalization of the HoG and HMAX Image Descriptors for Object Detection,"Recently, several powerful image features have been proposed whichcan be described as spatial histograms of oriented energy. Forinstance, the HoG, HMAX C1, SIFT, and Shape Context feature allrepresent an input image using with a discrete set of bins whichaccumulate evidence for oriented structures over a spatial regionand a range of orientations. In this work, we generalize thesetechniques to allow for a foveated input image, rather than arectilinear raster. It will be shown that improved object detectionaccuracy can be achieved via inputting a spectrum of imagemeasurements, from sharp, fine-scale image sampling within a smallspatial region within the target to coarse-scale sampling of a widefield of view around the target. Several alternative featuregeneration algorithms are proposed and tested which suitably makeuse of foveated image inputs. In the experiments we show thatfeatures generated from the foveated input format produce detectorsof greater accuracy, as measured for four object types from commonlyavailable data-sets. Finally, a flexible algorithm for generatingfeatures is described and tested which is independent of inputtopology and uses ICA to learn appropriate filters.",MIT-CSAIL-TR-2008-019; CBCL-271,8 p.,,,"Object Detection, ICA, Multi-Scale, Image Features",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,652,"a multi-scale generalization of the hog and hmax image descriptors for object detection recently, several powerful image features have been proposed whichcan be described as spatial histograms of oriented energy. forinstance, the hog, hmax c1, sift, and shape context feature allrepresent an input image using with a discrete set of bins whichaccumulate evidence for oriented structures over a spatial regionand a range of orientations. in this work, we generalize thesetechniques to allow for a foveated input image, rather than arectilinear raster. it will be shown that improved object detectionaccuracy can be achieved via inputting a spectrum of imagemeasurements, from sharp, fine-scale image sampling within a smallspatial region within the target to coarse-scale sampling of a widefield of view around the target. several alternative featuregeneration algorithms are proposed and tested which suitably makeuse of foveated image inputs. in the experiments we show thatfeatures generated from the foveated input format produce detectorsof greater accuracy, as measured for four object types from commonlyavailable data-sets. finally, a flexible algorithm for generatingfeatures is described and tested which is independent of inputtopology and uses ica to learn appropriate filters.",object recognition/detection
John Leonard,"Rosen, David M.; Carlone, Luca; Bandeira, Afonso S.; Leonard, John J.",2017-02-07T23:00:06Z,2017-02-07T23:00:06Z,2017-02-05,http://hdl.handle.net/1721.1/106885,MIT-CSAIL-TR-2017-002,SE-Sync: A Certifiably Correct Algorithm for Synchronization over the Special Euclidean Group,"Many important geometric estimation problems naturally take the form of synchronization over the special Euclidean group: estimate the values of a set of unknown poses given noisy measurements of a subset of their pairwise relative transforms. Examples of this class include the foundational problems of pose-graph simultaneous localization and mapping (SLAM) (in robotics), camera motion estimation (in computer vision), and sensor network localization (in distributed sensing), among others. This inference problem is typically formulated as a nonconvex maximum-likelihood estimation that is computationally hard to solve in general. Nevertheless, in this paper we present an algorithm that is able to efficiently recover certifiably globally optimal solutions of the special Euclidean synchronization problem in a non-adversarial noise regime. The crux of our approach is the development of a semidefinite relaxation of the maximum-likelihood estimation whose minimizer provides an exact MLE so long as the magnitude of the noise corrupting the available measurements falls below a certain critical threshold; furthermore, whenever exactness obtains, it is possible to verify this fact a posteriori, thereby certifying the optimality of the recovered estimate. We develop a specialized optimization scheme for solving large-scale instances of this semidefinite relaxation by exploiting its low-rank, geometric, and graph-theoretic structure to reduce it to an equivalent optimization problem defined on a low-dimensional Riemannian manifold, and then design a Riemannian truncated-Newton trust-region method to solve this reduction efficiently. Finally, we combine this fast optimization approach with a simple rounding procedure to produce our algorithm, SE-Sync. Experimental evaluation on a variety of simulated and real-world pose-graph SLAM datasets shows that SE-Sync is capable of recovering certifiably globally optimal solutions when the available measurements are corrupted by noise up to an order of magnitude greater than that typically encountered in robotics and computer vision applications, and does so more than an order of magnitude faster than the Gauss-Newton-based approach that forms the basis of current state-of-the-art techniques.",,"49 pages, 20 figures",,,Simultaneous localization and mapping (SLAM); Maximum-likelihood estimation; Convex relaxation; Low-rank semidefinite programming; Riemannian optimization,Marine Robotics,,,,,,,,,,,,,2017-02-07T23:00:06Z,,,,,,,,,,,,,,,,2017,1038,"se-sync: a certifiably correct algorithm for synchronization over the special euclidean group many important geometric estimation problems naturally take the form of synchronization over the special euclidean group: estimate the values of a set of unknown poses given noisy measurements of a subset of their pairwise relative transforms. examples of this class include the foundational problems of pose-graph simultaneous localization and mapping (slam) (in robotics), camera motion estimation (in computer vision), and sensor network localization (in distributed sensing), among others. this inference problem is typically formulated as a nonconvex maximum-likelihood estimation that is computationally hard to solve in general. nevertheless, in this paper we present an algorithm that is able to efficiently recover certifiably globally optimal solutions of the special euclidean synchronization problem in a non-adversarial noise regime. the crux of our approach is the development of a semidefinite relaxation of the maximum-likelihood estimation whose minimizer provides an exact mle so long as the magnitude of the noise corrupting the available measurements falls below a certain critical threshold; furthermore, whenever exactness obtains, it is possible to verify this fact a posteriori, thereby certifying the optimality of the recovered estimate. we develop a specialized optimization scheme for solving large-scale instances of this semidefinite relaxation by exploiting its low-rank, geometric, and graph-theoretic structure to reduce it to an equivalent optimization problem defined on a low-dimensional riemannian manifold, and then design a riemannian truncated-newton trust-region method to solve this reduction efficiently. finally, we combine this fast optimization approach with a simple rounding procedure to produce our algorithm, se-sync. experimental evaluation on a variety of simulated and real-world pose-graph slam datasets shows that se-sync is capable of recovering certifiably globally optimal solutions when the available measurements are corrupted by noise up to an order of magnitude greater than that typically encountered in robotics and computer vision applications, and does so more than an order of magnitude faster than the gauss-newton-based approach that forms the basis of current state-of-the-art techniques.",robotics
Leslie Kaelbling,"McAllester, David; Milch, Brian; Goodman, Noah D.",2008-05-05T15:45:52Z,2008-05-05T15:45:52Z,2008-05-03,http://hdl.handle.net/1721.1/41516,,Random-World Semantics and Syntactic Independence for Expressive Languages,"We consider three desiderata for a language combining logic and probability: logical expressivity, random-world semantics, and the existence of a useful syntactic condition for probabilistic independence. Achieving these three desiderata simultaneously is nontrivial. Expressivity can be achieved by using a formalism similar to a programming language, but standard approaches to combining programming languages with probabilities sacrifice random-world semantics. Naive approaches to restoring random-world semantics undermine syntactic independence criteria. Our main result is a syntactic independence criterion that holds for a broad class of highly expressive logics under random-world semantics. We explore various examples including Bayesian networks, probabilistic context-free grammars, and an example from Mendelian genetics. Our independence criterion supports a case-factor inference technique that reproduces both variable elimination for BNs and the inside algorithm for PCFGs.",MIT-CSAIL-TR-2008-025,6 p.,,,,Learning and Intelligent Systems,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,658,"random-world semantics and syntactic independence for expressive languages we consider three desiderata for a language combining logic and probability: logical expressivity, random-world semantics, and the existence of a useful syntactic condition for probabilistic independence. achieving these three desiderata simultaneously is nontrivial. expressivity can be achieved by using a formalism similar to a programming language, but standard approaches to combining programming languages with probabilities sacrifice random-world semantics. naive approaches to restoring random-world semantics undermine syntactic independence criteria. our main result is a syntactic independence criterion that holds for a broad class of highly expressive logics under random-world semantics. we explore various examples including bayesian networks, probabilistic context-free grammars, and an example from mendelian genetics. our independence criterion supports a case-factor inference technique that reproduces both variable elimination for bns and the inside algorithm for pcfgs.",language models
,"Walfish, Michael; Stribling, Jeremy; Krohn, Maxwell; Balakrishnan, Hari; Morris, Robert; Shenker, Scott",2005-12-22T01:35:08Z,2005-12-22T01:35:08Z,2004-06-24,http://hdl.handle.net/1721.1/30481,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Middleboxes No Longer Considered Harmful,"Intermediate network elements, such as network address translators (NATs), firewalls, and transparent caches are now commonplace. The usual reaction in the network architecture community to these so-called middleboxes is a combination of scorn (because they violate important architectural principles) and dismay (because these violations make the Internet less flexible). While we acknowledge these concerns, we also recognize that middleboxes have become an Internet fact of life for important reasons. To retain their functions while eliminating their dangerous side-effects, we propose an extension to the Internet architecture, called the Delegation-Oriented Architecture (DOA), that not only allows, but also facilitates, the deployment of middleboxes. DOA involves two relatively modest changes to the current architecture: (a) a set of references that are carried in packets and serve as persistent host identifiers and (b) a way to resolve these references to delegates chosen by the referenced host.",MIT-CSAIL-TR-2004-042; MIT-LCS-TR-954,16 p.; 31058710 bytes; 1338617 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,347,"middleboxes no longer considered harmful intermediate network elements, such as network address translators (nats), firewalls, and transparent caches are now commonplace. the usual reaction in the network architecture community to these so-called middleboxes is a combination of scorn (because they violate important architectural principles) and dismay (because these violations make the internet less flexible). while we acknowledge these concerns, we also recognize that middleboxes have become an internet fact of life for important reasons. to retain their functions while eliminating their dangerous side-effects, we propose an extension to the internet architecture, called the delegation-oriented architecture (doa), that not only allows, but also facilitates, the deployment of middleboxes. doa involves two relatively modest changes to the current architecture: (a) a set of references that are carried in packets and serve as persistent host identifiers and (b) a way to resolve these references to delegates chosen by the referenced host.",object recognition/detection
,"Kuncak, Viktor; Rinard, Martin",2005-12-22T02:14:54Z,2005-12-22T02:14:54Z,2004-10-25,http://hdl.handle.net/1721.1/30498,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Spatial Conjunction as Second-Order Logic,"Spatial conjunction is a powerful construct for reasoning about dynamically allocateddata structures, as well as concurrent, distributed and mobile computation. Whileresearchers have identified many uses of spatial conjunction, its precise expressive powercompared to traditional logical constructs was not previously known.In this paper we establish the expressive power of spatial conjunction. We construct anembedding from first-order logic with spatial conjunction into second-order logic, and moresurprisingly, an embedding from full second order logic into first-order logic with spatialconjunction. These embeddings show that the satisfiability of formulas in first-order logicwith spatial conjunction is equivalent to the satisfiability of formulas in second-order logic.These results explain the great expressive power of spatial conjunction and can be usedto show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidablelogic. As one example, we show that adding unrestricted spatial conjunction totwo-variable logic leads to undecidability.On the side of decidability, the embedding into second-order logic immediately implies thedecidability of first-order logic with a form of spatial conjunction over trees. The embeddinginto spatial conjunction also has useful consequences: because a restricted form of spatialconjunction in two-variable logic preserves decidability, we obtain that a correspondinglyrestricted form of second-order quantification in two-variable logic is decidable. The resultinglanguage generalizes the first-order theory of boolean algebra over sets and is useful inreasoning about the contents of data structures in object-oriented languages.",MIT-CSAIL-TR-2004-067; MIT-LCS-TR-970,16 p.; 15950544 bytes; 692945 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,377,"on spatial conjunction as second-order logic spatial conjunction is a powerful construct for reasoning about dynamically allocateddata structures, as well as concurrent, distributed and mobile computation. whileresearchers have identified many uses of spatial conjunction, its precise expressive powercompared to traditional logical constructs was not previously known.in this paper we establish the expressive power of spatial conjunction. we construct anembedding from first-order logic with spatial conjunction into second-order logic, and moresurprisingly, an embedding from full second order logic into first-order logic with spatialconjunction. these embeddings show that the satisfiability of formulas in first-order logicwith spatial conjunction is equivalent to the satisfiability of formulas in second-order logic.these results explain the great expressive power of spatial conjunction and can be usedto show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidablelogic. as one example, we show that adding unrestricted spatial conjunction totwo-variable logic leads to undecidability.on the side of decidability, the embedding into second-order logic immediately implies thedecidability of first-order logic with a form of spatial conjunction over trees. the embeddinginto spatial conjunction also has useful consequences: because a restricted form of spatialconjunction in two-variable logic preserves decidability, we obtain that a correspondinglyrestricted form of second-order quantification in two-variable logic is decidable. the resultinglanguage generalizes the first-order theory of boolean algebra over sets and is useful inreasoning about the contents of data structures in object-oriented languages.",language models
Gerald Sussman,"Evans, Isaac; Lynch, Joseph",2013-06-03T23:30:05Z,2013-06-03T23:30:05Z,2013-05-24,http://hdl.handle.net/1721.1/79057,MIT-CSAIL-TR-2013-010,Organon: A Symbolic Constraint Framework & Solver,"Organon is an open source system for expressing and solving complex symbolic constraints between generic entities. Our design avoids restricting the programmer s ability to phrase constraints; Organon acts purely as a framework that defines and holds together the key concepts of forms, constraints, and solvers. It has three main components: (1) Forms: Abstract representations of the entities to be constrained. (2) Constraints: Functions that symbolically express requirements on the relationships between forms as well as provide information a solver can use to improve the constraint s satisfaction. (3) Solvers: Functions which inspect instantiations of forms and manipulate them in an attempt to satisfy a set of objective constraints.",,33 p.,,,scheme; propagator; exponential solver; annealing solver,"Robotics, Vision & Sensor Networks",,,,,,,,,,,,,2013-06-03T23:30:06Z,,,,,,,,,,,,,,,,2013,925,"organon: a symbolic constraint framework & solver organon is an open source system for expressing and solving complex symbolic constraints between generic entities. our design avoids restricting the programmer s ability to phrase constraints; organon acts purely as a framework that defines and holds together the key concepts of forms, constraints, and solvers. it has three main components: (1) forms: abstract representations of the entities to be constrained. (2) constraints: functions that symbolically express requirements on the relationships between forms as well as provide information a solver can use to improve the constraint s satisfaction. (3) solvers: functions which inspect instantiations of forms and manipulate them in an attempt to satisfy a set of objective constraints.",language models
Tomaso Poggio,"De Vito, Ernesto; Belkin, Mikhail; Rosasco, Lorenzo",2008-08-20T19:15:07Z,2008-08-20T19:15:07Z,2008-08-19,http://hdl.handle.net/1721.1/41940,MIT-CSAIL-TR-2008-052; CBCL-274,A Note on Perturbation Results for Learning Empirical Operators,"A large number of learning algorithms, for example, spectral clustering, kernel Principal Components Analysis and many manifold methods are based on estimating eigenvalues and eigenfunctions of operators defined by a similarity function or a kernel, given empirical data. Thus for the analysis of algorithms, it is an important problem to be able to assess the quality of such approximations. The contribution of our paper is two-fold:  1. We use a technique based on a concentration inequality for Hilbert spaces to provide new much simplified proofs for a number of results in spectral approximation.  2. Using these methods we provide several new results for estimating spectral properties of the graph Laplacian operator extending and strengthening results from [26].",,22 p.,,,perturbation theory; statistical learning theory; kernel methods; spectral methods,Center for Biological and Computational Learning (CBCL),,Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2008,682,"a note on perturbation results for learning empirical operators a large number of learning algorithms, for example, spectral clustering, kernel principal components analysis and many manifold methods are based on estimating eigenvalues and eigenfunctions of operators defined by a similarity function or a kernel, given empirical data. thus for the analysis of algorithms, it is an important problem to be able to assess the quality of such approximations. the contribution of our paper is two-fold:  1. we use a technique based on a concentration inequality for hilbert spaces to provide new much simplified proofs for a number of results in spectral approximation.  2. using these methods we provide several new results for estimating spectral properties of the graph laplacian operator extending and strengthening results from [26].",privacy/ethics
Tomaso Poggio,"Poggio, Tomaso; Mutch, Jim; Anselmi, Fabio; Tacchetti, Andrea; Rosasco, Lorenzo; Leibo, Joel Z.",2013-08-12T02:30:12Z,2013-08-12T02:30:12Z,2013-08-06,http://hdl.handle.net/1721.1/79828,MIT-CSAIL-TR-2013-019; CBCL-313,Does invariant recognition predict tuning of neurons in sensory cortex?,"Tuning properties of simple cells in cortical V1 can be described in terms of a ""universal shape"" characterized by parameter values which hold across different species. This puzzling set of findings begs for a general explanation grounded on an evolutionarily important computational function of the visual cortex. We ask here whether these properties are predicted by the hypothesis that the goal of the ventral stream is to compute for each image a ""signature"" vector which is invariant to geometric transformations, with the the additional assumption that the mechanism for continuously learning and maintaining invariance consists of the memory storage of a sequence of neural images of a few objects undergoing transformations (such as translation, scale changes and rotation) via Hebbian synapses. For V1 simple cells the simplest version of this hypothesis is the online Oja rule which implies that the tuning of neurons converges to the eigenvectors of the covariance of their input. Starting with a set of dendritic fields spanning a range of sizes, simulations supported by a direct mathematical analysis show that the solution of the associated ""cortical equation"" provides a set of Gabor-like wavelets with parameter values that are in broad agreement with the physiology data. We show however that the simple version of the Hebbian assumption does not predict all the physiological properties. The same theoretical framework also provides predictions about the tuning of cells in V4 and in the face patch AL which are in qualitative agreement with physiology data.",,10 p.,,,,,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,2013-08-12T02:30:12Z,,,,,,,,,,,,,,,,2013,932,"does invariant recognition predict tuning of neurons in sensory cortex? tuning properties of simple cells in cortical v1 can be described in terms of a ""universal shape"" characterized by parameter values which hold across different species. this puzzling set of findings begs for a general explanation grounded on an evolutionarily important computational function of the visual cortex. we ask here whether these properties are predicted by the hypothesis that the goal of the ventral stream is to compute for each image a ""signature"" vector which is invariant to geometric transformations, with the the additional assumption that the mechanism for continuously learning and maintaining invariance consists of the memory storage of a sequence of neural images of a few objects undergoing transformations (such as translation, scale changes and rotation) via hebbian synapses. for v1 simple cells the simplest version of this hypothesis is the online oja rule which implies that the tuning of neurons converges to the eigenvectors of the covariance of their input. starting with a set of dendritic fields spanning a range of sizes, simulations supported by a direct mathematical analysis show that the solution of the associated ""cortical equation"" provides a set of gabor-like wavelets with parameter values that are in broad agreement with the physiology data. we show however that the simple version of the hebbian assumption does not predict all the physiological properties. the same theoretical framework also provides predictions about the tuning of cells in v4 and in the face patch al which are in qualitative agreement with physiology data.",face detection
,"Suh, G. Edward; Clarke, Dwaine; Gassend, Blaise; van Dijk, Marten; Devadas, Srinivas",2023-03-29T15:36:27Z,2023-03-29T15:36:27Z,2002-11,https://hdl.handle.net/1721.1/149968,MIT-LCS-TR-872,Hardware Mechanisms for Memory Integrity Checking,"Memory integrity verification is a useful primitive when implementing  secure processors that are resistant to attacks on hardware components.  This paper proposes new hardware schemes to verify the integrity of  untrusted external memory using a very small amount of trusted on-chip  storage. Our schemes maintain incremental multiset hashes of all memory  reads and writes at run-time, and can verify a {\\em sequence} of memory  operations at a later time. We study the advantages and disadvantages of  the two new schemes and two existing integrity checking schemes, MACs  and hash trees, when implemented in hardware in a microprocessor.  Simulations show that the new schemes outperform existing schemes of  equivalent functionality when integrity verification is infrequent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,169,"hardware mechanisms for memory integrity checking memory integrity verification is a useful primitive when implementing  secure processors that are resistant to attacks on hardware components.  this paper proposes new hardware schemes to verify the integrity of  untrusted external memory using a very small amount of trusted on-chip  storage. our schemes maintain incremental multiset hashes of all memory  reads and writes at run-time, and can verify a {\\em sequence} of memory  operations at a later time. we study the advantages and disadvantages of  the two new schemes and two existing integrity checking schemes, macs  and hash trees, when implemented in hardware in a microprocessor.  simulations show that the new schemes outperform existing schemes of  equivalent functionality when integrity verification is infrequent.",language models
Eric Grimson,"Zollei, Lilla",2006-01-25T21:04:03Z,2006-01-25T21:04:03Z,2006-01-25,http://hdl.handle.net/1721.1/30970,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Unified Information Theoretic Framework for Pair- and Group-wise Registration of Medical Images,"The field of medical image analysis has been rapidly growing for the past two decades. Besides a significant growth in computational power, scanner performance, and storage facilities, this acceleration is partially due to an unprecedented increase in the amount of data sets accessible for researchers. Medical experts traditionally rely on manual comparisons of images, but the abundance of information now available makes this task increasingly difficult. Such a challenge prompts for more automation in processing the images.In order to carry out any sort of comparison among multiple medical images, onefrequently needs to identify the proper correspondence between them. This step allows us to follow the changes that happen to anatomy throughout a time interval, to identify differences between individuals, or to acquire complementary information from different data modalities. Registration achieves such a correspondence. In this dissertation we focus on the unified analysis and characterization of statistical registration approaches.We formulate and interpret a select group of pair-wise registration methods in the context of a unified statistical and information theoretic framework. This clarifies the implicit assumptions of each method and yields a better understanding of their relative strengths and weaknesses. This guides us to a new registration algorithm that incorporates the advantages of the previously described methods. Next we extend the unified formulation with analysis of the group-wise registration algorithms that align a population as opposed to pairs of data sets. Finally, we present our group-wise registration framework, stochastic congealing. The algorithm runs in a simultaneous fashion, with every member of the population approaching the central tendency of the collection at the same time. It eliminates the need for selecting a particular referenceframe a priori, resulting in a non-biased estimate of a digital template. Our algorithm adopts an information theoretic objective function which is optimized via a gradientbased stochastic approximation process embedded in a multi-resolution setting. We demonstrate the accuracy and performance characteristics of stochastic congealing via experiments on both synthetic and real images.",MIT-CSAIL-TR-2006-005,152 p.; 254354390 bytes; 7278255 bytes,application/postscript; application/pdf,en_US,"population alignment, spatial normalization, congealing",Vision,,,,PhD thesis,,,,,,,,,,,,,,,,,,,,,,,,,2006,487,"a unified information theoretic framework for pair- and group-wise registration of medical images the field of medical image analysis has been rapidly growing for the past two decades. besides a significant growth in computational power, scanner performance, and storage facilities, this acceleration is partially due to an unprecedented increase in the amount of data sets accessible for researchers. medical experts traditionally rely on manual comparisons of images, but the abundance of information now available makes this task increasingly difficult. such a challenge prompts for more automation in processing the images.in order to carry out any sort of comparison among multiple medical images, onefrequently needs to identify the proper correspondence between them. this step allows us to follow the changes that happen to anatomy throughout a time interval, to identify differences between individuals, or to acquire complementary information from different data modalities. registration achieves such a correspondence. in this dissertation we focus on the unified analysis and characterization of statistical registration approaches.we formulate and interpret a select group of pair-wise registration methods in the context of a unified statistical and information theoretic framework. this clarifies the implicit assumptions of each method and yields a better understanding of their relative strengths and weaknesses. this guides us to a new registration algorithm that incorporates the advantages of the previously described methods. next we extend the unified formulation with analysis of the group-wise registration algorithms that align a population as opposed to pairs of data sets. finally, we present our group-wise registration framework, stochastic congealing. the algorithm runs in a simultaneous fashion, with every member of the population approaching the central tendency of the collection at the same time. it eliminates the need for selecting a particular referenceframe a priori, resulting in a non-biased estimate of a digital template. our algorithm adopts an information theoretic objective function which is optimized via a gradientbased stochastic approximation process embedded in a multi-resolution setting. we demonstrate the accuracy and performance characteristics of stochastic congealing via experiments on both synthetic and real images.",high performance computing
,"Tanudjaja, Francisco; Mui, Lik",2023-03-29T14:42:18Z,2023-03-29T14:42:18Z,2001-05,https://hdl.handle.net/1721.1/149307,MIT-LCS-TM-618,Persona: A Contextualized and Personalized Web Search,"Recent advances in graph-based search techniques derived from Kleinberg's work [1] have been impressive. This paper further improves the graph-based search algorithm in two dimensions. Firstly, variants of Kleinberg's techniques do not take into account the semantics of the query string nor of the nodes being searched. As a result, polysemy of query words cannot be resolved. This paper presents an interactive query scheme utilizing the simple web ontology provided by the Open Directory Project to resolve meanings of a user query. Secondly, we extend a recently proposed personalized version of the Kleinberg algorithm [3]. Simulation results are presented to illustrate the sensitivity of our technique. We outline the implementation of our algorithm in the Persona personalized web search system.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,52,"persona: a contextualized and personalized web search recent advances in graph-based search techniques derived from kleinberg's work [1] have been impressive. this paper further improves the graph-based search algorithm in two dimensions. firstly, variants of kleinberg's techniques do not take into account the semantics of the query string nor of the nodes being searched. as a result, polysemy of query words cannot be resolved. this paper presents an interactive query scheme utilizing the simple web ontology provided by the open directory project to resolve meanings of a user query. secondly, we extend a recently proposed personalized version of the kleinberg algorithm [3]. simulation results are presented to illustrate the sensitivity of our technique. we outline the implementation of our algorithm in the persona personalized web search system.",language models
Joshua Tenenbaum,"Tenenbaum, Joshua B.; Jonas, Eric M.; Mansinghka, Vikash K.",2008-11-24T16:30:26Z,2008-11-24T16:30:26Z,2008-11-24,http://hdl.handle.net/1721.1/43712,MIT-CSAIL-TR-2008-069,Stochastic Digital Circuits for Probabilistic Inference,"We introduce combinational stochastic logic, an abstraction that generalizes deterministic digital circuit design (based on Boolean logic gates) to the probabilistic setting. We show how this logic can be combined with techniques from contemporary digital design to generate stateless and stateful circuits for exact and approximate sampling from a range of probability distributions. We focus on Markov chain Monte Carlo algorithms for Markov random fields, using massively parallel circuits. We implement these circuits on commodity reconfigurable logic and estimate the resulting performance in time, space and price. Using our approach, these simple and general algorithms could be affordably run for thousands of iterations on models with hundreds of thousands of variables in real time.",,10 p.,,,cognitive science; robustness; Bayesian inference; artificial intelligence,Computational Cognitive Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,699,"stochastic digital circuits for probabilistic inference we introduce combinational stochastic logic, an abstraction that generalizes deterministic digital circuit design (based on boolean logic gates) to the probabilistic setting. we show how this logic can be combined with techniques from contemporary digital design to generate stateless and stateful circuits for exact and approximate sampling from a range of probability distributions. we focus on markov chain monte carlo algorithms for markov random fields, using massively parallel circuits. we implement these circuits on commodity reconfigurable logic and estimate the resulting performance in time, space and price. using our approach, these simple and general algorithms could be affordably run for thousands of iterations on models with hundreds of thousands of variables in real time.",high performance computing
,"Arkoudas, Konstantine",2005-12-22T02:37:00Z,2005-12-22T02:37:00Z,2005-10-06,http://hdl.handle.net/1721.1/30570,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Combining diagrammatic and symbolic reasoning,"We introduce a domain-independent framework for heterogeneous natural deduction that combines diagrammatic and sentential reasoning. The framework is presented in the form of a family of denotational proof languages (DPLs). Diagrams are represented as possibly partial descriptions of finite system states. This allows us to dealwith incomplete information, which we formalize by admitting sets as attribute values. We introduce a notion of attribute interpretations that enables us to interpret  first-order signatures into such system states, and develop a formal semantic framework based on Kleene\'s strong three-valued logic. We extend the assumption-base semantics of DPLs to accodomodate diagrammatic reasoning by introducing general inference mechanisms  for the valid extraction of information from diagrams and for the incorporation of sentential information into diagrams. A rigorous big-step operational semantics is given, on the basis of which we prove that our framework is sound. In addition, we specify detailed algorithms for implementing proof checkers for the resulting languages, and discuss associated efficiency issues.",MIT-CSAIL-TR-2005-059; MIT-LCS-TR-1002,58 p.; 48633844 bytes; 2003976 bytes,application/postscript; application/pdf,en_US,,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,459,"combining diagrammatic and symbolic reasoning we introduce a domain-independent framework for heterogeneous natural deduction that combines diagrammatic and sentential reasoning. the framework is presented in the form of a family of denotational proof languages (dpls). diagrams are represented as possibly partial descriptions of finite system states. this allows us to dealwith incomplete information, which we formalize by admitting sets as attribute values. we introduce a notion of attribute interpretations that enables us to interpret  first-order signatures into such system states, and develop a formal semantic framework based on kleene\'s strong three-valued logic. we extend the assumption-base semantics of dpls to accodomodate diagrammatic reasoning by introducing general inference mechanisms  for the valid extraction of information from diagrams and for the incorporation of sentential information into diagrams. a rigorous big-step operational semantics is given, on the basis of which we prove that our framework is sound. in addition, we specify detailed algorithms for implementing proof checkers for the resulting languages, and discuss associated efficiency issues.",language models
,"Feamster, Nick; Balakrishnan, Hari",2005-12-22T01:31:12Z,2005-12-22T01:31:12Z,2004-05-17,http://hdl.handle.net/1721.1/30471,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Verifying the Correctness of Wide-Area Internet Routing,"Several studies have shown that wide-area Internet routing is fragile, with failures occurring for a variety of reasons. Routing fragility is largely due to the flexible and powerful ways in which BGP can be configured to perform various tasks, which range from implementing the policies of commercial relationships to configuring backup paths. Configuring routers in an AS is like writing a distributed program, and BGP's flexible configuration and today's relatively low-level configuration languages make the process error-prone. The primary method used by operators to determine whether their complex configurations are correct is to try them out in operation.We believe that there is a need for a systematic approach to verifying router configurations before they are deployed. This paper develops a static analysis framework for configuration checking, and uses it in the design of rcc, a ``router configuration checker''. rcc takes as input a set of router configurations and flags anomalies and errors, based on a set of well-defined correctness conditions. We have used rcc to check BGP configurations from 9 operational networks, testing nearly 700 real-world router configurations in the process. Every network we analyzed had configuration errors, some of which were potentially serious and had previously gone unnoticed. Our analysis framework and results also suggest ways in which BGP and configuration languages should be improved. rcc has also been downloaded by 30 network operators to date.",MIT-CSAIL-TR-2004-031; MIT-LCS-TR-948,14 p.; 28764880 bytes; 1265172 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,330,"verifying the correctness of wide-area internet routing several studies have shown that wide-area internet routing is fragile, with failures occurring for a variety of reasons. routing fragility is largely due to the flexible and powerful ways in which bgp can be configured to perform various tasks, which range from implementing the policies of commercial relationships to configuring backup paths. configuring routers in an as is like writing a distributed program, and bgp's flexible configuration and today's relatively low-level configuration languages make the process error-prone. the primary method used by operators to determine whether their complex configurations are correct is to try them out in operation.we believe that there is a need for a systematic approach to verifying router configurations before they are deployed. this paper develops a static analysis framework for configuration checking, and uses it in the design of rcc, a ``router configuration checker''. rcc takes as input a set of router configurations and flags anomalies and errors, based on a set of well-defined correctness conditions. we have used rcc to check bgp configurations from 9 operational networks, testing nearly 700 real-world router configurations in the process. every network we analyzed had configuration errors, some of which were potentially serious and had previously gone unnoticed. our analysis framework and results also suggest ways in which bgp and configuration languages should be improved. rcc has also been downloaded by 30 network operators to date.",language models
Michael Ernst,"Tip, Frank; Ernst, Michael D.; Dig, Danny; Dolby, Julian; Kiezun, Adam; Artzi, Shay; Paradkar, Amit",2009-03-27T16:00:07Z,2009-03-27T16:00:07Z,2009-03-26,http://hdl.handle.net/1721.1/44956,MIT-CSAIL-TR-2009-010,Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit State Model Checking,"Web script crashes and malformed dynamically-generated web pages are common errors, and they seriously impact the usability of web applications. Current tools for web-page validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests, so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 302 faults in 6 PHP web applications.",,17 p.,,,Software Testing; PHP; Dynamic Analysis,Program Analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,713,"finding bugs in web applications using dynamic test generation and explicit state model checking web script crashes and malformed dynamically-generated web pages are common errors, and they seriously impact the usability of web applications. current tools for web-page validation cannot handle the dynamically generated pages that are ubiquitous on today's internet. we present a dynamic test generation technique for the domain of dynamic web applications. the technique utilizes both combined concrete and symbolic execution and explicit-state model checking. the technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests, so that the resulting bug reports are small and useful in finding and fixing the underlying faults. our tool apollo implements the technique for the php programming language. apollo generates test inputs for a web application, monitors the application for crashes, and validates that the output conforms to the html specification. this paper presents apollo's algorithms and implementation, and an experimental evaluation that revealed 302 faults in 6 php web applications.",language models
Anant Agarwal,"Lau, Eric; Miller, Jason E; Choi, Inseok; Yeung, Donald; Amarasinghe, Saman; Agarwal, Anant",2011-03-25T21:15:08Z,2011-03-25T21:15:08Z,2011-03-25,http://hdl.handle.net/1721.1/61978,MIT-CSAIL-TR-2011-017,Multicore Performance Optimization Using Partner Cores,"As the push for parallelism continues to increase the number of cores on a chip, and add to the complexity of system design, the task of optimizing performance at the application level becomes nearly impossible for the programmer. Much effort has been spent on developing techniques for optimizing performance at runtime, but many techniques for modern processors employ the use of speculative threads or performance counters. These approaches result in stolen cycles, or the use of an extra core, and such expensive penalties put demanding constraints on the gains provided by such methods. While processors have grown in power and complexity, the technology for small, efficient cores has emerged. We introduce the concept of Partner Cores for maximizing hardware power efficiency; these are low-area, low-power cores situated on-die, tightly coupled to each main processor core. We demonstrate that such cores enable performance improvement without incurring expensive penalties, and carry out potential applications that are impossible on a traditional chip multiprocessor.",,7 p.,,,self-aware; adaptive,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2011,844,"multicore performance optimization using partner cores as the push for parallelism continues to increase the number of cores on a chip, and add to the complexity of system design, the task of optimizing performance at the application level becomes nearly impossible for the programmer. much effort has been spent on developing techniques for optimizing performance at runtime, but many techniques for modern processors employ the use of speculative threads or performance counters. these approaches result in stolen cycles, or the use of an extra core, and such expensive penalties put demanding constraints on the gains provided by such methods. while processors have grown in power and complexity, the technology for small, efficient cores has emerged. we introduce the concept of partner cores for maximizing hardware power efficiency; these are low-area, low-power cores situated on-die, tightly coupled to each main processor core. we demonstrate that such cores enable performance improvement without incurring expensive penalties, and carry out potential applications that are impossible on a traditional chip multiprocessor.",high performance computing
,"Teller, Seth",2023-03-29T15:34:30Z,2023-03-29T15:34:30Z,2001-09,https://hdl.handle.net/1721.1/149928,MIT-LCS-TR-825,"Scalable, Controlled Imagery Capture in Urban Environments","We describe the design considerations underlying a system for scalable, automated capture of precisely controlled imagery in urban scenes. The system operates for architectural scenes in which, from every camera position, some  two vanishing points are visible. It has been used to capture thousands of controlled images in outdoor environments spanning hundreds of meters. The proposed system architecture forms the foundation for a future, fully robotic outdoor mapping capability for urban areas, analogous to existing, satellite-based robotic mapping systems which acquire images and models of natural terrain.  Four key ideas distinguish our approach from other methods. First, our sensor acquires georeferencing metadata with every image, enabling related images to be efficiently identified and registered. Second, the sensor acquires omni-directional images; we show strong experimental evidence that such images are fundamentally more powerful observations than conventional (narrow-FOV) images. Third, the system uses a probabilistic, projective error formulation to account for uncertainty. By treating measurement error in an appropriate depth-free framework, and by deferring decisions about camera calibration and scene structure until many noisy observations can be fused, the system achieves superior robustness and accuracy. Fourth, the system's computational requirements scale linearly in the input size, the area of the acquisition region, and the size of the output model. This is in contrast to most previous methods, which either assume constant-size inputs or exhibit quadratic running time (or worse) asymptotically. These attributes enable the system to operate in a regime of scale and physical extent which is unachievable by any other method, whether manual or automated. Consequently, it can acquire the most complex calibrated terrestrial image sets in existence, while operating faster thanany existing manual or algorithmic method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,76,"scalable, controlled imagery capture in urban environments we describe the design considerations underlying a system for scalable, automated capture of precisely controlled imagery in urban scenes. the system operates for architectural scenes in which, from every camera position, some  two vanishing points are visible. it has been used to capture thousands of controlled images in outdoor environments spanning hundreds of meters. the proposed system architecture forms the foundation for a future, fully robotic outdoor mapping capability for urban areas, analogous to existing, satellite-based robotic mapping systems which acquire images and models of natural terrain.  four key ideas distinguish our approach from other methods. first, our sensor acquires georeferencing metadata with every image, enabling related images to be efficiently identified and registered. second, the sensor acquires omni-directional images; we show strong experimental evidence that such images are fundamentally more powerful observations than conventional (narrow-fov) images. third, the system uses a probabilistic, projective error formulation to account for uncertainty. by treating measurement error in an appropriate depth-free framework, and by deferring decisions about camera calibration and scene structure until many noisy observations can be fused, the system achieves superior robustness and accuracy. fourth, the system's computational requirements scale linearly in the input size, the area of the acquisition region, and the size of the output model. this is in contrast to most previous methods, which either assume constant-size inputs or exhibit quadratic running time (or worse) asymptotically. these attributes enable the system to operate in a regime of scale and physical extent which is unachievable by any other method, whether manual or automated. consequently, it can acquire the most complex calibrated terrestrial image sets in existence, while operating faster thanany existing manual or algorithmic method.",high performance computing
Barbara Liskov,"Cowling, James; Myers, Daniel; Liskov, Barbara; Rodrigues, Rodrigo; Shrira, Liuba",2007-02-13T06:17:18Z,2007-02-13T06:17:18Z,2007-02-12,http://hdl.handle.net/1721.1/35888,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,HQ Replication: Properties and Optimizations,"There are currently two approaches to providing Byzantine-fault-tolerant state machine replication: a replica-based approach, e.g., BFT, that uses communication between replicas to agree on a proposed ordering of requests, and a quorum-based approach, such as Q/U, in which clients contact replicas directly to optimistically execute operations. Both approaches have shortcomings: the quadratic cost of inter-replica communication is unnecessary when there is no contention, and Q/U requires a large number of replicas and performs poorly under contention.We present HQ, a hybrid Byzantine-fault-tolerant state machine replication protocol that overcomes these problems. HQ employs a lightweight quorum-based protocol when there is no contention, but  uses BFT to resolve contention when it arises.  Furthermore, HQ uses only 3f+1 replicas to tolerate f faults, providing optimal resilience to node failures.We implemented a prototype of HQ, and we compare its performance to BFT and Q/U analytically and experimentally. Additionally, in this work we use a new implementation of BFT designed to scale as the number of faults increases.  Our results show that both HQ and our new implementation of BFT scale as f increases; additionally our hybrid approach of using BFT to handle contention works well.",MIT-CSAIL-TR-2007-009,18 p.,,,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,575,"hq replication: properties and optimizations there are currently two approaches to providing byzantine-fault-tolerant state machine replication: a replica-based approach, e.g., bft, that uses communication between replicas to agree on a proposed ordering of requests, and a quorum-based approach, such as q/u, in which clients contact replicas directly to optimistically execute operations. both approaches have shortcomings: the quadratic cost of inter-replica communication is unnecessary when there is no contention, and q/u requires a large number of replicas and performs poorly under contention.we present hq, a hybrid byzantine-fault-tolerant state machine replication protocol that overcomes these problems. hq employs a lightweight quorum-based protocol when there is no contention, but  uses bft to resolve contention when it arises.  furthermore, hq uses only 3f+1 replicas to tolerate f faults, providing optimal resilience to node failures.we implemented a prototype of hq, and we compare its performance to bft and q/u analytically and experimentally. additionally, in this work we use a new implementation of bft designed to scale as the number of faults increases.  our results show that both hq and our new implementation of bft scale as f increases; additionally our hybrid approach of using bft to handle contention works well.",high performance computing
Michael Ernst,"Artzi, Shay; Kiezun, Adam; Dolby, Julian; Tip, Frank; Dig, Danny; Paradkar, Amit; Ernst, Michael D.",2008-02-06T14:15:11Z,2008-02-06T14:15:11Z,2008-02-06,http://hdl.handle.net/1721.1/40249,,Finding Bugs In Dynamic Web Applications,"Web script crashes and malformed dynamically-generated web pages are common errors, and they seriously impact usability of web applications. Currenttools for web-page validation cannot handle the dynamically-generatedpages that are ubiquitous on today's Internet.In this work, we apply a dynamic test generation technique, based oncombined concrete and symbolic execution, to the domain of dynamic webapplications. The technique generates tests automatically andminimizes the bug-inducing inputs to reduce duplication and to makethe bug reports small and easy to understand and fix.We implemented the technique in Apollo, an automated tool thatfound dozens of bugs in real PHP applications. Apollo generatestest inputs for the web application, monitors the application forcrashes, and validates that the output conforms to the HTMLspecification. This paper presents Apollo's algorithms andimplementation, and an experimental evaluation that revealed a totalof 214 bugs in 4 open-source PHP web applications.",MIT-CSAIL-TR-2008-006,12 p.,,,html; syntax; validation; dynamic; bug,Program Analysis,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,637,"finding bugs in dynamic web applications web script crashes and malformed dynamically-generated web pages are common errors, and they seriously impact usability of web applications. currenttools for web-page validation cannot handle the dynamically-generatedpages that are ubiquitous on today's internet.in this work, we apply a dynamic test generation technique, based oncombined concrete and symbolic execution, to the domain of dynamic webapplications. the technique generates tests automatically andminimizes the bug-inducing inputs to reduce duplication and to makethe bug reports small and easy to understand and fix.we implemented the technique in apollo, an automated tool thatfound dozens of bugs in real php applications. apollo generatestest inputs for the web application, monitors the application forcrashes, and validates that the output conforms to the htmlspecification. this paper presents apollo's algorithms andimplementation, and an experimental evaluation that revealed a totalof 214 bugs in 4 open-source php web applications.",high performance computing
,"Livadas, Carolos; Lynch, Nancy A.",2005-12-19T23:14:28Z,2005-12-19T23:14:28Z,2003-08-11,http://hdl.handle.net/1721.1/30410,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,A Reliable Broadcast Scheme for Sensor Networks,"In this short technical report, we present a simple yet effective reliable broadcast protocol for sensor networks. This protocol disseminates packets throughout the sensor network by flooding and recovers from losses resulting from collisions by having hosts retransmit packets whenever they notice that their neighbors have fallen behind. Such retransmissions serve to flood the appropriate packets throughout the regions of the sensor network that did not receive the given packets as a result of prior flooding attempts.",MIT-CSAIL-TR-2003-010; MIT-LCS-TR-915,5 p.; 6233240 bytes; 285081 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,251,"a reliable broadcast scheme for sensor networks in this short technical report, we present a simple yet effective reliable broadcast protocol for sensor networks. this protocol disseminates packets throughout the sensor network by flooding and recovers from losses resulting from collisions by having hosts retransmit packets whenever they notice that their neighbors have fallen behind. such retransmissions serve to flood the appropriate packets throughout the regions of the sensor network that did not receive the given packets as a result of prior flooding attempts.",language models
"Amarasinghe, Saman; Agarwal, Anant","Barua, Rajeev",2023-03-29T15:32:37Z,2023-03-29T15:32:37Z,2000-01,https://hdl.handle.net/1721.1/149907,MIT-LCS-TR-799,Maps:  A Compiler-Managed Memory System for Software-Exposed Architectures,"Microprocessors must exploit both instruction-level parallelism (ILP) and memory parallelism for high performance.  Sophisticated techniques for ILP have boosted the ability of modern-day microprocessors to exploit ILP when available. Unfortunately, impro",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,4,"maps:  a compiler-managed memory system for software-exposed architectures microprocessors must exploit both instruction-level parallelism (ilp) and memory parallelism for high performance.  sophisticated techniques for ilp have boosted the ability of modern-day microprocessors to exploit ilp when available. unfortunately, impro",high performance computing
Gerald Sussman,"Beal, Jacob",2006-06-01T16:22:48Z,2006-06-01T16:22:48Z,2005-07,http://hdl.handle.net/1721.1/32986,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Amorphous Medium Language,"Programming reliable behavior on a large mesh network composed of unreliable parts is difficult. Amorphous Medium Language addresses this problem by abstracting robustness and networking issues away from the programmer via language of geometric primitives and homeostasis maintenance.AML is designed to operate on a high diameter network composed of thousands to billions of nodes, and does not assume coordinate, naming, or routing services. Computational processes are distributed through geometric regions of the space approximated by the network and specify behavior in terms of homeostasis conditions and actions to betaken when homeostasis is violated.AML programs are compiled for local execution using previously developed amorphous computing primitives which provide robustness against ongoing failures and joins and localize the impact of changes in topology. I show some examples of how AML allows complex robust behavior to be expressed in simple programs and some preliminary results from simulation.",MIT-CSAIL-TR-2006-040,7 p.; 408752 bytes; 2548880 bytes,application/pdf; application/postscript,en_US,distributed computing sensor networks,Mathematics and Computation,,,,,"LSMAS Workshop, AAMAS'05, July 25-29, 2005, Utrecht, Netherlands.",,,,,,,,,,,,,,,,,,,,,,,,2005,442,"amorphous medium language programming reliable behavior on a large mesh network composed of unreliable parts is difficult. amorphous medium language addresses this problem by abstracting robustness and networking issues away from the programmer via language of geometric primitives and homeostasis maintenance.aml is designed to operate on a high diameter network composed of thousands to billions of nodes, and does not assume coordinate, naming, or routing services. computational processes are distributed through geometric regions of the space approximated by the network and specify behavior in terms of homeostasis conditions and actions to betaken when homeostasis is violated.aml programs are compiled for local execution using previously developed amorphous computing primitives which provide robustness against ongoing failures and joins and localize the impact of changes in topology. i show some examples of how aml allows complex robust behavior to be expressed in simple programs and some preliminary results from simulation.",language models
,"Kohler, Eddie; Chen, Benjie; Kaashoek, M. Frans; Morris, Robert T.; Poletto, Massimiliano",2023-03-29T15:33:47Z,2023-03-29T15:33:47Z,2000-08,https://hdl.handle.net/1721.1/149915,MIT-LCS-TR-812,Programming Language Techniques for Modular Router Configurations,"This paper applies programming language techniques to a high-level system description, both to optimize the system and to prove useful properties about it. The system in question is Click, a modular software router framework. Click routers are built from components called elements. Elements are written in C++, but the user creates a configuration using a simple, declarative data flow language. This language is amenable to data flow analysis and other conventional programming language techniques. Applied to a router configuration, these techniques have high-level results---for example, optimizing the router or verifying its high-level properties. This paper describes several programming language techniques that have been useful in practice, including optimization tools that remove virtual function calls from router definitions and remove redundant parts of adjacent routers. We also present performance results for an extensively optimized standards-compliant IP router. On conventional PC hardware, this router can forward up to 456,000 64-byte packets per second.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,19,"programming language techniques for modular router configurations this paper applies programming language techniques to a high-level system description, both to optimize the system and to prove useful properties about it. the system in question is click, a modular software router framework. click routers are built from components called elements. elements are written in c++, but the user creates a configuration using a simple, declarative data flow language. this language is amenable to data flow analysis and other conventional programming language techniques. applied to a router configuration, these techniques have high-level results---for example, optimizing the router or verifying its high-level properties. this paper describes several programming language techniques that have been useful in practice, including optimization tools that remove virtual function calls from router definitions and remove redundant parts of adjacent routers. we also present performance results for an extensively optimized standards-compliant ip router. on conventional pc hardware, this router can forward up to 456,000 64-byte packets per second.",high performance computing
Brian Williams,"Blackmore, Lars",2006-04-28T18:22:27Z,2006-04-28T18:22:27Z,2006-04-28,http://hdl.handle.net/1721.1/32538,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,"A Probabilistic Particle Control Approach to Optimal, Robust Predictive Control","Autonomous vehicles need to be able to plan trajectories to a specified goal that avoid obstacles, and are robust to the inherent uncertainty in the problem. This uncertainty arises due to uncertain state estimation, disturbances and modeling errors. Previous solutions to the robust path planning problem solved this problem using a finite horizon optimal stochastic control approach. This approach finds the optimal path subject to chance constraints, which ensure that the probability of collision with obstacles is below a given threshold. This approach is limited to problems where all uncertain distributions are Gaussian, and typically result in highly conservative plans. In many cases, however, the Gaussian assumption is invalid; for example in the case of localization, the belief state about a vehicles position can consist of highly non-Gaussian, even multimodal, distributions.In this paper we present a novel method for finite horizon stochastic control ofdynamic systems subject to chance constraints. The method approximates the distribution of the system state using a finite number of particles. By expressing these particles in terms of the control variables, we are able to approximate the original stochastic control problem as a deterministic one; furthermore the approximation becomes exact as the number of particles tends to infinity. For a general class of chance constrained problems with linear system dynamics, we show that the approximate problem can be solved using efficient Mixed-Integer Linear Programming techniques. We apply the new method to aircraft control in turbulence, and show simulation results that demonstrate the efficacy of the approach.",MIT-CSAIL-TR-2006-031,15 p.; 7896873 bytes; 673988 bytes,application/postscript; application/pdf,en_US,,Model-based Embedded and Robotic Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2006,514,"a probabilistic particle control approach to optimal, robust predictive control autonomous vehicles need to be able to plan trajectories to a specified goal that avoid obstacles, and are robust to the inherent uncertainty in the problem. this uncertainty arises due to uncertain state estimation, disturbances and modeling errors. previous solutions to the robust path planning problem solved this problem using a finite horizon optimal stochastic control approach. this approach finds the optimal path subject to chance constraints, which ensure that the probability of collision with obstacles is below a given threshold. this approach is limited to problems where all uncertain distributions are gaussian, and typically result in highly conservative plans. in many cases, however, the gaussian assumption is invalid; for example in the case of localization, the belief state about a vehicles position can consist of highly non-gaussian, even multimodal, distributions.in this paper we present a novel method for finite horizon stochastic control ofdynamic systems subject to chance constraints. the method approximates the distribution of the system state using a finite number of particles. by expressing these particles in terms of the control variables, we are able to approximate the original stochastic control problem as a deterministic one; furthermore the approximation becomes exact as the number of particles tends to infinity. for a general class of chance constrained problems with linear system dynamics, we show that the approximate problem can be solved using efficient mixed-integer linear programming techniques. we apply the new method to aircraft control in turbulence, and show simulation results that demonstrate the efficacy of the approach.",high performance computing
Brian Williams,"Williams, Brian C.; Ono, Masahiro",2009-04-23T17:15:13Z,2009-04-23T17:15:13Z,2009-04-22,http://hdl.handle.net/1721.1/45142,MIT-CSAIL-TR-2009-016,Risk Allocation for Multi-agent Systems using Tatonnement,"This paper proposes a new market-based distributed planning algorithm for multi-agent systems under uncertainty, called MIRA (Market-based Iterative Risk Allocation). In large coordination problems, from power grid management to multi-vehicle missions, multiple agents act collectively in order to optimize the performance of the system, while satisfying mission constraints. These optimal plans are particularly susceptible to risk when uncertainty is introduced. We present a distributed planning algorithm that minimizes the system cost while ensuring that the probability of violating mission constraints is below a user-specified level. We build upon the paradigm of risk allocation (Ono and Williams, AAAI-08), in which the planner optimizes not only the sequence of actions, but also its allocation of risk among each constraint at each time step. We extend the concept of risk allocation to multi-agent systems by highlighting risk as a good that is traded in a computational market. The equilibrium price of risk that balances the supply and demand is found by an iterative price adjustment process called tatonnement (also known as Walrasian auction). The simulation results demonstrate the efficiency and optimality of the proposed distributed planner.",,18 p.,,,Robust MPC; Chance constraint; RMPC; Brent's method; Grouping,Model-based Embedded and Robotic Systems,This research is funded by The Boeing Company grant MIT-BA-GTA-1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,719,"risk allocation for multi-agent systems using tatonnement this paper proposes a new market-based distributed planning algorithm for multi-agent systems under uncertainty, called mira (market-based iterative risk allocation). in large coordination problems, from power grid management to multi-vehicle missions, multiple agents act collectively in order to optimize the performance of the system, while satisfying mission constraints. these optimal plans are particularly susceptible to risk when uncertainty is introduced. we present a distributed planning algorithm that minimizes the system cost while ensuring that the probability of violating mission constraints is below a user-specified level. we build upon the paradigm of risk allocation (ono and williams, aaai-08), in which the planner optimizes not only the sequence of actions, but also its allocation of risk among each constraint at each time step. we extend the concept of risk allocation to multi-agent systems by highlighting risk as a good that is traded in a computational market. the equilibrium price of risk that balances the supply and demand is found by an iterative price adjustment process called tatonnement (also known as walrasian auction). the simulation results demonstrate the efficiency and optimality of the proposed distributed planner.",language models
Silvio Micali,"Chen, Jing; Micali, Silvio",2010-12-30T09:30:03Z,2010-12-30T09:30:03Z,2010-12-20,http://hdl.handle.net/1721.1/60371,MIT-CSAIL-TR-2010-060,Conservative Rationalizability and The Second-Knowledge Mechanism,"In mechanism design, the traditional way of modeling the players' incomplete information about their opponents is ""assuming a Bayesian."" This assumption, however, is very strong and does not hold in many real applications. Accordingly, we put forward (1) a set-theoretic way to model the knowledge that a player might have about his opponents, and (2) a new class of mechanisms capable of leveraging such more conservative knowledge in a robust way. In auctions of a single good, we show that such a new mechanism can perfectly guarantee a revenue benchmark (always lying in between the second highest and the highest valuation) that no classical mechanism can even approximate in any robust way.",,23 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,824,"conservative rationalizability and the second-knowledge mechanism in mechanism design, the traditional way of modeling the players' incomplete information about their opponents is ""assuming a bayesian."" this assumption, however, is very strong and does not hold in many real applications. accordingly, we put forward (1) a set-theoretic way to model the knowledge that a player might have about his opponents, and (2) a new class of mechanisms capable of leveraging such more conservative knowledge in a robust way. in auctions of a single good, we show that such a new mechanism can perfectly guarantee a revenue benchmark (always lying in between the second highest and the highest valuation) that no classical mechanism can even approximate in any robust way.",language models
Nickolai Zeldovich,"Popa, Raluca Ada; Zeldovich, Nickolai; Balakrishnan, Hari",2011-02-01T20:15:04Z,2011-02-01T20:15:04Z,2011-01-26,http://hdl.handle.net/1721.1/60876,MIT-CSAIL-TR-2011-005,CryptDB: A Practical Encrypted Relational DBMS,"CryptDB is a DBMS that provides provable and practical privacy in the face of a compromised database server or curious database administrators. CryptDB works by executing SQL queries over encrypted data. At its core are three novel ideas: an SQL-aware encryption strategy that maps SQL operations to encryption schemes, adjustable query-based encryption which allows CryptDB to adjust the encryption level of each data item based on user queries, and onion encryption to efficiently change data encryption levels. CryptDB only empowers the server to execute queries that the users requested, and achieves maximum privacy given the mix of queries issued by the users. The database server fully evaluates queries on encrypted data and sends the result back to the client for final decryption; client machines do not perform any query processing and client-side applications run unchanged. Our evaluation shows that CryptDB has modest overhead: on the TPC-C benchmark on Postgres, CryptDB reduces throughput by 27% compared to regular Postgres. Importantly, CryptDB does not change the innards of existing DBMSs: we realized the implementation of CryptDB using client-side query rewriting/encrypting, user-defined functions, and server-side tables for public key information. As such, CryptDB is portable; porting CryptDB to MySQL required changing 86 lines of code, mostly at the connectivity layer.",,13 p.,,,confidentiality; privacy; cloud computing; outsourced databases; queries over encrypted data,Parallel and Distributed Operating Systems,,Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported,http://creativecommons.org/licenses/by-nc-nd/3.0/,,,,,,,,,,,,,,,,,,,,,,,,,,2011,832,"cryptdb: a practical encrypted relational dbms cryptdb is a dbms that provides provable and practical privacy in the face of a compromised database server or curious database administrators. cryptdb works by executing sql queries over encrypted data. at its core are three novel ideas: an sql-aware encryption strategy that maps sql operations to encryption schemes, adjustable query-based encryption which allows cryptdb to adjust the encryption level of each data item based on user queries, and onion encryption to efficiently change data encryption levels. cryptdb only empowers the server to execute queries that the users requested, and achieves maximum privacy given the mix of queries issued by the users. the database server fully evaluates queries on encrypted data and sends the result back to the client for final decryption; client machines do not perform any query processing and client-side applications run unchanged. our evaluation shows that cryptdb has modest overhead: on the tpc-c benchmark on postgres, cryptdb reduces throughput by 27% compared to regular postgres. importantly, cryptdb does not change the innards of existing dbmss: we realized the implementation of cryptdb using client-side query rewriting/encrypting, user-defined functions, and server-side tables for public key information. as such, cryptdb is portable; porting cryptdb to mysql required changing 86 lines of code, mostly at the connectivity layer.",privacy/ethics
Rob Miller,"Miller, Rob; Karger, David; Marcus, Adam; Bernstein, Michael",2009-10-13T12:31:44Z,2009-10-13T12:31:44Z,2009-10-07,http://hdl.handle.net/1721.1/49426,MIT-CSAIL-TR-2009-048,Understanding and Supporting Directed Content Sharing on the Web,"To find interesting, personally relevant web content, we often rely on friends and colleagues to pass links along as they encounter them. In this paper, we study and augment link-sharing via e-mail, the most popular means of sharing web content today. Armed with survey data indicating that active sharers of novel web content are often those that actively seek it out, we present FeedMe, a plug-in for Google Reader that makes directed sharing of content a more salient part of the user experience. Our survey research indicates that sharing is moderated by concern about relevancy to the recipient, a desire to send only novel content to the recipient, and the effort required to share. FeedMe allays these concerns by recommending friends who may be interested in seeing the content, providing information on what the recipient has seen and how many emails they have received recently, and giving recipients the opportunity to provide lightweight feedback when they appreciate shared content. FeedMe introduces a novel design space for mixed-initiative social recommenders: friends who know the user voluntarily vet the material on the user  s behalf. We present a two week field experiment (N=60) demonstrating that FeedMe  s recommendations and social awareness features made it easier and more enjoyable to share content that recipients appreciated and would not have found otherwise.",,10 p.,,,friendsourcing; Social link sharing; blogs; RSS,User Interface Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2009,753,"understanding and supporting directed content sharing on the web to find interesting, personally relevant web content, we often rely on friends and colleagues to pass links along as they encounter them. in this paper, we study and augment link-sharing via e-mail, the most popular means of sharing web content today. armed with survey data indicating that active sharers of novel web content are often those that actively seek it out, we present feedme, a plug-in for google reader that makes directed sharing of content a more salient part of the user experience. our survey research indicates that sharing is moderated by concern about relevancy to the recipient, a desire to send only novel content to the recipient, and the effort required to share. feedme allays these concerns by recommending friends who may be interested in seeing the content, providing information on what the recipient has seen and how many emails they have received recently, and giving recipients the opportunity to provide lightweight feedback when they appreciate shared content. feedme introduces a novel design space for mixed-initiative social recommenders: friends who know the user voluntarily vet the material on the user  s behalf. we present a two week field experiment (n=60) demonstrating that feedme  s recommendations and social awareness features made it easier and more enjoyable to share content that recipients appreciated and would not have found otherwise.",language models
Dina Katabi,"Katti, Sachin; Cohen, Jeffrey; Katabi, Dina",2007-02-23T23:21:45Z,2007-02-23T23:21:45Z,2007-02-23,http://hdl.handle.net/1721.1/36344,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Information Slicing: Anonymity Using Unreliable Overlays,"This paper proposes a new approach to anonymous communication called information slicing. Typically, anonymizers use onion routing, where a message is encrypted in layers with the public keys of the nodes along the path. Instead, our approach scrambles the message, divides it into pieces, and sends the pieces along disjoint paths. We show that information slicing addresses message confidentiality as well as source and destination anonymity. Surprisingly, it does not need any public key cryptography. Further, our approach naturally addresses the problem of node failures. These characteristics make it a good fit for use over dynamic peer-to-peer overlays. We evaluate the anonymity ofinformation slicing via analysis and simulations.  Our prototype implementation on PlanetLab shows that it achieves higher throughput than onion routing and effectively copes with node churn.",MIT-CSAIL-TR-2007-013,15 p,,,Privacy; Security; Overlay Networks,Networks & Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,578,"information slicing: anonymity using unreliable overlays this paper proposes a new approach to anonymous communication called information slicing. typically, anonymizers use onion routing, where a message is encrypted in layers with the public keys of the nodes along the path. instead, our approach scrambles the message, divides it into pieces, and sends the pieces along disjoint paths. we show that information slicing addresses message confidentiality as well as source and destination anonymity. surprisingly, it does not need any public key cryptography. further, our approach naturally addresses the problem of node failures. these characteristics make it a good fit for use over dynamic peer-to-peer overlays. we evaluate the anonymity ofinformation slicing via analysis and simulations.  our prototype implementation on planetlab shows that it achieves higher throughput than onion routing and effectively copes with node churn.",high performance computing
Shafi Goldwasser,"Pass, Rafael; Vaikuntanathan, Vinod",2008-05-05T15:46:14Z,2008-05-05T15:46:14Z,2008-04-16,http://hdl.handle.net/1721.1/41518,,New-Age Cryptography,"We introduce new and general complexity theoretic hardness assumptions. These assumptions abstract out concrete properties of a random oracle and are significantly stronger than traditional cryptographic hardness assumptions; however, assuming their validity we can resolve a number of longstandingopen problems in cryptography.",MIT-CSAIL-TR-2008-022,28 pp.,,,"Cryptographic Assumptions, Non-malleable Commitment, Non-malleable Zero-knowledge",Theory of Computation,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,655,"new-age cryptography we introduce new and general complexity theoretic hardness assumptions. these assumptions abstract out concrete properties of a random oracle and are significantly stronger than traditional cryptographic hardness assumptions; however, assuming their validity we can resolve a number of longstandingopen problems in cryptography.",language models
Martin Rinard,"Kim, Deokhwan; Rinard, Martin C.",2010-12-03T21:00:05Z,2010-12-03T21:00:05Z,2010-12-03,http://hdl.handle.net/1721.1/60078,MIT-CSAIL-TR-2010-056,Verification of Semantic Commutativity Conditions and Inverse Operations on Linked Data Structures,"Commuting operations play a critical role in many parallel computing systems. We present a new technique for verifying commutativity conditions, which are logical formulas that characterize when operations commute. Because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not identical) data structure states in different execution orders. We have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (ListSet and HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and ArrayList). This effort involved the specification and verification of 765 commutativity conditions. Many speculative parallel systems need to undo the effects of speculatively executed operations. Inverse operations, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). We present a new technique for verifying such inverse operations. We have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state. Together, the commutativity conditions and inverse operations provide a key resource that language designers and system developers can draw on to build parallel languages and systems with strong correctness guarantees.",,673 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,821,"verification of semantic commutativity conditions and inverse operations on linked data structures commuting operations play a critical role in many parallel computing systems. we present a new technique for verifying commutativity conditions, which are logical formulas that characterize when operations commute. because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not identical) data structure states in different execution orders. we have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (listset and hashset) as well as data structures that export a map interface (associationlist, hashtable, and arraylist). this effort involved the specification and verification of 765 commutativity conditions. many speculative parallel systems need to undo the effects of speculatively executed operations. inverse operations, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). we present a new technique for verifying such inverse operations. we have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state. together, the commutativity conditions and inverse operations provide a key resource that language designers and system developers can draw on to build parallel languages and systems with strong correctness guarantees.",language models
,"Livadas, Carolos; Lynch, Nancy A.",2023-03-29T15:36:17Z,2023-03-29T15:36:17Z,2002-11,https://hdl.handle.net/1721.1/149964,MIT-LCS-TR-868,A Formal Venture into Reliable Multicast Territory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,168,a formal venture into reliable multicast territory ,high performance computing
,"Fekete, Alan; Keidar, Idit",2023-03-29T14:42:00Z,2023-03-29T14:42:00Z,2000-11,https://hdl.handle.net/1721.1/149300,MIT-LCS-TM-610,A General Framework for Highly Available Services based on Group Communication,"We present a general framework for building highly available services. The framework uses group communication to coordinate a collection of servers. Our framework is configurable, in that one can adjust parameters such as the number of servers and the extent to which they are synchronized. We analyze the scenarios that can lead to the service availability being temporarily comprised, and we discuss the tradeoffs that govern the choice of parameters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2000,26,"a general framework for highly available services based on group communication we present a general framework for building highly available services. the framework uses group communication to coordinate a collection of servers. our framework is configurable, in that one can adjust parameters such as the number of servers and the extent to which they are synchronized. we analyze the scenarios that can lead to the service availability being temporarily comprised, and we discuss the tradeoffs that govern the choice of parameters.",language models
,"Katabi, Dina; Blake, Charles",2023-03-29T14:42:39Z,2023-03-29T14:42:39Z,2002-02,https://hdl.handle.net/1721.1/149315,MIT-LCS-TM-626,A Note on the Stability Requirements of Adaptive Virtual Queue,Choosing the correct value for the parameters of an Active Queue Management (AQM) scheme is a well-known hard problem. The Adaptive Virtual Queue (AVQ) attempts at solving this problem by using stability requirements to devise a rule for setting its parameter. This memo shows that the AVQ rule for setting its parameter is impractical for many real-life situations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2002,110,a note on the stability requirements of adaptive virtual queue choosing the correct value for the parameters of an active queue management (aqm) scheme is a well-known hard problem. the adaptive virtual queue (avq) attempts at solving this problem by using stability requirements to devise a rule for setting its parameter. this memo shows that the avq rule for setting its parameter is impractical for many real-life situations.,language models
Anant Agarwal,"Agarwal, Anant; Miller, Jason; Beckmann, Nathan; Wentzlaff, David",2010-02-12T07:15:04Z,2010-02-12T07:15:04Z,2010-02-11,http://hdl.handle.net/1721.1/51733,MIT-CSAIL-TR-2010-008,Core Count vs Cache Size for Manycore Architectures in the Cloud,"The number of cores which fit on a single chip is growing at an exponential rate while off-chip main memory bandwidth is growing at a linear rate at best. This core count to off-chip bandwidth disparity causes per-core memory bandwidth to decrease as process technology advances. Continuing per-core off-chip bandwidth reduction will cause multicore and manycore chip architects to rethink the optimal grain size of a core and the on-chip cache configuration in order to save main memory bandwidth. This work introduces an analytic model to study the tradeoffs of utilizing increased chip area for larger caches versus more cores. We focus this study on constructing manycore architectures well suited for the emerging application space of cloud computing where many independent applications are consolidated onto a single chip. This cloud computing application mix favors small, power-efficient cores. The model is exhaustively evaluated across a large range of cache and core-count configurations utilizing SPEC Int 2000 miss rates and CACTI timing and area models to determine the optimal cache configurations and the number of cores across four process nodes. The model maximizes aggregate computational throughput and is applied to SRAM and logic process DRAM caches. As an example, our study demonstrates that the optimal manycore configuration in the 32nm node for a 200 mm^2 die uses on the order of 158 cores, with each core containing a 64KB L1I cache, a 16KB L1D cache, and a 1MB L2 embedded-DRAM cache. This study finds that the optimal cache size will continue to grow as process technology advances, but the tradeoff between more cores and larger caches is a complex tradeoff in the face of limited off-chip bandwidth and the non-linearities of cache miss rates and memory controller queuing delay.",,13 p.,,,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2010,776,"core count vs cache size for manycore architectures in the cloud the number of cores which fit on a single chip is growing at an exponential rate while off-chip main memory bandwidth is growing at a linear rate at best. this core count to off-chip bandwidth disparity causes per-core memory bandwidth to decrease as process technology advances. continuing per-core off-chip bandwidth reduction will cause multicore and manycore chip architects to rethink the optimal grain size of a core and the on-chip cache configuration in order to save main memory bandwidth. this work introduces an analytic model to study the tradeoffs of utilizing increased chip area for larger caches versus more cores. we focus this study on constructing manycore architectures well suited for the emerging application space of cloud computing where many independent applications are consolidated onto a single chip. this cloud computing application mix favors small, power-efficient cores. the model is exhaustively evaluated across a large range of cache and core-count configurations utilizing spec int 2000 miss rates and cacti timing and area models to determine the optimal cache configurations and the number of cores across four process nodes. the model maximizes aggregate computational throughput and is applied to sram and logic process dram caches. as an example, our study demonstrates that the optimal manycore configuration in the 32nm node for a 200 mm^2 die uses on the order of 158 cores, with each core containing a 64kb l1i cache, a 16kb l1d cache, and a 1mb l2 embedded-dram cache. this study finds that the optimal cache size will continue to grow as process technology advances, but the tradeoff between more cores and larger caches is a complex tradeoff in the face of limited off-chip bandwidth and the non-linearities of cache miss rates and memory controller queuing delay.",language models
Sam Madden,"Newton, Ryan; Girod, Lewis; Craig, Michael; Madden, Sam; Morrisett, Greg",2008-01-31T19:00:11Z,2008-01-31T19:00:11Z,2008-01-31,http://hdl.handle.net/1721.1/40095,,WaveScript: A Case-Study in Applying a Distributed Stream-Processing Language,"Applications that combine live data streams with embedded, parallel,and distributed processing are becoming more commonplace. WaveScriptis a domain-specific language that brings high-level, type-safe,garbage-collected programming to these domains. This is made possibleby three primary implementation techniques. First, we employ a novelevaluation strategy that uses a combination of interpretation andreification to partially evaluate programs into stream dataflowgraphs. Second, we use profile-driven compilation to enable manyoptimizations that are normally only available in the synchronous(rather than asynchronous) dataflow domain. Finally, we incorporatean extensible system for rewrite rules to capture algebraic propertiesin specific domains (such as signal processing).We have used our language to build and deploy a sensor-network for theacoustic localization of wild animals, in particular, theYellow-Bellied marmot. We evaluate WaveScript's performance on thisapplication, showing that it yields good performance on both embeddedand desktop-class machines, including distributed execution andsubstantial parallel speedups. Our language allowed us to implementthe application rapidly, while outperforming a previous Cimplementation by over 35%, using fewer than half the lines of code.We evaluate the contribution of our optimizations to this success.",MIT-CSAIL-TR-2008-005; CBCL-270,11 p.,,,,Computation Structures,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,636,"wavescript: a case-study in applying a distributed stream-processing language applications that combine live data streams with embedded, parallel,and distributed processing are becoming more commonplace. wavescriptis a domain-specific language that brings high-level, type-safe,garbage-collected programming to these domains. this is made possibleby three primary implementation techniques. first, we employ a novelevaluation strategy that uses a combination of interpretation andreification to partially evaluate programs into stream dataflowgraphs. second, we use profile-driven compilation to enable manyoptimizations that are normally only available in the synchronous(rather than asynchronous) dataflow domain. finally, we incorporatean extensible system for rewrite rules to capture algebraic propertiesin specific domains (such as signal processing).we have used our language to build and deploy a sensor-network for theacoustic localization of wild animals, in particular, theyellow-bellied marmot. we evaluate wavescript's performance on thisapplication, showing that it yields good performance on both embeddedand desktop-class machines, including distributed execution andsubstantial parallel speedups. our language allowed us to implementthe application rapidly, while outperforming a previous cimplementation by over 35%, using fewer than half the lines of code.we evaluate the contribution of our optimizations to this success.",high performance computing
Howard Shrobe,"Khan, Muhammad Taimoor; Serpanos, Dimitrios; Shrobe, Howard",2015-03-03T21:00:05Z,2015-03-03T21:00:05Z,2015-03-03,http://hdl.handle.net/1721.1/95774,MIT-CSAIL-TR-2015-007,On the Formal Semantics of the Cognitive Middleware AWDRAT,"The purpose of this work is two fold: on one hand we want to formalize the behavior of critical components of the self generating and adapting cognitive middleware AWDRAT such that the formalism not only helps to understand the semantics and technical details of the middleware but also opens an opportunity to extend the middleware to support other complex application domains of cybersecurity; on the other hand, the formalism serves as a prerequisite for our proof of the behavioral correctness of the critical components to ensure the safety of the middleware itself. However, here we focus only on the core and critical component of the middleware, i.e. Execution Monitor which is a part of the module ""Architectural Differencer"" of AWDRAT. The role of the execution monitor is to identify inconsistencies between run-time observations of the target system and predictions of the System Architectural Model. Therefore, to achieve this goal, we first define the formal (denotational) semantics of the observations (run-time events) and predictions (executable specifications as of System Architectural Model); then based on the aforementioned formal semantics, we formalize the behavior of the ""Execution Monitor"" of the middleware.",,60 p.,,,cyber-security; reference-monitor; wrappers; executable specification,Cybersecurity,,,,,,AIRE,,,,,,,2015-03-03T21:00:05Z,,,,,,,,,,,,,,,,2015,987,"on the formal semantics of the cognitive middleware awdrat the purpose of this work is two fold: on one hand we want to formalize the behavior of critical components of the self generating and adapting cognitive middleware awdrat such that the formalism not only helps to understand the semantics and technical details of the middleware but also opens an opportunity to extend the middleware to support other complex application domains of cybersecurity; on the other hand, the formalism serves as a prerequisite for our proof of the behavioral correctness of the critical components to ensure the safety of the middleware itself. however, here we focus only on the core and critical component of the middleware, i.e. execution monitor which is a part of the module ""architectural differencer"" of awdrat. the role of the execution monitor is to identify inconsistencies between run-time observations of the target system and predictions of the system architectural model. therefore, to achieve this goal, we first define the formal (denotational) semantics of the observations (run-time events) and predictions (executable specifications as of system architectural model); then based on the aforementioned formal semantics, we formalize the behavior of the ""execution monitor"" of the middleware.",language models
Dina Katabi,"Abari, Omid; Rahul, Hariharan; Katabi, Dina",2014-04-28T18:30:03Z,2014-04-28T18:30:03Z,2014-04-27,http://hdl.handle.net/1721.1/86298,MIT-CSAIL-TR-2014-010,One Clock to Rule Them All: A Primitive for Distributed Wireless Protocols at the Physical Layer,"Implementing distributed wireless protocols at the physical layer today is challenging because different nodes have different clocks, each of which has slightly different frequencies. This causes the nodes to have frequency offset relative to each other, as a result of which transmitted signals from these nodes do not combine in a predictable manner over time. Past work tackles this challenge and builds distributed PHY layer systems by attempting to address the effects of the frequency offset and compensating for it in the transmitted signals. In this paper, we address this challenge by addressing the root cause - the different clocks with different frequencies on the different nodes. We present AirClock, a new wireless coordination primitive that enables multiple nodes to act as if they are driven by a single clock that they receive wirelessly over the air. AirClock presents a synchronized abstraction to the physical layer, and hence enables direct implementation of diverse kinds of distributed PHY protocols. We illustrate AirClock's versatility by using it to build three different systems: distributed MIMO, distributed rate adaptation for wireless sensors, and pilotless OFDM, and show that they can provide significant performance benefits over today's systems.",,14 p.,,,Clock Synchronization; Wireless; Distributed MIMO; Distributed Rate Adaptation; Frequency Synchronization; Wireless Sensor Networks,Networks & Mobile Systems,,,,,,,,,,,,,2014-04-28T18:30:03Z,,,,,,,,,,,,,,,,2014,960,"one clock to rule them all: a primitive for distributed wireless protocols at the physical layer implementing distributed wireless protocols at the physical layer today is challenging because different nodes have different clocks, each of which has slightly different frequencies. this causes the nodes to have frequency offset relative to each other, as a result of which transmitted signals from these nodes do not combine in a predictable manner over time. past work tackles this challenge and builds distributed phy layer systems by attempting to address the effects of the frequency offset and compensating for it in the transmitted signals. in this paper, we address this challenge by addressing the root cause - the different clocks with different frequencies on the different nodes. we present airclock, a new wireless coordination primitive that enables multiple nodes to act as if they are driven by a single clock that they receive wirelessly over the air. airclock presents a synchronized abstraction to the physical layer, and hence enables direct implementation of diverse kinds of distributed phy protocols. we illustrate airclock's versatility by using it to build three different systems: distributed mimo, distributed rate adaptation for wireless sensors, and pilotless ofdm, and show that they can provide significant performance benefits over today's systems.",language models
Nancy Lynch,"Lynch, Nancy; Pereira, Olivier; Kaynar, Dilsun; Cheung, Ling; Canetti, Ran",2008-11-24T06:00:04Z,2008-11-24T06:00:04Z,2008-11-22,http://hdl.handle.net/1721.1/43711,MIT-CSAIL-TR-2008-068,"Modeling Computational Security in Long-Lived Systems, Version 2","For many cryptographic protocols, security relies on the assumption that adversarial entities have limited computational power. This type of security degrades progressively over the lifetime of a protocol. However, some cryptographic services, such as timestamping services or digital archives, are long-lived in nature; they are expected to be secure and operational for a very long time (i.e., super-polynomial). In such cases, security cannot be guaranteed in the traditional sense: a computationally secure protocol may become insecure if the attacker has a super-polynomial number of interactions with the protocol. This paper proposes a new paradigm for the analysis of long-lived security protocols. We allow entities to be active for a potentially unbounded amount of real time, provided they perform only a polynomial amount of work per unit of real time. Moreover, the space used by these entities is allocated dynamically and must be polynomially bounded. We propose a new notion of long-term implementation, which is an adaptation of computational indistinguishability to the long-lived setting. We show that long-term implementation is preserved under polynomial parallel composition and exponential sequential composition. We illustrate the use of this new paradigm by analyzing some security properties of the long-lived timestamping protocol of Haber and Kamat.",,27 p.,,,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2008,698,"modeling computational security in long-lived systems, version 2 for many cryptographic protocols, security relies on the assumption that adversarial entities have limited computational power. this type of security degrades progressively over the lifetime of a protocol. however, some cryptographic services, such as timestamping services or digital archives, are long-lived in nature; they are expected to be secure and operational for a very long time (i.e., super-polynomial). in such cases, security cannot be guaranteed in the traditional sense: a computationally secure protocol may become insecure if the attacker has a super-polynomial number of interactions with the protocol. this paper proposes a new paradigm for the analysis of long-lived security protocols. we allow entities to be active for a potentially unbounded amount of real time, provided they perform only a polynomial amount of work per unit of real time. moreover, the space used by these entities is allocated dynamically and must be polynomially bounded. we propose a new notion of long-term implementation, which is an adaptation of computational indistinguishability to the long-lived setting. we show that long-term implementation is preserved under polynomial parallel composition and exponential sequential composition. we illustrate the use of this new paradigm by analyzing some security properties of the long-lived timestamping protocol of haber and kamat.",language models
,"Kaminsky, Michael; Peterson, Eric; Fu, Kevin; Mazires, David; Kaashoek, M. Frans",2023-03-29T15:36:55Z,2023-03-29T15:36:55Z,2003-01,https://hdl.handle.net/1721.1/149978,MIT-LCS-TR-884,"REX: Secure, modular remote execution through file descriptor passing","The ubiquitous SSH package has demonstrated the importance of   secure remote login and execution.  This paper presents a new system,   REX, designed to provide remote login and execution in the context of   the SFS secure distributed file system.  REX departs from traditional   remote login design and is built around two main mechanisms---file   descriptor passing and a user agent process.        File descriptor passing allows REX to be split into several   smaller pieces; privileged code can run as its own process to   provide enhanced security guarantees.  REX also emulates secure file   descriptor passing over network connections, allowing users to build   extensions to REX outside of the core REX software.        REX uses and extends SFS's agent mechanism to provide a   transparent distributed computing environment to users.  The   agent stores private keys, server nicknames, and other per-user   configuration state; REX makes the SFS agent available to programs   that it executes on remote machines.        We have an implementation of REX and demonstrate that its   flexibility does not come at the cost of performance.  Initial REX   connections are comparable to those of SSH in speed, while subsequent   connections are much faster because REX exploits the SFS agent to   cache connection state to avoid costly public-key operations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,190,"rex: secure, modular remote execution through file descriptor passing the ubiquitous ssh package has demonstrated the importance of   secure remote login and execution.  this paper presents a new system,   rex, designed to provide remote login and execution in the context of   the sfs secure distributed file system.  rex departs from traditional   remote login design and is built around two main mechanisms---file   descriptor passing and a user agent process.        file descriptor passing allows rex to be split into several   smaller pieces; privileged code can run as its own process to   provide enhanced security guarantees.  rex also emulates secure file   descriptor passing over network connections, allowing users to build   extensions to rex outside of the core rex software.        rex uses and extends sfs's agent mechanism to provide a   transparent distributed computing environment to users.  the   agent stores private keys, server nicknames, and other per-user   configuration state; rex makes the sfs agent available to programs   that it executes on remote machines.        we have an implementation of rex and demonstrate that its   flexibility does not come at the cost of performance.  initial rex   connections are comparable to those of ssh in speed, while subsequent   connections are much faster because rex exploits the sfs agent to   cache connection state to avoid costly public-key operations.",high performance computing
,"Kuncak, Viktor; Rinard, Martin",2005-12-22T02:19:36Z,2005-12-22T02:19:36Z,2004-11-30,http://hdl.handle.net/1721.1/30509,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,On Decision Procedures for Set-Value Fields,"An important feature of object-oriented programming languages is the ability todynamically instantiate user-defined container data structures such as lists, trees,and hash tables. Programs implement such data structures using references todynamically allocated objects, which allows data structures to store unboundednumbers of objects, but makes reasoning about programs more difficult. Reasoningabout object-oriented programs with complex data structures is simplified if datastructure operations are specified in terms of abstract sets of objects associatedwith each data structure. For example, an insertion into a data structure in thisapproach becomes simply an insertion into a dynamically changing set-valued fieldof an object, as opposed to a manipulation of a dynamically linked structure linkedto the object.In this paper we explore reasoning techniques for programs that manipulate datastructures specified using set-valued abstract fields associated with container objects.We compare the expressive power and the complexity of specification languagesbased on 1) decidable prefix vocabulary classes of first-order logic, 2) twovariablelogic with counting, and 3) Nelson-Oppen combinations of multisortedtheories. Such specification logics can be used for verification of object-orientedprograms with supplied invariants. Moreover, by selecting an appropriate subsetof properties expressible in such logic, the decision procedures for these logics yieldautomated computation of lattice operations in abstract interpretation domain, aswell as automated computation of abstract program semantics.",MIT-CSAIL-TR-2004-079; MIT-LCS-TR-975,16 p.; 19825924 bytes; 756829 bytes,application/postscript; application/pdf,en_US,,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,389,"on decision procedures for set-value fields an important feature of object-oriented programming languages is the ability todynamically instantiate user-defined container data structures such as lists, trees,and hash tables. programs implement such data structures using references todynamically allocated objects, which allows data structures to store unboundednumbers of objects, but makes reasoning about programs more difficult. reasoningabout object-oriented programs with complex data structures is simplified if datastructure operations are specified in terms of abstract sets of objects associatedwith each data structure. for example, an insertion into a data structure in thisapproach becomes simply an insertion into a dynamically changing set-valued fieldof an object, as opposed to a manipulation of a dynamically linked structure linkedto the object.in this paper we explore reasoning techniques for programs that manipulate datastructures specified using set-valued abstract fields associated with container objects.we compare the expressive power and the complexity of specification languagesbased on 1) decidable prefix vocabulary classes of first-order logic, 2) twovariablelogic with counting, and 3) nelson-oppen combinations of multisortedtheories. such specification logics can be used for verification of object-orientedprograms with supplied invariants. moreover, by selecting an appropriate subsetof properties expressible in such logic, the decision procedures for these logics yieldautomated computation of lattice operations in abstract interpretation domain, aswell as automated computation of abstract program semantics.",language models
John Leonard,"Benjamin, Michael R.",2017-05-17T16:00:05Z,2017-05-17T16:00:05Z,2017-05-16,http://hdl.handle.net/1721.1/109146,MIT-CSAIL-TR-2017-009,Autonomous COLREGS Modes and Velocity Functions,"This paper concerns an implementation of an autonomy system for unmanned surface vessels operating in accordance with the Coast Guard Collision Regulations (COLREGS). The autonomy system is implemented by associating a dedicated ownship behavior module for each contact for collision avoidance. For each behavior, a mode determination is made based on the COLREGS rules, ownship position and trajectory, and the contact position and trajectory. Based on the mode, an appropriate objective function is generated, over the set of possible ownship maneuvers, to bias the vehicle in accordance with the COLREGS. The focus on this paper is solely on (a) the mode determination algorithms, (b) the requisite ownship and contact terms regarding position, trajectory and relative position utilized in the mode determination algorithms, and (c) the form and equations used in making the objective functions associated with each mode.",,47 p.,,,Autonomy; Marine Autonomy; Unmanned Surface Vehicles; Collision Avoidance; Multi-Objective Optimization; Interval Programming; IvP; MOOS-IvP; pHelmIvP; Collision Regulations; Velocity Functions; Objective Functions; Autonomous Surface Vehicle; Autonomous Marine Vehicle; Rules of the Road,Marine Robotics,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,,,,,,,,,,2017-05-17T16:00:05Z,,,,,,,,,,,,,,,,2017,1043,"autonomous colregs modes and velocity functions this paper concerns an implementation of an autonomy system for unmanned surface vessels operating in accordance with the coast guard collision regulations (colregs). the autonomy system is implemented by associating a dedicated ownship behavior module for each contact for collision avoidance. for each behavior, a mode determination is made based on the colregs rules, ownship position and trajectory, and the contact position and trajectory. based on the mode, an appropriate objective function is generated, over the set of possible ownship maneuvers, to bias the vehicle in accordance with the colregs. the focus on this paper is solely on (a) the mode determination algorithms, (b) the requisite ownship and contact terms regarding position, trajectory and relative position utilized in the mode determination algorithms, and (c) the form and equations used in making the objective functions associated with each mode.",language models
Tomaso Poggio,"Poggio, Tomaso",2012-05-31T20:15:04Z,2012-05-31T20:15:04Z,2012-05-31,http://hdl.handle.net/1721.1/70970,MIT-CSAIL-TR-2012-014; CBCL-308,"The Levels of Understanding framework, revised","I discuss the ""levels of understanding"" framework described in Marr's Vision and propose a revised and updated version of it to capture the changes in computation and neuroscience over the last 30 years.",,9 p.,,,"artificial intelligence, computer vision",Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2012,894,"the levels of understanding framework, revised i discuss the ""levels of understanding"" framework described in marr's vision and propose a revised and updated version of it to capture the changes in computation and neuroscience over the last 30 years.",language models
Barbara Liskov,"Popic, Victoria",2010-09-29T21:00:22Z,2010-09-29T21:00:22Z,2010-09-29,http://hdl.handle.net/1721.1/58772,MIT-CSAIL-TR-2010-048,Audit Trails in the Aeolus Distributed Security Platform,"This thesis provides a complete design and implementation of audit trail collection and storage for Aeolus, a distributed security platform based on information flow control. An information flow control system regulates all activities that concern information security. By recording all the operations monitored by Aeolus, our audit trails capture all actions that can affect system security. In our system, event records are collected on each system node and shipped to a centralized location, where they are stored and processed. To correlate audit trail events of different system nodes we store event dependencies directly in the event records. Each audit trail record keeps links to its immediate predecessors. Therefore, our audit trails form dependency graphs that capture the causal relationship among system events. These graphs can be used to reconstruct the chains of events leading to a given system state. Our results show that audit trail collection imposes a small overhead on system performance.",,86 p.,,,,Programming Methodology,,,,MEng thesis,,,,,,,,,,,,,,,,,,,,,,,,,2010,814,"audit trails in the aeolus distributed security platform this thesis provides a complete design and implementation of audit trail collection and storage for aeolus, a distributed security platform based on information flow control. an information flow control system regulates all activities that concern information security. by recording all the operations monitored by aeolus, our audit trails capture all actions that can affect system security. in our system, event records are collected on each system node and shipped to a centralized location, where they are stored and processed. to correlate audit trail events of different system nodes we store event dependencies directly in the event records. each audit trail record keeps links to its immediate predecessors. therefore, our audit trails form dependency graphs that capture the causal relationship among system events. these graphs can be used to reconstruct the chains of events leading to a given system state. our results show that audit trail collection imposes a small overhead on system performance.",language models
,"Ajmani, Sameer; Liskov, Barbara; Shrira, Liuba; Curtis, Dorothy",2005-12-22T02:37:15Z,2005-12-22T02:37:15Z,2005-10-06,http://hdl.handle.net/1721.1/30572,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Automatic Software Upgrades for Distributed Systems,"Upgrading the software of long-lived, highly-available distributedsystems is difficult.  It is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  Instead, upgrades must happengradually, and there may be long periods of time when different nodesrun different software versions and need to communicate usingincompatible protocols.  We present a methodology and infrastructurethat make it possible to upgrade distributed systems automatically whilelimiting service disruption.  We introduce new ways to reason aboutcorrectness in a multi-version system. We also describe a prototypeimplementation that supports automatic upgrades with modest overhead.",MIT-CSAIL-TR-2005-062; MIT-LCS-TR-1005,14 p.; 26794595 bytes; 1207166 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,462,"automatic software upgrades for distributed systems upgrading the software of long-lived, highly-available distributedsystems is difficult.  it is not possible to upgrade all the nodes in asystem at once, since some nodes may be unavailable and halting thesystem for an upgrade is unacceptable.  instead, upgrades must happengradually, and there may be long periods of time when different nodesrun different software versions and need to communicate usingincompatible protocols.  we present a methodology and infrastructurethat make it possible to upgrade distributed systems automatically whilelimiting service disruption.  we introduce new ways to reason aboutcorrectness in a multi-version system. we also describe a prototypeimplementation that supports automatic upgrades with modest overhead.",language models
,"Monteleoni, Claire; Balakrishnan, Hari; Feamster, Nick; Jaakkola, Tommi",2005-12-22T02:15:02Z,2005-12-22T02:15:02Z,2004-10-27,http://hdl.handle.net/1721.1/30499,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Managing the 802.11 Energy/Performance Tradeoff with Machine Learning,"This paper addresses the problem of managing the tradeoff betweenenergy consumption and performance in wireless devices implementingthe IEEE 802.11 standard. To save energy, the 802.11 specificationproposes a power-saving mode (PSM), where a device can sleep to saveenergy, periodically waking up to receive packets from a neighbor(e.g., an access point) that may have buffered packets for thesleeping device. Previous work has shown that a fixed polling time forwaking up degrades the performance of Web transfers, because networkactivity is bursty and time-varying. We apply a new online machinelearning algorithm to this problem and show, using ns simulation andtrace analysis, that it is able to adapt well to network activity. Thelearning process makes no assumptions about the underlying networkactivity being stationary or even Markov. Our learning power-savingalgorithm, LPSM, guides the learning using a ""loss function"" thatcombines the increased latency from potentially sleeping too long andthe wasted use of energy in waking up too soon.  In our nssimulations, LPSM saved 7%-20% more energy than 802.11 in power-savingmode, with an associated increase in average latency by a factor of1.02, and not more than 1.2.  LPSM is straightforward to implementwithin the 802.11 PSM framework.",MIT-CSAIL-TR-2004-068; MIT-LCS-TR-971,14 p.; 23210224 bytes; 1542849 bytes,application/postscript; application/pdf,en_US,,Networks and Mobile Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,378,"managing the 802.11 energy/performance tradeoff with machine learning this paper addresses the problem of managing the tradeoff betweenenergy consumption and performance in wireless devices implementingthe ieee 802.11 standard. to save energy, the 802.11 specificationproposes a power-saving mode (psm), where a device can sleep to saveenergy, periodically waking up to receive packets from a neighbor(e.g., an access point) that may have buffered packets for thesleeping device. previous work has shown that a fixed polling time forwaking up degrades the performance of web transfers, because networkactivity is bursty and time-varying. we apply a new online machinelearning algorithm to this problem and show, using ns simulation andtrace analysis, that it is able to adapt well to network activity. thelearning process makes no assumptions about the underlying networkactivity being stationary or even markov. our learning power-savingalgorithm, lpsm, guides the learning using a ""loss function"" thatcombines the increased latency from potentially sleeping too long andthe wasted use of energy in waking up too soon.  in our nssimulations, lpsm saved 7%-20% more energy than 802.11 in power-savingmode, with an associated increase in average latency by a factor of1.02, and not more than 1.2.  lpsm is straightforward to implementwithin the 802.11 psm framework.",language models
,"Krashinsky, Ronny",2023-03-29T15:36:49Z,2023-03-29T15:36:49Z,2003-01,https://hdl.handle.net/1721.1/149976,MIT-LCS-TR-882,Efficient Web Browsing for Mobile Clients using HTTP Compression,"Efficient web browsing on mobile computers presents a unique challenge.  These machines are different from other classes of client computers since they have relatively low-bandwidth connections and they are battery-powered and therefore limited by their energy consumption.  However, they tend to interact with the same servers for the delivery of web content.  This project investigates optimizing the final critical link between a mobile client and a stationary base station by compressing HTTP request and response messages.  Using a split proxy design, compression of individual request messages reduces bandwidth by 26% to 34% across a variety of benchmark traces, and applying compression to response messages yields savings of 59% to 82% of the compressible data.  Higher compression rates are achieved by using streaming compression algorithms to compress the streams of request and response messages.  In this case, the bandwidth for requests sees an order of magnitude improvement, and the response stream obtains additional savings of 7% to 25% on top of the savings achieved with per-response compression.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,191,"efficient web browsing for mobile clients using http compression efficient web browsing on mobile computers presents a unique challenge.  these machines are different from other classes of client computers since they have relatively low-bandwidth connections and they are battery-powered and therefore limited by their energy consumption.  however, they tend to interact with the same servers for the delivery of web content.  this project investigates optimizing the final critical link between a mobile client and a stationary base station by compressing http request and response messages.  using a split proxy design, compression of individual request messages reduces bandwidth by 26% to 34% across a variety of benchmark traces, and applying compression to response messages yields savings of 59% to 82% of the compressible data.  higher compression rates are achieved by using streaming compression algorithms to compress the streams of request and response messages.  in this case, the bandwidth for requests sees an order of magnitude improvement, and the response stream obtains additional savings of 7% to 25% on top of the savings achieved with per-response compression.",language models
Erik Demaine,"Hajiaghayi, MohammadTaghi; Kortsarz, Guy; Salavatipour, Mohammad R.",2006-01-05T20:37:59Z,2006-01-05T20:37:59Z,2005-11-15,http://hdl.handle.net/1721.1/30601,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Approximating Buy-at-Bulk k-Steiner trees,"In the buy-at-bulk $k$-Steiner tree (or rent-or-buy$k$-Steiner tree) problem we are given a graph $G(V,E)$ with a setof terminals $T\subseteq V$ including a particular vertex $s$ calledthe root, and an integer $k\leq |T|$. There are two cost functionson the edges of $G$, a buy cost $b:E\longrightarrow \RR^+$ and a rentcost $r:E\longrightarrow \RR^+$. The goal is to find a subtree $H$ of$G$ rooted at $s$ with at least $k$ terminals so that the cost$\sum_{e\in H} b(e)+\sum_{t\in T-s} dist(t,s)$ is minimize, where$dist(t,s)$ is the distance from $t$ to $s$ in $H$ with respect tothe $r$ cost. Our main result is  an $O(\log^5 n)$-approximation forthe buy-at-bulk $k$-Steiner tree problem.To achieve this we also design an approximation algorithm forbicriteria $k$-Steiner tree. In the bicriteria $k$-Steiner tree problem weare given a graph $G$ with edge costs $b(e)$ and distance costs$r(e)$ over the edges, and an integer $k$. Our goal is to find aminimum cost (under $b$-cost) $k$-Steiner tree such that thediameter under $r$-cost is at most some given bound $D$. An$(\alpha,\beta)$-approximation finds a subgraph of diameter at most$\alpha\cdot {D}$ (with respect to $r$) and cost with respect to$b$ of at most $\beta\cdot opt$ where $opt$ is the minimum cost ofany solution with diameter at most $D$. Marathe et al \cite{ravi}gave an $(O(\log n),O(\log n))$-approximation algorithm for thebicriteria Steiner tree problem. Their algorithm does not extend tothe bicriteria $k$-Steiner tree problem.Our algorithm for the buy-at-bulk $k$-Steiner tree problem relies on an$(O(\log^2 n),O(\log^4 n))$-approximation algorithm we develop for the(shallow-light) bicriteria  $k$-Steiner tree problem, which is ofindependent interest. Indeed, this is also one of the main tools we use to obtainthe first polylogarithmic approximation algorithm for non-uniformmulticommodity buy-at-bulk~\cite{HKS}.",MIT-CSAIL-TR-2006-001,14 p.; 18505768 bytes; 766175 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,474,"approximating buy-at-bulk k-steiner trees in the buy-at-bulk $k$-steiner tree (or rent-or-buy$k$-steiner tree) problem we are given a graph $g(v,e)$ with a setof terminals $t\subseteq v$ including a particular vertex $s$ calledthe root, and an integer $k\leq |t|$. there are two cost functionson the edges of $g$, a buy cost $b:e\longrightarrow \rr^+$ and a rentcost $r:e\longrightarrow \rr^+$. the goal is to find a subtree $h$ of$g$ rooted at $s$ with at least $k$ terminals so that the cost$\sum_{e\in h} b(e)+\sum_{t\in t-s} dist(t,s)$ is minimize, where$dist(t,s)$ is the distance from $t$ to $s$ in $h$ with respect tothe $r$ cost. our main result is  an $o(\log^5 n)$-approximation forthe buy-at-bulk $k$-steiner tree problem.to achieve this we also design an approximation algorithm forbicriteria $k$-steiner tree. in the bicriteria $k$-steiner tree problem weare given a graph $g$ with edge costs $b(e)$ and distance costs$r(e)$ over the edges, and an integer $k$. our goal is to find aminimum cost (under $b$-cost) $k$-steiner tree such that thediameter under $r$-cost is at most some given bound $d$. an$(\alpha,\beta)$-approximation finds a subgraph of diameter at most$\alpha\cdot {d}$ (with respect to $r$) and cost with respect to$b$ of at most $\beta\cdot opt$ where $opt$ is the minimum cost ofany solution with diameter at most $d$. marathe et al \cite{ravi}gave an $(o(\log n),o(\log n))$-approximation algorithm for thebicriteria steiner tree problem. their algorithm does not extend tothe bicriteria $k$-steiner tree problem.our algorithm for the buy-at-bulk $k$-steiner tree problem relies on an$(o(\log^2 n),o(\log^4 n))$-approximation algorithm we develop for the(shallow-light) bicriteria  $k$-steiner tree problem, which is ofindependent interest. indeed, this is also one of the main tools we use to obtainthe first polylogarithmic approximation algorithm for non-uniformmulticommodity buy-at-bulk~\cite{hks}.",high performance computing
Karen Sollins,"Simosa, Jorge D.",2013-06-04T20:15:04Z,2013-06-04T20:15:04Z,2013-06-04,http://hdl.handle.net/1721.1/79060,MIT-CSAIL-TR-2013-011,A Publish-Subscribe Implementation of Network Management,"As modern networks become highly integrated, heterogeneous, and experience exponential growth, the task of network management becomes increasingly unmanageable for network administrators and designers. The Knowledge Plane (KP) is designed to support a self-managing network, given the organizational constraints of network management, as well as to create synergy and exploit commonality among network applications. In this thesis, to build an Information Plane that is suitable to the requirements of the KP, we propose a publish/subscribe system that provides a clear and systematic framework for resolving tussles in the network. To evaluate the effectiveness of this design, we configured a network of PlanetLab nodes and conducted experiments involving a variety of file sizes and source-destination pairs. The results suggest that the system's performance is not only comparable to existing file transfer services, but that the system also introduces several performance gains that are unattainable with current network architectures.",,77 p.,,,,Advanced Network Architecture,,Attribution 3.0 Unported,http://creativecommons.org/licenses/by/3.0/,MEng thesis,,,,,,,,,2013-06-04T20:15:04Z,,,,,,,,,,,,,,,,2013,926,"a publish-subscribe implementation of network management as modern networks become highly integrated, heterogeneous, and experience exponential growth, the task of network management becomes increasingly unmanageable for network administrators and designers. the knowledge plane (kp) is designed to support a self-managing network, given the organizational constraints of network management, as well as to create synergy and exploit commonality among network applications. in this thesis, to build an information plane that is suitable to the requirements of the kp, we propose a publish/subscribe system that provides a clear and systematic framework for resolving tussles in the network. to evaluate the effectiveness of this design, we configured a network of planetlab nodes and conducted experiments involving a variety of file sizes and source-destination pairs. the results suggest that the system's performance is not only comparable to existing file transfer services, but that the system also introduces several performance gains that are unattainable with current network architectures.",language models
,"Rodrigues, Rodrigo; Liskov, Barbara",2005-12-22T01:40:26Z,2005-12-22T01:40:26Z,2004-08-13,http://hdl.handle.net/1721.1/30494,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Byzantine Fault Tolerance in Long-Lived Systems,"This paper proposes counter-measures that can be deployedas part of a replicated system to reduce the size ofW, and thus reduce the class of attacks to which the system is vulnerable. Obviously it will not be possible to withstandall attacks via this technique, in particular attacks with verysmall A. But we will propose techniques that can reduceWto quite a small value.In the remainder of this paper, we discuss how to lowerthe value of W. We begin by discussing attacks. Then wediscuss some prior work in this area and why it is insufficient.The final section describes the approach we propose.",MIT-CSAIL-TR-2004-055; MIT-LCS-TR-962,3 p.; 6194127 bytes; 262522 bytes,application/postscript; application/pdf,en_US,,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2004,364,"byzantine fault tolerance in long-lived systems this paper proposes counter-measures that can be deployedas part of a replicated system to reduce the size ofw, and thus reduce the class of attacks to which the system is vulnerable. obviously it will not be possible to withstandall attacks via this technique, in particular attacks with verysmall a. but we will propose techniques that can reducewto quite a small value.in the remainder of this paper, we discuss how to lowerthe value of w. we begin by discussing attacks. then wediscuss some prior work in this area and why it is insufficient.the final section describes the approach we propose.",language models
Silvio Micali,"Micali, Silvio; Chen, Jing",2009-11-10T18:15:12Z,2009-11-10T18:15:12Z,2009-11-10,http://hdl.handle.net/1721.1/49810,MIT-CSAIL-TR-2009-057,Rational Robustness for Mechanism Design,"The currently prevailing equilibrium-based approach to mechanism design suffers from a plurality of fundamental problems, and new conceptual frameworks are needed to solve or sufficiently alleviate them. In this paper, we put forward rational robustness, a new solution concept/implementation notion that is not equilibrium-based; prove its fundamental structural theorems; and compare it with prior notions. Our notion of implementation is specifically built so as to be robust against the problem of equilibrium selection. We prove it robust against other fundamental problems as well in different papers.",,22 p.,,,"Mechanism Design, Implementation, Rational Robustness, Distinguishably Dominated Strategies",,Work partially supported by ONR Contract Number N00014-09-1-0597.,,,first draft,,,,,,,,,,,,,,,,Theory of Computation,,,,,,,,,2009,763,"rational robustness for mechanism design the currently prevailing equilibrium-based approach to mechanism design suffers from a plurality of fundamental problems, and new conceptual frameworks are needed to solve or sufficiently alleviate them. in this paper, we put forward rational robustness, a new solution concept/implementation notion that is not equilibrium-based; prove its fundamental structural theorems; and compare it with prior notions. our notion of implementation is specifically built so as to be robust against the problem of equilibrium selection. we prove it robust against other fundamental problems as well in different papers.",image classification
Martin Rinard,"Long, Fan; Qi, Zichao; Achour, Sara; Rinard, Martin",2015-02-12T21:00:03Z,2015-02-12T21:00:03Z,2015-02-12,http://hdl.handle.net/1721.1/94520,MIT-CSAIL-TR-2015-004,Automatic Program Repair with Condition Synthesis and Compound Mutations,"We present PCR, a new automatic patch generation system. PCR uses a new condition synthesis technique to efficiently discover logical expressions that generate desired control- flow transfer patterns. Presented with a set of test cases, PCR deploys condition synthesis to find and repair incorrect if conditions that cause the application to produce the wrong result for one or more of the test cases. PCR also leverages condition synthesis to obtain a set of compound modifications that generate a rich, productive, and tractable search space of candidate patches. We evaluate PCR on a set of 105 defects from the GenProg benchmark set. For 40 of these defects, PCR generates plausible patches (patches that generate correct outputs for all inputs in the test suite used to validate the patch). For 12 of these defects, PCR generates correct patches that are functionally equivalent to developer patches that appear in subsequent versions. For comparison purposes, GenProg generates plausible patches for only 18 defects and correct patches for only 2 defects. AE generates plausible patches for only 27 defects and correct patches for only 3 defects.",,14 p.,,,,Computer Architecture,,,,,,,,,,,,,2015-02-12T21:00:03Z,,,,,,,,,,,,,,,,2015,984,"automatic program repair with condition synthesis and compound mutations we present pcr, a new automatic patch generation system. pcr uses a new condition synthesis technique to efficiently discover logical expressions that generate desired control- flow transfer patterns. presented with a set of test cases, pcr deploys condition synthesis to find and repair incorrect if conditions that cause the application to produce the wrong result for one or more of the test cases. pcr also leverages condition synthesis to obtain a set of compound modifications that generate a rich, productive, and tractable search space of candidate patches. we evaluate pcr on a set of 105 defects from the genprog benchmark set. for 40 of these defects, pcr generates plausible patches (patches that generate correct outputs for all inputs in the test suite used to validate the patch). for 12 of these defects, pcr generates correct patches that are functionally equivalent to developer patches that appear in subsequent versions. for comparison purposes, genprog generates plausible patches for only 18 defects and correct patches for only 2 defects. ae generates plausible patches for only 27 defects and correct patches for only 3 defects.",high performance computing
Barbara Liskov,"Vandiver, Benjamin Mead",2008-07-02T06:00:10Z,2008-07-02T06:00:10Z,2008-06-30,http://hdl.handle.net/1721.1/41873,,Detecting and Tolerating Byzantine Faults in Database Systems,"This thesis describes the design, implementation, and evaluation of a replication scheme to handle Byzantine faults in transaction processing database systems. The scheme compares answers from queries and updates on multiple replicas which are off-the-shelf database systems, to provide a single database that is Byzantine fault tolerant. The scheme works when the replicas are homogeneous, but it also allows heterogeneous replication in which replicas come from different vendors. Heterogeneous replicas reduce the impact of bugs and security compromises because they are implemented independently and are thus less likely to suffer correlated failures. A final component of the scheme is a repair mechanism that can correct the state of a faulty replica, ensuring the longevity of the scheme.The main challenge in designing a replication scheme for transaction processingsystems is ensuring that the replicas state does not diverge while allowing a high degree of concurrency. We have developed two novel concurrency control protocols, commit barrier scheduling (CBS) and snapshot epoch scheduling (SES) that provide strong consistency and good performance. The two protocols provide different types of consistency: CBS provides single-copy serializability and SES provides single-copy snapshot isolation. We have implemented both protocols in the context of a replicated SQL database. Our implementation has been tested with production versions of several commercial and open source databases as replicas. Our experiments show a configuration that can tolerate one faulty replica has only a modest performance overhead (about 10-20% for the TPC-C benchmark). Our implementation successfully masks several Byzantine faults observed in practice and we have used it to find a new bug in MySQL.",MIT-CSAIL-TR-2008-040,174 p.,,,,Programming Methodology,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,670,"detecting and tolerating byzantine faults in database systems this thesis describes the design, implementation, and evaluation of a replication scheme to handle byzantine faults in transaction processing database systems. the scheme compares answers from queries and updates on multiple replicas which are off-the-shelf database systems, to provide a single database that is byzantine fault tolerant. the scheme works when the replicas are homogeneous, but it also allows heterogeneous replication in which replicas come from different vendors. heterogeneous replicas reduce the impact of bugs and security compromises because they are implemented independently and are thus less likely to suffer correlated failures. a final component of the scheme is a repair mechanism that can correct the state of a faulty replica, ensuring the longevity of the scheme.the main challenge in designing a replication scheme for transaction processingsystems is ensuring that the replicas state does not diverge while allowing a high degree of concurrency. we have developed two novel concurrency control protocols, commit barrier scheduling (cbs) and snapshot epoch scheduling (ses) that provide strong consistency and good performance. the two protocols provide different types of consistency: cbs provides single-copy serializability and ses provides single-copy snapshot isolation. we have implemented both protocols in the context of a replicated sql database. our implementation has been tested with production versions of several commercial and open source databases as replicas. our experiments show a configuration that can tolerate one faulty replica has only a modest performance overhead (about 10-20% for the tpc-c benchmark). our implementation successfully masks several byzantine faults observed in practice and we have used it to find a new bug in mysql.",face detection
Brian Williams,"Effinger, Robert",2018-01-30T23:46:07Z,2018-01-30T23:46:07Z,2006-08-25,http://hdl.handle.net/1721.1/113365,MIT-CSAIL-TR-2018-005,Optimal Temporal Planning at Reactive Time Scales via Dynamic Backtracking Branch and Bound,"Autonomous robots are being considered for increasingly capable roles in our society, such as urban search and rescue, automation for assisted living, and lunar habitat construction. To fulfill these roles, teams of autonomous robots will need to cooperate together to accomplish complex mission objectives in uncertain and dynamic environments. In these environments, autonomous robots face a host of new challenges, such as responding robustly to timing uncertainties and perturbations, task and coordination failures, and equipment malfunctions. In order to address these challenges, this thesis advocates a novel planning approach, called temporally-flexible contingent planning. A temporally-flexible contingent plan is a compact encoding of methods for achieving the mission objectives which incorporates robustness through flexible task durations, redundant methods, constraints on when methods are applicable, and preferences between methods. This approach enables robots to adapt to unexpected changes on-the-fly by selecting alternative methods at runtime in order to satisfy as best possible the mission objectives. The drawback to this approach, however, is the computational overhead involved in selecting alternative methods at runtime in response to changes. If a robot takes too long to select a new plan, it could fail to achieve its near-term mission objectives and potentially incur damage. To alleviate this problem, and extend the range of applicability of temporally-flexible contingent planning to more demanding real-time systems, this thesis proposes a temporally-flexible contingent plan executive that selects new methods quickly and optimally in response to changes in a robot's health and environment. We enable fast and optimal method selection through two complimentary approaches. First, we frame optimal method selection as a constraint satisfaction problem (CSP) variant, called an Optimal Conditional CSP (OCCSP). Second, we extend fast CSP search algorithms, such as Dynamic Backtracking and Branch-and-Bound Search, to solve OCCSPs. Experiments on an autonomous rover test-bed and on randomly generated plans show that these contributions significantly improve the speed at which robots perform optimal method selection in response to changes in their health status and environment.",,115 p.,,,,Model-based Embedded and Robotic Systems,,Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International,http://creativecommons.org/licenses/by-nc-nd/4.0/,SM thesis,,,,,,,,,2018-01-30T23:46:07Z,,,,,,,,,,,,,,,,2006,536,"optimal temporal planning at reactive time scales via dynamic backtracking branch and bound autonomous robots are being considered for increasingly capable roles in our society, such as urban search and rescue, automation for assisted living, and lunar habitat construction. to fulfill these roles, teams of autonomous robots will need to cooperate together to accomplish complex mission objectives in uncertain and dynamic environments. in these environments, autonomous robots face a host of new challenges, such as responding robustly to timing uncertainties and perturbations, task and coordination failures, and equipment malfunctions. in order to address these challenges, this thesis advocates a novel planning approach, called temporally-flexible contingent planning. a temporally-flexible contingent plan is a compact encoding of methods for achieving the mission objectives which incorporates robustness through flexible task durations, redundant methods, constraints on when methods are applicable, and preferences between methods. this approach enables robots to adapt to unexpected changes on-the-fly by selecting alternative methods at runtime in order to satisfy as best possible the mission objectives. the drawback to this approach, however, is the computational overhead involved in selecting alternative methods at runtime in response to changes. if a robot takes too long to select a new plan, it could fail to achieve its near-term mission objectives and potentially incur damage. to alleviate this problem, and extend the range of applicability of temporally-flexible contingent planning to more demanding real-time systems, this thesis proposes a temporally-flexible contingent plan executive that selects new methods quickly and optimally in response to changes in a robot's health and environment. we enable fast and optimal method selection through two complimentary approaches. first, we frame optimal method selection as a constraint satisfaction problem (csp) variant, called an optimal conditional csp (occsp). second, we extend fast csp search algorithms, such as dynamic backtracking and branch-and-bound search, to solve occsps. experiments on an autonomous rover test-bed and on randomly generated plans show that these contributions significantly improve the speed at which robots perform optimal method selection in response to changes in their health status and environment.",high performance computing
Silvio Micali,"Azar, Pablo; Micali, Silvio",2012-05-09T22:45:07Z,2012-05-09T22:45:07Z,2012-05-08,http://hdl.handle.net/1721.1/70556,MIT-CSAIL-TR-2012-011,Optimal Parametric Auctions,"We study the problem of profit maximization in auctions of one good where the buyers' valuations are drawn from independent distributions. When these distributions are known to the seller, Myerson's optimal auction is a well-known mechanism for maximizing revenue. In many cases, however, the seller may not know the buyers' distributions. We propose an alternative model where the seller only knows the mean and the variance of each distribution. We call parametric an auction whose mechanism only uses these parameters. We construct parametric auctions both when the seller only has one copy of the good to sell, and when she has an infinite number of identical copies (i.e., when the good is digital). For a very large class of distributions, including (but not limited to) distributions with a monotone hazard rate, our auctions achieve a constant fraction of the revenue of Myerson's auction. When the seller has absolutely no knowledge about the distributions, it is well known that no auction can achieve a constant fraction of the optimal revenue when the players are not identically distributed. Our parametric model gives the seller a small amount of extra information, allowing her to construct auctions for which (1) no two bidders need to be drawn from identical distributions and (2) the revenue obtained is a constant fraction of the revenue in Myerson's optimal auction.",,18 p.,,,,,,,,,,,,,,,,,,,,,,,,Theory of Computation,,,,,,,,,2012,891,"optimal parametric auctions we study the problem of profit maximization in auctions of one good where the buyers' valuations are drawn from independent distributions. when these distributions are known to the seller, myerson's optimal auction is a well-known mechanism for maximizing revenue. in many cases, however, the seller may not know the buyers' distributions. we propose an alternative model where the seller only knows the mean and the variance of each distribution. we call parametric an auction whose mechanism only uses these parameters. we construct parametric auctions both when the seller only has one copy of the good to sell, and when she has an infinite number of identical copies (i.e., when the good is digital). for a very large class of distributions, including (but not limited to) distributions with a monotone hazard rate, our auctions achieve a constant fraction of the revenue of myerson's auction. when the seller has absolutely no knowledge about the distributions, it is well known that no auction can achieve a constant fraction of the optimal revenue when the players are not identically distributed. our parametric model gives the seller a small amount of extra information, allowing her to construct auctions for which (1) no two bidders need to be drawn from identical distributions and (2) the revenue obtained is a constant fraction of the revenue in myerson's optimal auction.",language models
Gerald Sussman,"Beal, Jacob",2006-06-01T16:22:22Z,2006-06-01T16:22:22Z,2004-09,http://hdl.handle.net/1721.1/32985,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Programming an Amorphous Computational Medium,"Amorphous computing considers the problem of controllingmillions of spatially distributed unreliable devices which communicateonly with nearby neighbors. To program such a system, we need a highleveldescription language for desired global behaviors, and a system tocompile such descriptions into locally executing code which robustly createsand maintains the desired global behavior. I survey existing amorphouscomputing primitives and give desiderata for a language describingcomputation on an amorphous computer. I then bring these together inAmorphous Medium Language, which computes on an amorphous computeras though it were a space-filling computational medium.",MIT-CSAIL-TR-2006-039,16 p.; 4880361 bytes; 386051 bytes,application/postscript; application/pdf,en_US,distributed computing sensor networks,Mathematics and Computation,,,,,"J.-P. Banatre et al. (Eds.): UPP 2004, LNCS 3566, pp. 121136, 2005.Springer-Verlag Berlin Heidelberg 2005",,,,,,,,,,,,,,,,,,,,,,,,2004,365,"programming an amorphous computational medium amorphous computing considers the problem of controllingmillions of spatially distributed unreliable devices which communicateonly with nearby neighbors. to program such a system, we need a highleveldescription language for desired global behaviors, and a system tocompile such descriptions into locally executing code which robustly createsand maintains the desired global behavior. i survey existing amorphouscomputing primitives and give desiderata for a language describingcomputation on an amorphous computer. i then bring these together inamorphous medium language, which computes on an amorphous computeras though it were a space-filling computational medium.",language models
,"Wang, Karen",2023-03-29T15:34:42Z,2023-03-29T15:34:42Z,2001-12,https://hdl.handle.net/1721.1/149932,MIT-LCS-TR-829,2RegionRED: a Congestion Control Mechanism for the High Speed Internet,This thesis proposes a new Active Queue Management (AQM) scheme called 2RegionRED.  It is superior to the classic Random Early Detection (RED) algorithm in that there is an intuitive way to set its parameters and it is self-tuning.  Its design is motivated by an original principle to sustain the smallest queue possible while still allowing for maximum link utilization.  2RegionRED uses the number of competing TCPs as its measure of load.  However it does not keep an explicit count.  The result is a novel algorithm that adjusts the drop rate according to two regions of operation: that requiring less than and greater than one drop per round-trip time (RTT).  This thesis also analyzes methods for measuring the persistent queue and proposes the ABSMIN method.  Simulations of 2RegionRED using ABSMIN reveal some difficulties and insights.  Basic comparisons to the Adaptive RED and Flow Proportional Queuing (FPQ) adaptive algorithms are also demonstrated through simulation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2001,96,2regionred: a congestion control mechanism for the high speed internet this thesis proposes a new active queue management (aqm) scheme called 2regionred.  it is superior to the classic random early detection (red) algorithm in that there is an intuitive way to set its parameters and it is self-tuning.  its design is motivated by an original principle to sustain the smallest queue possible while still allowing for maximum link utilization.  2regionred uses the number of competing tcps as its measure of load.  however it does not keep an explicit count.  the result is a novel algorithm that adjusts the drop rate according to two regions of operation: that requiring less than and greater than one drop per round-trip time (rtt).  this thesis also analyzes methods for measuring the persistent queue and proposes the absmin method.  simulations of 2regionred using absmin reveal some difficulties and insights.  basic comparisons to the adaptive red and flow proportional queuing (fpq) adaptive algorithms are also demonstrated through simulation,high performance computing
Trevor Darrell,"Urtasun, Raquel; Darrell, Trevor",2007-03-29T11:21:46Z,2007-03-29T11:21:46Z,2007-03-28,http://hdl.handle.net/1721.1/36901,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Discriminative Gaussian Process Latent Variable Model for Classification,"Supervised learning is difficult with high dimensional input spacesand very small training sets, but accurate classification may bepossible if the data lie on a low-dimensional manifold.  GaussianProcess Latent Variable Models can discover low dimensional manifoldsgiven only a small number of examples, but learn a latent spacewithout regard for class labels.  Existing methods for discriminativemanifold learning (e.g., LDA, GDA) do constrain the class distributionin the latent space, but are generally deterministic and may notgeneralize well with limited training data.  We introduce a method forGaussian Process Classification using latent variable models trainedwith discriminative priors over the latent space, which can learn adiscriminative latent space from a small training set.",MIT-CSAIL-TR-2007-021,8 p.,,,Gaussian Processes; Classification; Latent Variable Models; Machine Learning,Vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,588,"discriminative gaussian process latent variable model for classification supervised learning is difficult with high dimensional input spacesand very small training sets, but accurate classification may bepossible if the data lie on a low-dimensional manifold.  gaussianprocess latent variable models can discover low dimensional manifoldsgiven only a small number of examples, but learn a latent spacewithout regard for class labels.  existing methods for discriminativemanifold learning (e.g., lda, gda) do constrain the class distributionin the latent space, but are generally deterministic and may notgeneralize well with limited training data.  we introduce a method forgaussian process classification using latent variable models trainedwith discriminative priors over the latent space, which can learn adiscriminative latent space from a small training set.",high performance computing
Martin Rinard,"Kuncak, Viktor",2007-01-02T20:21:50Z,2007-01-02T20:21:50Z,2007-01-01,http://hdl.handle.net/1721.1/35258,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Quantifier-Free Boolean Algebra with Presburger Arithmetic is NP-Complete,"Boolean Algebra with Presburger Arithmetic (BAPA) combines1) Boolean algebras of sets of uninterpreted elements (BA)and 2) Presburger arithmetic operations (PA).  BAPA canexpress the relationship between integer variables andcardinalities of unbounded finite sets and can be used toexpress verification conditions in verification of datastructure consistency properties.In this report I consider the Quantifier-Free fragment ofBoolean Algebra with Presburger Arithmetic (QFBAPA).Previous algorithms for QFBAPA had non-deterministicexponential time complexity.  In this report I show thatQFBAPA is in NP, and is therefore NP-complete.  My resultyields an algorithm for checking satisfiability of QFBAPAformulas by converting them to polynomially sized formulasof quantifier-free Presburger arithmetic.  I expect thisalgorithm to substantially extend the range of QFBAPAproblems whose satisfiability can be checked in practice.",MIT-CSAIL-TR-2007-001,14 p.; 315999 bytes; 842090 bytes,application/pdf; application/postscript,en_US,Caratheodory theorem; integer linear programming; integer cone; Hilbert basis,Computer Architecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2007,566,"quantifier-free boolean algebra with presburger arithmetic is np-complete boolean algebra with presburger arithmetic (bapa) combines1) boolean algebras of sets of uninterpreted elements (ba)and 2) presburger arithmetic operations (pa).  bapa canexpress the relationship between integer variables andcardinalities of unbounded finite sets and can be used toexpress verification conditions in verification of datastructure consistency properties.in this report i consider the quantifier-free fragment ofboolean algebra with presburger arithmetic (qfbapa).previous algorithms for qfbapa had non-deterministicexponential time complexity.  in this report i show thatqfbapa is in np, and is therefore np-complete.  my resultyields an algorithm for checking satisfiability of qfbapaformulas by converting them to polynomially sized formulasof quantifier-free presburger arithmetic.  i expect thisalgorithm to substantially extend the range of qfbapaproblems whose satisfiability can be checked in practice.",high performance computing
Dina Katabi,"Adib, Fadel; Kabelac, Zachary; Katabi, Dina",2014-04-28T18:30:06Z,2014-04-28T18:30:06Z,2014-04-26,http://hdl.handle.net/1721.1/86299,MIT-CSAIL-TR-2014-008,Multi-Person Motion Tracking via RF Body Reflections,"Recently, we have witnessed the emergence of technologies that can localize a user and track her gestures based purely on radio reflections off the person's body. These technologies work even if the user is behind a wall or obstruction. However, for these technologies to be fully practical, they need to address major challenges such as scaling to multiple people, accurately localizing them and tracking their gestures, and localizing static users as opposed to requiring the user to move to be detectable. This paper presents WiZ, the first multi-person centimeter-scale motion tracking system that pinpoints people's locations based purely on RF reflections off their bodies. WiZ can also locate static users by sensing minute changes in their RF reflections due to breathing. Further, it can track concurrent gestures made by different individuals, even when they carry no wireless device on them. We implement a prototype of WiZ and show that it can localize up to five users each with a median accuracy of 8-18 cm and 7-11 cm in the x and y dimensions respectively. WiZ can also detect 3D pointing gestures of multiple users with a median orientation error of 8 -16 degrees for each of them. Finally, WiZ can track breathing motion and output the breath count of multiple people with high accuracy.",,14 p.,,,,Networks & Mobile Systems,,,,,,,,,,,,,2014-04-28T18:30:06Z,,,,,,,,,,,,,,,,2014,959,"multi-person motion tracking via rf body reflections recently, we have witnessed the emergence of technologies that can localize a user and track her gestures based purely on radio reflections off the person's body. these technologies work even if the user is behind a wall or obstruction. however, for these technologies to be fully practical, they need to address major challenges such as scaling to multiple people, accurately localizing them and tracking their gestures, and localizing static users as opposed to requiring the user to move to be detectable. this paper presents wiz, the first multi-person centimeter-scale motion tracking system that pinpoints people's locations based purely on rf reflections off their bodies. wiz can also locate static users by sensing minute changes in their rf reflections due to breathing. further, it can track concurrent gestures made by different individuals, even when they carry no wireless device on them. we implement a prototype of wiz and show that it can localize up to five users each with a median accuracy of 8-18 cm and 7-11 cm in the x and y dimensions respectively. wiz can also detect 3d pointing gestures of multiple users with a median orientation error of 8 -16 degrees for each of them. finally, wiz can track breathing motion and output the breath count of multiple people with high accuracy.",image classification
Trevor Darrell,"Quattoni, Ariadna; Carreras, Xavier; Collins, Michael; Darrell, Trevor",2008-07-24T20:00:14Z,2008-07-24T20:00:14Z,2008-07-23,http://hdl.handle.net/1721.1/41888,,A Projected Subgradient Method for Scalable Multi-Task Learning,"Recent approaches to multi-task learning have investigated the use of a variety of matrix norm regularization schemes for promoting feature sharing across tasks.In essence, these approaches aim at extending the l1 framework for sparse single task approximation to the multi-task setting. In this paper we focus on the computational complexity of training a jointly regularized model and propose an optimization algorithm whose complexity is linear with the number of training examples and O(n log n) with n being the number of parameters of the joint model. Our algorithm is based on setting jointly regularized loss minimization as a convex constrained optimization problem for which we develop an efficient projected gradient algorithm. The main contribution of this paper is the derivation of a gradient projection method with l1 constraints that can be performed efficiently and which has convergence rates.",MIT-CSAIL-TR-2008-045,8 p.,,,,Vision,,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2008,673,"a projected subgradient method for scalable multi-task learning recent approaches to multi-task learning have investigated the use of a variety of matrix norm regularization schemes for promoting feature sharing across tasks.in essence, these approaches aim at extending the l1 framework for sparse single task approximation to the multi-task setting. in this paper we focus on the computational complexity of training a jointly regularized model and propose an optimization algorithm whose complexity is linear with the number of training examples and o(n log n) with n being the number of parameters of the joint model. our algorithm is based on setting jointly regularized loss minimization as a convex constrained optimization problem for which we develop an efficient projected gradient algorithm. the main contribution of this paper is the derivation of a gradient projection method with l1 constraints that can be performed efficiently and which has convergence rates.",high performance computing
Tomaso Poggio,"Masquelier, Timothee; Serre, Thomas; Thorpe, Simon; Poggio, Tomaso",2007-12-27T13:45:13Z,2007-12-27T13:45:13Z,2007-12-26,http://hdl.handle.net/1721.1/39833,,Learning complex cell invariance from natural videos: A plausibility proof,"One of the most striking feature of the cortex is its ability to wire itself. Understanding how the visual cortex wires up through development and how visual experience refines connections into adulthood is a key question for Neuroscience. While computational models of the visual cortex are becoming increasingly detailed, the question of how such architecture could self-organize through visual experience is often overlooked. Here we focus on the class of hierarchical feedforward models of the ventral stream of the visual cortex, which extend the classical simple-to-complex cells model by Hubel and Wiesel (1962) to extra-striate areas, and have been shown to account for a host of experimental data. Such models assume two functional classes of simple and complex cells with specific predictions about their respective wiring and resulting functionalities.In these networks, the issue of learning, especially for complex cells, is perhaps the least well understood. In fact, in most of these models, the connectivity between simple and complex cells is not learned butrather hard-wired. Several algorithms have been proposed for learning invariances at the complex cell level based on a trace rule to exploit the temporal continuity of sequences of natural images, but very few can learn from natural cluttered image sequences.Here we propose a new variant of the trace rule that only reinforces the synapses between the most active cells, and therefore can handle cluttered environments. The algorithm has so far been developed and tested at the level of V1-like simple and complex cells: we verified that Gabor-like simple cell selectivity could emerge from competitive Hebbian learning. In addition, we show how the modified trace rule allows the subsequent complex cells to learn to selectively pool over simple cells with the same preferred orientation but slightly different positions thus increasing their tolerance to the precise position of the stimulus within their receptive fields.",MIT-CSAIL-TR-2007-060; CBCL-269,19 p.,,,primary visual cortex; slowness; temporal continuity; hubel and wiesel; feedforward hierarchical models,Center for Biological and Computational Learning (CBCL),,,,,,,,,,,,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory; ,,,,,,,,,,,,,,,,,2007,629,"learning complex cell invariance from natural videos: a plausibility proof one of the most striking feature of the cortex is its ability to wire itself. understanding how the visual cortex wires up through development and how visual experience refines connections into adulthood is a key question for neuroscience. while computational models of the visual cortex are becoming increasingly detailed, the question of how such architecture could self-organize through visual experience is often overlooked. here we focus on the class of hierarchical feedforward models of the ventral stream of the visual cortex, which extend the classical simple-to-complex cells model by hubel and wiesel (1962) to extra-striate areas, and have been shown to account for a host of experimental data. such models assume two functional classes of simple and complex cells with specific predictions about their respective wiring and resulting functionalities.in these networks, the issue of learning, especially for complex cells, is perhaps the least well understood. in fact, in most of these models, the connectivity between simple and complex cells is not learned butrather hard-wired. several algorithms have been proposed for learning invariances at the complex cell level based on a trace rule to exploit the temporal continuity of sequences of natural images, but very few can learn from natural cluttered image sequences.here we propose a new variant of the trace rule that only reinforces the synapses between the most active cells, and therefore can handle cluttered environments. the algorithm has so far been developed and tested at the level of v1-like simple and complex cells: we verified that gabor-like simple cell selectivity could emerge from competitive hebbian learning. in addition, we show how the modified trace rule allows the subsequent complex cells to learn to selectively pool over simple cells with the same preferred orientation but slightly different positions thus increasing their tolerance to the precise position of the stimulus within their receptive fields.",image classification
,"Durand, Frdo",2011-12-14T19:45:12Z,2011-12-14T19:45:12Z,2011-12-14,http://hdl.handle.net/1721.1/67677,MIT-CSAIL-TR-2011-052,A Frequency Analysis of Monte-Carlo and other Numerical Integration Schemes,"The numerical calculation of integrals is central to many computer graphics algorithms such as Monte-Carlo Ray Tracing. We show that such methods can be studied using Fourier analysis. Numerical error is shown to correspond to aliasing and the link between properties of the sampling pattern and the integrand is studied. The approach also permits the unified study of image aliasing and numerical integration, by considering a multidimensional domain where some dimensions are integrated while others are sampled.",,6 p.,,,Numerical Analysis; Integration; Fourier; Monte-Carlo; Aliasing; Rendering; Ray Tracing,Computer Graphics,,,,,,,,,,,,,,,,,,,,,,,,,en-US,,,,2011,877,"a frequency analysis of monte-carlo and other numerical integration schemes the numerical calculation of integrals is central to many computer graphics algorithms such as monte-carlo ray tracing. we show that such methods can be studied using fourier analysis. numerical error is shown to correspond to aliasing and the link between properties of the sampling pattern and the integrand is studied. the approach also permits the unified study of image aliasing and numerical integration, by considering a multidimensional domain where some dimensions are integrated while others are sampled.",privacy/ethics
Barbara Liskov,"Liskov, Barbara",2012-09-17T18:30:04Z,2012-09-17T18:30:04Z,2012-09-14,http://hdl.handle.net/1721.1/73017,MIT-CSAIL-TR-2012-030,Aeolus Reference Manual,This document describes the interface that the Aeolus information flow platform provides for users who are implementing applications using Java. The document explains how the Aeolus features are made available by means of a Java library.,,37 p.,,,information flow control; DIFC; data privacy,Programming Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2012,908,aeolus reference manual this document describes the interface that the aeolus information flow platform provides for users who are implementing applications using java. the document explains how the aeolus features are made available by means of a java library.,language models
,"Kiriansky, Vladimir; Bruening, Derek; Amarasinghe, Saman",2023-03-29T14:43:12Z,2023-03-29T14:43:12Z,2003-05,https://hdl.handle.net/1721.1/149325,MIT-LCS-TM-638,Execution Model Enforcement Via Program Shepherding,"Nearly all security attacks have one thing in common: they coerce the target program into performing actions that it was never intended to perform.  In short, they violate the program's execution model. The execution model encompasses the Application Binary Interface (ABI), higher-level specifications from the program's source programming language, and components specific to the program --- for example, which values a particular function pointer may take.  If this execution model were enforced, and only program actions that the programmer intended were allowed, a majority of current security holes would be closed.   In this paper, we employ program shepherding[26] to enforce a program's execution model.  Program shepherding monitors control flow in order to enforce a security policy.  We use static and dynamic analyses to automatically build a custom security policy for a target program which specifies the program's execution model.  We have implemented our analyses in the DynamoRIO [5] runtime code modification system.  The resulting system imposes minimal or no performance overhead, operates on unmodified native binaries, and requires no special hardware or operating system support.  Our static analyses require source code access but not recompilation.  The analysis process requires no user interaction, but is able to build a strict enough policy to prevent all deviations from the program's control flow graph and nearly all violations of the calling convention, greatly reducing the possibility of an unintended program action.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2003,221,"execution model enforcement via program shepherding nearly all security attacks have one thing in common: they coerce the target program into performing actions that it was never intended to perform.  in short, they violate the program's execution model. the execution model encompasses the application binary interface (abi), higher-level specifications from the program's source programming language, and components specific to the program --- for example, which values a particular function pointer may take.  if this execution model were enforced, and only program actions that the programmer intended were allowed, a majority of current security holes would be closed.   in this paper, we employ program shepherding[26] to enforce a program's execution model.  program shepherding monitors control flow in order to enforce a security policy.  we use static and dynamic analyses to automatically build a custom security policy for a target program which specifies the program's execution model.  we have implemented our analyses in the dynamorio [5] runtime code modification system.  the resulting system imposes minimal or no performance overhead, operates on unmodified native binaries, and requires no special hardware or operating system support.  our static analyses require source code access but not recompilation.  the analysis process requires no user interaction, but is able to build a strict enough policy to prevent all deviations from the program's control flow graph and nearly all violations of the calling convention, greatly reducing the possibility of an unintended program action.",language models
,"Lynch, Nancy; Mitra, Sayan; Nolte, Tina",2005-12-22T02:25:56Z,2005-12-22T02:25:56Z,2005-04-06,http://hdl.handle.net/1721.1/30535,Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory,Motion Coordination Using Virtual Nodes,"We describe how a virtual node abstraction layer can be used to coordinate the motion of real mobile nodes in a region of 2-space. In particular, we consider how nodes in a mobile ad hoc network can arrange themselves along a predetermined curve in the plane, and can maintain themselves in such a configuration in the presence of changes in the underlying mobile ad hoc network, specifically, when nodes may join or leave the system or may fail. Our strategy is to allow the mobile nodes to implement a virtual layer consisting of mobile client nodes, stationary Virtual Nodes (VNs) for predetermined zones in the plane, and local broadcast communication.  The VNs coordinate among themselves to distribute the client nodesbetween zones based on the length of the curve through those zones, while each VN directs its zone's local client nodes to move themselves to equally spaced locations on the local portion of the target curve.",MIT-CSAIL-TR-2005-023; MIT-LCS-TR-986,12 p.; 26698908 bytes; 1192908 bytes,application/postscript; application/pdf,en_US,,Theory of Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2005,419,"motion coordination using virtual nodes we describe how a virtual node abstraction layer can be used to coordinate the motion of real mobile nodes in a region of 2-space. in particular, we consider how nodes in a mobile ad hoc network can arrange themselves along a predetermined curve in the plane, and can maintain themselves in such a configuration in the presence of changes in the underlying mobile ad hoc network, specifically, when nodes may join or leave the system or may fail. our strategy is to allow the mobile nodes to implement a virtual layer consisting of mobile client nodes, stationary virtual nodes (vns) for predetermined zones in the plane, and local broadcast communication.  the vns coordinate among themselves to distribute the client nodesbetween zones based on the length of the curve through those zones, while each vn directs its zone's local client nodes to move themselves to equally spaced locations on the local portion of the target curve.",language models
