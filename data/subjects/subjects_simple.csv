research_paper_id,year,dc.description.abstract,subject_id,subject
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",1,AI
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",2,MIT
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",3,Artificial Intelligence
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",4,object detection
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",5,pattern recognition
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",6,people detection
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",7,face detection
13,2000,"This thesis presents a general, trainable  system for object detection in static images  and video sequences. The core system finds  a certain class of objects in static images of  completely unconstrained, cluttered scenes  without using motion, tracking, or handcrafted  models and without making any assumptions  on the scene structure or the number of  objects in the scene. The system uses a set  of training data of positive and negative  example images as input, transforms the  pixel images to a Haar wavelet  representation, and uses a support vector  machine classifier to learn the difference  between in-class and out-of-class patterns.  To detect objects in out-of-sample images,  we do a brute force search over all the  subwindows in the image. This system is  applied to face, people, and car detection with  excellent results. For our extensions to video  sequences, we augment the core static  detection system in several ways -- 1)  extending the representation to five frames, 2)  implementing an approximation to a Kalman  filter, and 3) modeling detections in an image  as a density and propagating this density  through time according to measured features.  In addition, we present a real-time version of  the system that is currently running in a  DaimlerChrysler experimental vehicle. As part  of this thesis, we also present a system that,  instead of detecting full patterns, uses a  component-based approach. We find it to be  more robust to occlusions, rotations in depth,  and severe lighting conditions for people  detection than the full body version. We also  experiment with various other representations  including pixels and principal components  and show results that quantify how the  number of features, color, and gray-level affect  performance.",8,car detection
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,1,AI
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,2,MIT
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,3,Artificial Intelligence
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,9,missing data
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,10,mixture models
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,11,statistical learning
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,12,EM algorithm
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,13,neural networks
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,14,kernel classifiers
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,15,Support Vector Machine
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,16,regularization networks
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,17,statistical learning theory
15,2000,We present distribution independent bounds on the generalization misclassification performance of a family of kernel classifiers with margin. Support Vector Machine classifiers (SVM) stem out of this class of machines. The bounds are derived through computations of the $V_gamma$ dimension of a family of loss functions where the SVM one belongs to. Bounds that use functions of margin distributions (i.e. functions of the slack variables of SVM) are derived.,18,V-gamma dimension.
55,2001,"Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for  feature detection in strokes and apply this technique using two approaches to  signal processing, one using simple average based thresholding and a second  using scale space.",1,AI
55,2001,"Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for  feature detection in strokes and apply this technique using two approaches to  signal processing, one using simple average based thresholding and a second  using scale space.",19,Feature Point Detection
55,2001,"Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for  feature detection in strokes and apply this technique using two approaches to  signal processing, one using simple average based thresholding and a second  using scale space.",20,Curve Approximation
55,2001,"Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for  feature detection in strokes and apply this technique using two approaches to  signal processing, one using simple average based thresholding and a second  using scale space.",21,Freehand Sketching
56,2001,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",1,AI
56,2001,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",22,tactile sensation
56,2001,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",23,finger
56,2001,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",24,robot
56,2001,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",25,anthropomorphic
56,2001,"The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane.   The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception.  In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an %engineered analog of the human hand with a complete haptic system.",26,skin
57,2001,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",1,AI
57,2001,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",27,predicate dispatching
57,2001,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",28,Common Lisp
57,2001,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",29,CLOS
57,2001,"I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",30,Weyl
60,2001,"In this report, a face recognition system that is capable of detecting and recognizing frontal and rotated faces was developed. Two face recognition methods focusing on the aspect of pose invariance are presented and evaluated - the whole face approach and the component-based approach. The main challenge of this project is to develop a system that is able to identify faces under different viewing angles in realtime. The development of such a system will enhance the capability and robustness of current face recognition technology.  The whole-face approach recognizes faces by classifying a single feature vector consisting of the gray values of the whole face image. The component-based approach  first locates the facial components and extracts them. These components are normalized and combined into a single feature vector for classification. The Support Vector Machine (SVM) is used as the classifier for both approaches. Extensive tests with respect to the robustness against pose changes are performed on a  database that includes faces rotated up to about 40 degrees in depth. The component-based approach clearly outperforms the whole-face approach on all tests. Although this approach isproven to be more reliable, it is still too slow for real-time applications. That is the reason why a real-time face recognition system using the whole-face approach is implemented to recognize people in color video sequences.",1,AI
60,2001,"In this report, a face recognition system that is capable of detecting and recognizing frontal and rotated faces was developed. Two face recognition methods focusing on the aspect of pose invariance are presented and evaluated - the whole face approach and the component-based approach. The main challenge of this project is to develop a system that is able to identify faces under different viewing angles in realtime. The development of such a system will enhance the capability and robustness of current face recognition technology.  The whole-face approach recognizes faces by classifying a single feature vector consisting of the gray values of the whole face image. The component-based approach  first locates the facial components and extracts them. These components are normalized and combined into a single feature vector for classification. The Support Vector Machine (SVM) is used as the classifier for both approaches. Extensive tests with respect to the robustness against pose changes are performed on a  database that includes faces rotated up to about 40 degrees in depth. The component-based approach clearly outperforms the whole-face approach on all tests. Although this approach isproven to be more reliable, it is still too slow for real-time applications. That is the reason why a real-time face recognition system using the whole-face approach is implemented to recognize people in color video sequences.",31,vision
61,2001,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",1,AI
61,2001,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",32,self-organisation
61,2001,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",33,multi agent
61,2001,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",34,developmental biology
61,2001,"In this thesis I present a language for  instructing a sheet of identically-programmed, flexible, autonomous  agents (``cells'') to assemble themselves into a predetermined  global shape, using local interactions. The global shape is described  as a folding construction on a continuous sheet, using a set of axioms  from paper-folding (origami). I provide a means of automatically  deriving the cell program, executed by all cells, from the global  shape description.  With this language, a wide variety of global  shapes and patterns can be synthesized, using only local interactions  between identically-programmed cells. Examples  include flat layered shapes, all plane Euclidean constructions, and a  variety of tessellation patterns. In contrast to approaches based on  cellular automata or evolution, the cell program is directly derived  from the global shape description and is composed from a small  number of biologically-inspired primitives: gradients, neighborhood query,  polarity inversion, cell-to-cell contact and flexible folding. The cell  programs are robust, without relying on regular cell  placement, global coordinates, or synchronous operation and can tolerate a  small amount of random cell death. I show that an average cell  neighborhood of 15 is sufficient to reliably self-assemble complex  shapes and geometric patterns on randomly distributed cells.  The language provides many insights into the  relationship between local and global descriptions of behavior,  such as the advantage of constructive languages, mechanisms for  achieving global robustness, and mechanisms for achieving scale-independent shapes from a single cell program. The language suggests a  mechanism by which many related shapes can be created by the same cell  program, in the manner of D'Arcy Thompson's famous coordinate  transformations. The thesis illuminates how complex morphology and  pattern can emerge from local interactions, and how one can engineer  robust self-assembly.",35,amorphous computing
63,2001,"We design and implement a system that recommends musicians to listeners. The basic idea is to keep track of what artists a user listens to, to find other users with similar tastes, and to recommend other artists that these similar listeners enjoy. The system utilizes a client-server architecture, a web-based interface, and an SQL database to store and process information. We describe Audiomomma-0.3, a proof-of-concept implementation of the above ideas.",1,AI
64,2001,"Market prices are well known to efficiently collect and aggregate diverse information regarding the value of commodities and assets. The role of markets has been particularly suitable to pricing financial securities. This article provides an alternative application of the pricing mechanism to marketing research - using pseudo-securities markets to measure preferences over new product concepts. Surveys, focus groups, concept tests and conjoint studies are methods traditionally used to measure individual and aggregate preferences. Unfortunately, these methods can be biased, costly and time-consuming to conduct. The present research is motivated by the desire to efficiently measure preferences and more accurately predict new product success, based on the efficiency and incentive-compatibility of security trading markets. The article describes a novel market research method, pro-vides insight into why the method should work, and compares the results of several trading experiments against other methodologies such as concept testing and conjoint analysis.",1,AI
65,2001,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",1,AI
65,2001,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",36,Image matching
65,2001,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",37,vector quantization
65,2001,"The image comparison operation ??sessing how well one image matches another ??rms a critical component of many image analysis systems and models of human visual processing. Two norms used commonly for this purpose are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric better captures the perceptual notion of image similarity than the other. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created via vector quantization. In both conditions the subjects showed a consistent preference for images matched using the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.",38,Minkowski metric
66,2001,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,1,AI
66,2001,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,39,Scene classification
66,2001,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,40,Navigation
66,2001,We propose a scheme for indoor place identification based on the recognition of global scene views. Scene views are encoded using a holistic representation that provides low-resolution spatial and spectral information. The holistic nature of the representation dispenses with the need to rely on specific objects or local landmarks and also renders it robust against variations in object configurations. We demonstrate the scheme on the problem of recognizing scenes in video sequences captured while walking through an office environment. We develop a method for distinguishing between 'diagnostic' and 'generic' views and also evaluate changes in system performances as a function of the amount of training data available and the complexity of the representation.,41,scene representation
69,2001,"The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation.   Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",1,AI
69,2001,"The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation.   Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",42,registration
69,2001,"The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation.   Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.",43,medical imaging
70,2001,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",1,AI
70,2001,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",44,reinforcement learning
70,2001,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",45,RL
70,2001,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",46,importance sampling
70,2001,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",47,estimation
70,2001,"This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms.  We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm.  Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making.",48,market-making
71,2001,"Brightness judgments are a key part of the primate brain's visual analysis of the environment. There is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. However, it is unclear precisely how a region's context influences its perceived brightness. Recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. This work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. Here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. We find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. These results point to the important role of low-level mechanisms in determining brightness percepts.",1,AI
71,2001,"Brightness judgments are a key part of the primate brain's visual analysis of the environment. There is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. However, it is unclear precisely how a region's context influences its perceived brightness. Recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. This work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. Here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. We find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. These results point to the important role of low-level mechanisms in determining brightness percepts.",49,brightness perception
71,2001,"Brightness judgments are a key part of the primate brain's visual analysis of the environment. There is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. However, it is unclear precisely how a region's context influences its perceived brightness. Recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. This work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. Here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. We find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. These results point to the important role of low-level mechanisms in determining brightness percepts.",50,perceptual organization
71,2001,"Brightness judgments are a key part of the primate brain's visual analysis of the environment. There is general consensus that the perceived brightness of an image region is based not only on its actual luminance, but also on the photometric structure of its neighborhood. However, it is unclear precisely how a region's context influences its perceived brightness. Recent research has suggested that brightness estimation may be based on a sophisticated analysis of scene layout in terms of transparency, illumination and shadows. This work has called into question the role of low-level mechanisms, such as lateral inhibition, as explanations for brightness phenomena. Here we describe experiments with displays for which low-level and high-level analyses make qualitatively different predictions, and with which we can quantitatively assess the trade-offs between low-level and high-level factors. We find that brightness percepts in these displays are governed by low-level stimulus properties, even when these percepts are inconsistent with higher-level interpretations of scene layout. These results point to the important role of low-level mechanisms in determining brightness percepts.",51,local mechanisms
72,2001,I present an algorithm which allows two agents to generate a simple language based only on observations of a shared environment. Vocabulary and roles for the language are learned in linear time. Communication is robust and degrades gradually as complexity increases. Dissimilar modes of experience will lead to a shared kernel vocabulary.,1,AI
72,2001,I present an algorithm which allows two agents to generate a simple language based only on observations of a shared environment. Vocabulary and roles for the language are learned in linear time. Communication is robust and degrades gradually as complexity increases. Dissimilar modes of experience will lead to a shared kernel vocabulary.,52,Adaptive Learning Hash-coding communication architecture algorithm
73,2001,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",1,AI
73,2001,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",53,multiclass
73,2001,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",54,SVM
73,2001,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",55,feature selection
73,2001,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",56,SRBCT
73,2001,"A novel approach to multiclass tumor classification using Artificial Neural Networks (ANNs) was introduced in a recent paper cite{Khan2001}. The method successfully classified and diagnosed small, round blue cell tumors (SRBCTs) of childhood into four distinct categories, neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), using cDNA gene expression profiles of samples that included both tumor biopsy material and cell lines. We report that using an approach similar to the one reported by Yeang et al cite{Yeang2001}, i.e. multiclass classification by combining outputs of binary classifiers, we achieved equal accuracy with much fewer features. We report the performances of 3 binary classifiers (k-nearest neighbors (kNN), weighted-voting (WV), and support vector machines (SVM)) with 3 feature selection techniques (Golub's Signal to Noise (SN) ratios cite{Golub99}, Fisher scores (FSc) and Mukherjee's SVM feature selection (SVMFS))cite{Sayan98}.",57,tumors
77,2001,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",1,AI
77,2001,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",58,naive bayes
77,2001,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",59,text
77,2001,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",60,classification
77,2001,"There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.",55,feature selection
78,2001,"This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",1,AI
78,2001,"This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",61,Robotics
78,2001,"This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls.  There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models.  Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces.  The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.",62,Soil Modeling
79,2001,"All intelligence relies on search --- for  example, the search for an intelligent agent's  next action. Search is only likely to succeed in resource-bounded agents if they have already  been biased towards finding the right answer.  In artificial agents, the primary source of bias is engineering.   This dissertation describes an approach,  Behavior-Oriented Design (BOD) for  engineering complex agents. A complex agent  is one that must arbitrate between potentially  conflicting goals or behaviors.  Behavior-oriented design builds on work in  behavior-based and hybrid architectures for agents, and the object  oriented approach to software engineering.   The primary contributions of this dissertation  are:     1.The BOD architecture: a modular  architecture with each module providing  specialized representations to facilitate  learning.    This includes one pre-specified module  and representation for action selection or  behavior arbitration. The specialized    representation underlying BOD action  selection is Parallel-rooted, Ordered,  Slip-stack Hierarchical (POSH) reactive plans.     2.The BOD development process: an  iterative process that alternately scales the  agent's capabilities then optimizes the agent  for    simplicity, exploiting tradeoffs between the  component representations. This ongoing  process for controlling complexity not only    provides bias for the behaving agent, but  also facilitates its maintenance and  extendibility.   The secondary contributions of this  dissertation include two implementations of  POSH action selection, a procedure for  identifying useful idioms in agent architectures and  using them to distribute knowledge across  agent paradigms, several examples of  applying BOD idioms to established architectures, an  analysis and comparison of the attributes and  design trends of a large number of agent architectures, a comparison of biological  (particularly mammalian) intelligence to  artificial agent architectures, a novel model of primate transitive inference, and many other  examples of BOD agents and BOD  development.",1,AI
80,2001,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",1,AI
80,2001,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",63,context
80,2001,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",64,image statistics
80,2001,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",65,Bayesian reasoning
80,2001,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",66,recognition
80,2001,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",67,focus of attention
81,2001,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",1,AI
81,2001,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",68,reflectance
81,2001,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",69,lighting
81,2001,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",70,BRDF
81,2001,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",71,surface
81,2001,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",72,illumination statistics
81,2001,"Humans recognize optical reflectance properties of surfaces such as metal, plastic, or paper from a single image without knowledge of illumination. We develop a machine vision system to perform similar recognition tasks automatically. Reflectance estimation under unknown, arbitrary illumination proves highly underconstrained due to the variety of potential illumination distributions and surface reflectance properties. We have found that the spatial structure of real-world illumination possesses some of the statistical regularities observed in the natural image statistics literature. A human or computer vision system may be able to exploit this prior information to determine the most likely surface reflectance given an observed image. We develop an algorithm for reflectance classification under unknown real-world illumination, which learns relationships between surface reflectance and certain features (statistics) computed from a single observed image. We also develop an automatic feature selection method.",73,natural images
82,2001,"This paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. Our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. We claim that our gait dynamics representation is rich enough for the task of recognition and classification. The use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. In addition, preliminary results are shown on gender classification using our gait dynamics features.",1,AI
82,2001,"This paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. Our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. We claim that our gait dynamics representation is rich enough for the task of recognition and classification. The use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. In addition, preliminary results are shown on gender classification using our gait dynamics features.",74,gait
82,2001,"This paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. Our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. We claim that our gait dynamics representation is rich enough for the task of recognition and classification. The use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. In addition, preliminary results are shown on gender classification using our gait dynamics features.",66,recognition
82,2001,"This paper describes a representation of the dynamics of human walking action for the purpose of person identification and classification by gait appearance. Our gait representation is based on simple features such as moments extracted from video silhouettes of human walking motion. We claim that our gait dynamics representation is rich enough for the task of recognition and classification. The use of our feature representation is demonstrated in the task of person recognition from video sequences of orthogonal views of people walking. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times, and under varying lighting environments. In addition, preliminary results are shown on gender classification using our gait dynamics features.",75,gender classification
83,2001,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",1,AI
83,2001,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",76,Invariance
83,2001,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",77,Optical Flow
83,2001,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",78,Color Constancy
83,2001,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",79,Object Recognition
83,2001,"We present a unifying framework in which ""object-independent"" modes of variation are learned from continuous-time data such as video sequences. These modes of variation can be used as ""generators"" to produce a manifold of images of a new object from a single  example of that object.   We develop the framework in the context of a well-known example: analyzing the modes of spatial deformations of a scene under camera movement. Our method learns a close approximation to the standard affine deformations that are expected from the geometry of the situation, and  does so in a completely unsupervised (i.e. ignorant of the geometry of the situation) fashion. We stress that it is learning a ""parameterization"", not just the parameter values, of the data. We then demonstrate how we have used the same framework to derive a novel  data-driven model of joint color change in images due to common lighting variations. The model is superior to previous models of color change in describing non-linear color changes due to lighting.",80,image manifold
84,2001,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",1,AI
84,2001,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",81,maximum operation
84,2001,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",82,invariance
84,2001,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",83,recurrent inhibition
84,2001,"Object recognition in the visual cortex is based on a hierarchical architecture, in which specialized brain regions along the ventral pathway extract object features of increasing levels of complexity, accompanied by greater invariance in stimulus size, position, and orientation. Recent theoretical studies postulate a non-linear pooling function, such as the maximum (MAX) operation could be fundamental in achieving such invariance. In this paper, we are concerned with neurally plausible mechanisms that may be involved in realizing the MAX operation. Four canonical circuits are proposed, each based on neural mechanisms that have been previously discussed in the context of cortical processing. Through simulations and mathematical analysis, we examine the relative performance and robustness of these mechanisms. We derive experimentally verifiable predictions for each circuit and discuss their respective physiological considerations.",84,shunting inhibition
85,2001,"Visibility constraints can aid the segmentation of foreground objects observed with multiple range images. In our approach, points are defined as foreground if they can be determined to occlude some {em empty space} in the scene. We present an efficient algorithm to estimate foreground points in each range view using explicit epipolar search. In cases where the background pattern is stationary, we show how visibility constraints from other views can generate virtual background values at points with no valid depth in the primary view. We demonstrate the performance of both algorithms for detecting people in indoor office environments.",1,AI
86,2001,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",1,AI
86,2001,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",85,Deduction
86,2001,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",86,formal proofs
86,2001,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",87,semantics
86,2001,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",88,proof checking
86,2001,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",89,soundness
86,2001,"This paper introduces Denotational Proof Languages (DPLs). DPLs are languages for presenting, discovering, and checking formal proofs. In particular, in this paper we discus type-alpha DPLs---a simple class of DPLs  for which termination is guaranteed and proof checking can be  performed in time linear in the size of the proof.  Type-alpha DPLs allow for lucid proof presentation and for efficient proof checking, but not for proof search.  Type-omega DPLs allow for search as well as simple presentation and checking, but termination is no longer guaranteed and  proof checking may diverge. We do not study type-omega DPLs here.   We start by listing some common characteristics of DPLs. We  then illustrate with a particularly simple example: a toy  type-alpha DPL called PAR, for deducing parities. We present the abstract syntax of PAR, followed by two  different kinds of formal semantics: evaluation and denotational.  We then relate the two semantics and show how proof checking  becomes tantamount to evaluation. We proceed to develop the  proof theory of PAR, formulating and studying certain  key notions such as observational equivalence that pervade all DPLs.   We then present NDL, a type-alpha DPL for classical zero-order  natural deduction. Our presentation of NDL mirrors that of PAR,  showing how every basic concept that was introduced in PAR resurfaces in NDL. We present sample proofs of several well-known tautologies of propositional logic that demonstrate our thesis that DPL proofs are  readable, writable, and concise. Next we contrast DPLs to typed logics based  on the Curry-Howard isomorphism, and discuss the distinction between pure and augmented DPLs. Finally we consider the issue of  implementing DPLs, presenting an implementation of PAR in SML and one in Athena, and end with some concluding remarks.",90,logic
87,2001,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",1,AI
87,2001,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",91,deduction
87,2001,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",92,computation
87,2001,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",93,proof search
87,2001,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",89,soundness
87,2001,"Type-omega DPLs (Denotational Proof Languages) are languages for proof presentation and search that offer strong soundness guarantees. LCF-type systems such as HOL offer similar guarantees, but their soundness relies heavily on static type systems. By contrast, DPLs  ensure soundness dynamically, through their evaluation semantics; no type system is necessary. This is possible owing to a novel two-tier syntax  that separates deductions from computations, and to the abstraction of assumption bases, which is factored into the semantics of the language and allows for sound evaluation.   Every type-omega DPL properly contains a type-alpha DPL, which can be used to present proofs in a lucid and detailed form, exclusively in terms of primitive inference rules. Derived inference rules are expressed  as user-defined methods, which are ""proof recipes"" that take arguments  and dynamically perform appropriate deductions. Methods arise naturally  via parametric abstraction over type-alpha proofs. In that light, the  evaluation of a method call can be viewed as a computation that carries  out a type-alpha deduction. The type-alpha proof ""unwound"" by such a method  call is called the ""certificate"" of the call. Certificates can be checked  by exceptionally simple type-alpha interpreters, and thus they are useful  whenever we wish to minimize our trusted base.   Methods are statically closed over lexical environments, but dynamically scoped over assumption bases. They can take other methods as arguments, they can iterate, and they can branch conditionally. These capabilities,  in tandem with the bifurcated syntax of type-omega DPLs and their dynamic assumption-base semantics, allow the user to define methods in  a style that is disciplined enough to ensure soundness yet fluid enough  to permit succinct and perspicuous expression of arbitrarily sophisticated derived inference rules.   We demonstrate every major feature of type-omega DPLs by defining and studying NDL-omega, a higher-order, lexically scoped, call-by-value type-omega DPL for classical zero-order natural deduction---a simple choice that allows us to focus on type-omega syntax and semantics rather than on the subtleties of the underlying logic. We start by illustrating how type-alpha DPLs naturally lead to type-omega DPLs by way of abstraction; present the formal syntax and semantics of NDL-omega; prove several results about it, including soundness; give numerous examples of methods; point out connections to the lambda-phi calculus, a very general framework for type-omega DPLs; introduce a notion of computational and deductive cost; define several instrumented interpreters for computing such costs and for generating certificates; explore the use of type-omega DPLs as general programming languages; show that DPLs do not have to be type-less by formulating a static Hindley-Milner polymorphic type system for NDL-omega; discuss some idiosyncrasies of type-omega DPLs such as the potential divergence of proof checking; and compare type-omega DPLs to other approaches to proof presentation and discovery. Finally, a complete implementation of NDL-omega in SML-NJ is given for users who want to run the examples and experiment with the language.",90,logic
88,2001,"We compare Naive Bayes and Support Vector Machines on the task of multiclass text classification. Using a variety of approaches to combine the underlying binary classifiers, we find that SVMs substantially outperform Naive Bayes. We present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets. We develop a new indicator of binary performance to show that the SVM's lower multiclass error is a result of its improved binary performance. Furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties.",1,AI
88,2001,"We compare Naive Bayes and Support Vector Machines on the task of multiclass text classification. Using a variety of approaches to combine the underlying binary classifiers, we find that SVMs substantially outperform Naive Bayes. We present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets. We develop a new indicator of binary performance to show that the SVM's lower multiclass error is a result of its improved binary performance. Furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties.",94,text classification
88,2001,"We compare Naive Bayes and Support Vector Machines on the task of multiclass text classification. Using a variety of approaches to combine the underlying binary classifiers, we find that SVMs substantially outperform Naive Bayes. We present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets. We develop a new indicator of binary performance to show that the SVM's lower multiclass error is a result of its improved binary performance. Furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties.",95,support vector machine
88,2001,"We compare Naive Bayes and Support Vector Machines on the task of multiclass text classification. Using a variety of approaches to combine the underlying binary classifiers, we find that SVMs substantially outperform Naive Bayes. We present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets. We develop a new indicator of binary performance to show that the SVM's lower multiclass error is a result of its improved binary performance. Furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties.",96,multiclass classification
89,2001,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",1,AI
89,2001,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",97,illumination
89,2001,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",68,reflectance
89,2001,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",98,natural image statistics
89,2001,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",99,human vision
89,2001,"Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.",70,BRDF
90,2001,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",1,AI
90,2001,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",97,illumination
90,2001,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",68,reflectance
90,2001,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",100,computer vision
90,2001,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",101,geometry
90,2001,"This paper describes a machine vision system that classifies reflectance properties of surfaces such as metal, plastic, or paper, under unknown real-world illumination. We demonstrate performance of our algorithm for surfaces of arbitrary geometry. Reflectance estimation under arbitrary omnidirectional illumination proves highly underconstrained. Our reflectance estimation algorithm succeeds by learning relationships between surface reflectance and certain statistics computed from an observed image, which depend on statistical regularities in the spatial structure of real-world illumination. Although the algorithm assumes known geometry, its statistical nature makes it robust to inaccurate geometry estimates.",98,natural image statistics
92,2001,"The human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. Here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. Contrary to predictions from previous studies [Enns and Rensink, 1990; Sun and Perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. Our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. Specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",1,AI
92,2001,"The human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. Here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. Contrary to predictions from previous studies [Enns and Rensink, 1990; Sun and Perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. Our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. Specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",102,Illumination
92,2001,"The human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. Here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. Contrary to predictions from previous studies [Enns and Rensink, 1990; Sun and Perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. Our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. Specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",103,natural scene perception
92,2001,"The human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. Here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. Contrary to predictions from previous studies [Enns and Rensink, 1990; Sun and Perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. Our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. Specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",104,lighting direction
92,2001,"The human visual system is adept at detecting and encoding statistical regularities in its spatio-temporal environment. Here we report an unexpected failure of this ability in the context of perceiving inconsistencies in illumination distributions across a scene. Contrary to predictions from previous studies [Enns and Rensink, 1990; Sun and Perona, 1996a, 1996b, 1997], we find that the visual system displays a remarkable lack of sensitivity to illumination inconsistencies, both in experimental stimuli and in images of real scenes. Our results allow us to draw inferences regarding how the visual system encodes illumination distributions across scenes. Specifically, they suggest that the visual system does not verify the global consistency of locally derived estimates of illumination direction.",105,pop-out
93,2001,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",1,AI
93,2001,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",106,Face detection
93,2001,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",107,image resolution
93,2001,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",108,contrast negation
93,2001,"The ability to detect faces in images is of critical ecological significance. It is a pre-requisite for other important face perception tasks such as person identification, gender classification and affect analysis. Here we address the question of how the visual system classifies images into face and non-face patterns. We focus on face detection in impoverished images, which allow us to explore information thresholds required for different levels of performance. Our experimental results provide lower bounds on image resolution needed for reliable discrimination between face and non-face patterns and help characterize the nature of facial representations used by the visual system under degraded viewing conditions. Specifically, they enable an evaluation of the contribution of luminance contrast, image orientation and local context on face-detection performance.",109,vertical inversion
94,2001,"An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n^3) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",1,AI
94,2001,"An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n^3) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",110,semi-supervised learning
94,2001,"An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n^3) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",111,incomplete data
94,2001,"An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n^3) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",112,EM
94,2001,"An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n^3) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems.",113,stable estimation
95,2001,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",1,AI
95,2001,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",91,deduction
95,2001,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",114,proofs
95,2001,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",115,simplifiation
95,2001,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",116,proof optimization
95,2001,"This paper presents an algorithm for simplifying NDL deductions. An array of simplifying transformations are rigorously defined. They are shown to be terminating, and to respect the formal semantis of the language. We also show that the transformations never increase the size or complexity of a deduction---in the worst case, they produce deductions of the same size and complexity as the original. We present several examples of proofs containing various types of ""detours"", and explain how our procedure eliminates them, resulting in smaller and cleaner deductions. All of the given transformations are fully implemented in SML-NJ. The complete code listing is presented, along with explanatory comments. Finally, although the transformations given here are defined for NDL, we point out that they can be applied to any type-alpha DPL that satisfies a few simple conditions.",117,deduction complexity
98,2001,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",1,AI
98,2001,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",118,depth
98,2001,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",119,monocular
98,2001,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",120,scale selection
98,2001,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",73,natural images
98,2001,"In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual ""size"" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the difficulty of the object recognition process. Here we propose a source of information for absolute depth estimation that does not rely on specific objects: we introduce a procedure for absolute depth estimation based on the recognition of the whole scene. The shape of the space of the scene and the structures present in the scene are strongly related to the scale of observation. We demonstrate that, by recognizing the properties of the structures present in the image, we can infer the scale of the scene, and therefore its absolute mean depth. We illustrate the interest in computing the mean depth of the scene with application to scene recognition and object detection.",121,scene recognition
99,2001,"Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",1,AI
99,2001,"Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",1,AI
99,2001,"Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",122,computational neuroscience
99,2001,"Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",123,object recognition
99,2001,"Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",124,macaque
99,2001,"Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",125,IT
99,2001,"Baylis & Driver (Nature Neuroscience, 2001) have recently presented data on the response of neurons in macaque inferotemporal cortex (IT) to various stimulus transformations. They report that neurons can generalize over contrast and mirror reversal, but not over figure-ground reversal. This finding is taken to demonstrate that ``the selectivity of IT neurons is not determined simply by the distinctive contours in a display, contrary to simple edge-based models of shape recognition'', citing our recently presented model of object recognition in cortex (Riesenhuber & Poggio, Nature Neuroscience, 1999). In this memo, I show that the main effects of the experiment can be obtained by performing the appropriate simulations in our simple feedforward model. This suggests for IT cell tuning that the possible contributions of explicit edge assignment processes postulated in (Baylis & Driver, 2001) might be smaller than expected.",82,invariance
100,2001,"One of the key challenges in face perception lies in determining the contribution of different cues to face identification. In this study, we focus on the role of color cues. Although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. Here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. Under such conditions, recognition performance with color images is significantly better than that with grayscale images. Our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",1,AI
100,2001,"One of the key challenges in face perception lies in determining the contribution of different cues to face identification. In this study, we focus on the role of color cues. Although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. Here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. Under such conditions, recognition performance with color images is significantly better than that with grayscale images. Our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",126,Face recognition
100,2001,"One of the key challenges in face perception lies in determining the contribution of different cues to face identification. In this study, we focus on the role of color cues. Although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. Here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. Under such conditions, recognition performance with color images is significantly better than that with grayscale images. Our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",127,color
100,2001,"One of the key challenges in face perception lies in determining the contribution of different cues to face identification. In this study, we focus on the role of color cues. Although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. Here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. Under such conditions, recognition performance with color images is significantly better than that with grayscale images. Our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",128,low-resolution
100,2001,"One of the key challenges in face perception lies in determining the contribution of different cues to face identification. In this study, we focus on the role of color cues. Although color appears to be a salient attribute of faces, past research has suggested that it confers little recognition advantage for identifying people. Here we report experimental results suggesting that color cues do play a role in face recognition and their contribution becomes evident when shape cues are degraded. Under such conditions, recognition performance with color images is significantly better than that with grayscale images. Our experimental results also indicate that the contribution of color may lie not so much in providing diagnostic cues to identity as in aiding low-level image-analysis processes such as segmentation.",129,grayscale
103,2002,"In a distributed model of intelligence, peer components need to communicate with one another. I present a system which enables two agents connected by a thick twisted bundle of wires to bootstrap a simple communication system from observations of a shared environment. The agents learn a large vocabulary of symbols, as well as inflections on those symbols which allow thematic role-frames to be transmitted. Language acquisition time is rapid and linear in the number of symbols and inflections. The final communication system is robust and performance degrades gradually in the face of problems.",1,AI
103,2002,"In a distributed model of intelligence, peer components need to communicate with one another. I present a system which enables two agents connected by a thick twisted bundle of wires to bootstrap a simple communication system from observations of a shared environment. The agents learn a large vocabulary of symbols, as well as inflections on those symbols which allow thematic role-frames to be transmitted. Language acquisition time is rapid and linear in the number of symbols and inflections. The final communication system is robust and performance degrades gradually in the face of problems.",130,distributed amorphous human intelligence genesis robust communication network
104,2002,"The utility of vision-based face tracking for dual pointing tasks is evaluated. We first describe a 3-D face tracking technique based on real-time parametric motion-stereo, which is non-invasive, robust, and self-initialized. The tracker provides a real-time estimate of a ?frontal face ray? whose intersection with the display surface plane is used as a second stream of input for scrolling or pointing, in paral-lel with hand input. We evaluated the performance of com-bined head/hand input on a box selection and coloring task: users selected boxes with one pointer and colors with a second pointer, or performed both tasks with a single pointer. We found that performance with head and one hand was intermediate between single hand performance and dual hand performance. Our results are consistent with previously reported dual hand conflict in symmetric pointing tasks, and suggest that a head-based input stream should be used for asymmetric control.",1,AI
105,2002,"We introduce a new method to describe, in a single image, changes in  shape over time. We acquire both range and image information with a  stationary stereo camera. From the pictures taken, we display a  composite image consisting of the image data from the  surface closest to the camera at every pixel. This reveals the 3-d  relationships over time by easy-to-interpret occlusion relationships  in the composite image. We call the composite a shape-time  photograph.   Small errors in depth measurements cause artifacts in the shape-time  images. We correct most of these using a Markov network to estimate  the most probable front surface, taking into account the depth  measurements, their uncertainties, and layer continuity assumptions.",1,AI
105,2002,"We introduce a new method to describe, in a single image, changes in  shape over time. We acquire both range and image information with a  stationary stereo camera. From the pictures taken, we display a  composite image consisting of the image data from the  surface closest to the camera at every pixel. This reveals the 3-d  relationships over time by easy-to-interpret occlusion relationships  in the composite image. We call the composite a shape-time  photograph.   Small errors in depth measurements cause artifacts in the shape-time  images. We correct most of these using a Markov network to estimate  the most probable front surface, taking into account the depth  measurements, their uncertainties, and layer continuity assumptions.",131,video summarization
105,2002,"We introduce a new method to describe, in a single image, changes in  shape over time. We acquire both range and image information with a  stationary stereo camera. From the pictures taken, we display a  composite image consisting of the image data from the  surface closest to the camera at every pixel. This reveals the 3-d  relationships over time by easy-to-interpret occlusion relationships  in the composite image. We call the composite a shape-time  photograph.   Small errors in depth measurements cause artifacts in the shape-time  images. We correct most of these using a Markov network to estimate  the most probable front surface, taking into account the depth  measurements, their uncertainties, and layer continuity assumptions.",132,stereo
118,2002,"Intuitively, we expect that averaging --- or  bagging --- different regressors with low correlation should  smooth their behavior and be somewhat similar to regularization. In this  note we make this intuition precise. Using an almost classical  definition of stability, we prove that a certain form of averaging  provides generalization bounds with a rate of convergence of the  same order as Tikhonov regularization --- similar to fashionable RKHS-based learning algorithms.",1,AI
118,2002,"Intuitively, we expect that averaging --- or  bagging --- different regressors with low correlation should  smooth their behavior and be somewhat similar to regularization. In this  note we make this intuition precise. Using an almost classical  definition of stability, we prove that a certain form of averaging  provides generalization bounds with a rate of convergence of the  same order as Tikhonov regularization --- similar to fashionable RKHS-based learning algorithms.",133,Bagging
118,2002,"Intuitively, we expect that averaging --- or  bagging --- different regressors with low correlation should  smooth their behavior and be somewhat similar to regularization. In this  note we make this intuition precise. Using an almost classical  definition of stability, we prove that a certain form of averaging  provides generalization bounds with a rate of convergence of the  same order as Tikhonov regularization --- similar to fashionable RKHS-based learning algorithms.",134,stability
118,2002,"Intuitively, we expect that averaging --- or  bagging --- different regressors with low correlation should  smooth their behavior and be somewhat similar to regularization. In this  note we make this intuition precise. Using an almost classical  definition of stability, we prove that a certain form of averaging  provides generalization bounds with a rate of convergence of the  same order as Tikhonov regularization --- similar to fashionable RKHS-based learning algorithms.",135,regularization
120,2002,"Tsunoda et al. (2001) recently studied the  nature of object representation in monkey  inferotemporal cortex using a combination of  optical imaging and extracellular recordings.  In particular, they examined IT neuron  responses to complex natural objects and  ""simplified"" versions thereof. In that study, in  42% of the cases, optical imaging revealed a  decrease in the number of activation patches  in IT as stimuli were ""simplified"". However, in  58% of the cases, ""simplification"" of the  stimuli actually led to the appearance of  additional activation patches in IT. Based on  these results, the authors propose a scheme  in which an object is represented by  combinations of active and inactive columns  coding for individual features.  We examine the patterns of activation caused  by the same stimuli as used by Tsunoda et al.  in our model of object recognition in cortex  (Riesenhuber 99). We find that object-tuned  units can show a pattern of appearance and  disappearance of features identical to the  experiment. Thus, the data of Tsunoda et al.  appear to be in quantitative agreement with a  simple object-based representation in which  an object's identity is coded by its similarities  to reference objects. Moreover, the agreement  of simulations and experiment suggests that  the simplification procedure used by Tsunoda  (2001) is not necessarily an accurate method  to determine neuronal tuning.",1,AI
120,2002,"Tsunoda et al. (2001) recently studied the  nature of object representation in monkey  inferotemporal cortex using a combination of  optical imaging and extracellular recordings.  In particular, they examined IT neuron  responses to complex natural objects and  ""simplified"" versions thereof. In that study, in  42% of the cases, optical imaging revealed a  decrease in the number of activation patches  in IT as stimuli were ""simplified"". However, in  58% of the cases, ""simplification"" of the  stimuli actually led to the appearance of  additional activation patches in IT. Based on  these results, the authors propose a scheme  in which an object is represented by  combinations of active and inactive columns  coding for individual features.  We examine the patterns of activation caused  by the same stimuli as used by Tsunoda et al.  in our model of object recognition in cortex  (Riesenhuber 99). We find that object-tuned  units can show a pattern of appearance and  disappearance of features identical to the  experiment. Thus, the data of Tsunoda et al.  appear to be in quantitative agreement with a  simple object-based representation in which  an object's identity is coded by its similarities  to reference objects. Moreover, the agreement  of simulations and experiment suggests that  the simplification procedure used by Tsunoda  (2001) is not necessarily an accurate method  to determine neuronal tuning.",136,computational neuroscience object recognition representation simplification
121,2002,"The Design Patterns book [GOF95] presents  24 time-tested patterns that consistently appear in well-designed  software systems. Each pattern is presented with a description of the  design problem the pattern addresses, as well as sample  implementation code and design considerations. This paper explores how the  patterns from the ""Gang of Four'', or ""GOF'' book, as it is often called,  appear when similar problems are addressed using a dynamic,  higher-order, object-oriented programming language. Some of the  patterns disappear -- that is, they are supported directly by language features,  some patterns are simpler or have a different focus, and some are  essentially unchanged.",1,AI
124,2002,"Most reinforcement learning methods operate  on propositional representations of the world state. Such  representations are often intractably large and generalize poorly. Using  a deictic representation is believed to be a viable  alternative: they promise generalization while allowing the use of  existing reinforcement-learning methods. Yet, there  are few experiments on learning with deictic representations reported  in the literature. In this paper we explore the effectiveness of two  forms of deictic representation and a naive propositional  representation in a simple blocks-world domain. We find,  empirically, that the deictic representations actually worsen performance.  We conclude with a discussion of possible causes of these  results and strategies for more effective learning in domains with objects.",1,AI
124,2002,"Most reinforcement learning methods operate  on propositional representations of the world state. Such  representations are often intractably large and generalize poorly. Using  a deictic representation is believed to be a viable  alternative: they promise generalization while allowing the use of  existing reinforcement-learning methods. Yet, there  are few experiments on learning with deictic representations reported  in the literature. In this paper we explore the effectiveness of two  forms of deictic representation and a naive propositional  representation in a simple blocks-world domain. We find,  empirically, that the deictic representations actually worsen performance.  We conclude with a discussion of possible causes of these  results and strategies for more effective learning in domains with objects.",137,Reinforcement Learning
124,2002,"Most reinforcement learning methods operate  on propositional representations of the world state. Such  representations are often intractably large and generalize poorly. Using  a deictic representation is believed to be a viable  alternative: they promise generalization while allowing the use of  existing reinforcement-learning methods. Yet, there  are few experiments on learning with deictic representations reported  in the literature. In this paper we explore the effectiveness of two  forms of deictic representation and a naive propositional  representation in a simple blocks-world domain. We find,  empirically, that the deictic representations actually worsen performance.  We conclude with a discussion of possible causes of these  results and strategies for more effective learning in domains with objects.",138,Partial Observability
124,2002,"Most reinforcement learning methods operate  on propositional representations of the world state. Such  representations are often intractably large and generalize poorly. Using  a deictic representation is believed to be a viable  alternative: they promise generalization while allowing the use of  existing reinforcement-learning methods. Yet, there  are few experiments on learning with deictic representations reported  in the literature. In this paper we explore the effectiveness of two  forms of deictic representation and a naive propositional  representation in a simple blocks-world domain. We find,  empirically, that the deictic representations actually worsen performance.  We conclude with a discussion of possible causes of these  results and strategies for more effective learning in domains with objects.",139,Representations
125,2002,"In a recent experiment, Freedman et al.  recorded from inferotemporal (IT) and prefrontal cortices (PFC) of monkeys  performing a ""cat/dog"" categorization task (Freedman 2001 and  Freedman, Riesenhuber, Poggio, Miller 2001). In this paper we analyze the tuning properties of view-tuned  units in our HMAX model of object recognition in cortex (Riesenhuber  1999) using the same paradigm and stimuli  as in the experiment. We then compare the simulation results to the monkey  inferotemporal neuron population data. We find that view-tuned  model IT units that were trained without any explicit category  information can show category-related tuning as observed in the  experiment. This suggests that the tuning properties of experimental IT  neurons might primarily be shaped by bottom-up stimulus-space  statistics, with little influence of top-down task-specific  information. The population of experimental PFC neurons, on the other hand,  shows tuning properties that cannot be explained just by stimulus  tuning. These analyses are compatible with a model of object recognition  in cortex (Riesenhuber 2000)  in which a population of shape-tuned  neurons provides a general basis for neurons tuned to  different recognition tasks.",1,AI
125,2002,"In a recent experiment, Freedman et al.  recorded from inferotemporal (IT) and prefrontal cortices (PFC) of monkeys  performing a ""cat/dog"" categorization task (Freedman 2001 and  Freedman, Riesenhuber, Poggio, Miller 2001). In this paper we analyze the tuning properties of view-tuned  units in our HMAX model of object recognition in cortex (Riesenhuber  1999) using the same paradigm and stimuli  as in the experiment. We then compare the simulation results to the monkey  inferotemporal neuron population data. We find that view-tuned  model IT units that were trained without any explicit category  information can show category-related tuning as observed in the  experiment. This suggests that the tuning properties of experimental IT  neurons might primarily be shaped by bottom-up stimulus-space  statistics, with little influence of top-down task-specific  information. The population of experimental PFC neurons, on the other hand,  shows tuning properties that cannot be explained just by stimulus  tuning. These analyses are compatible with a model of object recognition  in cortex (Riesenhuber 2000)  in which a population of shape-tuned  neurons provides a general basis for neurons tuned to  different recognition tasks.",140,categorization IT PFC computational neuroscience model HMAX
129,2002,"We describe an adaptive, mid-level approach  to the wireless device power management problem. Our approach  is based on reinforcement learning, a machine learning  framework for autonomous agents. We describe how our  framework can be applied to the power management problem in both  infrastructure and ad~hoc wireless networks. From this thesis we conclude that  mid-level power management policies can outperform low-level policies and  are more convenient to implement than high-level policies. We also  conclude that power management policies need to adapt to the  user and network, and that a mid-level power management framework  based on reinforcement learning fulfills these requirements.",1,AI
129,2002,"We describe an adaptive, mid-level approach  to the wireless device power management problem. Our approach  is based on reinforcement learning, a machine learning  framework for autonomous agents. We describe how our  framework can be applied to the power management problem in both  infrastructure and ad~hoc wireless networks. From this thesis we conclude that  mid-level power management policies can outperform low-level policies and  are more convenient to implement than high-level policies. We also  conclude that power management policies need to adapt to the  user and network, and that a mid-level power management framework  based on reinforcement learning fulfills these requirements.",44,reinforcement learning
129,2002,"We describe an adaptive, mid-level approach  to the wireless device power management problem. Our approach  is based on reinforcement learning, a machine learning  framework for autonomous agents. We describe how our  framework can be applied to the power management problem in both  infrastructure and ad~hoc wireless networks. From this thesis we conclude that  mid-level power management policies can outperform low-level policies and  are more convenient to implement than high-level policies. We also  conclude that power management policies need to adapt to the  user and network, and that a mid-level power management framework  based on reinforcement learning fulfills these requirements.",141,power management
129,2002,"We describe an adaptive, mid-level approach  to the wireless device power management problem. Our approach  is based on reinforcement learning, a machine learning  framework for autonomous agents. We describe how our  framework can be applied to the power management problem in both  infrastructure and ad~hoc wireless networks. From this thesis we conclude that  mid-level power management policies can outperform low-level policies and  are more convenient to implement than high-level policies. We also  conclude that power management policies need to adapt to the  user and network, and that a mid-level power management framework  based on reinforcement learning fulfills these requirements.",142,wireless networks
130,2002,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",1,AI
130,2002,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",143,Tamper-resistant hardware
130,2002,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",144,Microsoft Xbox
130,2002,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",145,Cryptography
130,2002,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",146,Privacy
130,2002,"This paper discusses the hardware foundations of the cryptosystem employed by the Xbox(TM) video game console from Microsoft. A secret boot block overlay is buried within a system ASIC. This secret boot block decrypts and verifies portions of an external FLASH-type ROM. The presence of the secret boot block is camouflaged by a decoy boot block in the external ROM. The code contained within the secret boot block is transferred to the CPU in the clear over a set of high-speed busses where it can be extracted using simple custom hardware. The paper concludes with recommendations for improving the Xbox security system. One lesson of this study is that the use of a high-performance bus alone is not a sufficient security measure, given the advent of inexpensive, fast rapid prototyping services and high-performance FPGAs.",147,Public Key Algos
137,2002,"The furious pace of Moore's Law is driving  computer architecture into a realm where the the speed of light is the  dominant factor in system latencies. The number of clock cycles to span  a chip are increasing, while the number of bits that can be accessed  within a clock cycle is decreasing. Hence, it is becoming more  difficult to hide latency. One alternative solution is to reduce latency by  migrating threads and data, but the overhead of existing  implementations has previously made migration an unserviceable solution so  far.  I present an architecture, implementation, and  mechanisms that reduces the overhead of migration to the point where  migration is a viable supplement to other latency hiding  mechanisms, such as multithreading. The architecture is abstract,  and presents programmers with a simple, uniform fine-grained  multithreaded parallel programming model with implicit memory management. In  other words, the spatial nature and implementation details (such as  the number of processors) of a parallel machine are entirely hidden from  the programmer. Compiler writers are  encouraged to devise programming languages for the machine that guide a  programmer to express their ideas in terms of objects, since objects exhibit  an inherent physical locality of data and code. The machine  implementation can then leverage this locality to automatically distribute  data and threads across the physical machine by using a set of  high performance migration mechanisms.  An implementation of this architecture could  migrate a null thread in 66 cycles -- over a factor of 1000 improvement  over previous work. Performance also scales well; the time  required to move a typical thread is only 4 to 5 times that of a null  thread. Data migration performance is similar, and scales  linearly with data block size. Since the performance of the migration  mechanism is on par with that of an L2 cache, the implementation  simulated in my work has no data caches and relies instead on  multithreading and the migration mechanism to hide and reduce access  latencies.",1,AI
137,2002,"The furious pace of Moore's Law is driving  computer architecture into a realm where the the speed of light is the  dominant factor in system latencies. The number of clock cycles to span  a chip are increasing, while the number of bits that can be accessed  within a clock cycle is decreasing. Hence, it is becoming more  difficult to hide latency. One alternative solution is to reduce latency by  migrating threads and data, but the overhead of existing  implementations has previously made migration an unserviceable solution so  far.  I present an architecture, implementation, and  mechanisms that reduces the overhead of migration to the point where  migration is a viable supplement to other latency hiding  mechanisms, such as multithreading. The architecture is abstract,  and presents programmers with a simple, uniform fine-grained  multithreaded parallel programming model with implicit memory management. In  other words, the spatial nature and implementation details (such as  the number of processors) of a parallel machine are entirely hidden from  the programmer. Compiler writers are  encouraged to devise programming languages for the machine that guide a  programmer to express their ideas in terms of objects, since objects exhibit  an inherent physical locality of data and code. The machine  implementation can then leverage this locality to automatically distribute  data and threads across the physical machine by using a set of  high performance migration mechanisms.  An implementation of this architecture could  migrate a null thread in 66 cycles -- over a factor of 1000 improvement  over previous work. Performance also scales well; the time  required to move a typical thread is only 4 to 5 times that of a null  thread. Data migration performance is similar, and scales  linearly with data block size. Since the performance of the migration  mechanism is on par with that of an L2 cache, the implementation  simulated in my work has no data caches and relies instead on  multithreading and the migration mechanism to hide and reduce access  latencies.",148,HPC parallel computer architecture queues fault tolerance programmability ADAM
138,2002,"Conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. In this  thesis, I introduce sparsely faceted arrays (SFAs), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. Sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the Cray T3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. A sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. By providing simple semantics and  making efficient use of memory, SFAs enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. I present example applications which use  SFAs, and describe and evaluate simple hardware mechanisms for  implementing SFAs.  Keeping track of which nodes have allocated  facets for a particular SFA is an important task that suggests the  need for automatic memory management, including garbage collection.  To address this need, I first argue that conventional tracing  techniques such as mark/sweep and copying GC are inherently unscalable in  parallel systems. I then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. I also discuss opportunities  for hardware support of this garbage collection strategy.  I have implemented a high-level hardware/OS  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. I describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of SFAs and SFA-aware garbage collection. Simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",1,AI
138,2002,"Conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. In this  thesis, I introduce sparsely faceted arrays (SFAs), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. Sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the Cray T3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. A sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. By providing simple semantics and  making efficient use of memory, SFAs enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. I present example applications which use  SFAs, and describe and evaluate simple hardware mechanisms for  implementing SFAs.  Keeping track of which nodes have allocated  facets for a particular SFA is an important task that suggests the  need for automatic memory management, including garbage collection.  To address this need, I first argue that conventional tracing  techniques such as mark/sweep and copying GC are inherently unscalable in  parallel systems. I then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. I also discuss opportunities  for hardware support of this garbage collection strategy.  I have implemented a high-level hardware/OS  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. I describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of SFAs and SFA-aware garbage collection. Simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",149,sparsely faceted arrays
138,2002,"Conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. In this  thesis, I introduce sparsely faceted arrays (SFAs), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. Sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the Cray T3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. A sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. By providing simple semantics and  making efficient use of memory, SFAs enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. I present example applications which use  SFAs, and describe and evaluate simple hardware mechanisms for  implementing SFAs.  Keeping track of which nodes have allocated  facets for a particular SFA is an important task that suggests the  need for automatic memory management, including garbage collection.  To address this need, I first argue that conventional tracing  techniques such as mark/sweep and copying GC are inherently unscalable in  parallel systems. I then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. I also discuss opportunities  for hardware support of this garbage collection strategy.  I have implemented a high-level hardware/OS  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. I describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of SFAs and SFA-aware garbage collection. Simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",150,shared memory
138,2002,"Conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. In this  thesis, I introduce sparsely faceted arrays (SFAs), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. Sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the Cray T3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. A sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. By providing simple semantics and  making efficient use of memory, SFAs enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. I present example applications which use  SFAs, and describe and evaluate simple hardware mechanisms for  implementing SFAs.  Keeping track of which nodes have allocated  facets for a particular SFA is an important task that suggests the  need for automatic memory management, including garbage collection.  To address this need, I first argue that conventional tracing  techniques such as mark/sweep and copying GC are inherently unscalable in  parallel systems. I then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. I also discuss opportunities  for hardware support of this garbage collection strategy.  I have implemented a high-level hardware/OS  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. I describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of SFAs and SFA-aware garbage collection. Simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",151,garbage collection
138,2002,"Conventional parallel computer architectures  do not provide support for non-uniformly distributed objects. In this  thesis, I introduce sparsely faceted arrays (SFAs), a new low-level mechanism for naming regions of memory, or facets, on different  processors in a distributed, shared memory parallel  processing system. Sparsely faceted arrays address the disconnect  between the global distributed arrays provided by conventional architectures  (e.g. the Cray T3 series), and the requirements of high-level  parallel programming methods that wish to use objects that are  distributed over only a subset of processing elements. A sparsely  faceted array names a virtual globally-distributed array, but actual  facets are lazily allocated. By providing simple semantics and  making efficient use of memory, SFAs enable efficient  implementation of a variety of non-uniformly distributed data structures and  related algorithms. I present example applications which use  SFAs, and describe and evaluate simple hardware mechanisms for  implementing SFAs.  Keeping track of which nodes have allocated  facets for a particular SFA is an important task that suggests the  need for automatic memory management, including garbage collection.  To address this need, I first argue that conventional tracing  techniques such as mark/sweep and copying GC are inherently unscalable in  parallel systems. I then present a parallel memory-management  strategy, based on reference-counting, that is capable of garbage  collecting sparsely faceted arrays. I also discuss opportunities  for hardware support of this garbage collection strategy.  I have implemented a high-level hardware/OS  simulator featuring hardware support for sparsely faceted arrays  and automatic garbage collection. I describe the simulator and  outline a few of the numerous details associated with a ""real""  implementation of SFAs and SFA-aware garbage collection. Simulation  results are used throughout this thesis in the evaluation of hardware  support mechanisms.",152,data structures
139,2002,"Stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. In this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  The sequence of buys and sells for a  particular stock, the order flow, we model as an Input-Output Hidden Markov  Model fit to historical data. When combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. Our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. We  demonstrate learning results for two separate real stocks.",1,AI
139,2002,"Stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. In this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  The sequence of buys and sells for a  particular stock, the order flow, we model as an Input-Output Hidden Markov  Model fit to historical data. When combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. Our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. We  demonstrate learning results for two separate real stocks.",153,input/output HMM
139,2002,"Stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. In this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  The sequence of buys and sells for a  particular stock, the order flow, we model as an Input-Output Hidden Markov  Model fit to historical data. When combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. Our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. We  demonstrate learning results for two separate real stocks.",48,market-making
139,2002,"Stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. In this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  The sequence of buys and sells for a  particular stock, the order flow, we model as an Input-Output Hidden Markov  Model fit to historical data. When combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. Our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. We  demonstrate learning results for two separate real stocks.",44,reinforcement learning
139,2002,"Stock markets employ specialized traders,  market-makers, designed to provide liquidity and volume to the market by  constantly supplying both supply and demand. In this paper, we  demonstrate a novel method for modeling the market as a dynamic system  and a reinforcement learning algorithm that learns profitable  market-making strategies when run on this model.  The sequence of buys and sells for a  particular stock, the order flow, we model as an Input-Output Hidden Markov  Model fit to historical data. When combined with the dynamics of  the order book, this creates a highly non-linear and difficult dynamic  system. Our reinforcement learning algorithm, based on likelihood ratios,  is run on this partially-observable environment. We  demonstrate learning results for two separate real stocks.",154,stock order flow model
140,2002,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",1,AI
140,2002,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",155,virtual machine
140,2002,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",156,SECD
140,2002,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",157,SECD machine
140,2002,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",158,denotational proof language
140,2002,"In this thesis, I designed and implemented a  virtual machine (VM) for a monomorphic  variant of Athena, a type-omega denotational  proof language (DPL). This machine  attempts to maintain the minimum state required to evaluate Athena phrases. This  thesis also includes the design and  implementation of a compiler for  monomorphic Athena that compiles to the VM.  Finally, it includes details on my  implementation of a read-eval-print loop that  glues together the VM core and the compiler  to provide a full, user-accessible  interface to monomorphic Athena. The Athena  VM provides the same basis for DPLs that the  SECD machine does for pure, functional  programming and the Warren Abstract Machine does for Prolog.",159,Athena
146,2002,"Evolutionary algorithms are a common tool in  engineering and in the study of natural  evolution. Here we take their use in a new  direction by showing how they can be made to  implement a universal computer. We  consider populations of individuals with  genes whose values are the variables of  interest. By allowing them to interact with one  another in a specified environment with  limited resources, we demonstrate the ability  to construct any arbitrary logic circuit. We  explore models based on the limits of small  and large populations, and show examples of  such a system in action, implementing a  simple logic circuit.",1,AI
150,2002,"The HMAX model has recently been proposed  by Riesenhuber & Poggio as a  hierarchical  model of position- and size-invariant object  recognition in visual cortex. It has also turned  out to model successfully a number of other  properties of the ventral visual stream (the  visual pathway thought to be crucial for object  recognition in cortex), and particularly of (view-tuned) neurons in macaque inferotemporal  cortex, the brain area at the top of the ventral  stream. The original modeling study only  used ``paperclip'' stimuli, as in the  corresponding physiology experiment, and did  not explore systematically how model units'  invariance properties depended on model  parameters. In this study, we aimed at a  deeper understanding of the inner workings of  HMAX and its performance for various  parameter settings and ``natural'' stimulus  classes. We examined HMAX responses for  different stimulus sizes and positions  systematically and found a dependence of  model units' responses on stimulus position  for which a quantitative description is offered.  Interestingly, we find that scale invariance  properties of hierarchical neural models are  not independent of stimulus class, as  opposed to translation invariance, even  though both are affine transformations within  the image plane.",1,AI
151,2002,"The visual recognition of complex movements  and actions is crucial for communication and  survival in many species. Remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. In recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  However, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  This paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. The  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. The  model makes specific predictions that  motivate new experiments.",1,AI
151,2002,"The visual recognition of complex movements  and actions is crucial for communication and  survival in many species. Remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. In recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  However, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  This paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. The  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. The  model makes specific predictions that  motivate new experiments.",160,biological motion
151,2002,"The visual recognition of complex movements  and actions is crucial for communication and  survival in many species. Remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. In recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  However, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  This paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. The  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. The  model makes specific predictions that  motivate new experiments.",161,action recognition
151,2002,"The visual recognition of complex movements  and actions is crucial for communication and  survival in many species. Remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. In recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  However, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  This paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. The  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. The  model makes specific predictions that  motivate new experiments.",162,visual pathways
151,2002,"The visual recognition of complex movements  and actions is crucial for communication and  survival in many species. Remarkable  sensitivity and robustness of biological  motion perception have been demonstrated in  psychophysical experiments. In recent years,  neurons and cortical areas involved in action  recognition have been identified in  neurophysiological and imaging studies.  However, the detailed neural mechanisms  that underlie the recognition of such complex  movement patterns remain largely unknown.  This paper reviews the experimental results  and summarizes them in terms of a  biologically plausible neural model. The  model is based on the key assumption that  action recognition is based on learned  prototypical patterns and exploits information  from the ventral and the dorsal pathway. The  model makes specific predictions that  motivate new experiments.",163,hierarchical processing
152,2002,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",1,AI
152,2002,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",164,direction
152,2002,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",165,visual cortex
152,2002,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",166,nonlinear dynamics
152,2002,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",167,lurching waves
152,2002,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",134,stability
152,2002,"Different theoretical models have tried to  investigate the feasibility of recurrent neural  mechanisms for achieving direction selectivity  in the visual cortex. The mathematical  analysis of such models has been restricted  so far to the case of purely linear networks.  We present an exact analytical solution of the  nonlinear dynamics of a class of direction  selective recurrent neural models with  threshold nonlinearity. Our mathematical  analysis shows that such networks have  form-stable stimulus-locked traveling pulse  solutions that are appropriate for modeling  the responses of direction selective cortical  neurons. Our analysis shows also that the  stability of such solutions can break down  giving raise to a different class of solutions  (""lurching activity waves"") that are  characterized by a specific spatio-temporal  periodicity. These solutions cannot arise in  models for direction selectivity with purely  linear spatio-temporal filtering.",168,recurre
155,2002,"A common objective in learning a model from  data is to recover its network structure, while the model  parameters are of minor interest. For example, we may wish to recover  regulatory networks from high-throughput data sources. In this paper  we examine how Bayesian regularization using a Dirichlet prior over the  model parameters affects the learned model structure in a  domain with discrete variables. Surprisingly, a weak prior in the  sense of smaller equivalent sample size leads to a strong  regularization of the model structure (sparse graph) given a sufficiently  large data set. In particular, the empty graph is obtained in the  limit of a vanishing strength of prior belief. This is  diametrically opposite to what one may expect in this limit, namely the  complete graph from an (unregularized) maximum likelihood estimate.  Since the prior affects the parameters as expected, the prior strength  balances a ""trade-off"" between regularizing the parameters or the  structure of the model. We demonstrate the benefits of optimizing this  trade-off in the sense of predictive accuracy.",1,AI
155,2002,"A common objective in learning a model from  data is to recover its network structure, while the model  parameters are of minor interest. For example, we may wish to recover  regulatory networks from high-throughput data sources. In this paper  we examine how Bayesian regularization using a Dirichlet prior over the  model parameters affects the learned model structure in a  domain with discrete variables. Surprisingly, a weak prior in the  sense of smaller equivalent sample size leads to a strong  regularization of the model structure (sparse graph) given a sufficiently  large data set. In particular, the empty graph is obtained in the  limit of a vanishing strength of prior belief. This is  diametrically opposite to what one may expect in this limit, namely the  complete graph from an (unregularized) maximum likelihood estimate.  Since the prior affects the parameters as expected, the prior strength  balances a ""trade-off"" between regularizing the parameters or the  structure of the model. We demonstrate the benefits of optimizing this  trade-off in the sense of predictive accuracy.",169,Regularization
155,2002,"A common objective in learning a model from  data is to recover its network structure, while the model  parameters are of minor interest. For example, we may wish to recover  regulatory networks from high-throughput data sources. In this paper  we examine how Bayesian regularization using a Dirichlet prior over the  model parameters affects the learned model structure in a  domain with discrete variables. Surprisingly, a weak prior in the  sense of smaller equivalent sample size leads to a strong  regularization of the model structure (sparse graph) given a sufficiently  large data set. In particular, the empty graph is obtained in the  limit of a vanishing strength of prior belief. This is  diametrically opposite to what one may expect in this limit, namely the  complete graph from an (unregularized) maximum likelihood estimate.  Since the prior affects the parameters as expected, the prior strength  balances a ""trade-off"" between regularizing the parameters or the  structure of the model. We demonstrate the benefits of optimizing this  trade-off in the sense of predictive accuracy.",170,Dirichlet Prior
156,2002,"The goal of low-level vision is to estimate an  underlying scene, given an observed image. Real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. We propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. Shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. We describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) Shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",1,AI
156,2002,"The goal of low-level vision is to estimate an  underlying scene, given an observed image. Real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. We propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. Shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. We describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) Shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",41,scene representation
156,2002,"The goal of low-level vision is to estimate an  underlying scene, given an observed image. Real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. We propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. Shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. We describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) Shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",171,shape
156,2002,"The goal of low-level vision is to estimate an  underlying scene, given an observed image. Real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. We propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. Shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. We describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) Shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",132,stereo
156,2002,"The goal of low-level vision is to estimate an  underlying scene, given an observed image. Real-world scenes  (e.g., albedos or shapes) can be very complex, conventionally requiring  high dimensional representations which are hard to estimate  and store. We propose a low-dimensional representation, called a  scene recipe, that relies on the image itself to describe the  complex scene configurations. Shape recipes are an  example: these are the regression coefficients that predict the  bandpassed shape from bandpassed image data. We describe the  benefits of this representation, and show two uses  illustrating their properties: (1) we improve stereo shape estimates by  learning shape recipes at low resolution and applying them at full resolution;  (2) Shape recipes implicitly contain information about lighting  and materials and we use them for material segmentation.",172,shape recipes
157,2002,"We present an algorithm that uses multiple  cues to recover shading and reflectance intrinsic images from a single  image. Using both color information and a classifier trained to  recognize gray-scale patterns, each image derivative is classified as being  caused by shading or a change in the surface's reflectance.  Generalized Belief Propagation is then used to propagate information from  areas where the correct classification is clear to areas where it is  ambiguous. We also show results on real images.",1,AI
157,2002,"We present an algorithm that uses multiple  cues to recover shading and reflectance intrinsic images from a single  image. Using both color information and a classifier trained to  recognize gray-scale patterns, each image derivative is classified as being  caused by shading or a change in the surface's reflectance.  Generalized Belief Propagation is then used to propagate information from  areas where the correct classification is clear to areas where it is  ambiguous. We also show results on real images.",173,intrinisic images
157,2002,"We present an algorithm that uses multiple  cues to recover shading and reflectance intrinsic images from a single  image. Using both color information and a classifier trained to  recognize gray-scale patterns, each image derivative is classified as being  caused by shading or a change in the surface's reflectance.  Generalized Belief Propagation is then used to propagate information from  areas where the correct classification is clear to areas where it is  ambiguous. We also show results on real images.",174,reflectance estimation
158,2002,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",1,AI
158,2002,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",175,Facial Expression Recognition
158,2002,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",176,Pose Estimation
158,2002,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",177,Viseme Recognition
158,2002,"This thesis proposes a methodology for the  design of man-machine interfaces by combining top-down and  bottom-up processes in vision. From a computational perspective, we  propose that the scientific-cognitive question of combining top-down and bottom-up knowledge is similar to the engineering  question of labeling a training set in a supervised learning problem.  We investigate these questions in the realm  of facial analysis. We propose the use of a linear morphable model  (LMM) for representing top-down structure and use it to model  various facial variations such as mouth shapes and expression, the pose of  faces and visual speech (visemes). We apply a supervised learning  method based on support vector machine (SVM) regression for  estimating the parameters of LMMs directly from pixel-based representations of  faces. We combine these methods for designing new, more self-contained systems for recognizing facial expressions, estimating facial pose and  for recognizing visemes.",54,SVM
163,2002,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",1,AI
163,2002,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",97,illumination
163,2002,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",68,reflectance
163,2002,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",98,natural image statistics
163,2002,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",31,vision
163,2002,"Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ""look like"" in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions.   Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature.   These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.",178,materials
164,2002,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",1,AI
164,2002,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",179,mode estimation
164,2002,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",180,compilation
164,2002,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",181,model-based
164,2002,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",182,reasoning
164,2002,"As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing  these missions are becoming progressively more complex. This  increases the potential for mission failure due to hardware malfunctions  and unexpected spacecraft behavior. A solution to this problem lies in the  development of an advanced fault management system. Fault  management enables spacecraft to respond to failures and take repair  actions so that it may continue its mission. The two main approaches  developed for spacecraft fault management have been rule-based and  model-based systems. Rules map sensor information to system  behaviors, thus achieving fast response times, and making the actions of  the fault management system explicit. These rules are developed by  having a human reason through the interactions between spacecraft  components. This process is limited by the number of interactions a  human can reason about correctly. In the model-based approach, the  human provides component models, and the fault management system  reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit  the underlying system models, whereas these are implicit in the rule-based approach. We propose a fault detection engine, Compiled Mode  Estimation (CME) that unifies the strengths of the rule-based and model-based approaches. CME uses a compiled model to determine spacecraft  behavior more accurately. Reasoning related to fault detection is  compiled in an off-line process into a set of concurrent, localized  diagnostic rules. These are then combined on-line along with sensor  information to reconstruct the diagnosis of the system. These rules  enable a human to inspect the diagnostic consequences of CME.  Additionally, CME is capable of reasoning through component  interactions automatically and still provide fast and correct responses.  The implementation of this engine has been tested against the NEAR  spacecraft advanced rule-based system, resulting in detection of failures  beyond that of the rules. This evolution in fault detection will enable future  missions to explore the furthest reaches of the solar system without the  burden of human intervention to repair failed components.",183,autonomy
170,2002,"Classical mechanics is deceptively simple. It  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. To address this problem we  use computational techniques to communicate a deeper  understanding of Classical Mechanics. Computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. Expressing the methods in a computer language forces them to be  unambiguous and computationally effective. The task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. Also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",1,AI
170,2002,"Classical mechanics is deceptively simple. It  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. To address this problem we  use computational techniques to communicate a deeper  understanding of Classical Mechanics. Computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. Expressing the methods in a computer language forces them to be  unambiguous and computationally effective. The task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. Also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",184,Education
170,2002,"Classical mechanics is deceptively simple. It  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. To address this problem we  use computational techniques to communicate a deeper  understanding of Classical Mechanics. Computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. Expressing the methods in a computer language forces them to be  unambiguous and computationally effective. The task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. Also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",185,Mechanics
170,2002,"Classical mechanics is deceptively simple. It  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. To address this problem we  use computational techniques to communicate a deeper  understanding of Classical Mechanics. Computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. Expressing the methods in a computer language forces them to be  unambiguous and computationally effective. The task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. Also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",186,Functional Programming
170,2002,"Classical mechanics is deceptively simple. It  is surprisingly easy to get the right answer with fallacious reasoning  or without real understanding. To address this problem we  use computational techniques to communicate a deeper  understanding of Classical Mechanics. Computational algorithms are  used to express the methods used in the analysis of dynamical  phenomena. Expressing the methods in a computer language forces them to be  unambiguous and computationally effective. The task of  formulating a method as a computer-executable program and debugging  that program is a powerful exercise in the learning process. Also, once  formalized procedurally, a mathematical idea becomes a tool that can  be used directly to compute results.",187,Symbolic Mathematics
171,2002,"Cyclic changes in the shape of a quasi-rigid  body on a curved manifold can lead to net translation and/or  rotation of the body in the manifold. Presuming space-time is a  curved manifold as portrayed by general relativity, translation in space can  be accomplished simply by cyclic changes in the shape of a body,  without any thrust or external forces.",1,AI
176,2002,"In many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. Stochastic methods such as particle filters  provide an appealing alternative. However, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. Drawing on ideas from regularized particle  filters and belief propagation (BP), this paper develops a  nonparametric belief propagation (NBP) algorithm applicable to  general graphs. Each NBP iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. The algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. Thus, NBP  extends particle filtering methods to the more general vision  problems that graphical models can describe. We apply the NBP  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",1,AI
176,2002,"In many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. Stochastic methods such as particle filters  provide an appealing alternative. However, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. Drawing on ideas from regularized particle  filters and belief propagation (BP), this paper develops a  nonparametric belief propagation (NBP) algorithm applicable to  general graphs. Each NBP iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. The algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. Thus, NBP  extends particle filtering methods to the more general vision  problems that graphical models can describe. We apply the NBP  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",188,graphical model
176,2002,"In many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. Stochastic methods such as particle filters  provide an appealing alternative. However, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. Drawing on ideas from regularized particle  filters and belief propagation (BP), this paper develops a  nonparametric belief propagation (NBP) algorithm applicable to  general graphs. Each NBP iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. The algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. Thus, NBP  extends particle filtering methods to the more general vision  problems that graphical models can describe. We apply the NBP  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",189,belief propagation
176,2002,"In many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. Stochastic methods such as particle filters  provide an appealing alternative. However, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. Drawing on ideas from regularized particle  filters and belief propagation (BP), this paper develops a  nonparametric belief propagation (NBP) algorithm applicable to  general graphs. Each NBP iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. The algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. Thus, NBP  extends particle filtering methods to the more general vision  problems that graphical models can describe. We apply the NBP  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",190,nonparametric inference
176,2002,"In many applications of graphical models  arising in computer vision, the hidden variables of interest are most  naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to  these continuous distributions, but for the high-dimensional  variables typically of interest, discrete inference becomes  infeasible. Stochastic methods such as particle filters  provide an appealing alternative. However, existing techniques fail  to exploit the rich structure of the graphical models describing  many vision problems. Drawing on ideas from regularized particle  filters and belief propagation (BP), this paper develops a  nonparametric belief propagation (NBP) algorithm applicable to  general graphs. Each NBP iteration uses an efficient sampling procedure  to update kernel-based approximations to the true, continuous  likelihoods. The algorithm can accomodate an extremely broad class of  potential functions, including nonparametric representations. Thus, NBP  extends particle filtering methods to the more general vision  problems that graphical models can describe. We apply the NBP  algorithm to infer component interrelationships in a parts-based face  model, allowing location and reconstruction of occluded features.",31,vision
177,2002,"I present a system for robust leaderless  organization of an amorphous network into hierarchical clusters. This  system, which assumes that nodes are spatially embedded and can only  talk to neighbors within a given radius, scales to networks of arbitrary  size and converges rapidly. The amount of data stored at each  node is logarithmic in the diameter of the network, and the hierarchical  structure produces an addressing scheme such that there is an  invertible relation between distance and address for any pair of nodes.  The system adapts automatically to stopping failures, network  partition, and reorganization.",1,AI
177,2002,"I present a system for robust leaderless  organization of an amorphous network into hierarchical clusters. This  system, which assumes that nodes are spatially embedded and can only  talk to neighbors within a given radius, scales to networks of arbitrary  size and converges rapidly. The amount of data stored at each  node is logarithmic in the diameter of the network, and the hierarchical  structure produces an addressing scheme such that there is an  invertible relation between distance and address for any pair of nodes.  The system adapts automatically to stopping failures, network  partition, and reorganization.",191,amorphous computing hierarchy leaderless distributed
178,2002,"Binary image classifiction is a problem that  has received much attention  in recent years. In this paper we evaluate a  selection of popular  techniques in an effort to find a feature set/ classifier combination which  generalizes well to full resolution image data.  We then apply that system  to images at one-half through one-sixteenth  resolution, and consider the  corresponding error rates. In addition, we  further observe generalization  performance as it depends on the number of  training images, and lastly,  compare the system's best error rates to that  of a human performing an  identical classification task given teh same  set of test images.",1,AI
179,2002,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",1,AI
179,2002,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",192,Theory of Learning
179,2002,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",193,Great Discoveries
179,2002,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",194,Consistency
179,2002,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",195,ERM
179,2002,"Solutions of learning problems by Empirical  Risk  Minimization (ERM) need to be consistent, so  that they  may be predictive. They also need to be well-posed, so  that they can be used robustly. We show that  a statistical form  of well-posedness, defined in terms of the  key property of  L-stability, is necessary and sufficient for  consistency of ERM.",196,Stability
180,2002,"In low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. The  approach proposed here is to let the image itself bear as much of the  representational burden as possible. In many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. The scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. We illustrate the use of this  representation for encoding shape information. We show how  this representation has appealing properties such as locality and  slow variation across space and scale. These properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",1,AI
180,2002,"In low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. The  approach proposed here is to let the image itself bear as much of the  representational burden as possible. In many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. The scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. We illustrate the use of this  representation for encoding shape information. We show how  this representation has appealing properties such as locality and  slow variation across space and scale. These properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",197,shape from X
180,2002,"In low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. The  approach proposed here is to let the image itself bear as much of the  representational burden as possible. In many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. The scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. We illustrate the use of this  representation for encoding shape information. We show how  this representation has appealing properties such as locality and  slow variation across space and scale. These properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",41,scene representation
180,2002,"In low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. The  approach proposed here is to let the image itself bear as much of the  representational burden as possible. In many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. The scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. We illustrate the use of this  representation for encoding shape information. We show how  this representation has appealing properties such as locality and  slow variation across space and scale. These properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",172,shape recipes
180,2002,"In low-level vision, the representation of scene  properties such as shape, albedo, etc., are very high  dimensional as they have to describe complicated structures. The  approach proposed here is to let the image itself bear as much of the  representational burden as possible. In many situations, scene and  image are closely related and it is possible to find a functional relationship  between them. The scene information can be represented in  reference to the image where the functional specifies how to translate the  image into the associated scene. We illustrate the use of this  representation for encoding shape information. We show how  this representation has appealing properties such as locality and  slow variation across space and scale. These properties provide a way of  improving shape estimates coming from other sources of information like  stereo.",132,stereo
181,2002,"A difficulty in the design of automated text  summarization   algorithms is in the objective evaluation.  Viewing summarization   as a tradeoff between length and  information content, we introduce   a technique based on a hierarchy of  classifiers to rank, through   model selection, different summarization  methods. This summary   evaluation technique allows for broader  comparison of   summarization methods than the traditional  techniques of summary   evaluation. We present an empirical study  of two simple, albeit   widely used, summarization methods that  shows the different usages   of this automated task-based evaluation  system and confirms the   results obtained with human-based  evaluation methods over smaller   corpora.",1,AI
182,2002,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",1,AI
182,2002,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",198,parallel
182,2002,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",199,network
182,2002,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",200,simulation
182,2002,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",201,hashing
182,2002,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",202,multithreading
182,2002,"Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or  even billions of nodes. Associated with such large systems is a new set of  design challenges. Many problems must be addressed by an architecture in  order for it to be successful; of these, we focus on three in particular.  First, a scalable memory system is required. Second, the network messaging  protocol must be fault-tolerant. Third, the overheads of thread creation,  thread management and synchronization must be extremely low.  This thesis presents the complete system design for Hamal, a shared-memory  architecture which addresses these concerns and is directly scalable to one  million nodes. Virtual memory and distributed objects are implemented in a  manner that requires neither inter-node synchronization nor the storage of  globally coherent translations at each node. We develop a lightweight  fault-tolerant messaging protocol that guarantees message delivery and  idempotence across a discarding network. A number of hardware mechanisms  provide efficient support for massive multithreading and fine-grained  synchronization.  Experiments are conducted in simulation, using a trace-driven network  simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters  for the messaging protocol which optimize performance. A discarding network  is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network.  Our simulations of Hamal demonstrate the effectiveness of its thread  management and synchronization primitives. In particular, we find  register-based synchronization to be an extremely efficient mechanism which  can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.",203,synchronization
192,2003,"We study the frequent problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving {\\em weighted} low rank approximation problems, which, unlike simple matrix factorization problems, do not admit a closed form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low rank representation, and extend the formulation to non-Gaussian noise models such as classification (collaborative filtering).",1,AI
192,2003,"We study the frequent problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving {\\em weighted} low rank approximation problems, which, unlike simple matrix factorization problems, do not admit a closed form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low rank representation, and extend the formulation to non-Gaussian noise models such as classification (collaborative filtering).",204,svd pca
195,2003,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",1,AI
195,2003,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",205,sketch
195,2003,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",206,design
195,2003,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",207,multimodal
195,2003,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",208,disambiguation
195,2003,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",209,segmentation
195,2003,"Sketches are commonly used in the early stages of  design. Our previous system allows users to sketch mechanical systems that  the computer interprets. However, some parts of the mechanical  system might be too hard or too complicated to express in the sketch.  Adding speech recognition to create a multimodal system would move  us toward our goal of creating a more natural user interface. This  thesis examines the relationship between the verbal and sketch input,  particularly how to segment and align the two inputs. Toward this end,  subjects were recorded while they sketched and talked. These  recordings were transcribed, and a set of rules to perform segmentation  and alignment was created. These rules represent the knowledge that  the computer needs to perform segmentation and alignment. The  rules successfully interpreted the 24 data sets that they were given.",210,alignment
196,2003,"Biological systems exhibit rich and complex behavior through the orchestrated interplay of a large array of components. It is hypothesized that separable subsystems with some degree of functional autonomy exist; deciphering their independent behavior and functionality would greatly facilitate understanding the system as a whole. Discovering and analyzing such subsystems are hence pivotal problems in the quest to gain a quantitative understanding of complex biological systems.  In this work, using approaches from machine learning, physics and graph theory, methods for the identification and analysis of such subsystems were developed. A novel methodology, based on a recent machine learning algorithm known as non-negative matrix factorization (NMF), was developed to discover such subsystems in a set of large-scale gene expression data. This set of subsystems was then used to predict functional relationships between genes, and this approach was shown to score significantly higher than conventional methods when benchmarking them against existing databases. Moreover, a mathematical treatment was developed to treat simple network subsystems based only on their topology (independent of particular parameter values). Application to a problem of experimental interest demonstrated the need for extentions to the conventional model to fully explain the experimental data.  Finally, the notion of a subsystem was evaluated from a topological perspective. A number of different protein networks were examined to analyze their topological properties with respect to separability, seeking to find separable subsystems. These networks were shown to exhibit separability in a nonintuitive fashion, while the separable subsystems were of strong biological significance. It was demonstrated that the separability property found was not due to incomplete or biased data, but is likely to reflect biological structure.",1,AI
197,2003,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",1,AI
197,2003,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",211,knowledge acquisition
197,2003,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",212,knowledge capture
197,2003,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",213,analogy
197,2003,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",214,natural language
197,2003,"The goal of the work reported here is to capture the  commonsense knowledge of non-expert human  contributors. Achieving this goal will enable more  intelligent human-computer interfaces and pave the  way for computers to reason about our world. In the  domain of natural language processing, it will provide  the world knowledge much needed for semantic  processing of natural language. To acquire knowledge  from contributors not trained in knowledge engineering,  I take the following four steps: (i) develop a knowledge  representation (KR) model for simple assertions in  natural language, (ii) introduce cumulative analogy, a  class of nearest-neighbor based analogical reasoning  algorithms over this representation, (iii) argue that  cumulative analogy is well suited for knowledge  acquisition (KA) based on a theoretical analysis of  effectiveness of KA with this approach, and (iv) test the  KR model and the effectiveness of the cumulative  analogy algorithms empirically. To investigate  effectiveness of cumulative analogy for KA empirically,  Learner, an open source system for KA by cumulative  analogy has been implemented, deployed, and  evaluated. (The site ""1001 Questions,"" is available at  http://teach-computers.org/learner.html). Learner  acquires assertion-level knowledge by constructing  shallow semantic analogies between a KA topic and its  nearest neighbors and posing these analogies as  natural language questions to human contributors.  Suppose, for example, that based on the knowledge  about ""newspapers"" already present in the knowledge  base, Learner judges ""newspaper"" to be similar to  ""book"" and ""magazine."" Further suppose that  assertions ""books contain information"" and ""magazines  contain information"" are also already in the knowledge  base. Then Learner will use cumulative analogy from  the similar topics to ask humans whether ""newspapers  contain information."" Because similarity between topics  is computed based on what is already known about  them, Learner exhibits bootstrapping behavior --- the  quality of its questions improves as it gathers more  knowledge. By summing evidence for and against  posing any given question, Learner also exhibits noise  tolerance, limiting the effect of incorrect similarities. The  KA power of shallow semantic analogy from nearest  neighbors is one of the main findings of this thesis. I  perform an analysis of commonsense knowledge  collected by another research effort that did not rely on  analogical reasoning and demonstrate that indeed  there is sufficient amount of correlation in the  knowledge base to motivate using cumulative analogy  from nearest neighbors as a KA method. Empirically,  evaluating the percentages of questions answered  affirmatively, negatively and judged to be nonsensical  in the cumulative analogy case compares favorably  with the baseline, no-similarity case that relies on  random objects rather than nearest neighbors. Of the  questions generated by cumulative analogy,  contributors answered 45% affirmatively, 28%  negatively and marked 13% as nonsensical; in the  control, no-similarity case 8% of questions were  answered affirmatively, 60% negatively and 26% were  marked as nonsensical.",182,reasoning
198,2003,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",1,AI
198,2003,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",215,POMDP
198,2003,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",216,policy search
198,2003,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",217,adaptive systems
198,2003,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",44,reinforcement learning
198,2003,"One objective of artificial intelligence is to model the  behavior of an intelligent agent interacting with its environment. The  environment's transformations can be modeled as a Markov chain,  whose state is partially observable to the agent and affected by its actions;  such processes are known as partially observable Markov decision processes  (POMDPs). While the environment's dynamics are assumed to obey certain  rules, the agent does not know them and must learn.  In this dissertation we focus on the agent's adaptation  as captured by the reinforcement learning framework. This means learning  a policy---a mapping of observations into actions---based  on feedback from the environment. The learning can be viewed as browsing  a set of policies while evaluating them by trial through interaction with the  environment.  The set of policies is constrained by the architecture of  the agent's controller. POMDPs require a controller to have  a memory. We investigate controllers with memory, including  controllers with  external memory, finite state controllers and distributed  controllers for multi-agent systems. For these various  controllers we work out the details of the algorithms which learn by ascending  the gradient of expected cumulative reinforcement.   Building on statistical learning theory and experiment  design theory, a policy evaluation algorithm is developed for the case of  experience re-use. We address the question of sufficient experience for  uniform convergence of policy evaluation and obtain sample complexity bounds  for various estimators. Finally, we demonstrate the performance of the  proposed algorithms on several domains, the most complex of which is simulated  adaptive packet routing in a telecommunication network.",218,adaptive behavior
199,2003,"In this paper, we present an approach to discretizing  multivariate continuous data while learning the  structure of a graphical model. We derive the joint  scoring function from the principle of predictive  accuracy, which inherently ensures the optimal trade-off between goodness of fit and model complexity  (including the number of discretization levels). Using  the so-called finest grid implied by the data, our scoring  function depends only on the number of data points in  the various discretization levels. Not only can it be  computed efficiently, but it is also independent of the  metric used in the continuous space. Our experiments  with gene expression data show that discretization  plays a crucial role regarding the resulting network  structure.",1,AI
199,2003,"In this paper, we present an approach to discretizing  multivariate continuous data while learning the  structure of a graphical model. We derive the joint  scoring function from the principle of predictive  accuracy, which inherently ensures the optimal trade-off between goodness of fit and model complexity  (including the number of discretization levels). Using  the so-called finest grid implied by the data, our scoring  function depends only on the number of data points in  the various discretization levels. Not only can it be  computed efficiently, but it is also independent of the  metric used in the continuous space. Our experiments  with gene expression data show that discretization  plays a crucial role regarding the resulting network  structure.",219,Discretization
199,2003,"In this paper, we present an approach to discretizing  multivariate continuous data while learning the  structure of a graphical model. We derive the joint  scoring function from the principle of predictive  accuracy, which inherently ensures the optimal trade-off between goodness of fit and model complexity  (including the number of discretization levels). Using  the so-called finest grid implied by the data, our scoring  function depends only on the number of data points in  the various discretization levels. Not only can it be  computed efficiently, but it is also independent of the  metric used in the continuous space. Our experiments  with gene expression data show that discretization  plays a crucial role regarding the resulting network  structure.",220,Graphical models
200,2003,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",1,AI
200,2003,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",221,visual speech
200,2003,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",222,speech animation
200,2003,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",223,face animation
200,2003,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",224,image morphing
200,2003,"abstract  With many visual speech animation techniques now  available, there is a clear need for systematic  perceptual evaluation schemes. We describe here our  scheme and its application to a new video-realistic  (potentially indistinguishable from real recorded video)  visual-speech animation system, called Mary 101.  Two types of experiments were performed: a)  distinguishing visually between real and synthetic  image- sequences of the same utterances, (""Turing  tests"") and b) gauging visual speech recognition by  comparing lip-reading performance of the real and  synthetic image-sequences of the same utterances  (""Intelligibility tests"").  Subjects that were presented randomly with either real  or synthetic image-sequences could not tell the  synthetic from the real sequences above chance level.  The same subjects when asked to lip-read the  utterances from the same image-sequences  recognized speech from real image-sequences  significantly better than from synthetic ones. However,  performance for both, real and synthetic, were at levels  suggested in the literature on lip-reading. We conclude  from the two experiments that the animation of Mary  101 is adequate for providing a percept of a talking  head. However, additional effort is required to improve  the animation for lip-reading purposes like  rehabilitation and language learning.  In addition, these two tasks could be considered as  explicit and implicit perceptual discrimination tasks. In  the explicit task (a), each stimulus is classified directly  as a synthetic or real image-sequence by detecting a  possible difference between the synthetic and the real  image-sequences. The implicit perceptual  discrimination task (b) consists of a comparison  between visual recognition of speech of real and  synthetic image-sequences. Our results suggest that  implicit perceptual discrimination is a more sensitive  method for discrimination between synthetic and real  image-sequences than explicit perceptual  discrimination.",225,lip reading
206,2003,"The central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. Notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. Past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. The results obtained are  therefore most relevant for understanding the  identification of faces at close range. However, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. Here, we report experiments  designed to address this issue. Our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  While we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. These  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",1,AI
206,2003,"The central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. Notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. Past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. The results obtained are  therefore most relevant for understanding the  identification of faces at close range. However, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. Here, we report experiments  designed to address this issue. Our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  While we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. These  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",126,Face recognition
206,2003,"The central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. Notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. Past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. The results obtained are  therefore most relevant for understanding the  identification of faces at close range. However, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. Here, we report experiments  designed to address this issue. Our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  While we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. These  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",226,features
206,2003,"The central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. Notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. Past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. The results obtained are  therefore most relevant for understanding the  identification of faces at close range. However, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. Here, we report experiments  designed to address this issue. Our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  While we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. These  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",227,low resolution
206,2003,"The central challenge in face recognition lies in  understanding the role different facial features play in  our judgments of identity. Notable in this regard are the  relative contributions of the internal (eyes, nose and  mouth) and external (hair and jaw-line) features. Past  studies that have investigated this issue have typically  used high-resolution images or good-quality line  drawings as facial stimuli. The results obtained are  therefore most relevant for understanding the  identification of faces at close range. However, given  that real-world viewing conditions are rarely optimal, it  is also important to know how image degradations,  such as loss of resolution caused by large viewing  distances, influence our ability to use internal and  external features. Here, we report experiments  designed to address this issue. Our data characterize  how the relative contributions of internal and external  features change as a function of image resolution.  While we replicated results of previous studies that  have shown internal features of familiar faces to be  more useful for recognition than external features at  high resolution, we found that the two feature sets  reverse in importance as resolution decreases. These  results suggest that the visual system uses a highly  non-linear cue-fusion strategy in combining internal  and external features along the dimension of image  resolution and that the configural cues that relate the  two feature sets play an important role in judgments of  facial identity.",228,degraded images
207,2003,"In a Communication Bootstrapping system, peer  components with different perceptual worlds invent symbols and syntax  based on correlations between their percepts. I propose that  Communication Bootstrapping can also be used to acquire functional  definitions of words and causal reasoning knowledge. I illustrate this  point with several examples, then sketch the architecture of a  system in progress which attempts to execute this task.",1,AI
208,2003,"While navigating in an environment, a vision system  has to be able to recognize where it is and what the  main objects in the scene are. In this paper we present  a context-based vision system for place and object  recognition. The goal is to identify familiar locations  (e.g., office 610, conference room 941, Main Street), to  categorize new environments (office, corridor, street)  and to use that information to provide contextual priors  for object recognition (e.g., table, chair, car, computer). We present a low-dimensional global image representation that provides  relevant information for place recognition and  categorization, and how such contextual information  introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60  locations (indoors and outdoors) and to suggest the  presence and locations of more than 20 different object  types. The algorithm has been integrated into a mobile system that provides real-time feedback to the  user.",1,AI
208,2003,"While navigating in an environment, a vision system  has to be able to recognize where it is and what the  main objects in the scene are. In this paper we present  a context-based vision system for place and object  recognition. The goal is to identify familiar locations  (e.g., office 610, conference room 941, Main Street), to  categorize new environments (office, corridor, street)  and to use that information to provide contextual priors  for object recognition (e.g., table, chair, car, computer). We present a low-dimensional global image representation that provides  relevant information for place recognition and  categorization, and how such contextual information  introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60  locations (indoors and outdoors) and to suggest the  presence and locations of more than 20 different object  types. The algorithm has been integrated into a mobile system that provides real-time feedback to the  user.",229,context-based vision
208,2003,"While navigating in an environment, a vision system  has to be able to recognize where it is and what the  main objects in the scene are. In this paper we present  a context-based vision system for place and object  recognition. The goal is to identify familiar locations  (e.g., office 610, conference room 941, Main Street), to  categorize new environments (office, corridor, street)  and to use that information to provide contextual priors  for object recognition (e.g., table, chair, car, computer). We present a low-dimensional global image representation that provides  relevant information for place recognition and  categorization, and how such contextual information  introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60  locations (indoors and outdoors) and to suggest the  presence and locations of more than 20 different object  types. The algorithm has been integrated into a mobile system that provides real-time feedback to the  user.",230,place recognition
208,2003,"While navigating in an environment, a vision system  has to be able to recognize where it is and what the  main objects in the scene are. In this paper we present  a context-based vision system for place and object  recognition. The goal is to identify familiar locations  (e.g., office 610, conference room 941, Main Street), to  categorize new environments (office, corridor, street)  and to use that information to provide contextual priors  for object recognition (e.g., table, chair, car, computer). We present a low-dimensional global image representation that provides  relevant information for place recognition and  categorization, and how such contextual information  introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60  locations (indoors and outdoors) and to suggest the  presence and locations of more than 20 different object  types. The algorithm has been integrated into a mobile system that provides real-time feedback to the  user.",123,object recognition
212,2003,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",1,AI
212,2003,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",231,information seeking
212,2003,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",232,search
212,2003,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",233,orienteering
212,2003,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",63,context
212,2003,"We report on a study of how people look for information within email, files, and the Web. When locating a document or searching for a specific answer, people relied on their contextual knowledge of their information target to help them find it, often associating the target with a specific document. They appeared to prefer to use this contextual information as a guide in navigating locally in small steps to the desired document rather than directly jumping to their target. We found this behavior was especially true for people with unstructured information organization. We discuss the implications of our findings for the design of personal information management tools.",234,Semantic Web
213,2003,"A Persistent Node is a redundant distributed mechanism for storing a key/value pair reliably in a geographically local network. In this paper, I develop a method of establishing Persistent Nodes in an amorphous matrix. I address issues of construction, usage, atomicity guarantees and reliability in the face of stopping failures. Applications include routing, congestion control, and data storage in gigascale networks.",1,AI
213,2003,"A Persistent Node is a redundant distributed mechanism for storing a key/value pair reliably in a geographically local network. In this paper, I develop a method of establishing Persistent Nodes in an amorphous matrix. I address issues of construction, usage, atomicity guarantees and reliability in the face of stopping failures. Applications include routing, congestion control, and data storage in gigascale networks.",235,robust atomic distributed amorphous
214,2003,"We present an image-based approach to infer 3D  structure parameters using a probabilistic ""shape+structure''  model. The 3D shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. Bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. We  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. Model matching and  parameter inference are done entirely in the image domain and require no  explicit 3D construction. Our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. Using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3D  locations of 19 joints on the body based on observed silhouette  contours from real images.",1,AI
214,2003,"We present an image-based approach to infer 3D  structure parameters using a probabilistic ""shape+structure''  model. The 3D shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. Bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. We  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. Model matching and  parameter inference are done entirely in the image domain and require no  explicit 3D construction. Our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. Using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3D  locations of 19 joints on the body based on observed silhouette  contours from real images.",236,3D structure
214,2003,"We present an image-based approach to infer 3D  structure parameters using a probabilistic ""shape+structure''  model. The 3D shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. Bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. We  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. Model matching and  parameter inference are done entirely in the image domain and require no  explicit 3D construction. Our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. Using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3D  locations of 19 joints on the body based on observed silhouette  contours from real images.",237,statistical shape model
214,2003,"We present an image-based approach to infer 3D  structure parameters using a probabilistic ""shape+structure''  model. The 3D shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. Bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. We  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. Model matching and  parameter inference are done entirely in the image domain and require no  explicit 3D construction. Our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. Using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3D  locations of 19 joints on the body based on observed silhouette  contours from real images.",238,multi-view imagery
214,2003,"We present an image-based approach to infer 3D  structure parameters using a probabilistic ""shape+structure''  model. The 3D shape of a class of objects may be represented by sets  of contours from silhouette views simultaneously observed from  multiple calibrated cameras. Bayesian reconstructions of new shapes can  then be estimated using a prior density constructed with a mixture model  and probabilistic principal components analysis. We  augment the shape model to incorporate structural features of interest;  novel examples with missing structure parameters may then be  reconstructed to obtain estimates of these parameters. Model matching and  parameter inference are done entirely in the image domain and require no  explicit 3D construction. Our shape model enables accurate  estimation of structure despite segmentation errors or missing views  in the input silhouettes, and works even with only a single input  view. Using a dataset of thousands of pedestrian images generated  from a synthetic model, we can perform accurate inference of the 3D  locations of 19 joints on the body based on observed silhouette  contours from real images.",239,pose estimation
215,2003,"Statistical shape and texture appearance models are  powerful image representations, but previously had  been restricted to 2D or simple 3D shapes. In this paper  we present a novel 3D morphable model based on  image-based rendering techniques, which can  represent complex lighting conditions, structures, and  surfaces. We describe how to construct a manifold of  the multi-view appearance of an object class using light  fields and show how to match a 2D image of an object  to a point on this manifold. In turn we use the  reconstructed light field to render novel views of the  object. Our technique overcomes the limitations of  polygon based appearance models and uses light  fields that are acquired in real-time.",1,AI
216,2003,"Example-based methods are effective for parameter  estimation problems when the underlying system is simple or the  dimensionality of the input is low. For complex and high-dimensional  problems such as pose estimation, the number of required examples and the  computational complexity rapidly becme prohibitively high. We  introduce a new algorithm that learns a set of hashing functions that  efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for  locality-sensitive hashing, which finds approximate neighbors in time  sublinear in the number of examples. This method depends critically  on the choice of hash functions; we show how to find the set of hash  functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and  accurately estimate the articulated pose of human figures from a large  database of example images.",1,AI
216,2003,"Example-based methods are effective for parameter  estimation problems when the underlying system is simple or the  dimensionality of the input is low. For complex and high-dimensional  problems such as pose estimation, the number of required examples and the  computational complexity rapidly becme prohibitively high. We  introduce a new algorithm that learns a set of hashing functions that  efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for  locality-sensitive hashing, which finds approximate neighbors in time  sublinear in the number of examples. This method depends critically  on the choice of hash functions; we show how to find the set of hash  functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and  accurately estimate the articulated pose of human figures from a large  database of example images.",240,parameter estimation
216,2003,"Example-based methods are effective for parameter  estimation problems when the underlying system is simple or the  dimensionality of the input is low. For complex and high-dimensional  problems such as pose estimation, the number of required examples and the  computational complexity rapidly becme prohibitively high. We  introduce a new algorithm that learns a set of hashing functions that  efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for  locality-sensitive hashing, which finds approximate neighbors in time  sublinear in the number of examples. This method depends critically  on the choice of hash functions; we show how to find the set of hash  functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and  accurately estimate the articulated pose of human figures from a large  database of example images.",241,nearest neighbor
216,2003,"Example-based methods are effective for parameter  estimation problems when the underlying system is simple or the  dimensionality of the input is low. For complex and high-dimensional  problems such as pose estimation, the number of required examples and the  computational complexity rapidly becme prohibitively high. We  introduce a new algorithm that learns a set of hashing functions that  efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for  locality-sensitive hashing, which finds approximate neighbors in time  sublinear in the number of examples. This method depends critically  on the choice of hash functions; we show how to find the set of hash  functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and  accurately estimate the articulated pose of human figures from a large  database of example images.",242,locally weighted learning
223,2003,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",1,AI
223,2003,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",243,Head pose estimation
223,2003,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",244,Stereo processing
223,2003,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",245,Cursor control
223,2003,"In this text, we present two stereo-based head tracking  techniques along with a fast 3D model acquisition  system. The first tracking technique is a robust  implementation of stereo-based head tracking  designed for interactive environments with uncontrolled  lighting. We integrate fast face detection and drift  reduction algorithms with a gradient-based stereo rigid  motion tracking technique. Our system can  automatically segment and track a user's head under large rotation and illumination variations. Precision and  usability of this approach are compared with previous  tracking methods for cursor control and target selection  in both desktop and interactive room environments.  The second tracking technique is designed to improve  the robustness of head pose tracking for fast  movements. Our iterative hybrid tracker combines  constraints from the ICP (Iterative Closest Point)  algorithm and normal flow constraint. This new  technique is more precise for small movements and  noisy depth than ICP alone, and more robust for large  movements than the normal flow constraint alone. We present experiments which  test the accuracy of our approach on sequences of real  and synthetic stereo images.  The 3D model acquisition system we present quickly  aligns intensity and depth images, and reconstructs a  textured 3D mesh. 3D views are registered with shape  alignment based on our iterative hybrid tracker. We  reconstruct the 3D model using a new Cubic Ray  Projection merging algorithm which takes advantage of  a novel data structure: the linked voxel space. We  present experiments to test the accuracy of our  approach on 3D face modelling using real-time stereo  images.",246,3D model acquisition
225,2003,"For a very large network deployed in space with only nearby nodes able to talk to each other, we want to do tasks like robust routing and data storage. One way to organize the network is via a hierarchy, but hierarchies often have a few critical nodes whose death can disrupt organization over long distances. I address this with a system of distributed aggregates called Persistent Nodes, such that spatially local failures disrupt the hierarchy in an area proportional to the diameter of the failure. I describe and analyze this system, which has been implemented in simulation.",1,AI
225,2003,"For a very large network deployed in space with only nearby nodes able to talk to each other, we want to do tasks like robust routing and data storage. One way to organize the network is via a hierarchy, but hierarchies often have a few critical nodes whose death can disrupt organization over long distances. I address this with a system of distributed aggregates called Persistent Nodes, such that spatially local failures disrupt the hierarchy in an area proportional to the diameter of the failure. I describe and analyze this system, which has been implemented in simulation.",247,amorphous distributed fault tolerant gigascale
226,2003,"This paper presents a novel algorithm for learning in a  class of stochastic Markov decision processes (MDPs)  with continuous state and action spaces that trades  speed for accuracy. A transform of the stochastic MDP  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. In this transformed MDP, the calculation of  values is greatly simplified. The online algorithm  estimates the model of the transformed MDP and  simultaneously does policy search against it. Bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. The algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic MDP, using less domain knowledge. All  code used in the experiments is available on the  project's web site.",1,AI
226,2003,"This paper presents a novel algorithm for learning in a  class of stochastic Markov decision processes (MDPs)  with continuous state and action spaces that trades  speed for accuracy. A transform of the stochastic MDP  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. In this transformed MDP, the calculation of  values is greatly simplified. The online algorithm  estimates the model of the transformed MDP and  simultaneously does policy search against it. Bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. The algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic MDP, using less domain knowledge. All  code used in the experiments is available on the  project's web site.",248,Reinforcement learning
226,2003,"This paper presents a novel algorithm for learning in a  class of stochastic Markov decision processes (MDPs)  with continuous state and action spaces that trades  speed for accuracy. A transform of the stochastic MDP  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. In this transformed MDP, the calculation of  values is greatly simplified. The online algorithm  estimates the model of the transformed MDP and  simultaneously does policy search against it. Bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. The algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic MDP, using less domain knowledge. All  code used in the experiments is available on the  project's web site.",249,bicycle
226,2003,"This paper presents a novel algorithm for learning in a  class of stochastic Markov decision processes (MDPs)  with continuous state and action spaces that trades  speed for accuracy. A transform of the stochastic MDP  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. In this transformed MDP, the calculation of  values is greatly simplified. The online algorithm  estimates the model of the transformed MDP and  simultaneously does policy search against it. Bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. The algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic MDP, using less domain knowledge. All  code used in the experiments is available on the  project's web site.",216,policy search
226,2003,"This paper presents a novel algorithm for learning in a  class of stochastic Markov decision processes (MDPs)  with continuous state and action spaces that trades  speed for accuracy. A transform of the stochastic MDP  into a deterministic one is presented which captures the  essence of the original dynamics, in a sense made  precise. In this transformed MDP, the calculation of  values is greatly simplified. The online algorithm  estimates the model of the transformed MDP and  simultaneously does policy search against it. Bounds  on the error of this approximation are proven, and  experimental results in a bicycle riding domain are  presented. The algorithm learns near optimal policies  in orders of magnitude fewer interactions with the  stochastic MDP, using less domain knowledge. All  code used in the experiments is available on the  project's web site.",250,markov decision processes
227,2003,"We present a statistical image-based shape +  structure model for Bayesian visual hull reconstruction  and 3D structure inference. The 3D shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. Bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. We show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. The proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3D models under  various noise conditions are shown. We further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the Bayesian  reconstruction process. Model matching and parameter  inference are done entirely in the image domain and  require no explicit 3D construction. Our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. Using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3D locations of 19 joints on the  body based on observed silhouette contours from real images.",1,AI
227,2003,"We present a statistical image-based shape +  structure model for Bayesian visual hull reconstruction  and 3D structure inference. The 3D shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. Bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. We show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. The proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3D models under  various noise conditions are shown. We further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the Bayesian  reconstruction process. Model matching and parameter  inference are done entirely in the image domain and  require no explicit 3D construction. Our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. Using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3D locations of 19 joints on the  body based on observed silhouette contours from real images.",251,visual hull
227,2003,"We present a statistical image-based shape +  structure model for Bayesian visual hull reconstruction  and 3D structure inference. The 3D shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. Bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. We show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. The proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3D models under  various noise conditions are shown. We further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the Bayesian  reconstruction process. Model matching and parameter  inference are done entirely in the image domain and  require no explicit 3D construction. Our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. Using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3D locations of 19 joints on the  body based on observed silhouette contours from real images.",236,3D structure
227,2003,"We present a statistical image-based shape +  structure model for Bayesian visual hull reconstruction  and 3D structure inference. The 3D shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. Bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. We show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. The proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3D models under  various noise conditions are shown. We further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the Bayesian  reconstruction process. Model matching and parameter  inference are done entirely in the image domain and  require no explicit 3D construction. Our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. Using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3D locations of 19 joints on the  body based on observed silhouette contours from real images.",252,shape model
227,2003,"We present a statistical image-based shape +  structure model for Bayesian visual hull reconstruction  and 3D structure inference. The 3D shape of a class of  objects is represented by sets of contours from  silhouette views simultaneously observed from multiple  calibrated cameras. Bayesian reconstructions of new  shapes are then estimated using a prior density constructed with a mixture model and probabilistic  principal components analysis. We show how the use  of a class-specific prior in a visual hull reconstruction  can reduce the effect of segmentation errors from the  silhouette extraction process. The proposed method is  applied to a data set of pedestrian images, and  improvements in the approximate 3D models under  various noise conditions are shown. We further  augment the shape model to incorporate structural  features of interest; unknown structural parameters for a  novel set of contours are then inferred via the Bayesian  reconstruction process. Model matching and parameter  inference are done entirely in the image domain and  require no explicit 3D construction. Our shape model  enables accurate estimation of structure despite  segmentation errors or missing views in the input  silhouettes, and works even with only a single input  view. Using a data set of thousands of pedestrian  images generated from a synthetic model, we can  accurately infer the 3D locations of 19 joints on the  body based on observed silhouette contours from real images.",253,Bayesian inference
228,2003,"Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman-Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",1,AI
228,2003,"Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman-Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",254,model-based autonomy
228,2003,"Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman-Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",255,distributed planning
228,2003,"Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman-Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.",256,distributed constraint satisfaction
233,2003,"This thesis describes and evaluates a market-making  algorithm for setting prices in financial markets with asymmetric  information, and analyzes the properties of artificial markets in which the  algorithm is used. The core of our algorithm is a technique for  maintaining an online probability density estimate of the underlying  value of a stock. Previous theoretical work on market-making has  led to price-setting equations for which solutions cannot be  achieved in practice, whereas empirical work on algorithms for  market-making has focused on sets of heuristics and rules that lack  theoretical justification. The algorithm presented in this thesis is  theoretically justified by results in finance, and at the same time  flexible enough to be easily extended by incorporating modules for  dealing with considerations like portfolio risk and competition from  other market-makers. We analyze the performance of our  algorithm experimentally in artificial markets with different  parameter settings and find that many reasonable real-world properties  emerge. For example, the spread increases in response to  uncertainty about the true value of a stock, average spreads tend to be higher  in more volatile markets, and market-makers with lower  average spreads perform better in environments with multiple competitive market-makers. In addition, the time series data generated by simple  markets populated with market-makers using our algorithm replicate  properties of real-world financial time series, such as volatility  clustering and the fat-tailed nature of return distributions, without the  need to specify explicit models for opinion propagation and  herd behavior in the trading crowd.",1,AI
234,2003,"The dream of pervasive computing is slowly becoming  a reality. A number of projects around the world are constantly contributing ideas and solutions  that are bound to change the way we interact with our environments and with one another. An  essential component of the future is a software infrastructure that is capable of supporting interactions  on scales ranging from a single physical space to intercontinental collaborations. Such  infrastructure must help applications adapt to very diverse environments and must protect people's  privacy and respect their personal preferences. In this paper we indicate a number of limitations present  in the software infrastructures proposed so far (including our previous work). We then describe the  framework for building an infrastructure that satisfies the abovementioned criteria. This  framework hinges on the concepts of delegation, arbitration and high-level service discovery.  Components of our own implementation of such an infrastructure are presented.",1,AI
234,2003,"The dream of pervasive computing is slowly becoming  a reality. A number of projects around the world are constantly contributing ideas and solutions  that are bound to change the way we interact with our environments and with one another. An  essential component of the future is a software infrastructure that is capable of supporting interactions  on scales ranging from a single physical space to intercontinental collaborations. Such  infrastructure must help applications adapt to very diverse environments and must protect people's  privacy and respect their personal preferences. In this paper we indicate a number of limitations present  in the software infrastructures proposed so far (including our previous work). We then describe the  framework for building an infrastructure that satisfies the abovementioned criteria. This  framework hinges on the concepts of delegation, arbitration and high-level service discovery.  Components of our own implementation of such an infrastructure are presented.",257,pervasive computing
235,2003,"Previous biological models of object recognition in  cortex have been evaluated using idealized scenes  and have hard-coded features, such as the HMAX  model by Riesenhuber and Poggio [10]. Because  HMAX uses the same set of features for all object  classes, it does not perform well in the task of detecting  a target object in clutter. This thesis presents a new  model that integrates learning of object-specific  features with the HMAX. The new model performs  better than the standard HMAX and comparably to a  computer vision system on face detection. Results from  experimenting with unsupervised learning of features  and the use of a biologically-plausible classifier are  presented.",1,AI
236,2003,"The face inversion effect has been widely documented  as an effect of the uniqueness of face processing. Using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. In simulations using HMAX, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. Using  many, sharply tuned units, an ``expert'' has a large inversion effect. On the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. As the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. Using the same representations as with  faces, we create experts and novices for cars. We also measure the  feasibility of a view-based model for recognition of rotated objects  using HMAX. Using faces, we show that transfer of learning to novel views is possible.  Given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. Although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",1,AI
236,2003,"The face inversion effect has been widely documented  as an effect of the uniqueness of face processing. Using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. In simulations using HMAX, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. Using  many, sharply tuned units, an ``expert'' has a large inversion effect. On the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. As the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. Using the same representations as with  faces, we create experts and novices for cars. We also measure the  feasibility of a view-based model for recognition of rotated objects  using HMAX. Using faces, we show that transfer of learning to novel views is possible.  Given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. Although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",258,Face Recognition
236,2003,"The face inversion effect has been widely documented  as an effect of the uniqueness of face processing. Using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. In simulations using HMAX, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. Using  many, sharply tuned units, an ``expert'' has a large inversion effect. On the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. As the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. Using the same representations as with  faces, we create experts and novices for cars. We also measure the  feasibility of a view-based model for recognition of rotated objects  using HMAX. Using faces, we show that transfer of learning to novel views is possible.  Given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. Although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",259,Representation
236,2003,"The face inversion effect has been widely documented  as an effect of the uniqueness of face processing. Using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. In simulations using HMAX, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. Using  many, sharply tuned units, an ``expert'' has a large inversion effect. On the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. As the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. Using the same representations as with  faces, we create experts and novices for cars. We also measure the  feasibility of a view-based model for recognition of rotated objects  using HMAX. Using faces, we show that transfer of learning to novel views is possible.  Given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. Although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",76,Invariance
236,2003,"The face inversion effect has been widely documented  as an effect of the uniqueness of face processing. Using a computational  model, we show that the face inversion effect is a byproduct of expertise  with respect to the face object class. In simulations using HMAX, a  hierarchical, shape based model, we show that the magnitude of the  inversion effect is a function of the specificity of the representation. Using  many, sharply tuned units, an ``expert'' has a large inversion effect. On the other hand, if fewer, broadly  tuned units are used, the expertise is lost, and this ``novice'' has a small inversion effect. As the size of the inversion effect  is a product of the representation, not the object class, given the right  training we can create experts and novices in any object class. Using the same representations as with  faces, we create experts and novices for cars. We also measure the  feasibility of a view-based model for recognition of rotated objects  using HMAX. Using faces, we show that transfer of learning to novel views is possible.  Given only one training view, the view-based model  can recognize a face at a new orientation via interpolation from the views to  which it had been tuned. Although the model can generalize well to upright faces, inverted  faces yield poor performance because the features change differently  under rotation.",260,HMAX
237,2003,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",1,AI
237,2003,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",261,context-aware
237,2003,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",262,activity
237,2003,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",263,intelligent environment
237,2003,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",264,ubiquitous
237,2003,"Location is a primary cue in many context-aware  computing systems, and is often represented as  a global coordinate, room number, or Euclidean  distance various landmarks. A user?s concept of  location, however, is often defined in terms of regions in  which common activities occur. We show  how to partition a space into such regions based on  patterns of observed user location and  motion. These regions, which we call activity zones,  represent regions of similar user activity, and  can be used to trigger application actions, retrieve  information based on previous context, and  present information to users. We suggest that context-aware applications can benefit from a  location representation learned from observing users.  We describe an implementation of our  system and present two example applications whose  behavior is controlled by users? entry, exit,  and presence in the zones.",265,3d tracker
238,2003,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",1,AI
238,2003,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",266,online learning
238,2003,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",267,relative loss bounds
238,2003,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",268,switching dynamics
238,2003,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",269,wireless
238,2003,"We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.",270,802.11
239,2003,"As AI has begun to reach out beyond its symbolic, objectivist roots into the embodied, experientialist realm, many projects are exploring different aspects of creating machines which interact with and respond to the world as humans do. Techniques for visual processing, object recognition, emotional response, gesture production and recognition, etc., are necessary components of a complete humanoid robot. However, most projects invariably concentrate on developing a few of these individual components, neglecting the issue of how all of these pieces would eventually fit together.  The focus of the work in this dissertation is on creating a framework into which such specific competencies can be embedded, in a way that they can interact with each other and build layers of new functionality. To be of any practical value, such a framework must satisfy the real-world constraints of functioning in real-time with noisy sensors and actuators. The humanoid robot Cog provides an unapologetically adequate platform from which to take on such a challenge.  This work makes three contributions to embodied AI. First, it offers a general-purpose architecture for developing behavior-based systems distributed over networks of PC's. Second, it provides a motor-control system that simulates several biological features which impact the development of motor behavior. Third, it develops a framework for a system which enables a robot to learn new behaviors via interacting with itself and the outside world. A few basic functional modules are built into this framework, enough to demonstrate the robot learning some very simple behaviors taught by a human trainer.  A primary motivation for this project is the notion that it is practically impossible to build an ""intelligent"" machine unless it is designed partly to build itself. This work is a proof-of-concept of such an approach to integrating multiple perceptual and motor systems into a complete learning agent.",1,AI
239,2003,"As AI has begun to reach out beyond its symbolic, objectivist roots into the embodied, experientialist realm, many projects are exploring different aspects of creating machines which interact with and respond to the world as humans do. Techniques for visual processing, object recognition, emotional response, gesture production and recognition, etc., are necessary components of a complete humanoid robot. However, most projects invariably concentrate on developing a few of these individual components, neglecting the issue of how all of these pieces would eventually fit together.  The focus of the work in this dissertation is on creating a framework into which such specific competencies can be embedded, in a way that they can interact with each other and build layers of new functionality. To be of any practical value, such a framework must satisfy the real-world constraints of functioning in real-time with noisy sensors and actuators. The humanoid robot Cog provides an unapologetically adequate platform from which to take on such a challenge.  This work makes three contributions to embodied AI. First, it offers a general-purpose architecture for developing behavior-based systems distributed over networks of PC's. Second, it provides a motor-control system that simulates several biological features which impact the development of motor behavior. Third, it develops a framework for a system which enables a robot to learn new behaviors via interacting with itself and the outside world. A few basic functional modules are built into this framework, enough to demonstrate the robot learning some very simple behaviors taught by a human trainer.  A primary motivation for this project is the notion that it is practically impossible to build an ""intelligent"" machine unless it is designed partly to build itself. This work is a proof-of-concept of such an approach to integrating multiple perceptual and motor systems into a complete learning agent.",271,cog humanoid robot embodied learning phd thesis metaphor pancake reaching vision
240,2003,"This thesis describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. A suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. Despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. Each of the integration methods are investigated for their advantages and disadvantages. An improved gait representation is built based on our experiences with the initial set of gait representations. In addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.",1,AI
240,2003,"This thesis describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. A suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. Despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. Each of the integration methods are investigated for their advantages and disadvantages. An improved gait representation is built based on our experiences with the initial set of gait representations. In addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.",272,gait recognition
240,2003,"This thesis describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. A suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. Despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. Each of the integration methods are investigated for their advantages and disadvantages. An improved gait representation is built based on our experiences with the initial set of gait representations. In addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.",75,gender classification
241,2003,"We develop efficient techniques for the non-rigid registration of medical images by using representations that adapt to the anatomy found in such images.   Images of anatomical structures typically have uniform intensity interiors and smooth boundaries. We create methods to represent such regions compactly using tetrahedra. Unlike voxel-based representations, tetrahedra can accurately describe the expected smooth surfaces of medical objects. Furthermore, the interior of such objects can be represented using a small number of tetrahedra. Rather than describing a medical object using tens of thousands of voxels, our representations generally contain only a few thousand elements.  Tetrahedra facilitate the creation of efficient non-rigid registration algorithms based on finite element methods (FEM). We create a fast, FEM-based method to non-rigidly register segmented anatomical structures from two subjects. Using our compact tetrahedral representations, this method generally requires less than one minute of processing time on a desktop PC.  We also create a novel method for the non-rigid registration of gray scale images. To facilitate a fast method, we create a tetrahedral representation of a displacement field that automatically adapts to both the anatomy in an image and to the displacement field. The resulting algorithm has a computational cost that is dominated by the number of nodes in the mesh (about 10,000), rather than the number of voxels in an image (nearly 10,000,000). For many non-rigid registration problems, we can find a transformation from one image to another in five minutes. This speed is important as it allows use of the algorithm during surgery.  We apply our algorithms to find correlations between the shape of anatomical structures and the presence of schizophrenia. We show that a study based on our representations outperforms studies based on other representations. We also use the results of our non-rigid registration algorithm as the basis of a segmentation algorithm. That algorithm also outperforms other methods in our tests, producing smoother segmentations and more accurately reproducing manual segmentations.",1,AI
241,2003,"We develop efficient techniques for the non-rigid registration of medical images by using representations that adapt to the anatomy found in such images.   Images of anatomical structures typically have uniform intensity interiors and smooth boundaries. We create methods to represent such regions compactly using tetrahedra. Unlike voxel-based representations, tetrahedra can accurately describe the expected smooth surfaces of medical objects. Furthermore, the interior of such objects can be represented using a small number of tetrahedra. Rather than describing a medical object using tens of thousands of voxels, our representations generally contain only a few thousand elements.  Tetrahedra facilitate the creation of efficient non-rigid registration algorithms based on finite element methods (FEM). We create a fast, FEM-based method to non-rigidly register segmented anatomical structures from two subjects. Using our compact tetrahedral representations, this method generally requires less than one minute of processing time on a desktop PC.  We also create a novel method for the non-rigid registration of gray scale images. To facilitate a fast method, we create a tetrahedral representation of a displacement field that automatically adapts to both the anatomy in an image and to the displacement field. The resulting algorithm has a computational cost that is dominated by the number of nodes in the mesh (about 10,000), rather than the number of voxels in an image (nearly 10,000,000). For many non-rigid registration problems, we can find a transformation from one image to another in five minutes. This speed is important as it allows use of the algorithm during surgery.  We apply our algorithms to find correlations between the shape of anatomical structures and the presence of schizophrenia. We show that a study based on our representations outperforms studies based on other representations. We also use the results of our non-rigid registration algorithm as the basis of a segmentation algorithm. That algorithm also outperforms other methods in our tests, producing smoother segmentations and more accurately reproducing manual segmentations.",273,non-rigid registration
241,2003,"We develop efficient techniques for the non-rigid registration of medical images by using representations that adapt to the anatomy found in such images.   Images of anatomical structures typically have uniform intensity interiors and smooth boundaries. We create methods to represent such regions compactly using tetrahedra. Unlike voxel-based representations, tetrahedra can accurately describe the expected smooth surfaces of medical objects. Furthermore, the interior of such objects can be represented using a small number of tetrahedra. Rather than describing a medical object using tens of thousands of voxels, our representations generally contain only a few thousand elements.  Tetrahedra facilitate the creation of efficient non-rigid registration algorithms based on finite element methods (FEM). We create a fast, FEM-based method to non-rigidly register segmented anatomical structures from two subjects. Using our compact tetrahedral representations, this method generally requires less than one minute of processing time on a desktop PC.  We also create a novel method for the non-rigid registration of gray scale images. To facilitate a fast method, we create a tetrahedral representation of a displacement field that automatically adapts to both the anatomy in an image and to the displacement field. The resulting algorithm has a computational cost that is dominated by the number of nodes in the mesh (about 10,000), rather than the number of voxels in an image (nearly 10,000,000). For many non-rigid registration problems, we can find a transformation from one image to another in five minutes. This speed is important as it allows use of the algorithm during surgery.  We apply our algorithms to find correlations between the shape of anatomical structures and the presence of schizophrenia. We show that a study based on our representations outperforms studies based on other representations. We also use the results of our non-rigid registration algorithm as the basis of a segmentation algorithm. That algorithm also outperforms other methods in our tests, producing smoother segmentations and more accurately reproducing manual segmentations.",274,medical image processing
242,2003,"We develop efficient techniques for the non-rigid registration of medical
images by using representations that adapt to the anatomy found in such
images.

 Images of anatomical structures typically have uniform intensity interiors
and smooth boundaries. We create methods to represent such regions
compactly using tetrahedra.  Unlike voxel-based representations, tetrahedra
can accurately describe the expected smooth surfaces of medical
objects. Furthermore, the interior of such objects can be represented using
a small number of tetrahedra. Rather than describing a medical object using
tens of thousands of voxels, our representations generally contain only a few
thousand elements.

Tetrahedra facilitate the creation of efficient non-rigid registration
algorithms based on finite element methods (FEM).  We create a fast,
FEM-based method to non-rigidly register segmented anatomical structures
from two subjects. Using our compact tetrahedral representations, this
method generally requires less than one minute of processing time on a desktop
PC.

We also create a novel method for the non-rigid registration of gray scale
images. To facilitate a fast method, we create a tetrahedral representation
of a displacement field that automatically adapts to both the anatomy in an
image and to the displacement field.  The resulting algorithm has a
computational cost that is dominated by the number of nodes in the mesh
(about 10,000), rather than the number of voxels in an image (nearly
10,000,000). For many non-rigid registration problems, we can find a
transformation from one image to another in five minutes. This speed is
important as it allows use of the algorithm during surgery.

We apply our algorithms to find correlations between the shape of
anatomical structures and the presence of schizophrenia. We show that a
study based on our representations outperforms studies based on other
representations. We also use the results of our non-rigid registration
algorithm as the basis of a segmentation algorithm. That algorithm also
outperforms other methods in our tests, producing smoother segmentations
and more accurately reproducing manual segmentations.",1,AI
242,2003,"We develop efficient techniques for the non-rigid registration of medical
images by using representations that adapt to the anatomy found in such
images.

 Images of anatomical structures typically have uniform intensity interiors
and smooth boundaries. We create methods to represent such regions
compactly using tetrahedra.  Unlike voxel-based representations, tetrahedra
can accurately describe the expected smooth surfaces of medical
objects. Furthermore, the interior of such objects can be represented using
a small number of tetrahedra. Rather than describing a medical object using
tens of thousands of voxels, our representations generally contain only a few
thousand elements.

Tetrahedra facilitate the creation of efficient non-rigid registration
algorithms based on finite element methods (FEM).  We create a fast,
FEM-based method to non-rigidly register segmented anatomical structures
from two subjects. Using our compact tetrahedral representations, this
method generally requires less than one minute of processing time on a desktop
PC.

We also create a novel method for the non-rigid registration of gray scale
images. To facilitate a fast method, we create a tetrahedral representation
of a displacement field that automatically adapts to both the anatomy in an
image and to the displacement field.  The resulting algorithm has a
computational cost that is dominated by the number of nodes in the mesh
(about 10,000), rather than the number of voxels in an image (nearly
10,000,000). For many non-rigid registration problems, we can find a
transformation from one image to another in five minutes. This speed is
important as it allows use of the algorithm during surgery.

We apply our algorithms to find correlations between the shape of
anatomical structures and the presence of schizophrenia. We show that a
study based on our representations outperforms studies based on other
representations. We also use the results of our non-rigid registration
algorithm as the basis of a segmentation algorithm. That algorithm also
outperforms other methods in our tests, producing smoother segmentations
and more accurately reproducing manual segmentations.",273,non-rigid registration
242,2003,"We develop efficient techniques for the non-rigid registration of medical
images by using representations that adapt to the anatomy found in such
images.

 Images of anatomical structures typically have uniform intensity interiors
and smooth boundaries. We create methods to represent such regions
compactly using tetrahedra.  Unlike voxel-based representations, tetrahedra
can accurately describe the expected smooth surfaces of medical
objects. Furthermore, the interior of such objects can be represented using
a small number of tetrahedra. Rather than describing a medical object using
tens of thousands of voxels, our representations generally contain only a few
thousand elements.

Tetrahedra facilitate the creation of efficient non-rigid registration
algorithms based on finite element methods (FEM).  We create a fast,
FEM-based method to non-rigidly register segmented anatomical structures
from two subjects. Using our compact tetrahedral representations, this
method generally requires less than one minute of processing time on a desktop
PC.

We also create a novel method for the non-rigid registration of gray scale
images. To facilitate a fast method, we create a tetrahedral representation
of a displacement field that automatically adapts to both the anatomy in an
image and to the displacement field.  The resulting algorithm has a
computational cost that is dominated by the number of nodes in the mesh
(about 10,000), rather than the number of voxels in an image (nearly
10,000,000). For many non-rigid registration problems, we can find a
transformation from one image to another in five minutes. This speed is
important as it allows use of the algorithm during surgery.

We apply our algorithms to find correlations between the shape of
anatomical structures and the presence of schizophrenia. We show that a
study based on our representations outperforms studies based on other
representations. We also use the results of our non-rigid registration
algorithm as the basis of a segmentation algorithm. That algorithm also
outperforms other methods in our tests, producing smoother segmentations
and more accurately reproducing manual segmentations.",274,medical image processing
249,2003,"We present a set of techniques that can be used to represent and detect shapes in images. Our methods revolve around a particular shape representation based on the description of objects using triangulated polygons. This representation is similar to the medial axis transform and has important properties from a computational perspective. The first problem we consider is the detection of non-rigid objects in images using deformable models. We present an efficient algorithm to solve this problem in a wide range of situations, and show examples in both natural and medical images. We also consider the problem of learning an accurate non-rigid shape model for a class of objects from examples. We show how to learn good models while constraining them to the form required by the detection algorithm. Finally, we consider the problem of low-level image segmentation and grouping. We describe a stochastic grammar that generates arbitrary triangulated polygons while capturing Gestalt principles of shape regularity. This grammar is used as a prior model over random shapes in a low level algorithm that detects objects in images.",1,AI
250,2003,"We present a set of techniques that can be used to represent anddetect shapes in images.  Our methods revolve around a particularshape representation based on the description of objects usingtriangulated polygons.  This representation is similar to the medialaxis transform and has important properties from a computationalperspective.  The first problem we consider is the detection ofnon-rigid objects in images using deformable models.  We present anefficient algorithm to solve this problem in a wide range ofsituations, and show examples in both natural and medical images.  Wealso consider the problem of learning an accurate non-rigid shapemodel for a class of objects from examples.  We show how to learn goodmodels while constraining them to the form required by the detectionalgorithm.  Finally, we consider the problem of low-level imagesegmentation and grouping.  We describe a stochastic grammar thatgenerates arbitrary triangulated polygons while capturing Gestaltprinciples of shape regularity.  This grammar is used as a prior modelover random shapes in a low level algorithm that detects objects inimages.",1,AI
252,2003,"Small failures should only disrupt a small part of a network.  One wayto do this is by marking the surrounding area as untrustworthy ---circumscribing the failure. This can be done with a distributedalgorithm using hierarchical clustering and neighbor relations, andthe resulting circumscription is near-optimal for convex failures.",1,AI
252,2003,"Small failures should only disrupt a small part of a network.  One wayto do this is by marking the surrounding area as untrustworthy ---circumscribing the failure. This can be done with a distributedalgorithm using hierarchical clustering and neighbor relations, andthe resulting circumscription is near-optimal for convex failures.",275,amorphous distributed ad-hoc computing self-organizing stopping failure
253,2003,"Small failures should only disrupt a small part of a network. One way to do this is by marking the surrounding area as untrustworthy --- circumscribing the failure. This can be done with a distributed algorithm using hierarchical clustering and neighbor relations, and the resulting circumscription is near-optimal for convex failures.",1,AI
253,2003,"Small failures should only disrupt a small part of a network. One way to do this is by marking the surrounding area as untrustworthy --- circumscribing the failure. This can be done with a distributed algorithm using hierarchical clustering and neighbor relations, and the resulting circumscription is near-optimal for convex failures.",275,amorphous distributed ad-hoc computing self-organizing stopping failure
254,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . We introduce the ""Dissociated Dipole,"" or ""Sticks"" operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",1,AI
254,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . We introduce the ""Dissociated Dipole,"" or ""Sticks"" operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",276,image representation
254,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . We introduce the ""Dissociated Dipole,"" or ""Sticks"" operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",66,recognition
254,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filter's span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. . We introduce the ""Dissociated Dipole,"" or ""Sticks"" operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",277,non-local filtering
255,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filterÂ’s span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. .We introduce the Â“Dissociated Dipole,Â” or Â“SticksÂ” operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",1,AI
255,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filterÂ’s span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. .We introduce the Â“Dissociated Dipole,Â” or Â“SticksÂ” operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",276,image representation
255,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filterÂ’s span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. .We introduce the Â“Dissociated Dipole,Â” or Â“SticksÂ” operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",66,recognition
255,2003,"A fundamental question in visual neuroscience is how to represent image structure. The most common representational schemes rely on differential operators that compare adjacent image regions. While well-suited to encoding local relationships, such operators have significant drawbacks. Specifically, each filterÂ’s span is confounded with the size of its sub-fields, making it difficult to compare small regions across large distances. We find that such long-distance comparisons are more tolerant to common image transformations than purely local ones, suggesting they may provide a useful vocabulary for image encoding. .We introduce the Â“Dissociated Dipole,Â” or Â“SticksÂ” operator, for encoding non-local image relationships. This operator de-couples filter span from sub-field size, enabling parametric movement between edge and region-based representation modes. We report on the perceptual plausibility of the operator, and the computational advantages of non-local encoding. Our results suggest that non-local encoding may be an effective scheme for representing image structure.",277,non-local filtering
257,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",1,AI
257,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",278,pedestrian
257,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",279,walking direction
257,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",60,classification
257,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",54,SVM
257,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",66,recognition
257,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",280,human motion
259,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",1,AI
259,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",278,pedestrian
259,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",279,walking direction
259,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",60,classification
259,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",54,SVM
259,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",66,recognition
259,2003,"The capability of estimating the walking direction of people would be useful in many applications such as those involving autonomous cars and robots.  We introduce an approach for estimating the walking direction of people from images, based on learning the correct classification of a still image by using SVMs. We find that the performance of the system can be improved by classifying each image of a walking sequence and combining the outputs of the classifier.  Experiments were performed to evaluate our system and estimate the trade-off between number of images in walking sequences and performance.",280,human motion
260,2003,"We introduce and explore an approach to estimating statisticalsignificance of classification accuracy, which is particularly usefulin scientific applications of machine learning where highdimensionality of the data and the small number of training examplesrender most standard convergence bounds too loose to yield ameaningful guarantee of the generalization ability of theclassifier. Instead, we estimate statistical significance of theobserved classification accuracy, or the likelihood of observing suchaccuracy by chance due to spurious correlations of thehigh-dimensional data patterns with the class labels in the giventraining set. We adopt permutation testing, a non-parametric techniquepreviously developed in classical statistics for hypothesis testing inthe generative setting (i.e., comparing two probabilitydistributions). We demonstrate the method on real examples fromneuroimaging studies and DNA microarray analysis and suggest atheoretical analysis of the procedure that relates the asymptoticbehavior of the test to the existing convergence bounds.",1,AI
260,2003,"We introduce and explore an approach to estimating statisticalsignificance of classification accuracy, which is particularly usefulin scientific applications of machine learning where highdimensionality of the data and the small number of training examplesrender most standard convergence bounds too loose to yield ameaningful guarantee of the generalization ability of theclassifier. Instead, we estimate statistical significance of theobserved classification accuracy, or the likelihood of observing suchaccuracy by chance due to spurious correlations of thehigh-dimensional data patterns with the class labels in the giventraining set. We adopt permutation testing, a non-parametric techniquepreviously developed in classical statistics for hypothesis testing inthe generative setting (i.e., comparing two probabilitydistributions). We demonstrate the method on real examples fromneuroimaging studies and DNA microarray analysis and suggest atheoretical analysis of the procedure that relates the asymptoticbehavior of the test to the existing convergence bounds.",281,Classification
260,2003,"We introduce and explore an approach to estimating statisticalsignificance of classification accuracy, which is particularly usefulin scientific applications of machine learning where highdimensionality of the data and the small number of training examplesrender most standard convergence bounds too loose to yield ameaningful guarantee of the generalization ability of theclassifier. Instead, we estimate statistical significance of theobserved classification accuracy, or the likelihood of observing suchaccuracy by chance due to spurious correlations of thehigh-dimensional data patterns with the class labels in the giventraining set. We adopt permutation testing, a non-parametric techniquepreviously developed in classical statistics for hypothesis testing inthe generative setting (i.e., comparing two probabilitydistributions). We demonstrate the method on real examples fromneuroimaging studies and DNA microarray analysis and suggest atheoretical analysis of the procedure that relates the asymptoticbehavior of the test to the existing convergence bounds.",282,Permutation testing
260,2003,"We introduce and explore an approach to estimating statisticalsignificance of classification accuracy, which is particularly usefulin scientific applications of machine learning where highdimensionality of the data and the small number of training examplesrender most standard convergence bounds too loose to yield ameaningful guarantee of the generalization ability of theclassifier. Instead, we estimate statistical significance of theobserved classification accuracy, or the likelihood of observing suchaccuracy by chance due to spurious correlations of thehigh-dimensional data patterns with the class labels in the giventraining set. We adopt permutation testing, a non-parametric techniquepreviously developed in classical statistics for hypothesis testing inthe generative setting (i.e., comparing two probabilitydistributions). We demonstrate the method on real examples fromneuroimaging studies and DNA microarray analysis and suggest atheoretical analysis of the procedure that relates the asymptoticbehavior of the test to the existing convergence bounds.",283,Statistical significance.
261,2003,"We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",1,AI
261,2003,"We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",281,Classification
261,2003,"We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",282,Permutation testing
261,2003,"We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier. Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set. We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i.e., comparing two probability distributions). We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds.",283,Statistical significance.
262,2003,"To engineer complex synthetic biological systems will require modulardesign, assembly, and characterization strategies. The RNApolymerase arrival rate (PAR) is defined to be the rate that RNApolymerases arrive at a specified location on the DNA. Designing andcharacterizing biological modules in terms of RNA polymerase arrivalrates provides for many advantages in the construction and modeling ofbiological systems.PARMESAN is an in vitro method for measuring polymerase arrival ratesusing pyrrolo-dC, a fluorescent DNA base that can substitute forcytosine. Pyrrolo-dC shows a detectable fluorescence difference whenin single-stranded versus double-stranded DNA. During transcription,RNA polymerase separates the two strands of DNA, leading to a changein the fluorescence of pyrrolo-dC. By incorporating pyrrolo-dC atspecific locations in the DNA, fluorescence changes can be taken as adirect measurement of the polymerase arrival rate.",1,AI
263,2003,"To engineer complex synthetic biological systems will require modular design, assembly, and characterization strategies. The RNA polymerase arrival rate (PAR) is defined to be the rate that RNA polymerases arrive at a specified location on the DNA. Designing and characterizing biological modules in terms of RNA polymerase arrival rates provides for many advantages in the construction and modeling of biological systems.  PARMESAN is an in vitro method for measuring polymerase arrival rates using pyrrolo-dC, a fluorescent DNA base that can substitute for cytosine. Pyrrolo-dC shows a detectable fluorescence difference when in single-stranded versus double-stranded DNA. During transcription, RNA polymerase separates the two strands of DNA, leading to a change in the fluorescence of pyrrolo-dC. By incorporating pyrrolo-dC at specific locations in the DNA, fluorescence changes can be taken as a direct measurement of the polymerase arrival rate.",1,AI
264,2003,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",1,AI
264,2003,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",284,learning
264,2003,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",285,image segmentation
264,2003,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",286,motion
264,2003,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",287,Markov random field
264,2003,"This memo describes the initial results of a project to create a self-supervised algorithm for learning object segmentation from video data. Developmental psychology and computational experience have demonstrated that the motion segmentation of objects is a simpler, more primitive process than the detection of object boundaries by static image cues. Therefore, motion information provides a plausible supervision signal for learning the static boundary detection task and for evaluating performance on a test set. A video camera and previously developed background subtraction algorithms can automatically produce a large database of motion-segmented images for minimal cost. The purpose of this work is to use the information in such a database to learn how to detect the object boundaries in novel images using static information, such as color, texture, and shape.  This work was funded in part by the Office of Naval Research contract #N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of 11/6/98, and in part by a National Science Foundation Graduate Student Fellowship.",189,belief propagation
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",1,AI
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",288,Shape Tuning
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",289,Shape Representation
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",290,Features
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",260,HMAX
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",291,Visual Cortex
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",292,Gratings
265,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4. We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature. In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments. Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli. Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions. However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",293,V4
266,2003,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",1,AI
266,2003,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",284,learning
266,2003,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",285,image segmentation
266,2003,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",286,motion
266,2003,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",287,Markov random field
266,2003,"This memo describes the initial results of a project to create aself-supervised algorithm for learning object segmentation from videodata. Developmental psychology and computational experience havedemonstrated that the motion segmentation of objects is a simpler,more primitive process than the detection of object boundaries bystatic image cues. Therefore, motion information provides a plausiblesupervision signal for learning the static boundary detection task andfor evaluating performance on a test set. A video camera andpreviously developed background subtraction algorithms canautomatically produce a large database of motion-segmented images forminimal cost. The purpose of this work is to use the information insuch a database to learn how to detect the object boundaries in novelimages using static information, such as color, texture, and shape.This work was funded in part by the Office of Naval Research contract#N00014-00-1-0298, in part by the Singapore-MIT Alliance agreement of11/6/98, and in part by a National Science Foundation Graduate StudentFellowship.",189,belief propagation
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",1,AI
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",288,Shape Tuning
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",289,Shape Representation
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",290,Features
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",260,HMAX
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",291,Visual Cortex
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",292,Gratings
267,2003,"The question of how shape is represented is of central interest to understanding visual processing in cortex. While tuning properties of the cells in early part of the ventral visual stream, thought to be responsible for object recognition in the primate, are comparatively well understood, several different theories have been proposed regarding tuning in higher visual areas, such as V4.  We used the model of object recognition in cortex presented by Riesenhuber and Poggio (1999), where more complex shape tuning in higher layers is the result of combining afferent inputs tuned to simpler features, and compared the tuning properties of model units in intermediate layers to those of V4 neurons from the literature.  In particular, we investigated the issue of shape representation in visual area V1 and V4 using oriented bars and various types of gratings (polar, hyperbolic, and Cartesian), as used in several physiology experiments.  Our computational model was able to reproduce several physiological findings, such as the broadening distribution of the orientation bandwidths and the emergence of a bias toward non-Cartesian stimuli.  Interestingly, the simulation results suggest that some V4 neurons receive input from afferents with spatially separated receptive fields, leading to experimentally testable predictions.  However, the simulations also show that the stimulus set of Cartesian and non-Cartesian gratings is not sufficiently complex to probe shape tuning in higher areas, necessitating the use of more complex stimulus sets.",293,V4
274,2003,"In this paper, I describe the application of genetic programming to evolve a controller for a robotic tank in a simulated environment.The purpose is to explore how genetic techniques can best be applied to produce controllers based on subsumption and behavior oriented languages such as REX.  As part of my implementation, I developed TableRex, a modification of REX that can be expressed on a fixed-lengthgenome.  Using a fixed subsumption architecture of TableRex modules, I evolved robots that beat some of the most competitive hand-coded adversaries.",1,AI
274,2003,"In this paper, I describe the application of genetic programming to evolve a controller for a robotic tank in a simulated environment.The purpose is to explore how genetic techniques can best be applied to produce controllers based on subsumption and behavior oriented languages such as REX.  As part of my implementation, I developed TableRex, a modification of REX that can be expressed on a fixed-lengthgenome.  Using a fixed subsumption architecture of TableRex modules, I evolved robots that beat some of the most competitive hand-coded adversaries.",294,genetic programming
274,2003,"In this paper, I describe the application of genetic programming to evolve a controller for a robotic tank in a simulated environment.The purpose is to explore how genetic techniques can best be applied to produce controllers based on subsumption and behavior oriented languages such as REX.  As part of my implementation, I developed TableRex, a modification of REX that can be expressed on a fixed-lengthgenome.  Using a fixed subsumption architecture of TableRex modules, I evolved robots that beat some of the most competitive hand-coded adversaries.",295,robotics
275,2003,"In this paper, I describe the application of genetic programming  to evolve a controller for a robotic tank in a simulated environment. The purpose is to explore how genetic techniques can best be applied  to produce controllers based on subsumption and behavior oriented  languages such as REX. As part of my implementation, I developed  TableRex, a modification of REX that can be expressed on a fixed-length genome. Using a fixed subsumption architecture of TableRex modules,  I evolved robots that beat some of the most competitive hand-coded  adversaries.",1,AI
275,2003,"In this paper, I describe the application of genetic programming  to evolve a controller for a robotic tank in a simulated environment. The purpose is to explore how genetic techniques can best be applied  to produce controllers based on subsumption and behavior oriented  languages such as REX. As part of my implementation, I developed  TableRex, a modification of REX that can be expressed on a fixed-length genome. Using a fixed subsumption architecture of TableRex modules,  I evolved robots that beat some of the most competitive hand-coded  adversaries.",294,genetic programming
275,2003,"In this paper, I describe the application of genetic programming  to evolve a controller for a robotic tank in a simulated environment. The purpose is to explore how genetic techniques can best be applied  to produce controllers based on subsumption and behavior oriented  languages such as REX. As part of my implementation, I developed  TableRex, a modification of REX that can be expressed on a fixed-length genome. Using a fixed subsumption architecture of TableRex modules,  I evolved robots that beat some of the most competitive hand-coded  adversaries.",295,robotics
280,2003,We present a component-based approach for recognizing objects under large pose changes. From a set of training images of a given object we extract a large number of components which are clustered based on the similarity of their image features and their locations within the object image. The cluster centers build an initial set of component templates from which we select a subset for the final recognizer. In experiments we evaluate different sizes and types of components and three standard techniques for component selection. The component classifiers are finally compared to global classifiers on a database of four objects.,1,AI
280,2003,We present a component-based approach for recognizing objects under large pose changes. From a set of training images of a given object we extract a large number of components which are clustered based on the similarity of their image features and their locations within the object image. The cluster centers build an initial set of component templates from which we select a subset for the final recognizer. In experiments we evaluate different sizes and types of components and three standard techniques for component selection. The component classifiers are finally compared to global classifiers on a database of four objects.,100,computer vision
280,2003,We present a component-based approach for recognizing objects under large pose changes. From a set of training images of a given object we extract a large number of components which are clustered based on the similarity of their image features and their locations within the object image. The cluster centers build an initial set of component templates from which we select a subset for the final recognizer. In experiments we evaluate different sizes and types of components and three standard techniques for component selection. The component classifiers are finally compared to global classifiers on a database of four objects.,123,object recognition
280,2003,We present a component-based approach for recognizing objects under large pose changes. From a set of training images of a given object we extract a large number of components which are clustered based on the similarity of their image features and their locations within the object image. The cluster centers build an initial set of component templates from which we select a subset for the final recognizer. In experiments we evaluate different sizes and types of components and three standard techniques for component selection. The component classifiers are finally compared to global classifiers on a database of four objects.,296,component object recognition
281,2003,We present a component-based approach for recognizing objectsunder large pose changes. From a set of training images of a givenobject we extract a large number of components which are clusteredbased on the similarity of their image features and their locations withinthe object image. The cluster centers build an initial set of componenttemplates from which we select a subset for the final recognizer.In experiments we evaluate different sizes and types of components andthree standard techniques for component selection. The component classifiersare finally compared to global classifiers on a database of fourobjects.,1,AI
281,2003,We present a component-based approach for recognizing objectsunder large pose changes. From a set of training images of a givenobject we extract a large number of components which are clusteredbased on the similarity of their image features and their locations withinthe object image. The cluster centers build an initial set of componenttemplates from which we select a subset for the final recognizer.In experiments we evaluate different sizes and types of components andthree standard techniques for component selection. The component classifiersare finally compared to global classifiers on a database of fourobjects.,100,computer vision
281,2003,We present a component-based approach for recognizing objectsunder large pose changes. From a set of training images of a givenobject we extract a large number of components which are clusteredbased on the similarity of their image features and their locations withinthe object image. The cluster centers build an initial set of componenttemplates from which we select a subset for the final recognizer.In experiments we evaluate different sizes and types of components andthree standard techniques for component selection. The component classifiersare finally compared to global classifiers on a database of fourobjects.,123,object recognition
281,2003,We present a component-based approach for recognizing objectsunder large pose changes. From a set of training images of a givenobject we extract a large number of components which are clusteredbased on the similarity of their image features and their locations withinthe object image. The cluster centers build an initial set of componenttemplates from which we select a subset for the final recognizer.In experiments we evaluate different sizes and types of components andthree standard techniques for component selection. The component classifiersare finally compared to global classifiers on a database of fourobjects.,296,component object recognition
282,2003,"Research in mobile ad-hoc networks has focused on situations in whichnodes have no control over their movements.  We investigate animportant but overlooked domain in which nodes do have controlover their movements.  Reinforcement learning methods can be used tocontrol both packet routing decisions and node mobility, dramaticallyimproving the connectivity of the network.  We first motivate theproblem by presenting theoretical bounds for the connectivityimprovement of partially mobile networks and then present superiorempirical results under a variety of different scenarios in which themobile nodes in our ad-hoc network are embedded with adaptive routingpolicies and learned movement policies.",1,AI
282,2003,"Research in mobile ad-hoc networks has focused on situations in whichnodes have no control over their movements.  We investigate animportant but overlooked domain in which nodes do have controlover their movements.  Reinforcement learning methods can be used tocontrol both packet routing decisions and node mobility, dramaticallyimproving the connectivity of the network.  We first motivate theproblem by presenting theoretical bounds for the connectivityimprovement of partially mobile networks and then present superiorempirical results under a variety of different scenarios in which themobile nodes in our ad-hoc network are embedded with adaptive routingpolicies and learned movement policies.",44,reinforcement learning
282,2003,"Research in mobile ad-hoc networks has focused on situations in whichnodes have no control over their movements.  We investigate animportant but overlooked domain in which nodes do have controlover their movements.  Reinforcement learning methods can be used tocontrol both packet routing decisions and node mobility, dramaticallyimproving the connectivity of the network.  We first motivate theproblem by presenting theoretical bounds for the connectivityimprovement of partially mobile networks and then present superiorempirical results under a variety of different scenarios in which themobile nodes in our ad-hoc network are embedded with adaptive routingpolicies and learned movement policies.",297,multi-agent learning
282,2003,"Research in mobile ad-hoc networks has focused on situations in whichnodes have no control over their movements.  We investigate animportant but overlooked domain in which nodes do have controlover their movements.  Reinforcement learning methods can be used tocontrol both packet routing decisions and node mobility, dramaticallyimproving the connectivity of the network.  We first motivate theproblem by presenting theoretical bounds for the connectivityimprovement of partially mobile networks and then present superiorempirical results under a variety of different scenarios in which themobile nodes in our ad-hoc network are embedded with adaptive routingpolicies and learned movement policies.",298,ad-hoc networking
283,2003,"Research in mobile ad-hoc networks has focused on situations in which nodes have no control over their movements. We investigate an important but overlooked domain in which nodes do have control over their movements. Reinforcement learning methods can be used to control both packet routing decisions and node mobility, dramatically improving the connectivity of the network. We first motivate the problem by presenting theoretical bounds for the connectivity improvement of partially mobile networks and then present superior empirical results under a variety of different scenarios in which the mobile nodes in our ad-hoc network are embedded with adaptive routing policies and learned movement policies.",1,AI
283,2003,"Research in mobile ad-hoc networks has focused on situations in which nodes have no control over their movements. We investigate an important but overlooked domain in which nodes do have control over their movements. Reinforcement learning methods can be used to control both packet routing decisions and node mobility, dramatically improving the connectivity of the network. We first motivate the problem by presenting theoretical bounds for the connectivity improvement of partially mobile networks and then present superior empirical results under a variety of different scenarios in which the mobile nodes in our ad-hoc network are embedded with adaptive routing policies and learned movement policies.",44,reinforcement learning
283,2003,"Research in mobile ad-hoc networks has focused on situations in which nodes have no control over their movements. We investigate an important but overlooked domain in which nodes do have control over their movements. Reinforcement learning methods can be used to control both packet routing decisions and node mobility, dramatically improving the connectivity of the network. We first motivate the problem by presenting theoretical bounds for the connectivity improvement of partially mobile networks and then present superior empirical results under a variety of different scenarios in which the mobile nodes in our ad-hoc network are embedded with adaptive routing policies and learned movement policies.",297,multi-agent learning
283,2003,"Research in mobile ad-hoc networks has focused on situations in which nodes have no control over their movements. We investigate an important but overlooked domain in which nodes do have control over their movements. Reinforcement learning methods can be used to control both packet routing decisions and node mobility, dramatically improving the connectivity of the network. We first motivate the problem by presenting theoretical bounds for the connectivity improvement of partially mobile networks and then present superior empirical results under a variety of different scenarios in which the mobile nodes in our ad-hoc network are embedded with adaptive routing policies and learned movement policies.",298,ad-hoc networking
284,2003,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",1,AI
284,2003,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",299,contour matching
284,2003,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",300,shape matching
284,2003,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",301,EMD
284,2003,"Weighted graph matching is a good way to align a pair of shapesrepresented by a set of descriptive local features; the set ofcorrespondences produced by the minimum cost of matching features fromone shape to the features of the other often reveals how similar thetwo shapes are.  However, due to the complexity of computing the exactminimum cost matching, previous algorithms could only run efficientlywhen using a limited number of features per shape, and could not scaleto perform retrievals from large databases.  We present a contourmatching algorithm that quickly computes the minimum weight matchingbetween sets of descriptive local features using a recently introducedlow-distortion embedding of the Earth Mover's Distance (EMD) into anormed space.  Given a novel embedded contour, the nearest neighborsin a database of embedded contours are retrieved in sublinear time viaapproximate nearest neighbors search.  We demonstrate our shapematching method on databases of 10,000 images of human figures and60,000 images of handwritten digits.",302,image retrieval
285,2003,"Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",1,AI
285,2003,"Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",299,contour matching
285,2003,"Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",300,shape matching
285,2003,"Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",301,EMD
285,2003,"Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost of matching features from one shape to the features of the other often reveals how similar the two shapes are. However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search. We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits.",302,image retrieval
287,2003,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",1,AI
287,2003,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",303,ad-hoc networks distributed algorithms atomic distributed shared memory
288,2003,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",1,AI
288,2003,"We present an algorithm to store data robustly in a large, geographically distributed network by means of localized regions of data storage that move in response to changing conditions. For example, data might migrate away from failures or toward regions of high demand. The PersistentNode algorithm provides this service robustly, but with limited safety guarantees. We use the RAMBO framework to transform PersistentNode into RamboNode, an algorithm that guarantees atomic consistency in exchange for increased cost and decreased liveness. In addition, a half-life analysis of RamboNode shows that it is robust against continuous low-rate failures. Finally, we provide experimental simulations for the algorithm on 2000 nodes, demonstrating how it services requests and examining how it responds to failures.",303,ad-hoc networks distributed algorithms atomic distributed shared memory
290,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45]. We modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",1,AI
290,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45]. We modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",123,object recognition
290,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45]. We modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",304,attention
290,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45]. We modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",31,vision
290,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferred stimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45]. We modulated firing rates of model neurons in accordance with experimental  results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object  recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",305,modeling
291,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45].We modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",1,AI
291,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45].We modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",123,object recognition
291,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45].We modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",304,attention
291,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45].We modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",31,vision
291,2004,"Numerous psychophysical experiments have shown an important role for attentional modulations in vision. Behaviorally, allocation of attention can improve performance in object detection and recognition tasks. At the neural level, attention increases firing rates of neurons in visual cortex whose preferredstimulus is currently attended to. However, it is not yet known how these two phenomena are linked, i.e., how the visual system could be ""tuned"" in a task-dependent fashion to improve task performance. To answer this question, we performed simulations with the HMAX model of object recognition in cortex [45].We modulated firing rates of model neurons in accordance with experimental   results about effects of feature-based attention on single neurons and measured changes in the model's performance in a variety of object recognition tasks. It turned out that recognition performance could only be improved under very limited circumstances and that attentional influences on the process of object recognition per se tend to display a lack of specificity or raise false alarm rates. These observations lead us to postulate a new role for the observed attention-related neural response modulations.",305,modeling
292,2004,"In this paper we focus on the problem of estimating a boundeddensity using a finite combination of densities from a givenclass. We consider the Maximum Likelihood Procedure (MLE) and the greedy procedure described by Li and Barron. Approximation and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\frac{1}{\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",1,AI
292,2004,"In this paper we focus on the problem of estimating a boundeddensity using a finite combination of densities from a givenclass. We consider the Maximum Likelihood Procedure (MLE) and the greedy procedure described by Li and Barron. Approximation and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\frac{1}{\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",306,density estimation
292,2004,"In this paper we focus on the problem of estimating a boundeddensity using a finite combination of densities from a givenclass. We consider the Maximum Likelihood Procedure (MLE) and the greedy procedure described by Li and Barron. Approximation and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\frac{1}{\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",307,MLE
293,2004,"Array technologies have made it possible to record simultaneously the expression pattern of thousands of genes. A fundamental problem in the analysis of gene expression data is the identification of highly relevant genes that either discriminate between phenotypic labels or are important with respect to the cellular process studied in the experiment: for example cell cycle or heat shock in yeast experiments, chemical or genetic perturbations of mammalian cell lines, and genes involved in class discovery for human tumors. In this paper we focus on the task of unsupervised gene selection. The problem of selecting a small subset of genes is particularly challenging as the datasets involved are typically characterized by a very small sample size ?? the order of few tens of tissue samples ??d by a very large feature space as the number of genes tend to be in the high thousands. We propose a model independent approach which scores candidate gene selections using spectral properties of the candidate affinity matrix. The algorithm is very straightforward to implement yet contains a number of remarkable properties which guarantee consistent sparse selections. To illustrate the value of our approach we applied our algorithm on five different datasets. The first consists of time course data from four well studied Hematopoietic cell lines (HL-60, Jurkat, NB4, and U937). The other four datasets include three well studied treatment outcomes (large cell lymphoma, childhood medulloblastomas, breast tumors) and one unpublished dataset (lymph status). We compared our approach both with other unsupervised methods (SOM,PCA,GS) and with supervised methods (SNR,RMB,RFE). The results clearly show that our approach considerably outperforms all the other unsupervised approaches in our study, is competitive with supervised methods and in some case even outperforms supervised approaches.",1,AI
294,2004,"Array technologies have made it possible to record simultaneouslythe expression pattern of thousands of genes. A fundamental problemin the analysis of gene expression data is the identification ofhighly relevant genes that either discriminate between phenotypiclabels or are important with respect to the cellular process studied inthe experiment: for example cell cycle or heat shock in yeast experiments,chemical or genetic perturbations of mammalian cell lines,and genes involved in class discovery for human tumors. In this paperwe focus on the task of unsupervised gene selection. The problemof selecting a small subset of genes is particularly challengingas the datasets involved are typically characterized by a very smallsample size Â— in the order of few tens of tissue samples Â— andby a very large feature space as the number of genes tend to bein the high thousands. We propose a model independent approachwhich scores candidate gene selections using spectral properties ofthe candidate affinity matrix. The algorithm is very straightforwardto implement yet contains a number of remarkable properties whichguarantee consistent sparse selections. To illustrate the value of ourapproach we applied our algorithm on five different datasets. Thefirst consists of time course data from four well studied Hematopoieticcell lines (HL-60, Jurkat, NB4, and U937). The other fourdatasets include three well studied treatment outcomes (large celllymphoma, childhood medulloblastomas, breast tumors) and oneunpublished dataset (lymph status). We compared our approachboth with other unsupervised methods (SOM,PCA,GS) and withsupervised methods (SNR,RMB,RFE). The results clearly showthat our approach considerably outperforms all the other unsupervisedapproaches in our study, is competitive with supervised methodsand in some case even outperforms supervised approaches.",1,AI
295,2004,"In this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class. We consider the Maximum Likelihood Procedure (MLE) and  the greedy procedure described by Li and Barron. Approximation  and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\\frac{1}{\\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",1,AI
295,2004,"In this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class. We consider the Maximum Likelihood Procedure (MLE) and  the greedy procedure described by Li and Barron. Approximation  and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\\frac{1}{\\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",306,density estimation
295,2004,"In this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class. We consider the Maximum Likelihood Procedure (MLE) and  the greedy procedure described by Li and Barron. Approximation  and estimation bounds are given for the above methods. We extend and improve upon the estimation results of Li and Barron, and in particular prove an $O(\\frac{1}{\\sqrt{n}})$ bound on the estimation error which does not depend on the number of densities in the estimated combination.",307,MLE
296,2004,"Recovering a volumetric model of a person, car, or other object of interest from a single snapshot would be useful for many computer graphics applications. 3D model estimation in general is hard, and currently requires active sensors, multiple views, or integration over time. For a known object class, however, 3D shape can be successfully inferred from a single snapshot. We present a method for generating a ``virtual visual hull''-- an estimate of the 3D shape of an object from a known class, given a single silhouette observed from an unknown viewpoint. For a given class, a large database of multi-view silhouette examples from calibrated, though possibly varied, camera rigs are collected. To infer a novel single view input silhouette's virtual visual hull, we search for 3D shapes in the database which are most consistent with the observed contour. The input is matched to component single views of the multi-view training examples. A set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. The 3D shape estimate for the input is then found by interpolating between the contours of these aligned views. When the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. We show results of our algorithm on real and synthetic images of people.",1,AI
296,2004,"Recovering a volumetric model of a person, car, or other object of interest from a single snapshot would be useful for many computer graphics applications. 3D model estimation in general is hard, and currently requires active sensors, multiple views, or integration over time. For a known object class, however, 3D shape can be successfully inferred from a single snapshot. We present a method for generating a ``virtual visual hull''-- an estimate of the 3D shape of an object from a known class, given a single silhouette observed from an unknown viewpoint. For a given class, a large database of multi-view silhouette examples from calibrated, though possibly varied, camera rigs are collected. To infer a novel single view input silhouette's virtual visual hull, we search for 3D shapes in the database which are most consistent with the observed contour. The input is matched to component single views of the multi-view training examples. A set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. The 3D shape estimate for the input is then found by interpolating between the contours of these aligned views. When the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. We show results of our algorithm on real and synthetic images of people.",308,visual hulls
296,2004,"Recovering a volumetric model of a person, car, or other object of interest from a single snapshot would be useful for many computer graphics applications. 3D model estimation in general is hard, and currently requires active sensors, multiple views, or integration over time. For a known object class, however, 3D shape can be successfully inferred from a single snapshot. We present a method for generating a ``virtual visual hull''-- an estimate of the 3D shape of an object from a known class, given a single silhouette observed from an unknown viewpoint. For a given class, a large database of multi-view silhouette examples from calibrated, though possibly varied, camera rigs are collected. To infer a novel single view input silhouette's virtual visual hull, we search for 3D shapes in the database which are most consistent with the observed contour. The input is matched to component single views of the multi-view training examples. A set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. The 3D shape estimate for the input is then found by interpolating between the contours of these aligned views. When the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. We show results of our algorithm on real and synthetic images of people.",309,silhouettes
296,2004,"Recovering a volumetric model of a person, car, or other object of interest from a single snapshot would be useful for many computer graphics applications. 3D model estimation in general is hard, and currently requires active sensors, multiple views, or integration over time. For a known object class, however, 3D shape can be successfully inferred from a single snapshot. We present a method for generating a ``virtual visual hull''-- an estimate of the 3D shape of an object from a known class, given a single silhouette observed from an unknown viewpoint. For a given class, a large database of multi-view silhouette examples from calibrated, though possibly varied, camera rigs are collected. To infer a novel single view input silhouette's virtual visual hull, we search for 3D shapes in the database which are most consistent with the observed contour. The input is matched to component single views of the multi-view training examples. A set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. The 3D shape estimate for the input is then found by interpolating between the contours of these aligned views. When the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. We show results of our algorithm on real and synthetic images of people.",310,nearest neighbors
297,2004,"Recovering a volumetric model of a person, car, or other objectof interest from a single snapshot would be useful for many computergraphics applications.  3D model estimation in general is hard, andcurrently requires active sensors, multiple views, or integration overtime.  For a known object class, however, 3D shape can be successfullyinferred from a single snapshot.  We present a method for generating a``virtual visual hull''-- an estimate of the 3D shape of an objectfrom a known class, given a single silhouette observed from an unknownviewpoint.  For a given class, a large database of multi-viewsilhouette examples from calibrated, though possibly varied, camerarigs are collected.  To infer a novel single view input silhouette'svirtual visual hull, we search for 3D shapes in the database which aremost consistent with the observed contour.  The input is matched tocomponent single views of the multi-view training examples.  A set ofviewpoint-aligned virtual views are generated from the visual hullscorresponding to these examples.  The 3D shape estimate for the inputis then found by interpolating between the contours of these alignedviews.  When the underlying shape is ambiguous given a single viewsilhouette, we produce multiple visual hull hypotheses; if a sequenceof input images is available, a dynamic programming approach isapplied to find the maximum likelihood path through the feasiblehypotheses over time.  We show results of our algorithm on real andsynthetic images of people.",1,AI
297,2004,"Recovering a volumetric model of a person, car, or other objectof interest from a single snapshot would be useful for many computergraphics applications.  3D model estimation in general is hard, andcurrently requires active sensors, multiple views, or integration overtime.  For a known object class, however, 3D shape can be successfullyinferred from a single snapshot.  We present a method for generating a``virtual visual hull''-- an estimate of the 3D shape of an objectfrom a known class, given a single silhouette observed from an unknownviewpoint.  For a given class, a large database of multi-viewsilhouette examples from calibrated, though possibly varied, camerarigs are collected.  To infer a novel single view input silhouette'svirtual visual hull, we search for 3D shapes in the database which aremost consistent with the observed contour.  The input is matched tocomponent single views of the multi-view training examples.  A set ofviewpoint-aligned virtual views are generated from the visual hullscorresponding to these examples.  The 3D shape estimate for the inputis then found by interpolating between the contours of these alignedviews.  When the underlying shape is ambiguous given a single viewsilhouette, we produce multiple visual hull hypotheses; if a sequenceof input images is available, a dynamic programming approach isapplied to find the maximum likelihood path through the feasiblehypotheses over time.  We show results of our algorithm on real andsynthetic images of people.",308,visual hulls
297,2004,"Recovering a volumetric model of a person, car, or other objectof interest from a single snapshot would be useful for many computergraphics applications.  3D model estimation in general is hard, andcurrently requires active sensors, multiple views, or integration overtime.  For a known object class, however, 3D shape can be successfullyinferred from a single snapshot.  We present a method for generating a``virtual visual hull''-- an estimate of the 3D shape of an objectfrom a known class, given a single silhouette observed from an unknownviewpoint.  For a given class, a large database of multi-viewsilhouette examples from calibrated, though possibly varied, camerarigs are collected.  To infer a novel single view input silhouette'svirtual visual hull, we search for 3D shapes in the database which aremost consistent with the observed contour.  The input is matched tocomponent single views of the multi-view training examples.  A set ofviewpoint-aligned virtual views are generated from the visual hullscorresponding to these examples.  The 3D shape estimate for the inputis then found by interpolating between the contours of these alignedviews.  When the underlying shape is ambiguous given a single viewsilhouette, we produce multiple visual hull hypotheses; if a sequenceof input images is available, a dynamic programming approach isapplied to find the maximum likelihood path through the feasiblehypotheses over time.  We show results of our algorithm on real andsynthetic images of people.",309,silhouettes
297,2004,"Recovering a volumetric model of a person, car, or other objectof interest from a single snapshot would be useful for many computergraphics applications.  3D model estimation in general is hard, andcurrently requires active sensors, multiple views, or integration overtime.  For a known object class, however, 3D shape can be successfullyinferred from a single snapshot.  We present a method for generating a``virtual visual hull''-- an estimate of the 3D shape of an objectfrom a known class, given a single silhouette observed from an unknownviewpoint.  For a given class, a large database of multi-viewsilhouette examples from calibrated, though possibly varied, camerarigs are collected.  To infer a novel single view input silhouette'svirtual visual hull, we search for 3D shapes in the database which aremost consistent with the observed contour.  The input is matched tocomponent single views of the multi-view training examples.  A set ofviewpoint-aligned virtual views are generated from the visual hullscorresponding to these examples.  The 3D shape estimate for the inputis then found by interpolating between the contours of these alignedviews.  When the underlying shape is ambiguous given a single viewsilhouette, we produce multiple visual hull hypotheses; if a sequenceof input images is available, a dynamic programming approach isapplied to find the maximum likelihood path through the feasiblehypotheses over time.  We show results of our algorithm on real andsynthetic images of people.",310,nearest neighbors
299,2004,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or perhaps on how to ease the system's continued evolution.  But if we look at life cycle costs, then we must conclude that the important question is how to make a system easy to operate.  To do this we need to make it easy for the operators to see what's going on and to then manipulate the system so that it does what it is supposed to.  This is a radically different criterion for success.What makes a computer system visible and controllable?  This is a difficult question, but it's clear that today's modern operating systems with nearly 50 million source lines of code are neither.  Strikingly, the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's mainstream sytsems, but with only 1 Million lines of code.  This paper is a retrospective examination of the features of the Lisp Machine hardware and software system.  Our key claim is that by building the Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained.It is our hope that this is a lesson that can impact tomorrow's designs.  We also speculate on how the spirit of the Lisp Machine could be extended to include a comprehensive access control model and how new layers of abstraction could further enrich this model.",1,AI
299,2004,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or perhaps on how to ease the system's continued evolution.  But if we look at life cycle costs, then we must conclude that the important question is how to make a system easy to operate.  To do this we need to make it easy for the operators to see what's going on and to then manipulate the system so that it does what it is supposed to.  This is a radically different criterion for success.What makes a computer system visible and controllable?  This is a difficult question, but it's clear that today's modern operating systems with nearly 50 million source lines of code are neither.  Strikingly, the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's mainstream sytsems, but with only 1 Million lines of code.  This paper is a retrospective examination of the features of the Lisp Machine hardware and software system.  Our key claim is that by building the Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained.It is our hope that this is a lesson that can impact tomorrow's designs.  We also speculate on how the spirit of the Lisp Machine could be extended to include a comprehensive access control model and how new layers of abstraction could further enrich this model.",311,Software Environments
299,2004,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or perhaps on how to ease the system's continued evolution.  But if we look at life cycle costs, then we must conclude that the important question is how to make a system easy to operate.  To do this we need to make it easy for the operators to see what's going on and to then manipulate the system so that it does what it is supposed to.  This is a radically different criterion for success.What makes a computer system visible and controllable?  This is a difficult question, but it's clear that today's modern operating systems with nearly 50 million source lines of code are neither.  Strikingly, the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's mainstream sytsems, but with only 1 Million lines of code.  This paper is a retrospective examination of the features of the Lisp Machine hardware and software system.  Our key claim is that by building the Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained.It is our hope that this is a lesson that can impact tomorrow's designs.  We also speculate on how the spirit of the Lisp Machine could be extended to include a comprehensive access control model and how new layers of abstraction could further enrich this model.",312,Computer Archicture
300,2004,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or  perhaps on how to ease the system's continued evolution. But if we look at life cycle costs, then we  must conclude that the important question is how to make a system easy to operate. To do this we  need to make it easy for the operators to see what's going on and to then manipulate the system so  that it does what it is supposed to. This is a radically different criterion for success.  What makes a computer system visible and controllable? This is a difficult question, but it's clear that  today's modern operating systems with nearly 50 million source lines of code are neither. Strikingly,  the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's  mainstream sytsems, but with only 1 Million lines of code. This paper is a retrospective examination of  the features of the Lisp Machine hardware and software system. Our key claim is that by building the  Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained. It is our hope that this is a lesson that can impact tomorrow's designs. We also speculate on how the  spirit of the Lisp Machine could be extended to include a comprehensive access control model and how  new layers of abstraction could further enrich this model.",1,AI
300,2004,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or  perhaps on how to ease the system's continued evolution. But if we look at life cycle costs, then we  must conclude that the important question is how to make a system easy to operate. To do this we  need to make it easy for the operators to see what's going on and to then manipulate the system so  that it does what it is supposed to. This is a radically different criterion for success.  What makes a computer system visible and controllable? This is a difficult question, but it's clear that  today's modern operating systems with nearly 50 million source lines of code are neither. Strikingly,  the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's  mainstream sytsems, but with only 1 Million lines of code. This paper is a retrospective examination of  the features of the Lisp Machine hardware and software system. Our key claim is that by building the  Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained. It is our hope that this is a lesson that can impact tomorrow's designs. We also speculate on how the  spirit of the Lisp Machine could be extended to include a comprehensive access control model and how  new layers of abstraction could further enrich this model.",311,Software Environments
300,2004,"Traditionally, we've focussed on the question of how to make a system easy to code the first time, or  perhaps on how to ease the system's continued evolution. But if we look at life cycle costs, then we  must conclude that the important question is how to make a system easy to operate. To do this we  need to make it easy for the operators to see what's going on and to then manipulate the system so  that it does what it is supposed to. This is a radically different criterion for success.  What makes a computer system visible and controllable? This is a difficult question, but it's clear that  today's modern operating systems with nearly 50 million source lines of code are neither. Strikingly,  the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's  mainstream sytsems, but with only 1 Million lines of code. This paper is a retrospective examination of  the features of the Lisp Machine hardware and software system. Our key claim is that by building the  Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained. It is our hope that this is a lesson that can impact tomorrow's designs. We also speculate on how the  spirit of the Lisp Machine could be extended to include a comprehensive access control model and how  new layers of abstraction could further enrich this model.",312,Computer Archicture
304,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",1,AI
304,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",123,object recognition
304,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",313,faces
304,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",314,psychophysics
304,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",315,inversion effect
304,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",316,neuroscience
304,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ Â“featurallyÂ” are much easier to distinguish when inverted than those that differ Â“configurallyÂ” (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) Â– a finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjectsÂ’ expectations, there is no difference between Â“featurallyÂ” and Â“configurallyÂ” transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",317,comput
305,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",1,AI
305,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",123,object recognition
305,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",313,faces
305,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",314,psychophysics
305,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",315,inversion effect
305,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",316,neuroscience
305,2004,"Understanding how the human visual system recognizes objects is one of the key challenges in neuroscience. Inspired by a large body of physiological evidence (Felleman and Van Essen, 1991; Hubel and Wiesel, 1962; Livingstone and Hubel, 1988; Tso et al., 2001; Zeki, 1993), a general class of recognition models has emerged which is based on a hierarchical organization of visual processing, with succeeding stages being sensitive to image features of increasing complexity (Hummel and Biederman, 1992; Riesenhuber and Poggio, 1999; Selfridge, 1959). However, these models appear to be incompatible with some well-known psychophysical results. Prominent among these are experiments investigating recognition impairments caused by vertical inversion of images, especially those of faces. It has been reported that faces that differ ""featurally"" are much easier to distinguish when inverted than those that differ ""configurally"" (Freire et al., 2000; Le Grand et al., 2001; Mondloch et al., 2002) ??finding that is difficult to reconcile with the aforementioned models. Here we show that after controlling for subjects' expectations, there is no difference between ""featurally"" and ""configurally"" transformed faces in terms of inversion effect. This result reinforces the plausibility of simple hierarchical models of object representation and recognition in cortex.",317,comput
308,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",1,AI
308,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",318,local descriptor
308,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",319,steerable filter
308,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",320,Gaussian derivatives
308,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor  based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",321,"selectivity,invariance"
309,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",1,AI
309,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",318,local descriptor
309,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",319,steerable filter
309,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",320,Gaussian derivatives
309,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",322,selectivity
309,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. We propose a performance criterion for a local descriptor based on the tradeoff between selectivity and invariance. In this paper, we evaluate several local descriptors with respect to selectivity and invariance. The descriptors that we evaluated are Gaussian derivatives up to the third order, gray image patches, and Laplacian-based descriptors with either three scales or one scale filters. We compare selectivity and invariance to several affine changes such as rotation, scale, brightness, and viewpoint. Comparisons have been made keeping the dimensionality of the descriptors roughly constant. The overall results indicate a good performance by the descriptor based on a set of oriented Gaussian filters. It is interesting that oriented receptive fields similar to the Gaussian derivatives as well as receptive fields similar to the Laplacian are found in primate visual cortex.",82,invariance
315,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",1,AI
315,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",323,Attention
315,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",63,context
315,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",324,saliency
315,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",121,scene recognition
315,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",4,object detection
317,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",1,AI
317,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",325,Object detection
317,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",326,sharing features
317,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",55,feature selection
317,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",53,multiclass
317,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.  We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",327,Boosting
318,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",1,AI
318,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",323,Attention
318,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",63,context
318,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",324,saliency
318,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",121,scene recognition
318,2004,"This article describes a model for including scene/context priors in attention guidance. In the proposed scheme, visual context information can be available early in the visual processing chain, in order to modulate the saliency of image regions and to provide an efficient short cut for object detection and recognition. The scene is represented by means of a low-dimensional global description obtained from low-level features. The global scene features are then used to predict the probability of presence of the target object in the scene, and its location and scale, before exploring the image. Scene information can then be used to modulate the saliency of image regions early during the visual processing in order to provide an efficient short cut for object detection and recognition.",4,object detection
320,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",1,AI
320,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",325,Object detection
320,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",326,sharing features
320,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",55,feature selection
320,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",53,multiclass
320,2004,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data, since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (run-time) computational complexity, and the (training-time) sample complexity, scales linearly with the number of classes to be detected. It seems unlikely that such an approach will scale up to allow recognition of hundreds or thousands of objects.We present a multi-class boosting procedure (joint boosting) that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required, and therefore the computational cost, is observed to scale approximately logarithmically with the number of classes. The features selected jointly are closer to edges and generic features typical of many natural structures instead of finding specific object parts. Those generic features generalize better and reduce considerably the computational cost of an algorithm for multi-class object detection.",327,Boosting
321,2004,"Among the various methods to combine classifiers, Boosting was originally thought as an stratagem to cascade pairs of classifiers through their disagreement. I recover the same idea from the work of Niyogi et al. to show how to loosen the requirement of weak learnability, central to Boosting, and introduce a new cascading stratagem. The paper concludes with an empirical study of an implementation of the cascade that, under assumptions that mirror the conditions imposed by Viola and Jones in [VJ01], has the property to preserve the generalization ability of boosting.",1,AI
323,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",1,AI
323,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",123,object recognition
323,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",318,local descriptor
323,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",328,rotation invariant
324,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",1,AI
324,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",123,object recognition
324,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",318,local descriptor
324,2004,"Local descriptors are increasingly used for the task of object recognition because of their perceived robustness with respect to occlusions and to global geometrical deformations. Such a descriptor--based on a set of oriented Gaussian derivative filters-- is used in our recognition system. We report here an evaluation of several techniques for orientation estimation to achieve rotation invariance of the descriptor. We also describe feature selection based on a single training image. Virtual images are generated by rotating and rescaling the image and robust features are selected. The results confirm robust performance in cluttered scenes, in the presence of partial occlusions, and when the object is embedded in different backgrounds.",328,rotation invariant
325,2004,"We formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  A unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. Additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. We analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. Among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",1,AI
325,2004,"We formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  A unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. Additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. We analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. Among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",42,registration
325,2004,"We formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  A unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. Additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. We analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. Among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",329,information theory
325,2004,"We formulate and interpret several multi-modal registration methods in the context of a unified statistical and information theoretic framework.  A unified interpretation clarifies the implicit assumptions of each method yielding a better understanding of their relative strengths and weaknesses. Additionally, we discuss a generative statistical model from which we derive a novel analysis tool, the ""auto-information function"", as a means of assessing and exploiting the common spatial dependencies inherent in multi-modal imagery. We analytically derive useful properties of the ""auto-information"" as well as verify them empirically on multi-modal imagery. Among the useful aspects of the ""auto-information function"" is that it can be computed from imaging modalities independently and it allows one to decompose the search space of registration problems.",330,unified framework
326,2004,"We formulate and interpret several multi-modal registration methods inthe context of a unified statistical and information theoretic framework. A unified interpretation clarifies the implicit assumptionsof each method yielding a better understanding of their relativestrengths and weaknesses. Additionally, we discuss a generativestatistical model from which we derive a novel analysis tool, the""auto-information function"", as a means of assessing and exploiting thecommon spatial dependencies inherent in multi-modal imagery. Weanalytically derive useful properties of the ""auto-information"" aswell as verify them empirically on multi-modal imagery. Among theuseful aspects of the ""auto-information function"" is that it canbe computed from imaging modalities independently and it allows one todecompose the search space of registration problems.",1,AI
326,2004,"We formulate and interpret several multi-modal registration methods inthe context of a unified statistical and information theoretic framework. A unified interpretation clarifies the implicit assumptionsof each method yielding a better understanding of their relativestrengths and weaknesses. Additionally, we discuss a generativestatistical model from which we derive a novel analysis tool, the""auto-information function"", as a means of assessing and exploiting thecommon spatial dependencies inherent in multi-modal imagery. Weanalytically derive useful properties of the ""auto-information"" aswell as verify them empirically on multi-modal imagery. Among theuseful aspects of the ""auto-information function"" is that it canbe computed from imaging modalities independently and it allows one todecompose the search space of registration problems.",42,registration
326,2004,"We formulate and interpret several multi-modal registration methods inthe context of a unified statistical and information theoretic framework. A unified interpretation clarifies the implicit assumptionsof each method yielding a better understanding of their relativestrengths and weaknesses. Additionally, we discuss a generativestatistical model from which we derive a novel analysis tool, the""auto-information function"", as a means of assessing and exploiting thecommon spatial dependencies inherent in multi-modal imagery. Weanalytically derive useful properties of the ""auto-information"" aswell as verify them empirically on multi-modal imagery. Among theuseful aspects of the ""auto-information function"" is that it canbe computed from imaging modalities independently and it allows one todecompose the search space of registration problems.",329,information theory
326,2004,"We formulate and interpret several multi-modal registration methods inthe context of a unified statistical and information theoretic framework. A unified interpretation clarifies the implicit assumptionsof each method yielding a better understanding of their relativestrengths and weaknesses. Additionally, we discuss a generativestatistical model from which we derive a novel analysis tool, the""auto-information function"", as a means of assessing and exploiting thecommon spatial dependencies inherent in multi-modal imagery. Weanalytically derive useful properties of the ""auto-information"" aswell as verify them empirically on multi-modal imagery. Among theuseful aspects of the ""auto-information function"" is that it canbe computed from imaging modalities independently and it allows one todecompose the search space of registration problems.",330,unified framework
332,2004,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems. This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander. To support such a system, this thesis presents the Spock generative planner. To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes. This is in contrast to traditional planners, whose operators represent simple atomic or durative actions. Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes. RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators. Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution. Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts. This thesis describes the Spock algorithm in detail, along with example problems and test results.",1,AI
332,2004,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems. This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander. To support such a system, this thesis presents the Spock generative planner. To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes. This is in contrast to traditional planners, whose operators represent simple atomic or durative actions. Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes. RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators. Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution. Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts. This thesis describes the Spock algorithm in detail, along with example problems and test results.",331,"planning ""temporal planning"""
333,2004,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",1,AI
333,2004,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",284,learning
333,2004,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",63,context
333,2004,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",332,categorization
333,2004,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",333,similarity
333,2004,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",334,Bridge
333,2004,"If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as ``A blue bird flew to the tree'' and ``The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.",335,thread memory
334,2004,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems.  This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander.  To support such a system, this thesis presents the Spock generative planner.  To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes.  This is in contrast to traditional planners, whose operators represent simple atomic or durative actions.  Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes.  RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators.  Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution.  Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts.  This thesis describes the Spock algorithm in detail, along with example problems and test results.",1,AI
334,2004,"Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems.  This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander.  To support such a system, this thesis presents the Spock generative planner.  To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes.  This is in contrast to traditional planners, whose operators represent simple atomic or durative actions.  Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes.  RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators.  Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution.  Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts.  This thesis describes the Spock algorithm in detail, along with example problems and test results.",331,"planning ""temporal planning"""
336,2004,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",1,AI
336,2004,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",284,learning
336,2004,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",63,context
336,2004,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",332,categorization
336,2004,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",333,similarity
336,2004,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",334,Bridge
336,2004,"If we are to understand how we can build machines capable of broadpurpose learning and reasoning, we must first aim to build systemsthat can represent, acquire, and reason about the kinds of commonsenseknowledge that we humans have about the world. This endeavor suggestssteps such as identifying the kinds of knowledge people commonly haveabout the world, constructing suitable knowledge representations, andexploring the mechanisms that people use to make judgments about theeveryday world. In this work, I contribute to these goals by proposingan architecture for a system that can learn commonsense knowledgeabout the properties and behavior of objects in the world. Thearchitecture described here augments previous machine learning systemsin four ways: (1) it relies on a seven dimensional notion of context,built from information recently given to the system, to learn andreason about objects' properties; (2) it has multiple methods that itcan use to reason about objects, so that when one method fails, it canfall back on others; (3) it illustrates the usefulness of reasoningabout objects by thinking about their similarity to other, betterknown objects, and by inferring properties of objects from thecategories that they belong to; and (4) it represents an attempt tobuild an autonomous learner and reasoner, that sets its own goals forlearning about the world and deduces new facts by reflecting on itsacquired knowledge. This thesis describes this architecture, as wellas a first implementation, that can learn from sentences such as ``Ablue bird flew to the tree'' and ``The small bird flew to the cage''that birds can fly. One of the main contributions of thiswork lies in suggesting a further set of salient ideas about how wecan build broader purpose commonsense artificial learners andreasoners.",335,thread memory
338,2004,"The next generations of both biological engineering and computer engineering demand that control be  exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems  may provide us with the ability to build cells that are capable of a plethora of activities, from  computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not  only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a  comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical  design tool built in Java, utilizing a database back end, and supports a range of simulations using an  XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can  compile designs into actual DNA, and then generate synthesis instructions to build the physical parts.  The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for  synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks  repository, which enables the sharing of BioBrick components between researchers, and vastly reduces  the barriers to entry for aspiring Synthetic Biologists.",1,AI
338,2004,"The next generations of both biological engineering and computer engineering demand that control be  exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems  may provide us with the ability to build cells that are capable of a plethora of activities, from  computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not  only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a  comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical  design tool built in Java, utilizing a database back end, and supports a range of simulations using an  XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can  compile designs into actual DNA, and then generate synthesis instructions to build the physical parts.  The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for  synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks  repository, which enables the sharing of BioBrick components between researchers, and vastly reduces  the barriers to entry for aspiring Synthetic Biologists.",336,BioJADE
338,2004,"The next generations of both biological engineering and computer engineering demand that control be  exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems  may provide us with the ability to build cells that are capable of a plethora of activities, from  computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not  only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a  comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical  design tool built in Java, utilizing a database back end, and supports a range of simulations using an  XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can  compile designs into actual DNA, and then generate synthesis instructions to build the physical parts.  The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for  synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks  repository, which enables the sharing of BioBrick components between researchers, and vastly reduces  the barriers to entry for aspiring Synthetic Biologists.",337,Synthetic Biology
338,2004,"The next generations of both biological engineering and computer engineering demand that control be  exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems  may provide us with the ability to build cells that are capable of a plethora of activities, from  computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not  only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a  comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical  design tool built in Java, utilizing a database back end, and supports a range of simulations using an  XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can  compile designs into actual DNA, and then generate synthesis instructions to build the physical parts.  The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for  synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks  repository, which enables the sharing of BioBrick components between researchers, and vastly reduces  the barriers to entry for aspiring Synthetic Biologists.",338,DNA
339,2004,"The next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical design tool built in Java, utilizing a database back end, and supports a range of simulations using an XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can compile designs into actual DNA, and then generate synthesis instructions to build the physical parts. The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks repository, which enables the sharing of BioBrick components between researchers, and vastly reduces the barriers to entry for aspiring Synthetic Biologists.",1,AI
339,2004,"The next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical design tool built in Java, utilizing a database back end, and supports a range of simulations using an XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can compile designs into actual DNA, and then generate synthesis instructions to build the physical parts. The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks repository, which enables the sharing of BioBrick components between researchers, and vastly reduces the barriers to entry for aspiring Synthetic Biologists.",336,BioJADE
339,2004,"The next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical design tool built in Java, utilizing a database back end, and supports a range of simulations using an XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can compile designs into actual DNA, and then generate synthesis instructions to build the physical parts. The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks repository, which enables the sharing of BioBrick components between researchers, and vastly reduces the barriers to entry for aspiring Synthetic Biologists.",337,Synthetic Biology
339,2004,"The next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical design tool built in Java, utilizing a database back end, and supports a range of simulations using an XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can compile designs into actual DNA, and then generate synthesis instructions to build the physical parts. The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks repository, which enables the sharing of BioBrick components between researchers, and vastly reduces the barriers to entry for aspiring Synthetic Biologists.",338,DNA
343,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",1,AI
343,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",3,Artificial Intelligence
343,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",339,Society of Mind
343,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic) or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.",340,Multi-Agent Systems
344,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from  particular drawbacks. High-level AI uses abstractions that often have no relation to the way real,  biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. I have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit.  I then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. The new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. As a  demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a  network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior.",1,AI
344,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from  particular drawbacks. High-level AI uses abstractions that often have no relation to the way real,  biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. I have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit.  I then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. The new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. As a  demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a  network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior.",3,Artificial Intelligence
344,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from  particular drawbacks. High-level AI uses abstractions that often have no relation to the way real,  biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. I have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit.  I then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. The new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. As a  demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a  network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior.",339,Society of Mind
344,2004,"Most Artificial Intelligence (AI) work can be characterized as either ``high-level'' (e.g., logical, symbolic)  or ``low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from  particular drawbacks. High-level AI uses abstractions that often have no relation to the way real,  biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are  needed to express complex structures and relationships. I have tried to combine the best features of  both approaches, by building a set of programming abstractions defined in terms of simple, biologically  plausible components. At the ``ground level'', I define a primitive, perceptron-like computational unit.  I then show how more abstract computational units may be implemented in terms of the primitive  units, and show the utility of the abstract units in sample networks. The new units make it possible to  build networks using concepts such as long-term memories, short-term memories, and frames. As a  demonstration of these abstractions, I have implemented a simulator for ``creatures'' controlled by a  network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as  catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that  it is possible to build systems that can interact effectively with a dynamic physical environment, yet use  symbolic representations to control aspects of their behavior.",340,Multi-Agent Systems
345,2004,"This paper investigates how people return to information in a dynamic information environment. For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed. Changes can benefit users by providing new information, but they hinder returning to previously viewed information. The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment. A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not. While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution. The implications of these observations for systems that support re-finding in dynamic environments are discussed.",1,AI
345,2004,"This paper investigates how people return to information in a dynamic information environment. For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed. Changes can benefit users by providing new information, but they hinder returning to previously viewed information. The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment. A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not. While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution. The implications of these observations for systems that support re-finding in dynamic environments are discussed.",341,re-finding
345,2004,"This paper investigates how people return to information in a dynamic information environment. For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed. Changes can benefit users by providing new information, but they hinder returning to previously viewed information. The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment. A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not. While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution. The implications of these observations for systems that support re-finding in dynamic environments are discussed.",342,information management
345,2004,"This paper investigates how people return to information in a dynamic information environment. For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed. Changes can benefit users by providing new information, but they hinder returning to previously viewed information. The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment. A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not. While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution. The implications of these observations for systems that support re-finding in dynamic environments are discussed.",343,dynamic information
346,2004,"This paper investigates how people return to information in a dynamic information environment.  For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed.  Changes can benefit users by providing new information, but they hinder returning to previously viewed information.  The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  The implications of these observations for systems that support re-finding in dynamic environments are discussed.",1,AI
346,2004,"This paper investigates how people return to information in a dynamic information environment.  For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed.  Changes can benefit users by providing new information, but they hinder returning to previously viewed information.  The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  The implications of these observations for systems that support re-finding in dynamic environments are discussed.",341,re-finding
346,2004,"This paper investigates how people return to information in a dynamic information environment.  For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed.  Changes can benefit users by providing new information, but they hinder returning to previously viewed information.  The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  The implications of these observations for systems that support re-finding in dynamic environments are discussed.",342,information management
346,2004,"This paper investigates how people return to information in a dynamic information environment.  For example, a person might want to return to Web content via a link encountered earlier on a Web page, only to learn that the link has since been removed.  Changes can benefit users by providing new information, but they hinder returning to previously viewed information.  The observational study presented here analyzed instances, collected via a Web search, where people expressed difficulty re-finding information because of changes to the information or its environment.  A number of interesting observations arose from this analysis, including that the path originally taken to get to the information target appeared important in its re-retrieval, whereas, surprisingly, the temporal aspects of when the information was seen before were not.  While people expressed frustration when problems arose, an explanation of why the change had occurred was often sufficient to allay that frustration, even in the absence of a solution.  The implications of these observations for systems that support re-finding in dynamic environments are discussed.",343,dynamic information
348,2004,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",1,AI
348,2004,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",325,Object detection
348,2004,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",63,context
348,2004,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",344,boosting
348,2004,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",345,BP
348,2004,"We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",346,random fields
349,2004,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",1,AI
349,2004,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",325,Object detection
349,2004,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",63,context
349,2004,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",344,boosting
349,2004,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",345,BP
349,2004,"We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random field (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in office and street scenes.",346,random fields
351,2004,"We give a one-pass, O~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. Together with known lower bounds, this resolves the main problem left open by Alon, Matias, Szegedy, STOC'96. Our algorithm enables deletions as well as insertions of stream elements.",1,AI
352,2004,"We give a one-pass, O~(m^{1-2/k})-space algorithm for estimating the k-th frequency moment of a data stream for any real k>2. Together with known lower bounds, this resolves the main problem left open by Alon, Matias, Szegedy, STOC'96. Our algorithm enables deletions as well as insertions of stream elements.",1,AI
353,2004,"We present a constant-factor approximation algorithm for computing anembedding of the shortest path metric of an unweighted graph into atree, that minimizes the multiplicative distortion.",1,AI
353,2004,"We present a constant-factor approximation algorithm for computing anembedding of the shortest path metric of an unweighted graph into atree, that minimizes the multiplicative distortion.",347,embeddings
353,2004,"We present a constant-factor approximation algorithm for computing anembedding of the shortest path metric of an unweighted graph into atree, that minimizes the multiplicative distortion.",348,approximation algorithms
353,2004,"We present a constant-factor approximation algorithm for computing anembedding of the shortest path metric of an unweighted graph into atree, that minimizes the multiplicative distortion.",349,trees
354,2004,"We present a constant-factor approximation algorithm for computing an embedding of the shortest path metric of an unweighted graph into a tree, that minimizes the multiplicative distortion.",1,AI
354,2004,"We present a constant-factor approximation algorithm for computing an embedding of the shortest path metric of an unweighted graph into a tree, that minimizes the multiplicative distortion.",347,embeddings
354,2004,"We present a constant-factor approximation algorithm for computing an embedding of the shortest path metric of an unweighted graph into a tree, that minimizes the multiplicative distortion.",348,approximation algorithms
354,2004,"We present a constant-factor approximation algorithm for computing an embedding of the shortest path metric of an unweighted graph into a tree, that minimizes the multiplicative distortion.",349,trees
359,2004,"We present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  Our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  This model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.We apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",1,AI
359,2004,"We present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  Our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  This model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.We apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",284,learning
359,2004,"We present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  Our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  This model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.We apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",332,categorization
359,2004,"We present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  Our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  This model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.We apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",350,relations
359,2004,"We present a framework for learning abstract relational knowledge with the aimof explaining how people acquire intuitive theories of physical, biological, orsocial systems.  Our approach is based on a generative relational model withlatent classes, and simultaneously determines the kinds of entities that existin a domain, the number of these latent classes, and the relations betweenclasses that are possible or likely.  This model goes beyond previouspsychological models of category learning,  which consider attributesassociated with individual categories but not relationships between categories.We apply this domain-general framework to two specific problems: learning thestructure of kinship systems and learning causal theories.",351,kinship
360,2004,"In this paper, we discuss a wireless grid in which users are highly mobile, and form ad-hoc and sometimes short-lived connections with other devices.  As they roam through networks, the users may choose to employ privacy-enhancing technologies to address their privacy needs and benefit from the computational power of the grid for a variety of tasks, including sharing content.  The high rate of mobility of the users on the wireless grid, when combined with privacy enhancing mechanisms and ad-hoc connections, makes it difficult to conclusively link devices and/or individuals with network activities and to hold them liable for particular downloads.  Protecting intellectual property in this scenario requires a solution that can work in absence of knowledge about behavior of particular individuals.  Building on previous work, we argue for a solution that ensures proper compensation to content owners without inhibiting use and dissemination of works.  Our proposal is based on digital tracking for measuring distribution volume of content and compensation of authors based on this accounting information.  The emphasis is on obtaining good estimates of rate of popularity of works, without keeping track of activities of individuals or devices.  The contribution of this paper is a revenue protection mechanism, Distribution Volume Tracking, that does not invade the privacy of users in the wireless grid and works even in the presence of privacy-enhancing technologies they may employ.",1,AI
360,2004,"In this paper, we discuss a wireless grid in which users are highly mobile, and form ad-hoc and sometimes short-lived connections with other devices.  As they roam through networks, the users may choose to employ privacy-enhancing technologies to address their privacy needs and benefit from the computational power of the grid for a variety of tasks, including sharing content.  The high rate of mobility of the users on the wireless grid, when combined with privacy enhancing mechanisms and ad-hoc connections, makes it difficult to conclusively link devices and/or individuals with network activities and to hold them liable for particular downloads.  Protecting intellectual property in this scenario requires a solution that can work in absence of knowledge about behavior of particular individuals.  Building on previous work, we argue for a solution that ensures proper compensation to content owners without inhibiting use and dissemination of works.  Our proposal is based on digital tracking for measuring distribution volume of content and compensation of authors based on this accounting information.  The emphasis is on obtaining good estimates of rate of popularity of works, without keeping track of activities of individuals or devices.  The contribution of this paper is a revenue protection mechanism, Distribution Volume Tracking, that does not invade the privacy of users in the wireless grid and works even in the presence of privacy-enhancing technologies they may employ.",352,Intellectual Property Protection
360,2004,"In this paper, we discuss a wireless grid in which users are highly mobile, and form ad-hoc and sometimes short-lived connections with other devices.  As they roam through networks, the users may choose to employ privacy-enhancing technologies to address their privacy needs and benefit from the computational power of the grid for a variety of tasks, including sharing content.  The high rate of mobility of the users on the wireless grid, when combined with privacy enhancing mechanisms and ad-hoc connections, makes it difficult to conclusively link devices and/or individuals with network activities and to hold them liable for particular downloads.  Protecting intellectual property in this scenario requires a solution that can work in absence of knowledge about behavior of particular individuals.  Building on previous work, we argue for a solution that ensures proper compensation to content owners without inhibiting use and dissemination of works.  Our proposal is based on digital tracking for measuring distribution volume of content and compensation of authors based on this accounting information.  The emphasis is on obtaining good estimates of rate of popularity of works, without keeping track of activities of individuals or devices.  The contribution of this paper is a revenue protection mechanism, Distribution Volume Tracking, that does not invade the privacy of users in the wireless grid and works even in the presence of privacy-enhancing technologies they may employ.",146,Privacy
360,2004,"In this paper, we discuss a wireless grid in which users are highly mobile, and form ad-hoc and sometimes short-lived connections with other devices.  As they roam through networks, the users may choose to employ privacy-enhancing technologies to address their privacy needs and benefit from the computational power of the grid for a variety of tasks, including sharing content.  The high rate of mobility of the users on the wireless grid, when combined with privacy enhancing mechanisms and ad-hoc connections, makes it difficult to conclusively link devices and/or individuals with network activities and to hold them liable for particular downloads.  Protecting intellectual property in this scenario requires a solution that can work in absence of knowledge about behavior of particular individuals.  Building on previous work, we argue for a solution that ensures proper compensation to content owners without inhibiting use and dissemination of works.  Our proposal is based on digital tracking for measuring distribution volume of content and compensation of authors based on this accounting information.  The emphasis is on obtaining good estimates of rate of popularity of works, without keeping track of activities of individuals or devices.  The contribution of this paper is a revenue protection mechanism, Distribution Volume Tracking, that does not invade the privacy of users in the wireless grid and works even in the presence of privacy-enhancing technologies they may employ.",353,Wireless Computational Grid
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",1,AI
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",123,object recognition
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",354,simple cell
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",355,complex cell
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",356,hmax
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",357,V1
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",125,IT
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",358,view-tuned unit
361,2004,"Riesenhuber \& Poggio recently proposed a model of object recognitionin cortex which, beyond integrating general beliefs about the visualsystem in a quantitative framework, made testable predictions aboutvisual processing. In particular, they showed that invariant objectrepresentation could be obtained with a selective pooling mechanismover properly chosen afferents through a {\sc max} operation: Forinstance, at the complex cells level, pooling over a group of simplecells at the same preferred orientation and position in space but atslightly different spatial frequency would provide scale tolerance,while pooling over a group of simple cells at the same preferredorientation and spatial frequency but at slightly different positionin space would provide position tolerance. Indirect support for suchmechanisms in the visual system come from the ability of thearchitecture at the top level to replicate shape tuning as well asshift and size invariance properties of ``view-tuned cells'' (VTUs)found in inferotemporal cortex (IT), the highest area in the ventralvisual stream, thought to be crucial in mediating object recognitionin cortex. There is also now good physiological evidence that a {\scmax} operation is performed at various levels along the ventralstream. However, in the original paper by Riesenhuber \& Poggio,tuning and pooling parameters of model units in early and intermediateareas were only qualitatively inspired by physiological data. Inparticular, many studies have investigated the tuning properties ofsimple and complex cells in primary visual cortex, V1. We show thatunits in the early levels of HMAX can be tuned to produce realisticsimple and complex cell-like tuning, and that the earlier findings onthe invariance properties of model VTUs still hold in this morerealistic version of the model.",359,in
362,2004,"Freehand sketching is a natural and crucial part of everyday humaninteraction, yet is almost totally unsupported by current user interfaces. With the increasing availability of tablet notebooks and pen based PDAs, sketchbased interaction has gained attention as a natural interaction modality.We are working to combine the flexibility and ease of use of paper and pencilwith the processing power of a computer, to produce a user interface fordesign that feels as natural as paper, yet is considerably smarter. One of themost basic tasks in accomplishing this is converting the original digitizedpen strokes in a sketch into the intended geometric objects. In this paper wedescribe an implemented system that combines multiple sources of knowledge toprovide robust early processing for freehand sketching. We also show how thisearly processing system can be used as part of a fast sketch recognition system with polynomial time segmentation and recognition algorithms.",1,AI
362,2004,"Freehand sketching is a natural and crucial part of everyday humaninteraction, yet is almost totally unsupported by current user interfaces. With the increasing availability of tablet notebooks and pen based PDAs, sketchbased interaction has gained attention as a natural interaction modality.We are working to combine the flexibility and ease of use of paper and pencilwith the processing power of a computer, to produce a user interface fordesign that feels as natural as paper, yet is considerably smarter. One of themost basic tasks in accomplishing this is converting the original digitizedpen strokes in a sketch into the intended geometric objects. In this paper wedescribe an implemented system that combines multiple sources of knowledge toprovide robust early processing for freehand sketching. We also show how thisearly processing system can be used as part of a fast sketch recognition system with polynomial time segmentation and recognition algorithms.",360,Sketch Recognition
362,2004,"Freehand sketching is a natural and crucial part of everyday humaninteraction, yet is almost totally unsupported by current user interfaces. With the increasing availability of tablet notebooks and pen based PDAs, sketchbased interaction has gained attention as a natural interaction modality.We are working to combine the flexibility and ease of use of paper and pencilwith the processing power of a computer, to produce a user interface fordesign that feels as natural as paper, yet is considerably smarter. One of themost basic tasks in accomplishing this is converting the original digitizedpen strokes in a sketch into the intended geometric objects. In this paper wedescribe an implemented system that combines multiple sources of knowledge toprovide robust early processing for freehand sketching. We also show how thisearly processing system can be used as part of a fast sketch recognition system with polynomial time segmentation and recognition algorithms.",361,Early Sketch Processing
362,2004,"Freehand sketching is a natural and crucial part of everyday humaninteraction, yet is almost totally unsupported by current user interfaces. With the increasing availability of tablet notebooks and pen based PDAs, sketchbased interaction has gained attention as a natural interaction modality.We are working to combine the flexibility and ease of use of paper and pencilwith the processing power of a computer, to produce a user interface fordesign that feels as natural as paper, yet is considerably smarter. One of themost basic tasks in accomplishing this is converting the original digitizedpen strokes in a sketch into the intended geometric objects. In this paper wedescribe an implemented system that combines multiple sources of knowledge toprovide robust early processing for freehand sketching. We also show how thisearly processing system can be used as part of a fast sketch recognition system with polynomial time segmentation and recognition algorithms.",362,Shape Approximation
366,2004,"While single neurons in inferior temporal (IT) cortex show differential responses to distinct complex stimuli, little is known about the responses of populations of neurons in IT.  We recorded single electrode data, including multi-unit activity (MUA) and local field potentials (LFP), from 618 sites in the inferior temporal cortex of macaque monkeys while the animals passively viewed 78 different pictures of complex stimuli. The LFPs were obtained by low-pass filtering the extracellular electrophysiological signal with a corner frequency of 300 Hz. As reported previously, we observed that spike counts from MUA showed selectivity for some of the pictures.  Strikingly, the LFP data, which is thought to constitute an average over large numbers of neurons, also showed significantly selective responses.  The LFP responses were less selective than the MUA responses both in terms of the proportion of selective sites as well as in the selectivity of each site. We observed that there was only little overlap between the selectivity of MUA and LFP recordings from the same electrode.  To assess the spatial organization of selective responses, we compared the selectivity of nearby sites recorded along the same penetration and sites recorded from different penetrations.  We observed that MUA selectivity was correlated on spatial scales up to 800 &#61549;m while the LFP selectivity was correlated over a larger spatial extent, with significant correlations between sites separated by several mm.  Our data support the idea that there is some topographical arrangement to the organization of selectivity in inferior temporal cortex and that this organization may be relevant for the representation of object identity in IT.",1,AI
366,2004,"While single neurons in inferior temporal (IT) cortex show differential responses to distinct complex stimuli, little is known about the responses of populations of neurons in IT.  We recorded single electrode data, including multi-unit activity (MUA) and local field potentials (LFP), from 618 sites in the inferior temporal cortex of macaque monkeys while the animals passively viewed 78 different pictures of complex stimuli. The LFPs were obtained by low-pass filtering the extracellular electrophysiological signal with a corner frequency of 300 Hz. As reported previously, we observed that spike counts from MUA showed selectivity for some of the pictures.  Strikingly, the LFP data, which is thought to constitute an average over large numbers of neurons, also showed significantly selective responses.  The LFP responses were less selective than the MUA responses both in terms of the proportion of selective sites as well as in the selectivity of each site. We observed that there was only little overlap between the selectivity of MUA and LFP recordings from the same electrode.  To assess the spatial organization of selective responses, we compared the selectivity of nearby sites recorded along the same penetration and sites recorded from different penetrations.  We observed that MUA selectivity was correlated on spatial scales up to 800 &#61549;m while the LFP selectivity was correlated over a larger spatial extent, with significant correlations between sites separated by several mm.  Our data support the idea that there is some topographical arrangement to the organization of selectivity in inferior temporal cortex and that this organization may be relevant for the representation of object identity in IT.",123,object recognition
366,2004,"While single neurons in inferior temporal (IT) cortex show differential responses to distinct complex stimuli, little is known about the responses of populations of neurons in IT.  We recorded single electrode data, including multi-unit activity (MUA) and local field potentials (LFP), from 618 sites in the inferior temporal cortex of macaque monkeys while the animals passively viewed 78 different pictures of complex stimuli. The LFPs were obtained by low-pass filtering the extracellular electrophysiological signal with a corner frequency of 300 Hz. As reported previously, we observed that spike counts from MUA showed selectivity for some of the pictures.  Strikingly, the LFP data, which is thought to constitute an average over large numbers of neurons, also showed significantly selective responses.  The LFP responses were less selective than the MUA responses both in terms of the proportion of selective sites as well as in the selectivity of each site. We observed that there was only little overlap between the selectivity of MUA and LFP recordings from the same electrode.  To assess the spatial organization of selective responses, we compared the selectivity of nearby sites recorded along the same penetration and sites recorded from different penetrations.  We observed that MUA selectivity was correlated on spatial scales up to 800 &#61549;m while the LFP selectivity was correlated over a larger spatial extent, with significant correlations between sites separated by several mm.  Our data support the idea that there is some topographical arrangement to the organization of selectivity in inferior temporal cortex and that this organization may be relevant for the representation of object identity in IT.",363,inferior temporal cortex
366,2004,"While single neurons in inferior temporal (IT) cortex show differential responses to distinct complex stimuli, little is known about the responses of populations of neurons in IT.  We recorded single electrode data, including multi-unit activity (MUA) and local field potentials (LFP), from 618 sites in the inferior temporal cortex of macaque monkeys while the animals passively viewed 78 different pictures of complex stimuli. The LFPs were obtained by low-pass filtering the extracellular electrophysiological signal with a corner frequency of 300 Hz. As reported previously, we observed that spike counts from MUA showed selectivity for some of the pictures.  Strikingly, the LFP data, which is thought to constitute an average over large numbers of neurons, also showed significantly selective responses.  The LFP responses were less selective than the MUA responses both in terms of the proportion of selective sites as well as in the selectivity of each site. We observed that there was only little overlap between the selectivity of MUA and LFP recordings from the same electrode.  To assess the spatial organization of selective responses, we compared the selectivity of nearby sites recorded along the same penetration and sites recorded from different penetrations.  We observed that MUA selectivity was correlated on spatial scales up to 800 &#61549;m while the LFP selectivity was correlated over a larger spatial extent, with significant correlations between sites separated by several mm.  Our data support the idea that there is some topographical arrangement to the organization of selectivity in inferior temporal cortex and that this organization may be relevant for the representation of object identity in IT.",364,local field potentials
367,2004,"The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties.This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes.Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",1,AI
367,2004,"The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties.This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes.Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",365,Humanoid Robots
367,2004,"The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties.This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes.Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",366,Developmental Learning
367,2004,"The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties.This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes.Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",367,Perception
367,2004,"The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ""compliant perception"" of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties.This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes.Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.",368,Human-robot Interactions
368,2004,"The interval programming model (IvP) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  The central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. The piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.In this paper the model and solution algorithms are described, and theapplicability of IvP to certain applications arediscussed.",1,AI
368,2004,"The interval programming model (IvP) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  The central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. The piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.In this paper the model and solution algorithms are described, and theapplicability of IvP to certain applications arediscussed.",369,multi-objective decision making
368,2004,"The interval programming model (IvP) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  The central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. The piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.In this paper the model and solution algorithms are described, and theapplicability of IvP to certain applications arediscussed.",370,behavior-based control
368,2004,"The interval programming model (IvP) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  The central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. The piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.In this paper the model and solution algorithms are described, and theapplicability of IvP to certain applications arediscussed.",371,action selection
368,2004,"The interval programming model (IvP) is a mathematical programmingmodel for representing and solving multi-objective optimizationproblems.  The central characteristic of the model is the use ofpiecewise linearly defined objective functions and a solution methodthat searches through the combination space of pieces rather thanthrough the actual decision space. The piecewise functions typicallyrepresent an approximation of some underlying function, but thisconcession is balanced on the positive side by relative freedom fromfunction form assumptions as well as the assurance of global optimality.In this paper the model and solution algorithms are described, and theapplicability of IvP to certain applications arediscussed.",372,MCDM
369,2004,"Alternative pre-messenger RNA splicing affects a majority of human genes and plays important roles in development and disease.  Alternative splicing (AS) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  Here we describe sequence features that distinguish exons subject to evolutionarily conserved AS, which we call 'alternative-conserved exons' (ACEs) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, ACEScan.  Genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted ACEs.  Alternative splicing was verified in both human and mouse tissues using an RT-PCR-sequencing protocol for 21 of 30 (70%) predicted ACEs tested, supporting the validity of a majority of ACEScan predictions.  By contrast, AS was observed in mouse tissues for only 2 of 15 (13%) tested exons which had EST or cDNA evidence of AS in human but were not predicted ACEs, and was never observed for eleven negative control exons in human or mouse tissues.  Predicted ACEs were much more likely to preserve reading frame, and less likely to disrupt protein domains than other AS events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, RNA processing and development.  Our results also imply that the vast majority of AS events represented in the human EST databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",1,AI
369,2004,"Alternative pre-messenger RNA splicing affects a majority of human genes and plays important roles in development and disease.  Alternative splicing (AS) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  Here we describe sequence features that distinguish exons subject to evolutionarily conserved AS, which we call 'alternative-conserved exons' (ACEs) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, ACEScan.  Genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted ACEs.  Alternative splicing was verified in both human and mouse tissues using an RT-PCR-sequencing protocol for 21 of 30 (70%) predicted ACEs tested, supporting the validity of a majority of ACEScan predictions.  By contrast, AS was observed in mouse tissues for only 2 of 15 (13%) tested exons which had EST or cDNA evidence of AS in human but were not predicted ACEs, and was never observed for eleven negative control exons in human or mouse tissues.  Predicted ACEs were much more likely to preserve reading frame, and less likely to disrupt protein domains than other AS events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, RNA processing and development.  Our results also imply that the vast majority of AS events represented in the human EST databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",373,alternative splicing
369,2004,"Alternative pre-messenger RNA splicing affects a majority of human genes and plays important roles in development and disease.  Alternative splicing (AS) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  Here we describe sequence features that distinguish exons subject to evolutionarily conserved AS, which we call 'alternative-conserved exons' (ACEs) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, ACEScan.  Genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted ACEs.  Alternative splicing was verified in both human and mouse tissues using an RT-PCR-sequencing protocol for 21 of 30 (70%) predicted ACEs tested, supporting the validity of a majority of ACEScan predictions.  By contrast, AS was observed in mouse tissues for only 2 of 15 (13%) tested exons which had EST or cDNA evidence of AS in human but were not predicted ACEs, and was never observed for eleven negative control exons in human or mouse tissues.  Predicted ACEs were much more likely to preserve reading frame, and less likely to disrupt protein domains than other AS events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, RNA processing and development.  Our results also imply that the vast majority of AS events represented in the human EST databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",374,comparative genomics
369,2004,"Alternative pre-messenger RNA splicing affects a majority of human genes and plays important roles in development and disease.  Alternative splicing (AS) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  Here we describe sequence features that distinguish exons subject to evolutionarily conserved AS, which we call 'alternative-conserved exons' (ACEs) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, ACEScan.  Genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted ACEs.  Alternative splicing was verified in both human and mouse tissues using an RT-PCR-sequencing protocol for 21 of 30 (70%) predicted ACEs tested, supporting the validity of a majority of ACEScan predictions.  By contrast, AS was observed in mouse tissues for only 2 of 15 (13%) tested exons which had EST or cDNA evidence of AS in human but were not predicted ACEs, and was never observed for eleven negative control exons in human or mouse tissues.  Predicted ACEs were much more likely to preserve reading frame, and less likely to disrupt protein domains than other AS events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, RNA processing and development.  Our results also imply that the vast majority of AS events represented in the human EST databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",60,classification
369,2004,"Alternative pre-messenger RNA splicing affects a majority of human genes and plays important roles in development and disease.  Alternative splicing (AS) events conserved since the divergence of human and mouse are likely of primary biological importance, but relatively few such events are known.  Here we describe sequence features that distinguish exons subject to evolutionarily conserved AS, which we call 'alternative-conserved exons' (ACEs) from other orthologous human/mouse exons, and integrate these features into an exon classification algorithm, ACEScan.  Genome-wide analysis of annotated orthologous human-mouse exon pairs identified ~2,000 predicted ACEs.  Alternative splicing was verified in both human and mouse tissues using an RT-PCR-sequencing protocol for 21 of 30 (70%) predicted ACEs tested, supporting the validity of a majority of ACEScan predictions.  By contrast, AS was observed in mouse tissues for only 2 of 15 (13%) tested exons which had EST or cDNA evidence of AS in human but were not predicted ACEs, and was never observed for eleven negative control exons in human or mouse tissues.  Predicted ACEs were much more likely to preserve reading frame, and less likely to disrupt protein domains than other AS events, and were enriched in genes expressed in the brain and in genes involved in transcriptional regulation, RNA processing and development.  Our results also imply that the vast majority of AS events represented in the human EST databases are not conserved in mouse, and therefore may represent aberrant, disease- or allele-specific, or highly lineage-restricted splicing events.",135,regularization
370,2004,"Throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  This thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state NMR spectroscopy.  A separate portion of the thesis isabout protein-DNA binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.The search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromNMR experiments.  Systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  Our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) OmniMerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  Although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  Propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of OmniMerge.(3) The search may be run in two stages, once at low-resolutionusing a side-effect of OmniMerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) An A* function prioritizes each subchain based onestimated future search costs.  Subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.A common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.In addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.These novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",1,AI
370,2004,"Throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  This thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state NMR spectroscopy.  A separate portion of the thesis isabout protein-DNA binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.The search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromNMR experiments.  Systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  Our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) OmniMerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  Although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  Propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of OmniMerge.(3) The search may be run in two stages, once at low-resolutionusing a side-effect of OmniMerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) An A* function prioritizes each subchain based onestimated future search costs.  Subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.A common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.In addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.These novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",375,Distance Geometry
370,2004,"Throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  This thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state NMR spectroscopy.  A separate portion of the thesis isabout protein-DNA binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.The search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromNMR experiments.  Systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  Our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) OmniMerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  Although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  Propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of OmniMerge.(3) The search may be run in two stages, once at low-resolutionusing a side-effect of OmniMerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) An A* function prioritizes each subchain based onestimated future search costs.  Subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.A common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.In addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.These novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",376,Nuclear Magnetic Resonance (NMR)
370,2004,"Throughout biological, chemical, and pharmaceutical research,conformational searches are used to explore the possiblethree-dimensional configurations of molecules.  This thesis describesa new systematic method for conformational search, including anapplication of the method to determining the structure of a peptidevia solid-state NMR spectroscopy.  A separate portion of the thesis isabout protein-DNA binding, with a three-dimensional macromolecularstructure determined by x-ray crystallography.The search method in this thesis enumerates all conformations of amolecule (at a given level of torsion angle resolution) that satisfy aset of local geometric constraints, such as constraints derived fromNMR experiments.  Systematic searches, historically used for smallmolecules, generally now use some form of divide-and-conquer forapplication to larger molecules.  Our method can achieve a significantimprovement in runtime by making some major and counter-intuitivemodifications to traditional divide-and-conquer:(1) OmniMerge divides a polymer into many alternative pairs ofsubchains and searches all the pairs, instead of simply cutting inhalf and searching two subchains.  Although the extra searches mayappear wasteful, the bottleneck stage of the overall search, which isto re-connect the conformations of the largest subchains, can be greatlyaccelerated by the availability of alternative pairs of sidechains.(2)  Propagation of disqualified conformations acrossoverlapping subchains can disqualify infeasible conformations veryrapidly, which further offsets the cost of searching the extrasubchains of OmniMerge.(3) The search may be run in two stages, once at low-resolutionusing a side-effect of OmniMerge to determine an optimalpartitioning of the molecule into efficient subchains; then again athigh-resolution while making use of the precomputed subchains.(4) An A* function prioritizes each subchain based onestimated future search costs.  Subchains with sufficiently lowpriority can be omitted from the search, which improves efficiency.A common theme of these four ideas is to make good choices about howto break the large search problem into lower-dimensional subproblems.In addition, the search method uses heuristic local searches withinthe overall systematic framework, to maintain the systematic guaranteewhile providing the empirical efficiency of stochastic search.These novel algorithms were implemented and the effectiveness of eachinnovation is demonstrated on a highly constrained peptide with 40degrees of freedom.",377,Molecular Modeling
375,2004,"One of the reasons that it is difficult to plan and act in real-worlddomains is that they are very large.  Existing research generallydeals with the large domain size using a static representation andexploiting a single type of domain structure.  In this paper, wecreate a framework that encapsulates existing and new abstraction andapproximation methods into modules, and combines arbitrary modulesinto a system that allows for dynamic representation changes.  We showthat the dynamic changes of representation allow our framework tosolve larger and more interesting domains than were previouslypossible, and while there are no optimality guarantees, suitablemodule choices gain tractability at little cost to optimality.",1,AI
380,2004,"The zebra finch is a standard experimental system for studying learning and generation of temporally extended motor patterns.  The first part of this project concerned the evaluation of simple models for the operation and structure of the network in the motor nucleus RA.  A directed excitatory chain with a global inhibitory network, for which experimental evidence exists, was found to produce waves of activity similar to those observed in RA; this similarity included one particularly important feature of the measured activity, synchrony between the onset of bursting in one neuron and the offset of bursting in another.  Other models, which were simpler and more analytically tractable, were also able to exhibit this feature, but not for parameter values quantitatively close to those observed.Another issue of interest concerns how these networks are initially learned by the bird during song acquisition.  The second part of the project concerned the analysis of exemplars of REINFORCE algorithms, a general class of algorithms for reinforcement learning in neural networks, which are on several counts more biologically plausible than standard prescriptions such as backpropagation.  The former compared favorably with backpropagation on tasks involving single input-output pairs, though a noise analysis suggested it should not perform so well.  On tasks involving trajectory learning, REINFORCE algorithms meet with some success, though the analysis that predicts their success on input-output-pair tasks fails to explain it for trajectories.",1,AI
382,2004,"The computational processes in the intermediate stages of the ventral pathway responsible for visual object recognition are not well understood. A recent physiological study by A. Pasupathy and C. Connor in intermediate area V4 using contour stimuli, proposes that a population of V4 neurons display bjectcentered,position-specific curvature tuning [18]. The Â“standard modelÂ” of object recognition, a recently developed model [23] to account for recognition properties of IT cells (extending classical suggestions by Hubel, Wiesel and others [9, 10, 19]), is used here to model the response of the V4 cells described in [18]. Our results show that a feedforward, network level mechanism can exhibit selectivity and invariance properties that correspond to the responses of the V4 cells described in [18]. These results suggest howobject-centered, position-specific curvature tuning of V4 cells may arise from combinations of complex V1 cell responses. Furthermore, the model makes predictions about the responses of the same V4 cells studied by Pasupathy and Connor to novel gray level patterns, such as gratings and natural images. Thesepredictions suggest specific experiments to further explore shape representation in V4.",1,AI
382,2004,"The computational processes in the intermediate stages of the ventral pathway responsible for visual object recognition are not well understood. A recent physiological study by A. Pasupathy and C. Connor in intermediate area V4 using contour stimuli, proposes that a population of V4 neurons display bjectcentered,position-specific curvature tuning [18]. The Â“standard modelÂ” of object recognition, a recently developed model [23] to account for recognition properties of IT cells (extending classical suggestions by Hubel, Wiesel and others [9, 10, 19]), is used here to model the response of the V4 cells described in [18]. Our results show that a feedforward, network level mechanism can exhibit selectivity and invariance properties that correspond to the responses of the V4 cells described in [18]. These results suggest howobject-centered, position-specific curvature tuning of V4 cells may arise from combinations of complex V1 cell responses. Furthermore, the model makes predictions about the responses of the same V4 cells studied by Pasupathy and Connor to novel gray level patterns, such as gratings and natural images. Thesepredictions suggest specific experiments to further explore shape representation in V4.",378,V4 Visual Cortex Object Recognition Standard Model
383,2004,"In this paper, we present and analyze a novel regularization technique based on enhancing our dataset with corrupted copies of the original data. The motivation is that since the learning algorithm lacks information about which parts of thedata are reliable, it has to produce more robust classification functions. We then demonstrate how this regularization leads to redundancy in the resulting  classifiers, which is somewhat in contrast to the common interpretations of the OccamÂ’s razor principle. Using this framework, we propose a simple addition to the gentle boosting algorithm which enables it to work with only a few examples. We test this new algorithm on a variety of datasets and show convincing results.",1,AI
384,2004,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",1,AI
384,2004,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",165,visual cortex
384,2004,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",123,object recognition
384,2004,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",7,face detection
384,2004,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",379,hierarchy
384,2004,"In this paper, we introduce a novel set of features for robust object recognition, which exhibits outstanding performances on a variety ofobject categories while being capable of learning from only a fewtraining examples. Each element of this set is a complex featureobtained by combining position- and scale-tolerant edge-detectors overneighboring positions and multiple orientations.Our system - motivated by a quantitative model of visual cortex -outperforms state-of-the-art systems on a variety of object imagedatasets from different groups. We also show that our system is ableto learn from very few examples with no prior category knowledge.  Thesuccess of the approach is also a suggestive plausibility proof for aclass of feed-forward models of object recognition in cortex. Finally,we conjecture the existence of a universal overcompletedictionary of features that could handle the recognition of all objectcategories.",380,feature learning
386,2004,"Matrices that can be factored into a product of two simpler matricescan serve as a useful and often natural model in the analysis oftabulated or high-dimensional data.  Models based on matrixfactorization (Factor Analysis, PCA) have been extensively used instatistical analysis and machine learning for over a century, withmany new formulations and models suggested in recent years (LatentSemantic Indexing, Aspect Models, Probabilistic PCA, Exponential PCA,Non-Negative Matrix Factorization and others).  In this thesis weaddress several issues related to learning with matrix factorizations:we study the asymptotic behavior and generalization ability ofexisting methods, suggest new optimization methods, and present anovel maximum-margin high-dimensional matrix factorizationformulation.",1,AI
387,2004,"Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes the minimal-cost correspondence between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.We evaluate our technique with scene recognition and texture classification tasks.",1,AI
387,2004,"Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes the minimal-cost correspondence between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.We evaluate our technique with scene recognition and texture classification tasks.",381,image matching
387,2004,"Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes the minimal-cost correspondence between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.We evaluate our technique with scene recognition and texture classification tasks.",123,object recognition
387,2004,"Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes the minimal-cost correspondence between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.We evaluate our technique with scene recognition and texture classification tasks.",382,content-based image retrieval
387,2004,"Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes the minimal-cost correspondence between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We also show how the feature representation may be extended to encode the distribution of geometric constraints between the invariant features appearing in each image.We evaluate our technique with scene recognition and texture classification tasks.",383,texture
391,2004,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",1,AI
391,2004,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",384,tree-width
391,2004,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",385,hypertrees
391,2004,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",386,Markov Networks
391,2004,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",387,maximum likelihood
391,2004,"Markov trees generalize naturally to bounded tree-width Markov networks, onwhich exact computations can still be done efficiently.  However, learning themaximum likelihood Markov network with tree-width greater than 1 is NP-hard, sowe discuss a few algorithms for approximating the optimal Markov network.  Wepresent a set of methods for training a density estimator.  Each method isspecified by three arguments: tree-width, model scoring metric (maximumlikelihood or minimum description length), and model representation (using onejoint distribution or several class-conditional distributions).  On thesemethods, we give empirical results on density estimation and classificationtasks and explore the implications of these arguments.",388,MDL
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,1,AI
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,389,tuning
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,390,gain control
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,391,normalization
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,392,Gaussian
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,393,neuron
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,394,cortex
392,2004,Tuning to an optimal stimulus is a widespread property of neurons in cortex. We propose that such tuning is a consequence of normalization or gain control circuits. We also present a biologically plausible neural circuitry of tuning.,395,biophysics
393,2004,"Â“Winner-take-allÂ” networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",1,AI
393,2004,"Â“Winner-take-allÂ” networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",396,WTA
393,2004,"Â“Winner-take-allÂ” networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",397,Borda machine
393,2004,"Â“Winner-take-allÂ” networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",398,Condorcet procedure
393,2004,"Â“Winner-take-allÂ” networks typically pick as winners that alternative with the largest excitatory input. This choice is far from optimal when there is uncertainty in the strength of the inputs, and when information is available about how alternatives may be related. In the Social Choice community, many other procedures will yield more robust winners. The Borda Count and the pair-wise Condorcet tally are among the most favored. Their implementations are simple modifications of classical recurrent networks.",399,neural network
398,2005,"Inspired by the robustness and flexibility of biological systems, we are developing linguistic and programming tools to allow us to program spatial systems populated by vast numbers of unreliable components interconnected in unknown, irregular, and time-varying ways. We organize our computations around geometry, making the fact that our system is made up of discrete individuals implicit. Geometry allows us to specify requirements in terms of the behavior of the space occupied by the aggregate rather than the behavior of individuals, thereby decreasing complexity. So we describe the behavior of space explicitly, abstracting away the discrete nature of the components. As an example, we present the Amorphous Medium Language, which describes behavior in terms of homeostatic maintenance of constraints on nested regions of space.",1,AI
398,2005,"Inspired by the robustness and flexibility of biological systems, we are developing linguistic and programming tools to allow us to program spatial systems populated by vast numbers of unreliable components interconnected in unknown, irregular, and time-varying ways. We organize our computations around geometry, making the fact that our system is made up of discrete individuals implicit. Geometry allows us to specify requirements in terms of the behavior of the space occupied by the aggregate rather than the behavior of individuals, thereby decreasing complexity. So we describe the behavior of space explicitly, abstracting away the discrete nature of the components. As an example, we present the Amorphous Medium Language, which describes behavior in terms of homeostatic maintenance of constraints on nested regions of space.",400,amorphous robust biological spatial sensor networks language programming medium
400,2005,"In this thesis I will be concerned with linking the observed speechsignal to the configuration of articulators.Due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. The typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.I argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. The patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.My patterns are an alternative to frequency. I use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. These features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. No linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.The measurements I use are neurophysiologically plausible: theauditory system could be using similar methods.I have demonstrated this approach by constructing a robust techniquefor categorizing the English voiced stops as the consonants B, D, or Gbased on the vocalic portions of their releases. The classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",1,AI
400,2005,"In this thesis I will be concerned with linking the observed speechsignal to the configuration of articulators.Due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. The typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.I argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. The patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.My patterns are an alternative to frequency. I use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. These features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. No linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.The measurements I use are neurophysiologically plausible: theauditory system could be using similar methods.I have demonstrated this approach by constructing a robust techniquefor categorizing the English voiced stops as the consonants B, D, or Gbased on the vocalic portions of their releases. The classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",401,speech processing
400,2005,"In this thesis I will be concerned with linking the observed speechsignal to the configuration of articulators.Due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. The typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.I argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. The patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.My patterns are an alternative to frequency. I use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. These features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. No linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.The measurements I use are neurophysiologically plausible: theauditory system could be using similar methods.I have demonstrated this approach by constructing a robust techniquefor categorizing the English voiced stops as the consonants B, D, or Gbased on the vocalic portions of their releases. The classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",402,stop consonants
400,2005,"In this thesis I will be concerned with linking the observed speechsignal to the configuration of articulators.Due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. The typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.I argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. The patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.My patterns are an alternative to frequency. I use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. These features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. No linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.The measurements I use are neurophysiologically plausible: theauditory system could be using similar methods.I have demonstrated this approach by constructing a robust techniquefor categorizing the English voiced stops as the consonants B, D, or Gbased on the vocalic portions of their releases. The classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",403,pitch period
400,2005,"In this thesis I will be concerned with linking the observed speechsignal to the configuration of articulators.Due to the potentially rapid motion of the articulators, the speechsignal can be highly non-stationary. The typical linear analysistechniques that assume quasi-stationarity may not have sufficienttime-frequency resolution to determine the place of articulation.I argue that the traditional low and high-level primitives of speechprocessing, frequency and phonemes, are inadequate and should bereplaced by a representation with three layers: 1. short pitch periodresonances and other spatio-temporal patterns 2. articulatorconfiguration trajectories 3. syllables. The patterns indicatearticulator configuration trajectories (how the tongue, jaws, etc. aremoving), which are interpreted as syllables and words.My patterns are an alternative to frequency. I use shorttime-domain features of the sound waveform, which can be extractedfrom each vowel pitch period pattern, to identify the positions of thearticulators with high reliability. These features are importantbecause by capitalizing on detailed measurements within a single pitchperiod, the rapid articulator movements can be tracked. No linearsignal processing approach can achieve the combination of sensitivityto short term changes and measurement accuracy resulting from thesenonlinear techniques.The measurements I use are neurophysiologically plausible: theauditory system could be using similar methods.I have demonstrated this approach by constructing a robust techniquefor categorizing the English voiced stops as the consonants B, D, or Gbased on the vocalic portions of their releases. The classificationrecognizes 93.5%, 81.8% and 86.1% of the b, d and gto ae transitions with false positive rates 2.9%, 8.7% and2.6% respectively.",404,spatio-temporal patterns
402,2005,"Differential geometry is deceptively simple.  It is surprisingly easyto get the right answer with unclear and informal symbol manipulation.To address this problem we use computer programs to communicate aprecise understanding of the computations in differential geometry.Expressing the methods of differential geometry in a computer languageforces them to be unambiguous and computationally effective.  The taskof formulating a method as a computer-executable program and debuggingthat program is a powerful exercise in the learning process.  Also,once formalized procedurally, a mathematical idea becomes a tool thatcan be used directly to compute results.",1,AI
402,2005,"Differential geometry is deceptively simple.  It is surprisingly easyto get the right answer with unclear and informal symbol manipulation.To address this problem we use computer programs to communicate aprecise understanding of the computations in differential geometry.Expressing the methods of differential geometry in a computer languageforces them to be unambiguous and computationally effective.  The taskof formulating a method as a computer-executable program and debuggingthat program is a powerful exercise in the learning process.  Also,once formalized procedurally, a mathematical idea becomes a tool thatcan be used directly to compute results.",405,Scheme  differential geometry calculus manifolds
403,2005,"Traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. At the same time, computer algorithms for the synthesis of natural textures have improved dramatically. The current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. We employ Portilla and SimoncelliÂ’s 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). We find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",1,AI
403,2005,"Traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. At the same time, computer algorithms for the synthesis of natural textures have improved dramatically. The current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. We employ Portilla and SimoncelliÂ’s 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). We find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",406,texture perception
403,2005,"Traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. At the same time, computer algorithms for the synthesis of natural textures have improved dramatically. The current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. We employ Portilla and SimoncelliÂ’s 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). We find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",407,texture synthesis
403,2005,"Traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. At the same time, computer algorithms for the synthesis of natural textures have improved dramatically. The current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. We employ Portilla and SimoncelliÂ’s 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). We find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",314,psychophysics
403,2005,"Traditionally, human texture perception has been studied using artificial textures made of random-dot patterns or abstract structured elements. At the same time, computer algorithms for the synthesis of natural textures have improved dramatically. The current study seeks to unify these two fields of research through a psychophysical assessment of a particular computational model, thus providing a sense of what image statistics are most vital for representing a range of natural textures. We employ Portilla and SimoncelliÂ’s 2000 model of texture synthesis for this task (a parametric model of analysis and synthesis designed to mimic computations carried out by the human visual system). We find an intriguing interaction between texture type (periodic v. structured) and image statistics (autocorrelation function and filter magnitude correlations), suggesting different processing strategies may be employed for these two texture families under pre-attentive viewing.",73,natural images
406,2005,"The ISO/IEC 8802-11:1999(E) specification uses a 32-bit CRC for error detection and whole-packet retransmissions for recovery. In long-distance orhigh-interference links where the probability of a bit error is high,this strategy results in excessive losses, because any erroneous bitcauses an entire packet to be discarded. By ignoring the CRC andadding redundancy to 802.11 payloads in software, we achievedsubstantially reduced loss rates on indoor and outdoor long-distancelinks and extended line-of-sight range outdoors by 70 percent.",1,AI
408,2005,"Given a set of images containing multiple object categories,we seek to discover those categories and their image locations withoutsupervision.  We achieve this using generative modelsfrom the statistical text literature: probabilistic Latent SemanticAnalysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysisthese are used to discover topics in a corpus using the bag-of-wordsdocument representation. Here we discover topics as object categories, sothat an image containing instances of several categories is modelled as amixture of topics.The models are applied to images by using avisual analogue of a word, formed by vector quantizing SIFT like regiondescriptors.  We investigate a set of increasingly demanding scenarios,starting with image sets containing only two object categories through tosets containing multiple categories (including airplanes, cars, faces,motorbikes, spotted cats) and background clutter. The object categoriessample both intra-class and scale variation, and both the categories andtheir approximate spatial layout are found without supervision.We also demonstrate classification of unseen images and images containingmultiple objects. Performance of the proposed unsupervised method is compared tothe semi-supervised approach of Fergus et al.",1,AI
410,2005,"Localized operators, like Gabor wavelets and difference-of-Gaussian filters, are considered to be useful tools for image representation. This is due to their ability to form a Â‘sparse codeÂ’ that can serve as a basis set for high-fidelity reconstruction of natural images. However, for many visual tasks, the more appropriate criterion of representational efficacy is Â‘recognitionÂ’, rather than Â‘reconstructionÂ’. It is unclear whether simple local features provide the stability necessary to subserve robust recognition of complex objects. In this paper, we search the space of two-lobed differential operators for those that constitute a good representational code under recognition/discrimination criteria. We find that a novel operator, which we call the Â‘dissociated dipoleÂ’ displays useful properties in this regard. We describe simple computational experiments to assess the merits of such dipoles relative to the more traditional local operators. The results suggest that non-local operators constitute a vocabulary that is stable across a range of image transformations.",1,AI
410,2005,"Localized operators, like Gabor wavelets and difference-of-Gaussian filters, are considered to be useful tools for image representation. This is due to their ability to form a Â‘sparse codeÂ’ that can serve as a basis set for high-fidelity reconstruction of natural images. However, for many visual tasks, the more appropriate criterion of representational efficacy is Â‘recognitionÂ’, rather than Â‘reconstructionÂ’. It is unclear whether simple local features provide the stability necessary to subserve robust recognition of complex objects. In this paper, we search the space of two-lobed differential operators for those that constitute a good representational code under recognition/discrimination criteria. We find that a novel operator, which we call the Â‘dissociated dipoleÂ’ displays useful properties in this regard. We describe simple computational experiments to assess the merits of such dipoles relative to the more traditional local operators. The results suggest that non-local operators constitute a vocabulary that is stable across a range of image transformations.",123,object recognition
410,2005,"Localized operators, like Gabor wavelets and difference-of-Gaussian filters, are considered to be useful tools for image representation. This is due to their ability to form a Â‘sparse codeÂ’ that can serve as a basis set for high-fidelity reconstruction of natural images. However, for many visual tasks, the more appropriate criterion of representational efficacy is Â‘recognitionÂ’, rather than Â‘reconstructionÂ’. It is unclear whether simple local features provide the stability necessary to subserve robust recognition of complex objects. In this paper, we search the space of two-lobed differential operators for those that constitute a good representational code under recognition/discrimination criteria. We find that a novel operator, which we call the Â‘dissociated dipoleÂ’ displays useful properties in this regard. We describe simple computational experiments to assess the merits of such dipoles relative to the more traditional local operators. The results suggest that non-local operators constitute a vocabulary that is stable across a range of image transformations.",408,face recognition
410,2005,"Localized operators, like Gabor wavelets and difference-of-Gaussian filters, are considered to be useful tools for image representation. This is due to their ability to form a Â‘sparse codeÂ’ that can serve as a basis set for high-fidelity reconstruction of natural images. However, for many visual tasks, the more appropriate criterion of representational efficacy is Â‘recognitionÂ’, rather than Â‘reconstructionÂ’. It is unclear whether simple local features provide the stability necessary to subserve robust recognition of complex objects. In this paper, we search the space of two-lobed differential operators for those that constitute a good representational code under recognition/discrimination criteria. We find that a novel operator, which we call the Â‘dissociated dipoleÂ’ displays useful properties in this regard. We describe simple computational experiments to assess the merits of such dipoles relative to the more traditional local operators. The results suggest that non-local operators constitute a vocabulary that is stable across a range of image transformations.",409,sparse coding
412,2005,"Objects can exhibit different dynamics at different scales, a property that isoftenexploited by visual tracking algorithms. A local dynamicmodel is typically used to extract image features that are then used as inputsto a system for tracking the entire object using a global dynamic model.Approximate local dynamicsmay be brittle---point trackers drift due to image noise and adaptivebackground models adapt to foreground objects that becomestationary---but constraints from the global model can make them more robust.We propose a probabilistic framework for incorporating globaldynamics knowledge into the local feature extraction processes.A global tracking algorithm can beformulated as a generative model and used to predict feature values thatinfluence the observation process of thefeature extractor. We combine such models in a multichain graphicalmodel framework.We show the utility of our framework for improving feature tracking and thusshapeand motion estimates in a batch factorization algorithm.We also propose an approximate filtering algorithm appropriate for onlineapplications, and demonstrate its application to problems such as backgroundsubtraction, structure from motion and articulated body tracking.",1,AI
412,2005,"Objects can exhibit different dynamics at different scales, a property that isoftenexploited by visual tracking algorithms. A local dynamicmodel is typically used to extract image features that are then used as inputsto a system for tracking the entire object using a global dynamic model.Approximate local dynamicsmay be brittle---point trackers drift due to image noise and adaptivebackground models adapt to foreground objects that becomestationary---but constraints from the global model can make them more robust.We propose a probabilistic framework for incorporating globaldynamics knowledge into the local feature extraction processes.A global tracking algorithm can beformulated as a generative model and used to predict feature values thatinfluence the observation process of thefeature extractor. We combine such models in a multichain graphicalmodel framework.We show the utility of our framework for improving feature tracking and thusshapeand motion estimates in a batch factorization algorithm.We also propose an approximate filtering algorithm appropriate for onlineapplications, and demonstrate its application to problems such as backgroundsubtraction, structure from motion and articulated body tracking.",410,graphical models
412,2005,"Objects can exhibit different dynamics at different scales, a property that isoftenexploited by visual tracking algorithms. A local dynamicmodel is typically used to extract image features that are then used as inputsto a system for tracking the entire object using a global dynamic model.Approximate local dynamicsmay be brittle---point trackers drift due to image noise and adaptivebackground models adapt to foreground objects that becomestationary---but constraints from the global model can make them more robust.We propose a probabilistic framework for incorporating globaldynamics knowledge into the local feature extraction processes.A global tracking algorithm can beformulated as a generative model and used to predict feature values thatinfluence the observation process of thefeature extractor. We combine such models in a multichain graphicalmodel framework.We show the utility of our framework for improving feature tracking and thusshapeand motion estimates in a batch factorization algorithm.We also propose an approximate filtering algorithm appropriate for onlineapplications, and demonstrate its application to problems such as backgroundsubtraction, structure from motion and articulated body tracking.",411,feature extraction
412,2005,"Objects can exhibit different dynamics at different scales, a property that isoftenexploited by visual tracking algorithms. A local dynamicmodel is typically used to extract image features that are then used as inputsto a system for tracking the entire object using a global dynamic model.Approximate local dynamicsmay be brittle---point trackers drift due to image noise and adaptivebackground models adapt to foreground objects that becomestationary---but constraints from the global model can make them more robust.We propose a probabilistic framework for incorporating globaldynamics knowledge into the local feature extraction processes.A global tracking algorithm can beformulated as a generative model and used to predict feature values thatinfluence the observation process of thefeature extractor. We combine such models in a multichain graphicalmodel framework.We show the utility of our framework for improving feature tracking and thusshapeand motion estimates in a batch factorization algorithm.We also propose an approximate filtering algorithm appropriate for onlineapplications, and demonstrate its application to problems such as backgroundsubtraction, structure from motion and articulated body tracking.",412,tracking
413,2005,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",1,AI
413,2005,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",413,kernel
413,2005,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",414,unordered sets
413,2005,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",415,correspondence
413,2005,"Discriminative learning is challenging when examples are setsof local image features, and the sets vary in cardinality and lackany sort of meaningful ordering.  Kernel-based classificationmethods can learn complex decision boundaries, but a kernelsimilarity measure for unordered set inputs must somehow solve forcorrespondences -- generally a computationally expensive task thatbecomes impractical for large set sizes.  We present a new fastkernel function which maps unordered feature sets tomulti-resolution histograms and computes a weighted histogramintersection in this space.  This ``pyramid match"" computation islinear in the number of features, and it implicitly findscorrespondences based on the finest resolution histogram cell wherea matched pair first appears. Since the kernel does not penalize thepresence of extra features, it is robust to clutter.  We show thekernel function is positive-definite, making it valid for use inlearning algorithms whose optimal solutions are guaranteed only forMercer kernels.  We demonstrate our algorithm on object recognitiontasks and show it to be dramatically faster than currentapproaches.",123,object recognition
415,2005,"This paper bridges the gap between variable selection methods (e.g., Pearson coefficients, KS test) and dimensionality reductionalgorithms (e.g., PCA, LDA). Variable selection algorithms encounter difficulties dealing with highly correlated data,since many features are similar in quality. Dimensionality reduction algorithms tend to combine all variables and cannotselect a subset of significant variables.Our approach combines both methodologies by applying variable selection followed by dimensionality reduction. Thiscombination makes sense only when using the same utility function in both stages, which we do. The resulting algorithmbenefits from complex features as variable selection algorithms do, and at the same time enjoys the benefits of dimensionalityreduction.1",1,AI
415,2005,"This paper bridges the gap between variable selection methods (e.g., Pearson coefficients, KS test) and dimensionality reductionalgorithms (e.g., PCA, LDA). Variable selection algorithms encounter difficulties dealing with highly correlated data,since many features are similar in quality. Dimensionality reduction algorithms tend to combine all variables and cannotselect a subset of significant variables.Our approach combines both methodologies by applying variable selection followed by dimensionality reduction. Thiscombination makes sense only when using the same utility function in both stages, which we do. The resulting algorithmbenefits from complex features as variable selection algorithms do, and at the same time enjoys the benefits of dimensionalityreduction.1",416,Computer Vision
415,2005,"This paper bridges the gap between variable selection methods (e.g., Pearson coefficients, KS test) and dimensionality reductionalgorithms (e.g., PCA, LDA). Variable selection algorithms encounter difficulties dealing with highly correlated data,since many features are similar in quality. Dimensionality reduction algorithms tend to combine all variables and cannotselect a subset of significant variables.Our approach combines both methodologies by applying variable selection followed by dimensionality reduction. Thiscombination makes sense only when using the same utility function in both stages, which we do. The resulting algorithmbenefits from complex features as variable selection algorithms do, and at the same time enjoys the benefits of dimensionalityreduction.1",417,Statistical Learning
415,2005,"This paper bridges the gap between variable selection methods (e.g., Pearson coefficients, KS test) and dimensionality reductionalgorithms (e.g., PCA, LDA). Variable selection algorithms encounter difficulties dealing with highly correlated data,since many features are similar in quality. Dimensionality reduction algorithms tend to combine all variables and cannotselect a subset of significant variables.Our approach combines both methodologies by applying variable selection followed by dimensionality reduction. Thiscombination makes sense only when using the same utility function in both stages, which we do. The resulting algorithmbenefits from complex features as variable selection algorithms do, and at the same time enjoys the benefits of dimensionalityreduction.1",418,Variable Selection
416,2005,"This paper presents a statistical framework which combines the registration of an atlas with the segmentation of MR images. We use an Expectation Maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. An example of the approach is given for a brain structure-dependent affine mapping approach. The algorithm produces high quality segmentations for brain tissues as well as their substructures. We demonstrate the approach on a set of 30 brain MR images. In addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",1,AI
416,2005,"This paper presents a statistical framework which combines the registration of an atlas with the segmentation of MR images. We use an Expectation Maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. An example of the approach is given for a brain structure-dependent affine mapping approach. The algorithm produces high quality segmentations for brain tissues as well as their substructures. We demonstrate the approach on a set of 30 brain MR images. In addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",419,Expectation Maximization
416,2005,"This paper presents a statistical framework which combines the registration of an atlas with the segmentation of MR images. We use an Expectation Maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. An example of the approach is given for a brain structure-dependent affine mapping approach. The algorithm produces high quality segmentations for brain tissues as well as their substructures. We demonstrate the approach on a set of 30 brain MR images. In addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",420,Segmentation
416,2005,"This paper presents a statistical framework which combines the registration of an atlas with the segmentation of MR images. We use an Expectation Maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. An example of the approach is given for a brain structure-dependent affine mapping approach. The algorithm produces high quality segmentations for brain tissues as well as their substructures. We demonstrate the approach on a set of 30 brain MR images. In addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",421,Registration
416,2005,"This paper presents a statistical framework which combines the registration of an atlas with the segmentation of MR images. We use an Expectation Maximization-based algorithm to find a solution within the model, which simultaneously estimates image inhomogeneities, anatomical labelmap, and a mapping from the atlas to the image space. An example of the approach is given for a brain structure-dependent affine mapping approach. The algorithm produces high quality segmentations for brain tissues as well as their substructures. We demonstrate the approach on a set of 30 brain MR images. In addition, we show that the approach performs better than similar methods which separate the registration from the segmentation problem.",422,Medical Image Analysis
420,2005,"We describe a system in which simple, identical, autonomous robots assemble two-dimensional structures out of identical building blocks.  We show that, in a system divided in this way into mobile units and structural units, giving the blocks limited communication abilities enables robots to have sufficient global structural knowledge to rapidly build elaborate pre-designed structures.  In this way we extend the principle of stigmergy (storing information in the environment) used by social insects, by increasing the capabilities of the blocks that represent that environmental information.  As a result, arbitrary solid structures can be built using a few fixed, local behaviors, without requiring construction to be planned out in detail.",1,AI
422,2005,"Examples are a powerful tool for teaching both humans and computers.In order to learn from examples, however, a student must first extractthe examples from its stream of perception. Snapshot learning is ageneral approach to this problem, in which relevant samples ofperception are used as examples.  Learning from these examples can inturn improve the judgement of the snapshot mechanism, improving thequality of future examples.  One way to implement snapshot learning isthe Top-Cliff heuristic, which identifies relevant samples using ageneralized notion of peaks. I apply snapshot learning with theTop-Cliff heuristic to solve a distributed learning problem and showthat the resulting system learns rapidly and robustly, and canhallucinate useful examples in a perceptual stream from a teacherlesssystem.",1,AI
422,2005,"Examples are a powerful tool for teaching both humans and computers.In order to learn from examples, however, a student must first extractthe examples from its stream of perception. Snapshot learning is ageneral approach to this problem, in which relevant samples ofperception are used as examples.  Learning from these examples can inturn improve the judgement of the snapshot mechanism, improving thequality of future examples.  One way to implement snapshot learning isthe Top-Cliff heuristic, which identifies relevant samples using ageneralized notion of peaks. I apply snapshot learning with theTop-Cliff heuristic to solve a distributed learning problem and showthat the resulting system learns rapidly and robustly, and canhallucinate useful examples in a perceptual stream from a teacherlesssystem.",423,unsupervised supervised learning examples
423,2005,"We develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel Hilbert spaces for supervised learning.  We show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  In fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",1,AI
423,2005,"We develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel Hilbert spaces for supervised learning.  We show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  In fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",424,optimal rates
423,2005,"We develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel Hilbert spaces for supervised learning.  We show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  In fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",425,regularized least-squares
423,2005,"We develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel Hilbert spaces for supervised learning.  We show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  In fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",426,reproducing kernel Hilbert space
423,2005,"We develop a theoretical analysis of generalization performances of regularized least-squares on reproducing kernel Hilbert spaces for supervised learning.  We show that the concept of effective dimension of an integral operator plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples.  In fact, a minimax analysis is performed which shows asymptotic optimality of the above-mentioned criterion.",427,effe
424,2005,"In human-human dialogues, face-to-face meetings are often preferred over phone conversations.One explanation is that non-verbal modalities such as gesture provide additionalinformation, making communication more efficient and accurate. If so, computerprocessing of natural language could improve by attending to non-verbal modalitiesas well. We consider the problem of sentence segmentation, using hand-annotatedgesture features to improve recognition. We find that gesture features correlate wellwith sentence boundaries, but that these features improve the overall performance of alanguage-only system only marginally. This finding is in line with previous research onthis topic. We provide a regression analysis, revealing that for sentence boundarydetection, the gestural features are largely redundant with the language model andpause features. This suggests that gestural features can still be useful when speech recognition is inaccurate.",1,AI
424,2005,"In human-human dialogues, face-to-face meetings are often preferred over phone conversations.One explanation is that non-verbal modalities such as gesture provide additionalinformation, making communication more efficient and accurate. If so, computerprocessing of natural language could improve by attending to non-verbal modalitiesas well. We consider the problem of sentence segmentation, using hand-annotatedgesture features to improve recognition. We find that gesture features correlate wellwith sentence boundaries, but that these features improve the overall performance of alanguage-only system only marginally. This finding is in line with previous research onthis topic. We provide a regression analysis, revealing that for sentence boundarydetection, the gestural features are largely redundant with the language model andpause features. This suggests that gestural features can still be useful when speech recognition is inaccurate.",428,gesture
424,2005,"In human-human dialogues, face-to-face meetings are often preferred over phone conversations.One explanation is that non-verbal modalities such as gesture provide additionalinformation, making communication more efficient and accurate. If so, computerprocessing of natural language could improve by attending to non-verbal modalitiesas well. We consider the problem of sentence segmentation, using hand-annotatedgesture features to improve recognition. We find that gesture features correlate wellwith sentence boundaries, but that these features improve the overall performance of alanguage-only system only marginally. This finding is in line with previous research onthis topic. We provide a regression analysis, revealing that for sentence boundarydetection, the gestural features are largely redundant with the language model andpause features. This suggests that gestural features can still be useful when speech recognition is inaccurate.",429,natural language processing
424,2005,"In human-human dialogues, face-to-face meetings are often preferred over phone conversations.One explanation is that non-verbal modalities such as gesture provide additionalinformation, making communication more efficient and accurate. If so, computerprocessing of natural language could improve by attending to non-verbal modalitiesas well. We consider the problem of sentence segmentation, using hand-annotatedgesture features to improve recognition. We find that gesture features correlate wellwith sentence boundaries, but that these features improve the overall performance of alanguage-only system only marginally. This finding is in line with previous research onthis topic. We provide a regression analysis, revealing that for sentence boundarydetection, the gestural features are largely redundant with the language model andpause features. This suggests that gestural features can still be useful when speech recognition is inaccurate.",207,multimodal
425,2005,"We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",1,AI
425,2005,"We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",430,sensor network
425,2005,"We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",431,localization
425,2005,"We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",432,bayesian filter
425,2005,"We introduce Simultaneous Localization and Tracking (SLAT), the  problem of tracking a target in a sensor network while  simultaneously localizing and calibrating the nodes of the network.  Our proposed solution, LaSLAT, is a Bayesian filter providing  on-line probabilistic estimates of sensor locations and target  tracks. It does not require globally accessible beacon signals or  accurate ranging between the nodes.  When applied to a network of 27  sensor nodes, our algorithm can localize the nodes to within one or  two centimeters.",433,extended kalman filter
427,2005,We show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel Hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  We first briefly introduce central concepts on operator-valued kernels.  Then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,1,AI
427,2005,We show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel Hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  We first briefly introduce central concepts on operator-valued kernels.  Then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,424,optimal rates
427,2005,We show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel Hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  We first briefly introduce central concepts on operator-valued kernels.  Then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,426,reproducing kernel Hilbert space
427,2005,We show that recent results in [3] on risk bounds for regularized least-squares on reproducing kernel Hilbert spaces can be straightforwardly extended to the vector-valued regression setting.  We first briefly introduce central concepts on operator-valued kernels.  Then we show how risk bounds can be expressed in terms of a generalization of effective dimension.,434,effective dimension
429,2005,"We study properties of algorithms which minimize (or almost minimize) empirical error over a Donsker class of functions. We show that the L2-diameter of the set of almost-minimizers is converging to zero in probability. Therefore, as the number of samples grows, it is becoming unlikely that adding a point (or a number of points) to the training set will result in a large jump (in L2 distance) to a new hypothesis. We also show that under some conditions the expected errors of the almost-minimizers are becoming close with a rate faster than n^{-1/2}.",1,AI
429,2005,"We study properties of algorithms which minimize (or almost minimize) empirical error over a Donsker class of functions. We show that the L2-diameter of the set of almost-minimizers is converging to zero in probability. Therefore, as the number of samples grows, it is becoming unlikely that adding a point (or a number of points) to the training set will result in a large jump (in L2 distance) to a new hypothesis. We also show that under some conditions the expected errors of the almost-minimizers are becoming close with a rate faster than n^{-1/2}.",435,empirical risk minimization
429,2005,"We study properties of algorithms which minimize (or almost minimize) empirical error over a Donsker class of functions. We show that the L2-diameter of the set of almost-minimizers is converging to zero in probability. Therefore, as the number of samples grows, it is becoming unlikely that adding a point (or a number of points) to the training set will result in a large jump (in L2 distance) to a new hypothesis. We also show that under some conditions the expected errors of the almost-minimizers are becoming close with a rate faster than n^{-1/2}.",134,stability
429,2005,"We study properties of algorithms which minimize (or almost minimize) empirical error over a Donsker class of functions. We show that the L2-diameter of the set of almost-minimizers is converging to zero in probability. Therefore, as the number of samples grows, it is becoming unlikely that adding a point (or a number of points) to the training set will result in a large jump (in L2 distance) to a new hypothesis. We also show that under some conditions the expected errors of the almost-minimizers are becoming close with a rate faster than n^{-1/2}.",436,empirical processes
430,2005,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",1,AI
430,2005,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",437,Natural Language Processing
430,2005,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",438,N-grams
430,2005,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",439,Text Similarity
430,2005,"We present a system to determine content similarity of documents. More specifically, our goal is to identify book chapters that are translations of the same original chapter; this task requires identification of not only the different topics in the documents but also the particular flow of these topics. We experiment with different representations employing n-grams of lexical chains and test these representations on a corpus of approximately 1000 chapters gathered from books with multiple parallel translations.  Our representations include the cosine similarity of attribute vectors of n-grams of lexical chains, the cosine similarity of tf*idf-weighted keywords, and the cosine similarity of unweighted lexical chains (unigrams of lexical chains) as well as multiplicative combinations of the similarity measures produced by these approaches. Our results identify fourgrams of unordered lexical chains as a particularly useful representation for text similarity evaluation.",440,Lexical Chains
432,2005,"This thesis presents a method of object classification using the idea of deformable shape matching.  Three types of visual features, geometric blur, C1 and SIFT, are used to generate feature descriptors.  These feature descriptors are then used to find point correspondences between pairs of images.  Various morphable models are created by small subsets of these correspondences using thin-plate spline.  Given these morphs, a simple algorithm, least median of squares (LMEDS), is used to find the best morph.  A scoring metric, using both LMEDS and distance transform, is used to classify test images based on a nearest neighbor algorithm.  We perform the experiments on the Caltech 101 dataset [5].  To ease computation, for each test image, a shortlist is created containing 10 of the most likely candidates.  We were unable to duplicate the performance of [1] in the shortlist stage because we did not use hand-segmentation to extract objects for our training images.  However, our gain from the shortlist to correspondence stage is comparable to theirs.  In our experiments, we improved from 21% to 28% (gain of 33%), while [1] improved from 41% to 48% (gain of 17%).  We find that using a non-shape based approach, C2 [14], the overall classification rate of 33.61% is higher than all of the shaped based methods tested in our experiments.",1,AI
432,2005,"This thesis presents a method of object classification using the idea of deformable shape matching.  Three types of visual features, geometric blur, C1 and SIFT, are used to generate feature descriptors.  These feature descriptors are then used to find point correspondences between pairs of images.  Various morphable models are created by small subsets of these correspondences using thin-plate spline.  Given these morphs, a simple algorithm, least median of squares (LMEDS), is used to find the best morph.  A scoring metric, using both LMEDS and distance transform, is used to classify test images based on a nearest neighbor algorithm.  We perform the experiments on the Caltech 101 dataset [5].  To ease computation, for each test image, a shortlist is created containing 10 of the most likely candidates.  We were unable to duplicate the performance of [1] in the shortlist stage because we did not use hand-segmentation to extract objects for our training images.  However, our gain from the shortlist to correspondence stage is comparable to theirs.  In our experiments, we improved from 21% to 28% (gain of 33%), while [1] improved from 41% to 48% (gain of 17%).  We find that using a non-shape based approach, C2 [14], the overall classification rate of 33.61% is higher than all of the shaped based methods tested in our experiments.",123,object recognition
432,2005,"This thesis presents a method of object classification using the idea of deformable shape matching.  Three types of visual features, geometric blur, C1 and SIFT, are used to generate feature descriptors.  These feature descriptors are then used to find point correspondences between pairs of images.  Various morphable models are created by small subsets of these correspondences using thin-plate spline.  Given these morphs, a simple algorithm, least median of squares (LMEDS), is used to find the best morph.  A scoring metric, using both LMEDS and distance transform, is used to classify test images based on a nearest neighbor algorithm.  We perform the experiments on the Caltech 101 dataset [5].  To ease computation, for each test image, a shortlist is created containing 10 of the most likely candidates.  We were unable to duplicate the performance of [1] in the shortlist stage because we did not use hand-segmentation to extract objects for our training images.  However, our gain from the shortlist to correspondence stage is comparable to theirs.  In our experiments, we improved from 21% to 28% (gain of 33%), while [1] improved from 41% to 48% (gain of 17%).  We find that using a non-shape based approach, C2 [14], the overall classification rate of 33.61% is higher than all of the shaped based methods tested in our experiments.",441,shape-based
433,2005,"This paper presents an approach to model selection for regularized least-squares on reproducing kernel Hilbert spaces in the semi-supervised setting.  The role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  The main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  The empirical effective dimension can be computed from independent unlabelled samples.  This makes the approach particularly appealing in the semi-supervised setting.",1,AI
433,2005,"This paper presents an approach to model selection for regularized least-squares on reproducing kernel Hilbert spaces in the semi-supervised setting.  The role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  The main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  The empirical effective dimension can be computed from independent unlabelled samples.  This makes the approach particularly appealing in the semi-supervised setting.",424,optimal rates
433,2005,"This paper presents an approach to model selection for regularized least-squares on reproducing kernel Hilbert spaces in the semi-supervised setting.  The role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  The main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  The empirical effective dimension can be computed from independent unlabelled samples.  This makes the approach particularly appealing in the semi-supervised setting.",434,effective dimension
433,2005,"This paper presents an approach to model selection for regularized least-squares on reproducing kernel Hilbert spaces in the semi-supervised setting.  The role of effective dimension was recently shown to be crucial in the definition of a rule for the choice of the regularization parameter, attaining asymptotic optimal performances in a minimax sense.  The main goal of the present paper is showing how the effective dimension can be replaced by an empirical counterpart while conserving optimality.  The empirical effective dimension can be computed from independent unlabelled samples.  This makes the approach particularly appealing in the semi-supervised setting.",110,semi-supervised learning
434,2005,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",1,AI
434,2005,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",442,Localization
434,2005,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",443,Target Tracking
434,2005,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",444,Sensor Network
434,2005,"In this thesis we present LaSLAT, a sensor network algorithm thatsimultaneously localizes sensors, calibrates sensing hardware, andtracks unconstrained moving targets using only range measurementsbetween the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probabilitydistribution over the quantities of interest as measurementsarrive. The algorithm is distributable, and requires only a constantamount of space with respect to the number of measurementsincorporated. LaSLAT is easy to adapt to new types of hardware and newphysical environments due to its use of intuitive probabilitydistributions: one adaptation demonstrated in this thesis uses amixture measurement model to detect and compensate for bad acousticrange measurements due to echoes.We also present results from a centralized Java implementation ofLaSLAT on both two- and three-dimensional sensor networks in whichranges are obtained using the Cricket ranging system. LaSLAT is ableto localize sensors to within several centimeters of their groundtruth positions while recovering a range measurement bias for eachsensor and the complete trajectory of the mobile.",445,Calibration
435,2005,"We present a novel framework to exert a topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. A method, based on the concept of simple point borrowed from digital topology, was recently proposed to achieve a strict topology preservation during a level set evolution. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. We demonstrate the strength of our method on a wide range of numerical experiments.",1,AI
435,2005,"We present a novel framework to exert a topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. A method, based on the concept of simple point borrowed from digital topology, was recently proposed to achieve a strict topology preservation during a level set evolution. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. We demonstrate the strength of our method on a wide range of numerical experiments.",446,digital topology
435,2005,"We present a novel framework to exert a topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. A method, based on the concept of simple point borrowed from digital topology, was recently proposed to achieve a strict topology preservation during a level set evolution. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. We demonstrate the strength of our method on a wide range of numerical experiments.",447,level set
435,2005,"We present a novel framework to exert a topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. A method, based on the concept of simple point borrowed from digital topology, was recently proposed to achieve a strict topology preservation during a level set evolution. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. We demonstrate the strength of our method on a wide range of numerical experiments.",448,active contour
438,2005,"Many high-dimensional time-varying signals can be modeled as a  sequence of noisy nonlinear observations of a low-dimensional  dynamical process.  Given high-dimensional observations and a  distribution describing the dynamical process, we present a  computationally inexpensive approximate algorithm for estimating the  inverse of this mapping. Once this mapping is learned, we can invert  it to construct a generative model for the signals. Our algorithm  can be thought of as learning a manifold of images by taking into  account the dynamics underlying the low-dimensional representation  of these images. It also serves as a nonlinear system identification  procedure that estimates the inverse of the observation function in  nonlinear dynamic system.  Our algorithm reduces to a generalized  eigenvalue problem, so it does not suffer from the computational or  local minimum issues traditionally associated with nonlinear system  identification, allowing us to apply it to the problem of learning  generative models for video sequences.",1,AI
438,2005,"Many high-dimensional time-varying signals can be modeled as a  sequence of noisy nonlinear observations of a low-dimensional  dynamical process.  Given high-dimensional observations and a  distribution describing the dynamical process, we present a  computationally inexpensive approximate algorithm for estimating the  inverse of this mapping. Once this mapping is learned, we can invert  it to construct a generative model for the signals. Our algorithm  can be thought of as learning a manifold of images by taking into  account the dynamics underlying the low-dimensional representation  of these images. It also serves as a nonlinear system identification  procedure that estimates the inverse of the observation function in  nonlinear dynamic system.  Our algorithm reduces to a generalized  eigenvalue problem, so it does not suffer from the computational or  local minimum issues traditionally associated with nonlinear system  identification, allowing us to apply it to the problem of learning  generative models for video sequences.",449,"Manifold learning,nonlinear system identification"
438,2005,"Many high-dimensional time-varying signals can be modeled as a  sequence of noisy nonlinear observations of a low-dimensional  dynamical process.  Given high-dimensional observations and a  distribution describing the dynamical process, we present a  computationally inexpensive approximate algorithm for estimating the  inverse of this mapping. Once this mapping is learned, we can invert  it to construct a generative model for the signals. Our algorithm  can be thought of as learning a manifold of images by taking into  account the dynamics underlying the low-dimensional representation  of these images. It also serves as a nonlinear system identification  procedure that estimates the inverse of the observation function in  nonlinear dynamic system.  Our algorithm reduces to a generalized  eigenvalue problem, so it does not suffer from the computational or  local minimum issues traditionally associated with nonlinear system  identification, allowing us to apply it to the problem of learning  generative models for video sequences.",450,unsupervised learning
443,2005,"Understanding the complex brain computations leading to object recognition requires quantitatively characterizing the information represented in inferior temporal cortex (IT), the highest stage of the primate visual stream. A read-out technique based on a trainable classifier is used to characterize the neural coding of selectivity and invariance at the population level. The activity of very small populations of independently recorded IT neurons (~100 randomly selected cells) over very short time intervals (as small as 12.5 ms) contains surprisingly accurate and robust information about both object Â‘identityÂ’ and Â‘categoryÂ’, which is furthermore highly invariant to object position and scale. Significantly, selectivity and invariance are present even for novel objects, indicating that these properties arise from the intrinsic circuitry and do not require object-specific learning. Within the limits of the technique, there is no detectable difference in the latency or temporal resolution of the IT information supporting so-called Â‘categorizationÂ’ (a.k. basic level) and Â‘identificationÂ’ (a.k. subordinate level) tasks.  Furthermore, where information, in particular information about stimulus location and scale, can also be read-out from the same small population of IT neurons. These results show how it is possible to decode invariant object information rapidly, accurately and robustly from a small population in IT and provide insights into the nature of the neural code for different kinds of object-related information.",1,AI
443,2005,"Understanding the complex brain computations leading to object recognition requires quantitatively characterizing the information represented in inferior temporal cortex (IT), the highest stage of the primate visual stream. A read-out technique based on a trainable classifier is used to characterize the neural coding of selectivity and invariance at the population level. The activity of very small populations of independently recorded IT neurons (~100 randomly selected cells) over very short time intervals (as small as 12.5 ms) contains surprisingly accurate and robust information about both object Â‘identityÂ’ and Â‘categoryÂ’, which is furthermore highly invariant to object position and scale. Significantly, selectivity and invariance are present even for novel objects, indicating that these properties arise from the intrinsic circuitry and do not require object-specific learning. Within the limits of the technique, there is no detectable difference in the latency or temporal resolution of the IT information supporting so-called Â‘categorizationÂ’ (a.k. basic level) and Â‘identificationÂ’ (a.k. subordinate level) tasks.  Furthermore, where information, in particular information about stimulus location and scale, can also be read-out from the same small population of IT neurons. These results show how it is possible to decode invariant object information rapidly, accurately and robustly from a small population in IT and provide insights into the nature of the neural code for different kinds of object-related information.",123,object recognition
443,2005,"Understanding the complex brain computations leading to object recognition requires quantitatively characterizing the information represented in inferior temporal cortex (IT), the highest stage of the primate visual stream. A read-out technique based on a trainable classifier is used to characterize the neural coding of selectivity and invariance at the population level. The activity of very small populations of independently recorded IT neurons (~100 randomly selected cells) over very short time intervals (as small as 12.5 ms) contains surprisingly accurate and robust information about both object Â‘identityÂ’ and Â‘categoryÂ’, which is furthermore highly invariant to object position and scale. Significantly, selectivity and invariance are present even for novel objects, indicating that these properties arise from the intrinsic circuitry and do not require object-specific learning. Within the limits of the technique, there is no detectable difference in the latency or temporal resolution of the IT information supporting so-called Â‘categorizationÂ’ (a.k. basic level) and Â‘identificationÂ’ (a.k. subordinate level) tasks.  Furthermore, where information, in particular information about stimulus location and scale, can also be read-out from the same small population of IT neurons. These results show how it is possible to decode invariant object information rapidly, accurately and robustly from a small population in IT and provide insights into the nature of the neural code for different kinds of object-related information.",451,neural coding
443,2005,"Understanding the complex brain computations leading to object recognition requires quantitatively characterizing the information represented in inferior temporal cortex (IT), the highest stage of the primate visual stream. A read-out technique based on a trainable classifier is used to characterize the neural coding of selectivity and invariance at the population level. The activity of very small populations of independently recorded IT neurons (~100 randomly selected cells) over very short time intervals (as small as 12.5 ms) contains surprisingly accurate and robust information about both object Â‘identityÂ’ and Â‘categoryÂ’, which is furthermore highly invariant to object position and scale. Significantly, selectivity and invariance are present even for novel objects, indicating that these properties arise from the intrinsic circuitry and do not require object-specific learning. Within the limits of the technique, there is no detectable difference in the latency or temporal resolution of the IT information supporting so-called Â‘categorizationÂ’ (a.k. basic level) and Â‘identificationÂ’ (a.k. subordinate level) tasks.  Furthermore, where information, in particular information about stimulus location and scale, can also be read-out from the same small population of IT neurons. These results show how it is possible to decode invariant object information rapidly, accurately and robustly from a small population in IT and provide insights into the nature of the neural code for different kinds of object-related information.",363,inferior temporal cortex
444,2005,"Object recognition systems relying on local descriptors are increasingly used because of their perceived robustness with respect to occlusions and to global geometrical deformations.  Descriptors of this type -- based on a set of oriented Gaussian derivative filters -- are used in our recognition system.  In this paper, we explore a multi-view 3D object recognition system that does not use explicit geometrical information. The basic idea is to find discriminant features to describe an object across different views.  A boosting procedure is used to select features out of a large feature pool of local features collected from the positive training examples.  We describe experiments on face images with excellent recognition rate.",1,AI
444,2005,"Object recognition systems relying on local descriptors are increasingly used because of their perceived robustness with respect to occlusions and to global geometrical deformations.  Descriptors of this type -- based on a set of oriented Gaussian derivative filters -- are used in our recognition system.  In this paper, we explore a multi-view 3D object recognition system that does not use explicit geometrical information. The basic idea is to find discriminant features to describe an object across different views.  A boosting procedure is used to select features out of a large feature pool of local features collected from the positive training examples.  We describe experiments on face images with excellent recognition rate.",452,3D multiview
444,2005,"Object recognition systems relying on local descriptors are increasingly used because of their perceived robustness with respect to occlusions and to global geometrical deformations.  Descriptors of this type -- based on a set of oriented Gaussian derivative filters -- are used in our recognition system.  In this paper, we explore a multi-view 3D object recognition system that does not use explicit geometrical information. The basic idea is to find discriminant features to describe an object across different views.  A boosting procedure is used to select features out of a large feature pool of local features collected from the positive training examples.  We describe experiments on face images with excellent recognition rate.",123,object recognition
444,2005,"Object recognition systems relying on local descriptors are increasingly used because of their perceived robustness with respect to occlusions and to global geometrical deformations.  Descriptors of this type -- based on a set of oriented Gaussian derivative filters -- are used in our recognition system.  In this paper, we explore a multi-view 3D object recognition system that does not use explicit geometrical information. The basic idea is to find discriminant features to describe an object across different views.  A boosting procedure is used to select features out of a large feature pool of local features collected from the positive training examples.  We describe experiments on face images with excellent recognition rate.",453,SVM and boosting classifiers
454,2005,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a Â“top-cycleÂ”.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voterÂ’s view of how alternatives are related. As more and more voterÂ’s preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",1,AI
454,2005,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a Â“top-cycleÂ”.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voterÂ’s view of how alternatives are related. As more and more voterÂ’s preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",454,collective choice
454,2005,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a Â“top-cycleÂ”.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voterÂ’s view of how alternatives are related. As more and more voterÂ’s preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",455,uncertainty
454,2005,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a Â“top-cycleÂ”.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voterÂ’s view of how alternatives are related. As more and more voterÂ’s preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",456,voting
454,2005,"When groups of individuals make choices among several alternatives, the most compelling social outcome is the Condorcet winner, namely the alternative beating all others in a pair-wise contest. Obviously the Condorcet winner cannot be overturned if one sub-group proposes another alternative it happens to favor.  However, in some cases, and especially with haphazard voting, there will be no clear unique winner, with the outcome consisting of a triple of pair-wise winners that each beat different subsets of the alternatives (i.e. a Â“top-cycleÂ”.)  We explore the sensitivity of Condorcet winners to various perturbations in the voting process that lead to top-cycles. Surprisingly, variations in the number of votes for each alternative is much less important than consistency in a voterÂ’s view of how alternatives are related. As more and more voterÂ’s preference orderings on alternatives depart from a shared model of the domain, then unique Condorcet outcomes become increasingly unlikely.",457,top-cycles
456,2005,"Research in object detection and recognition in cluttered scenes requires large image collections with ground truth labels.  The labels should provide information about the object classes present in each image, as well as their shape and locations, and possibly other attributes such as pose.  Such data is useful for testing, as well as for supervised learning.  This project provides a web-based annotation tool that makes it easy to annotate images, and to instantly sharesuch annotations with the community.  This tool, plus an initial set of 10,000 images (3000 of which have been labeled), can be found at http://www.csail.mit.edu/$\sim$brussell/research/LabelMe/intro.html",1,AI
456,2005,"Research in object detection and recognition in cluttered scenes requires large image collections with ground truth labels.  The labels should provide information about the object classes present in each image, as well as their shape and locations, and possibly other attributes such as pose.  Such data is useful for testing, as well as for supervised learning.  This project provides a web-based annotation tool that makes it easy to annotate images, and to instantly sharesuch annotations with the community.  This tool, plus an initial set of 10,000 images (3000 of which have been labeled), can be found at http://www.csail.mit.edu/$\sim$brussell/research/LabelMe/intro.html",458,object recognition detection database annotation tool
457,2005,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",1,AI
457,2005,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",459,Unsupervised
457,2005,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",460,activity analysis
457,2005,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",461,scene modeling
457,2005,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",412,tracking
457,2005,"Current computer vision techniques can effectively monitor gross activities in sparse environments.  Unfortunately, visual stimulus is often not sufficient for reliably discriminating between many types of activity.  In many cases where the visual information required for a particular task is extremely subtle or non-existent, there is often audio stimulus that is extremely salient for a particular classification or anomaly detection task.  Unfortunately unlike visual events, independent sounds are often very ambiguous and not sufficient to define useful events themselves.  Without an effective method of learning causally-linked temporal sequences of sound events that are coupled to the visual events, these sound events are generally only useful for independent anomalous sounds detection, e.g., detecting a gunshot or breaking glass.  This paper outlines a method for automatically detecting a set of audio events and visual events in a particular environment, for determining statistical anomalies, for automatically clustering these detected events into meaningful clusters, and for learning salient temporal relationships between the audio and visual events.  This results in a compact description of the different types of compound audio-visual events in an environment.",462,event detection
458,2005,"Partially observable Markov decision processes (POMDPs) are a well studied paradigm for programming autonomous robots, where the robot sequentially chooses actions to achieve long term goals efficiently.  Unfortunately, for real world robots and other similar domains, the uncertain outcomes of the actions and the fact that the true world state may not be completely observable make learning of models of the world extremely difficult, and using them algorithmically infeasible.  In this paper we show that learning POMDP models and planning with them can become significantly easier when we incorporate into our algorithms the notions of spatial and tempral abstraction.  We demonstrate the superiority of our algorithms by comparing them with previous flat approaches for large scale robot navigation.",1,AI
463,2005,"This paper introduces algorithms for learning how to trade usinginsider (superior) information in Kyle's model of financial markets.Prior results in finance theory relied on the insider having perfectknowledge of the structure and parameters of the market. I show herethat it is possible to learn the equilibrium trading strategy whenits form is known even without knowledge of the parameters governingtrading in the model. However, the rate of convergence toequilibrium is slow, and an approximate algorithm that does notconverge to the equilibrium strategy achieves better utility whenthe horizon is limited. I analyze this approximate algorithm fromthe perspective of reinforcement learning and discuss the importanceof domain knowledge in designing a successful learning algorithm.",1,AI
465,2005,"Previous studies have shown that dyslexic individuals who supplement windowed reading practice with intensive small-scale hand-eye coordination tasks exhibit marked improvement in their reading skills. Here we examine whether similar hand-eye coordination activities, in the form of artwork performed by children in kindergarten, first and second grades, could reduce the number of students at-risk for reading problems. Our results suggest that daily hand-eye coordination activities significantly reduce the number of students at-risk. We believe that the effectiveness of these activities derives from their ability to prepare the students perceptually for reading.",1,AI
465,2005,"Previous studies have shown that dyslexic individuals who supplement windowed reading practice with intensive small-scale hand-eye coordination tasks exhibit marked improvement in their reading skills. Here we examine whether similar hand-eye coordination activities, in the form of artwork performed by children in kindergarten, first and second grades, could reduce the number of students at-risk for reading problems. Our results suggest that daily hand-eye coordination activities significantly reduce the number of students at-risk. We believe that the effectiveness of these activities derives from their ability to prepare the students perceptually for reading.",463,dyslexia
465,2005,"Previous studies have shown that dyslexic individuals who supplement windowed reading practice with intensive small-scale hand-eye coordination tasks exhibit marked improvement in their reading skills. Here we examine whether similar hand-eye coordination activities, in the form of artwork performed by children in kindergarten, first and second grades, could reduce the number of students at-risk for reading problems. Our results suggest that daily hand-eye coordination activities significantly reduce the number of students at-risk. We believe that the effectiveness of these activities derives from their ability to prepare the students perceptually for reading.",464,prevention
467,2005,"We consider regularized least-squares (RLS) with a Gaussian kernel. Weprove that if we let the Gaussian bandwidth $\sigma \rightarrow\infty$ while letting the regularization parameter $\lambda\rightarrow 0$, the RLS solution tends to a polynomial whose order iscontrolled by the relative rates of decay of $\frac{1}{\sigma^2}$ and$\lambda$: if $\lambda = \sigma^{-(2k+1)}$, then, as $\sigma \rightarrow\infty$, the RLS solution tends to the $k$th order polynomial withminimal empirical error.  We illustrate the result with an example.",1,AI
467,2005,"We consider regularized least-squares (RLS) with a Gaussian kernel. Weprove that if we let the Gaussian bandwidth $\sigma \rightarrow\infty$ while letting the regularization parameter $\lambda\rightarrow 0$, the RLS solution tends to a polynomial whose order iscontrolled by the relative rates of decay of $\frac{1}{\sigma^2}$ and$\lambda$: if $\lambda = \sigma^{-(2k+1)}$, then, as $\sigma \rightarrow\infty$, the RLS solution tends to the $k$th order polynomial withminimal empirical error.  We illustrate the result with an example.",465,machine learning
467,2005,"We consider regularized least-squares (RLS) with a Gaussian kernel. Weprove that if we let the Gaussian bandwidth $\sigma \rightarrow\infty$ while letting the regularization parameter $\lambda\rightarrow 0$, the RLS solution tends to a polynomial whose order iscontrolled by the relative rates of decay of $\frac{1}{\sigma^2}$ and$\lambda$: if $\lambda = \sigma^{-(2k+1)}$, then, as $\sigma \rightarrow\infty$, the RLS solution tends to the $k$th order polynomial withminimal empirical error.  We illustrate the result with an example.",135,regularization
473,2005,"We present an algorithm for c-approximate nearest neighbor problem in a d-dimensional Euclidean space,  achieving query time ofO(dn^{1/c^2+o(1)}) and space O(dn + n^{1+1/c^2+o(1)}).",1,AI
473,2005,"We present an algorithm for c-approximate nearest neighbor problem in a d-dimensional Euclidean space,  achieving query time ofO(dn^{1/c^2+o(1)}) and space O(dn + n^{1+1/c^2+o(1)}).",466,locality sensitive hashing
473,2005,"We present an algorithm for c-approximate nearest neighbor problem in a d-dimensional Euclidean space,  achieving query time ofO(dn^{1/c^2+o(1)}) and space O(dn + n^{1+1/c^2+o(1)}).",241,nearest neighbor
473,2005,"We present an algorithm for c-approximate nearest neighbor problem in a d-dimensional Euclidean space,  achieving query time ofO(dn^{1/c^2+o(1)}) and space O(dn + n^{1+1/c^2+o(1)}).",467,high dimensions
475,2005,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,1,AI
475,2005,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,266,online learning
475,2005,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,468,regret bounds
475,2005,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,469,non-stationarity
475,2005,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,470,HMM
475,2005,We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization of the switching-rate parameter that governs the switching dynamics. We demonstrate the algorithm in the context of wireless networks.,142,wireless networks
476,2005,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",1,AI
476,2005,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",471,active learning
476,2005,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",472,perceptron
476,2005,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",473,label-complexity
476,2005,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",474,mistake bound
476,2005,"We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon^2})$ labels to learn linear separators within generalization error $\epsilon$.  We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error $\epsilon$ after asking for just $\tilde{O}(d \log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.",475,selective sampling
477,2005,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",1,AI
477,2005,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",429,natural language processing
477,2005,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",476,syntactic information
477,2005,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",477,content
477,2005,"This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem.  Theapproach presented facilitates identification of intellectual propertyusing expression fingerprints. Copyright law protects expression of content.  Recognizing literaryworks for copyright protection requires identification of theexpression of their content.  The expression fingerprints described inthis thesis use a novel set of linguistic features that capture boththe content presented in documents and the manner of expression usedin conveying this content.  These fingerprints consist of bothsyntactic and semantic elements of language.  Examples of thesyntactic elements of expression include structures of embedding andembedded verb phrases.  The semantic elements of expression consist ofhigh-level, broad semantic categories.  Syntactic and semantic elements of expression enable generation ofmodels that correctly identify books and their paraphrases 82% of thetime, providing a significant (approximately 18%) improvement over modelsthat use tfidf-weighted keywords.  The performance of models builtwith these features is also better than models created with standardfeatures used in stylometry (e.g., function words), which yield anaccuracy of 62%.In the non-digital world, copyright holders collect revenues bycontrolling distribution of their works.  Current approaches to thedigital copyright problem attempt to provide copyright holders withthe same kind of control over distribution by employing Digital RightsManagement (DRM) systems.  However, DRM systems also enable copyrightholders to control and limit fair use, to inhibit others' speech, andto collect private information about individual users of digitalworks.Digital tracking technologies enable alternate solutions to thedigital copyright problem; some of these solutions can protectcreative incentives of copyright holders in the absence of controlover distribution of works.  Expression fingerprints facilitatedigital tracking even when literary works are DRM- and watermark-free,and even when they are paraphrased.  As such, they enable meteringpopularity of works and make practicable solutions that encouragelarge-scale dissemination and unrestricted use of digital works andthat protect the revenues of copyright holders, for example throughtaxation-based revenue collection and distribution systems, withoutimposing limits on distribution.",478,expression
481,2005,"We describe a state-space tracking approach based on a Conditional Random Field(CRF) model, where the observation potentials are \emph{learned} from data. Wefind functions that embed both state and observation into a space wheresimilarity corresponds to $L_1$ distance, and define an observation potentialbased on distance in this space. This potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. We show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse HMM, resulting in real-time system performance. The resulting system isused for human pose tracking in video sequences.",1,AI
481,2005,"We describe a state-space tracking approach based on a Conditional Random Field(CRF) model, where the observation potentials are \emph{learned} from data. Wefind functions that embed both state and observation into a space wheresimilarity corresponds to $L_1$ distance, and define an observation potentialbased on distance in this space. This potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. We show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse HMM, resulting in real-time system performance. The resulting system isused for human pose tracking in video sequences.",479,articulated tracking
481,2005,"We describe a state-space tracking approach based on a Conditional Random Field(CRF) model, where the observation potentials are \emph{learned} from data. Wefind functions that embed both state and observation into a space wheresimilarity corresponds to $L_1$ distance, and define an observation potentialbased on distance in this space. This potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. We show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse HMM, resulting in real-time system performance. The resulting system isused for human pose tracking in video sequences.",480,grid filter
481,2005,"We describe a state-space tracking approach based on a Conditional Random Field(CRF) model, where the observation potentials are \emph{learned} from data. Wefind functions that embed both state and observation into a space wheresimilarity corresponds to $L_1$ distance, and define an observation potentialbased on distance in this space. This potential is extremely fast to compute and in conjunction with a grid-filtering framework can be used to reduce acontinuous state estimation problem to a discrete one. We show how a statetemporal prior in the grid-filter can be computed in a manner similar to asparse HMM, resulting in real-time system performance. The resulting system isused for human pose tracking in video sequences.",481,conditional random field
482,2005,In this paper we describe a technique of classifier combination used in a human identification system. The system integrates all available features from multi-modal sources within a Bayesian framework. The framework allows representinga class of popular classifier combination rules and methods within a single formalism. It relies on a Â“per-classÂ” measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. The method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. We show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,1,AI
482,2005,In this paper we describe a technique of classifier combination used in a human identification system. The system integrates all available features from multi-modal sources within a Bayesian framework. The framework allows representinga class of popular classifier combination rules and methods within a single formalism. It relies on a Â“per-classÂ” measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. The method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. We show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,482,classifier combination
482,2005,In this paper we describe a technique of classifier combination used in a human identification system. The system integrates all available features from multi-modal sources within a Bayesian framework. The framework allows representinga class of popular classifier combination rules and methods within a single formalism. It relies on a Â“per-classÂ” measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. The method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. We show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,408,face recognition
482,2005,In this paper we describe a technique of classifier combination used in a human identification system. The system integrates all available features from multi-modal sources within a Bayesian framework. The framework allows representinga class of popular classifier combination rules and methods within a single formalism. It relies on a Â“per-classÂ” measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. The method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. We show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,483,identification
482,2005,In this paper we describe a technique of classifier combination used in a human identification system. The system integrates all available features from multi-modal sources within a Bayesian framework. The framework allows representinga class of popular classifier combination rules and methods within a single formalism. It relies on a Â“per-classÂ” measure of confidence derived from performance of each classifier on training data that is shown to improve performance on a synthetic data set. The method is especially relevant in autonomous surveillance setting where varying time scales and missing features are a common occurrence. We show an application of this technique to the real-world surveillance database of video and audio recordings of people collected over several weeks in the office setting.,484,multi-modal
483,2005,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",1,AI
483,2005,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",123,object recognition
483,2005,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",485,standard model
483,2005,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",486,theory
483,2005,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",165,visual cortex
483,2005,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",487,ventral stream
483,2005,"We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics.",356,hmax
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",488,UUV
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",489,AUV
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",490,Behavior Based Architecture
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",491,Unmanned Surface Vehicles
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",492,Behavior Based Control
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",493,Unmanned Marine Vehicles
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",494,Action Selection
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",495,Collision Avoidance
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",496,Multi-Objective Optimization
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",497,Unmanned Vehicles
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",498,MOOS
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",499,USV
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",500,Autonomous Decision Making
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",501,Autonomous Marine Vehicles
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",502,Autonomous Helm
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",503,Arbitration
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",504,Autonomous Vehicles
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",505,Underwater Vehicles
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",1,AI
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",506,MOOSDB
733,2009,"This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moosivp.org.",507,Behaviors
783,2010,"Most unmanned missions in space and undersea are commanded by a ""script"" that specifies a sequence of discrete commands and continuous actions. Currently such scripts are mostly hand-generated by human operators. This introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. For discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals to be accomplished, and the goal-directed systems automatically output the correct, executable command sequences. Increasingly, the control of autonomous systems involves performing actions with a mix of discrete and continuous effects. For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and take sample, and continuous actions, like descend and ascend, which are influenced by the dynamical model of the vehicle. A hybrid planner generates a sequence of discrete and continuous actions that achieve the mission goals. In this thesis, I present a novel approach to solve the generative planning problem for temporally extended goals for hybrid systems, involving both continuous and discrete actions. The planner, Kongming, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for continuous actions. Second, it engages novel reformulation schemes to handle temporally flexible actions and temporally extended goals. I have successfully demonstrated controlling an AUV in the Atlantic ocean using mission scripts solely generated by Kongming. I have also empirically evaluated Kongming on various real-world scenarios in the underwater domain and the air vehicle domain, and found it successfully and efficiently generates valid and optimal plans.",508,combinatorial optimization
783,2010,"Most unmanned missions in space and undersea are commanded by a ""script"" that specifies a sequence of discrete commands and continuous actions. Currently such scripts are mostly hand-generated by human operators. This introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. For discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals to be accomplished, and the goal-directed systems automatically output the correct, executable command sequences. Increasingly, the control of autonomous systems involves performing actions with a mix of discrete and continuous effects. For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and take sample, and continuous actions, like descend and ascend, which are influenced by the dynamical model of the vehicle. A hybrid planner generates a sequence of discrete and continuous actions that achieve the mission goals. In this thesis, I present a novel approach to solve the generative planning problem for temporally extended goals for hybrid systems, involving both continuous and discrete actions. The planner, Kongming, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for continuous actions. Second, it engages novel reformulation schemes to handle temporally flexible actions and temporally extended goals. I have successfully demonstrated controlling an AUV in the Atlantic ocean using mission scripts solely generated by Kongming. I have also empirically evaluated Kongming on various real-world scenarios in the underwater domain and the air vehicle domain, and found it successfully and efficiently generates valid and optimal plans.",509,AI planning
783,2010,"Most unmanned missions in space and undersea are commanded by a ""script"" that specifies a sequence of discrete commands and continuous actions. Currently such scripts are mostly hand-generated by human operators. This introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. For discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals to be accomplished, and the goal-directed systems automatically output the correct, executable command sequences. Increasingly, the control of autonomous systems involves performing actions with a mix of discrete and continuous effects. For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and take sample, and continuous actions, like descend and ascend, which are influenced by the dynamical model of the vehicle. A hybrid planner generates a sequence of discrete and continuous actions that achieve the mission goals. In this thesis, I present a novel approach to solve the generative planning problem for temporally extended goals for hybrid systems, involving both continuous and discrete actions. The planner, Kongming, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for continuous actions. Second, it engages novel reformulation schemes to handle temporally flexible actions and temporally extended goals. I have successfully demonstrated controlling an AUV in the Atlantic ocean using mission scripts solely generated by Kongming. I have also empirically evaluated Kongming on various real-world scenarios in the underwater domain and the air vehicle domain, and found it successfully and efficiently generates valid and optimal plans.",510,autonomous systems
1023,2016,"This thesis discusses novel principles to improve the theoretical analyses of a class of methods, aiming to provide theoretically driven yet practically useful methods. The thesis focuses on a class of methods, called bound-based search, which includes several planning algorithms (e.g., the A* algorithm and the UCT algorithm), several optimization methods (e.g., Bayesian optimization and Lipschitz optimization), and some learning algorithms (e.g., PAC-MDP algorithms). For Bayesian optimization, this work solves an open problem and achieves an exponential convergence rate. For learning algorithms, this thesis proposes a new analysis framework, called PAC-RMDP, and improves the previous theoretical bounds. The PAC-RMDP framework also provides a unifying view of some previous near-Bayes optimal and PAC-MDP algorithms. All proposed algorithms derived on the basis of the new principles produced competitive results in our numerical experiments with standard benchmark tests.",511,PAC-MDP
1023,2016,"This thesis discusses novel principles to improve the theoretical analyses of a class of methods, aiming to provide theoretically driven yet practically useful methods. The thesis focuses on a class of methods, called bound-based search, which includes several planning algorithms (e.g., the A* algorithm and the UCT algorithm), several optimization methods (e.g., Bayesian optimization and Lipschitz optimization), and some learning algorithms (e.g., PAC-MDP algorithms). For Bayesian optimization, this work solves an open problem and achieves an exponential convergence rate. For learning algorithms, this thesis proposes a new analysis framework, called PAC-RMDP, and improves the previous theoretical bounds. The PAC-RMDP framework also provides a unifying view of some previous near-Bayes optimal and PAC-MDP algorithms. All proposed algorithms derived on the basis of the new principles produced competitive results in our numerical experiments with standard benchmark tests.",509,AI planning
1023,2016,"This thesis discusses novel principles to improve the theoretical analyses of a class of methods, aiming to provide theoretically driven yet practically useful methods. The thesis focuses on a class of methods, called bound-based search, which includes several planning algorithms (e.g., the A* algorithm and the UCT algorithm), several optimization methods (e.g., Bayesian optimization and Lipschitz optimization), and some learning algorithms (e.g., PAC-MDP algorithms). For Bayesian optimization, this work solves an open problem and achieves an exponential convergence rate. For learning algorithms, this thesis proposes a new analysis framework, called PAC-RMDP, and improves the previous theoretical bounds. The PAC-RMDP framework also provides a unifying view of some previous near-Bayes optimal and PAC-MDP algorithms. All proposed algorithms derived on the basis of the new principles produced competitive results in our numerical experiments with standard benchmark tests.",512,Global optimization
1055,2018,"Learning models of decision-making behavior during sequential tasks is useful across a variety of applications, including human-machine interaction. In this paper, we present an approach to learning such models within Markovian domains based on observing and querying a decision-making agent. In contrast to classical approaches to behavior learning, we do not assume complete knowledge of the state features that impact an agent's decisions. Using tools from Bayesian nonparametric inference and time series of agents  decisions, we first provide an inference algorithm to identify the presence of any unmodeled state features that impact decision making, as well as likely candidate models. In order to identify the best model among these candidates, we next provide an active querying approach that resolves model ambiguity by querying the decision maker. Results from our evaluations demonstrate that, using the proposed algorithms, an observer can identify the presence of latent state features, recover their dynamics, and estimate their impact on decisions during sequential tasks.",513,"Decision Making, Graphical Models, Human-AI Collaboration"
